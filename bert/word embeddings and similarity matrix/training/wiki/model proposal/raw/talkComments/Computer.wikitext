{{Talk header}}
{{Vital article|level=2|topic=Technology|class=C}}
{{Article history
|action1=FAC
|action1date=03:44, 7 April 2006
|action1link=Wikipedia:Featured article candidates/Computer/archive1
|action1result=not promoted
|action1oldid=47352504

|action2=GAR
|action2date=7 April 2006
|action2link=Talk:Computer/Archive 3#Removal of "good article"
|action2result=Delisted
|action2oldid=47455350

|action3=PR
|action3date=28 November 2006
|action3link=Wikipedia:Peer review/Computer/archive1
|action3result=Reviewed
|action3oldid=90496516

|action4=FAC
|action4date=19 December 2006
|action4link=Wikipedia:Featured article candidates/Computer/archive2
|action4result=Failed
|action4oldid=95204877

|currentstatus=FFAC
}}
{{WikiProjectBannerShell|1=
{{WikiProject Computer science|class=c|importance=Top}}
{{WikiProject Home living|class=c|importance=mid}}
{{WikiProject Computing|class=c|importance=top|network=|network-importance=high|hardware=yes|hardware-importance=top|software=yes|software-importance=mid|early-computing=yes|early-computing-importance=high}}
{{WikiProject Engineering |class=C |importance=High}}
{{WikiProject Technology|class=c|importance=Top}}
{{WikiProject Video games|class=c|importance=mid}}
{{WikiProject Systems|class=c|importance=Top}}
}}
{{American English}}
{{User:MiszaBot/config
|maxarchivesize = 300K
|counter = 5
|algo = old(60d)
|archive = Talk:Computer/Archive %(counter)d
}}

{{Auto archiving notice|age=60|dounreplied=yes|bot=Lowercase sigmabot III|small=yes}}
{{WP1.0|v0.5=pass|class=c|category=Engtech|core=yes|VA=yes|small=yes|importance=high}}

<!-- Please add new comments at the BOTTOM of this page -->

== Semi-protected edit request on 1 September 2018 ==

{{edit semi-protected|Computer|answered=yes}}
Plese let me help you guys to  edit  this page [[User:Jonnie jpt|Jonnie jpt]] ([[User talk:Jonnie jpt|talk]]) 16:55, 1 September 2018 (UTC)
:[[File:Red information icon with gradient background.svg|20px|link=]] '''Not done:'''<!-- Template:ESp --> Hi [[User:Jonnie jpt|Jonnie jpt]]! Wikipedia would always be glad to have more volunteers, but unfortunately this article has been a frequent target of vandalism, so editing by newly registered users has been disabled. If you have a specific fact you'd like to add, write it down here in the format "change XXX to YYY" or "after the text ZZZ add new text WWW", reactivate this request, and we will be happy to make the change for you&ndash;just be sure to be specific, or otherwise we may not be able to understand your requested edit. If you would like the ability to edit this article yourself, please make at least [[WP:AUTOCONFIRM|six more edits]] and you'll be able to edit semi-protected articles like this one. Best, [[User:Altamel|Altamel]] ([[User talk:Altamel|talk]]) 18:28, 1 September 2018 (UTC)

== Is the history section too British-centric? ==

The content of the History section of this article is very British-centric and might lead one to believe that most major advancements in digital computing occurred principally in the U.K. Even a cursory examination of the works of [[Brian Randell]], an Englishman himself and probably the preeminent and best known historian of early digital computing, shows a more balanced approach to American accomplishments and advancements. I am not suggesting an edit specifically, but I am suggesting as part of clean-up of this article, a little more balanced approach in this section would improve completeness and quality. [[User:Ray Trygstad|Ray Trygstad]] ([[User talk:Ray Trygstad|talk]]) 17:53, 7 September 2018 (UTC)

== Semi-protected edit request on 22 November 2018 ==

{{edit semi-protected|Computer|answered=yes}}
please ad my links to this https://yogeshksahu.blogspot.com/2018/07/computer-what-is-computer.html  [[User:Yogeshwarsahu|Yogeshwarsahu]] ([[User talk:Yogeshwarsahu|talk]]) 13:53, 22 November 2018 (UTC)

:{{Not done}}.  Not without some compelling reason.  &ndash;[[User:Deacon Vorbis|Deacon Vorbis]]&nbsp;([[User Talk:Deacon Vorbis|carbon]]&nbsp;&bull;&nbsp;[[Special:Contributions/Deacon Vorbis|videos]]) 14:19, 22 November 2018 (UTC)

== Semi-protected edit request on 17 February 2019 ==

19th century -> 20th century

Under Etymology
"From the end of the 19th century the word began to take on its more familiar meaning, a machine that carries out computations.[3]"  <!-- Template:Unsigned IP --><small class="autosigned">—&nbsp;Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/199.58.98.69|199.58.98.69]] ([[User talk:199.58.98.69#top|talk]]) 07:29, 18 February 2019 (UTC)</small> <!--Autosigned by SineBot-->

:{{Not done}}.  "19th century" was intended and correct in the context of beginning to take on the modern meaning.  By 1960 the "machine" meaning was fully established, although the "human" meaning had not quite disappeared.  By 1970, the "human" meaning was fully obsolete. --[[User:R. S. Shaw|R. S. Shaw]] ([[User talk:R. S. Shaw|talk]]) 20:56, 21 February 2019 (UTC)

     I, uh, 20th century IS 1900s. 19th century means 1800s.
     [[Special:Contributions/199.58.98.69|199.58.98.69]] ([[User talk:199.58.98.69|talk]]) 22:25, 21 February 2019 (UTC)

== Semi-protected edit request on 22 March 2019 ==

{{edit semi-protected|Computer|answered=yes}}
I am suggesting these changes to improve the page's grammar and writing style.

1. Please change this:
The sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.

To this:
The sector was developed in the late 16th century and found application in gunnery, surveying and navigation. It was a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots.

Because: The original text is a run-on sentence that can be restructured to improve readability.
 
2. Please Change this:
Babbage's failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. 

To this:
Babbage's failure to complete the analytical engine can be attributed to difficulties of politics and financing, as well as his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else. 

Because: The writing style can be improved by omitting unnecessary words.

3. Please Change this:
Rather than the harder-to-implement decimal system (used in Charles Babbage's earlier design), using a binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time.

To this:
Rather than the more difficult to implement decimal system (used in Charles Babbage's earlier design), using a binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time.

Because: "harder-to-implement" is not grammatically correct.

4. Please change this:
This design was also all-electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.[30]

To this:
This design was also fully electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.[30]

Because: "all-electronic" is also not grammatically correct.
 
5. Please change this:

The U.S.-built ENIAC[37] (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US.

To this:

The ENIAC[37] (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US.

Because: The repetition of at the start and the end of the sentence makes it redundant. [[User:SFU-CMPT376W|SFU-CMPT376W]] ([[User talk:SFU-CMPT376W|talk]]) 11:00, 22 March 2019 (UTC)
:{{reply to|SFU-CMPT376W}}
:
:# {{not done}} I agree that the parenthetical "''a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots''" is way too long, but pushing it to the end of the paragraph makes the result sound weird. We should describe what the instrument does before how it's used. We can workshop this paragraph together and then I can make the finalized changes for you.
:# {{done}}
:# {{not done}} "Harder-to-implement" is a valid English construction.
:# {{not done}} "All-electronic" is also a valid English construction.
:# {{done}}
:&ndash; <span style="white-space: nowrap;">[[User:Qzekrom|Qzekrom]] [[User talk:Qzekrom|💬]] <sup>they</sup><sub>them</sub></span> 08:43, 23 March 2019 (UTC)
{{Aan}}

I heard something about variable computers. If someone knows what they are and how they work, can they type it in?

--------------------------------------------------------

Is my thermostat a computer?  :-)

No it is a simple feedback device unless it is a programable thermostat.

Suppose it's programmable.

Then yes, it is a computer.  I think a more natural way of speaking would be to say that it has a computer in it.

Well, no; it has an embedded chip...that doesn't make it a computer, does it?

We have to start with definition. To most people 'computer' means personal computer and even if they think about a supercomputer they see a more powerful pc. If we however stick to the definition 'device that process data' then computer will have much broader meaning. ENIAC was a computer but it did not resemble present computers. A computer computes data therefore any device that does it can be called computer. A programmable thermostat has small computer inside and one of the more sophisticated thermostats might be more powerful than ENIAC.
----
I think a ''strong'' connotation of computer nowadays is that it is universal (ie. it can perform any computational task).  A thermostat can be incredibly sophisticated but it will still only tell you when to turn on the heater.  A pac-man machine will only play pac-man.  But a computer can do either of those things, or much more, so long as you give it instructions on how to do so.

----

A ''computer'' used to mean a person that computed, eventually a person 
that computed using an adding machine.  Many of these ''computers'' were
women.  The computations were often systems of differential equations 
(or other linear systems), for example,  solving problems in ballistics.


----

I intend to give this page a serious working over as the result of some interesting discussions on [[talk:Konrad Zuse]] and on the other "history of computing" related pages. [[User:Robert Merkel|Robert Merkel]]

----
As the author of the page (though it has been improved somewhat since) i think a complete rewrite would be nice. I wrote it mostly in desperation that so important a topic had only a one like entry. The current page is better than that but not particularly good.

However, i suggest not deleting anything from the page until you have a complete article that covers all the important stuff already there (and hopefully more!). One way might be to rule a line at the top (or bottom) and start your rewrite in a seperate section. When you have enough there the old version could be removed.

I have seen a few other pages where mediocre articles were deleted by someone who then ran out of steam before completing their rewrite, leaving something worse than the original. Leaving both versions available during the transition protects somewhat against this disaster. Best of luck here!

----

I didn't see the above comment until I had committed my rewrite (it was actually a good idea you had, if somebody can restore the old article and hang it somewhere that'd be good).

(Done. It's at the end of the new one. New one is looking good!)

It is approximately half "feature-complete" at this point.  Seeing we already have a great deal of other material on computing topics, I intend to concentrate merely on the "what is a computer" question, with very brief overviews of the other two subheadings.
 

Any suggestions (or just plain edits) on how to improve my explanation of why Turing-completeness is important would be appreciated.  [[User:Robert Merkel|Robert Merkel]]
----
On the commercial computing side, data processing machines began to enter serious use circa 1895 in the USA and during the early 20th century many other places. These machines usually were based on punched cards and could do operations such as sorting, tabulating, counting, and in some cases calculating a total, but were probably not general enough to be considered computers. When computers became cheap enough to be competitive, computers took over because they can do all this, and have much more flexibility. Many of the technologies used in computers 1945-1970 were originally developed for earlier data processing machines and some of the same companies were involved (IBM and Burroughs, maybe Sperry, probably others in other countries). In the history section this seems somehow relevant, but you write so much better than me i leave it to you to decide if, or how, to add it.
----
Yes, the new one is really looking good! --[[User:LMS|LMS]]
----
I think the old version should be moved here. Also, even though the main article could be expanded almost without limit, it might be good to move or remove all the metacomments so that it will read like most of the other articles. [[User:David spector|David]] 19:49 Sep 21, 2002 (UTC)

----

"It's now commonplace for operating systems to include web browsers, text editors, e-mail programs, network interfaces, movie-players and other programs that were once quite exotic special-order programs." Is this realy true even in windows? In Linux vim or emacs, xine or mplayer, kmail, evolution mutt or pine ,konqueror galeon opera mozilla or firebird would never be considered part of the OS which most people would consider the kernel, likewise for OSX and its BSD derived kernel. Do people realy regard notepad and IE and outlook and windows media player as part of the OS - I know microsoft claims it for IE, but the rest? I'm sure Microsoft would like to claim almost any aplication where they have competition is "part of the OS" so they can happily bundle it with the OS, but bundeling with and being part of are two different things. This passage should be defended or removed, is there any example of an OS where eg the text editor is part of the OS? Generally where the functionality can be provided by an alternative program it cannot be considered part of the OS. The example of network interfaces is the one example of a function which has genuinely been taken over by the os as compared to a third party program (cf. trumpet winsock)
[[User:Htaccess|Htaccess]]

: There's not really a clear line between what is part of the OS and what isn't--usually the core programs are considered a part of it. Programs like QuickTime and Safari could be considered part of Mac OS X because they are first party and are integrated at the system level instead of just the application level. Even a text editor may be--vi is part of the Single Unix Specification. Network interfaces have moved from the userland to the kernel, but the kernel is not the same thing as the OS. --[[User:HunterX|HunterX]] 22:51, 2005 May 11 (UTC)
----

I really like this article but feel it is missing an important piece about the user of computers to decode in WW2. Is that somewhere else and if so can we put in a link to it? --[[user talk:BozMo|(talk)]][[User:BozMo|BozMo]] 13:37, 14 May 2004 (UTC)

: Well, that's what most of the work in building an encyclopedia consists of, isn't it? :-)  As a start, see maybe: [[Ultra]] . [[User:Kim Bruning|Kim Bruning]] 14:09, 14 May 2004 (UTC)


----

Oh dear. For some reason [[John von Neumann]] isn't mentioned in the definition section. Odd that. Other missing people are [[Alan Turing]] and the pair [[Alonzo Church]] and  [[Stephen Kleene]].

These folks provide 3 different detailed definitions of ''computer'', all of which are currently in actual use in the field. :-)

The 3 POVs (with a ''very'' short summary <small>''(so [[YMMV]])''</small> ) are:
* [[Von Neumann architecture]]
** A computer is a device with a [[Central processing unit|CPU]], Memory, and some input/output system
* [[Turing Machine]]
** A computer is at least a [[Finite State Automaton]], augmented with an infinite length tape for storage, or anything that can simulate one.
* [[Lambda calculus]]
** A computer is anything that can perform lambda calculus.

These 3 definitions overlap:
* A von Neumann architecture can simulate a turing machine, with the understanding that most implementations can't actually simulate an infinite length tape, just a very long one.
* Originally people thought that von Neumann machines weren't particularly suited to performing lambda calculus. Over time people have gotten practice. Nowadays , a language like [[Ocaml]] might actually run faster than a language (like [[C programming language|C]]) that was specifically designed for von Neumann architectures.
* A Turing machine can be simulated in lambda calculus, and lambda calculus can be performed by a turing machine.

This means that all 3 POVs of are ''logically'' equivalent, but they do each bring a slightly different way of looking at computers to the table.

I guess I'd better look into this better sometime. [[User:Kim Bruning|Kim Bruning]] 23:06, 28 Jul 2004 (UTC)
: Partially done, and extended this comment as a result. [[User:Kim Bruning|Kim Bruning]] 07:40, 29 Jul 2004 (UTC)

----

I just completely re-wrote the definition section. Turing and Von neumann now get mentions but church does not (maybe you could add him in a suitable place). Given the general title I tried to stay away from too much theory and leave the details for other pages..

I think the following would be useful:

*A scematic diagram of a simple von neumann architecture to illustrate the "How a computer works section".
*More information in the computer applications section 
*Also a sprinkling of more up to date references in various sections

I also moved a lot of etymology to here [[wiktionary:computer]] which seems a much beter place for it.

[[User:Virtual Traveler|John Harris]] 

----

What do I want from a computer? Well, love, but I've pretty much given up hope on that one. Ten years ago a guy named Steve made a computer called NeXT, and things still suck. This is intolerable. Why don't I have a minimal instruction set computing (MISC)-type box on my desk? Something like a Forth engine (cf. Chuck Moore's F21 CPU and similar), with Lisp as the app language (coded itself in machine Forth, of course). Forth can be implemented surprisingly efficiently with a stack machine. No register mess, which makes context switching basically painless and trivial. Easy, cheap, frequent procedure calling is encouraged, with implicit low coupling and modularity thanks to the ever-present stack. Programs are small and efficient because so few instructions are required on a complete stack machine. Oh, yes, while I'm at it, I'd like the whole thing implemented in asynchronous logic CMOS please, so I can implant it in my cerebral cortex and run it off body heat and the occasional shot of glucose. Sigh. Well, I'm a cs geek, but there are times I wish I was in comp. engg...


The picture below shows one of my MBs that fried mystriously, it was connected to a highedups which showed now power issures at all.  But obliouvsly something went wrong.   Any way thought the pic might be of use to someone. [[User:Belizian|Belizian]] 20:26, 6 Oct 2004 (UTC)

|[[Image:Mbf.jpg|thumb|100px|none|[[Fried Motherboard]], And it didnn't come from outside something with board itself caused this]]

== "[[Computar]]"?? ==

[[Computar]] redirects here. Is that right? Neither [http://www.dictionary.com/ Dictionary.com] nor [http://www.google.com/ Google] seem to have anything useful. [[User:Brianjd|Brianjd]]

----

I removed the information on CPU manufacturers because it was misleading and incomplete. Intel and AMD both make x86 chips and, it doesn't make sense to mention "Motorolla" (68k) and Cyrix (another x86) and not PPC which is still used for a minority of desktops (or for that matter other low-key x86 players like Transmeta or VIA). In any case, information on manufacturers isn't really necessary to a general discussion of ALUs. --[[User:HunterX|HunterX]] 08:16, 2005 Jan 7 (UTC)

----

I think the newly added general principles section is a good idea but I'm not sure Claude Shannon deserves special mention. His work on information theory was very important but this is only incedental to computers. Also he was not the only one thinking about boolean algebra Konrad Zuse was doing the same thing in Germany.  Alan Turing was certainly more important than Shannon but again he is only one of many. In general I think name droping in  a genral principles section is not very helpful since it will evolve into a who's who of computer pioneers.  
-- [[User:Virtual Traveler|John Harris]] 2005, Jan 8

Uh, the reason Shannon is there is NOT because of his work on information theory.  Before you make a fool of yourself further, please read the Shannon article in full, and the attached discussion page, to understand why he is listed in ''this'' article.  Shannon had ''two'' major accomplishments in life, (1) founding digital circuit or logic design theory, and (2) founding information theory.  It is the first one which was his major fundamental contribution to modern computers.  Otherwise we might still be doing electronic computers the old-fashioned way, totally ad hoc, like Zuse, Stibitz, Atanasoff, etc., instead of doing them the fancy way, with modern Boolean algebra.

--[[User:Coolcaesar|Coolcaesar]] 05:37, 12 Feb 2005 (UTC)
----
In the etymology section, it might be worth mentionning that "Computer" is of latin origin, it comes from the verb "computare" which means "to count". It also gave the French verb "compter" which has the same meaning.


== Error on graph ==

Somebody needs to fix the image Image:PPTSuperComputersPRINT.jpg -- the vertical axis says that [[FLOPS]] = "floating point operations" where FLOPS is actually "floating point operations ''per second''." [[User:Gary D Robson|Gary D Robson]] 17:33, 14 July 2005 (UTC)

----

In case anyone was looking at the history, the reversion that I made was against the vandalism by 219.95.97.41, not 219.95.97.4 like I accidently said. [[User:DarthVader|DarthVader]] 14:04, 21 July 2005 (UTC)

==History?==
Where's the history section?  I know, I know...[[Wikipedia:Be bold in updating pages|be bold]], but is it just somewhere I haven't seen yet? --[[User:Wtshymanski|Wtshymanski]] 19:47, 22 July 2005 (UTC)

:See [[History of computing hardware]].  But also see my comment below - history has been displaced by all the other random crud that this article has accumulated.  --[[User:Robert Merkel|Robert Merkel]] 04:35, 29 July 2005 (UTC)

== Stop adding random crap! ==

Don't take this personally, but this article has become a near-unreadable collection of people's personal hobby-horses.

One unfortunate problem of encyclopedia articles by committee is that while committees are very good at ''adding'' things, they are terrible at ''removing'' them.  This particularly shows itself in "overview"-type articles as this one is, because everybody wonders why their little bit of expertise isn't mentioned in the main article (it's in fact discussed at length in more narrowly focussed ones).  The "classification" section, for instance, is incredibly unwieldy, and in my view adds little of value for the reader.  This article therefore needs a thorough renovation, which I intend to start work on tonight.

Please, just don't add whatever random fact comes into your head to an overview article!  Think about whether it actually adds value to the likely reader!  --[[User:Robert Merkel|Robert Merkel]] 04:34, 29 July 2005 (UTC)

:New version started at [[Computer/Temp]]. --[[User:Robert Merkel|Robert Merkel]] 13:59, 29 July 2005 (UTC)

::Yep, have at it. An article here will always be mediocre until someone takes it upon themselves to make it better. Lots of people can do that, and do so for lots of articles all the time, but not every article has someone get to it. Keep us updated here on major changes in the temp page version. My general comment is to try to thing generally about the whole topic and prioritize what are truly the most important topics, and what needs to be covered and what doesn't. This article will be particularly dicey to avoid POV, because everyone has a different way of interacting with the computer and a different focus. Try to be as general as possible. - [[User:Taxman|Taxman]] <sup><small>[[User talk:Taxman|Talk]]</sup></small> 15:19, July 29, 2005 (UTC)

::Nice re-write. Since I wrote the original "unwieldy" classification section I thought I might add a few comments. Don't worry I'm not offended I'm just amazed it lasted so long! The new introduction seems a bit heavy to me. Introducing the  Church-Turing thesis in the second sentence seems a bit agressive. I think the first pragraph of this type of general article should be aimed at an educated 12 year old. I'd suggest leaving the theory of computers till at least the second paragraph. The History section is a nice summary. But it does illustrate a problem I had. Which names should be listed in such a section. You have Babbage, Zuse and Shannon. Obviously important, but why not Turing and Eckart etc. I could never find a solution to this problem someone is always going to want to add someone else. Finally I think there is a bit of revisionism going on here. The computer scientists and theorists seem to be taking all the credit when in reality it was a bunch of annonymous engineers who really made computers work (I understand what I am saying here and will stand by the claim). Where are the mentions for Tommy Flowers, Mauchly and Stibitz. All these men made contributions of significance. They used to say that "science owes more to the steam engine that the steam engine owes to science" I think a similar statement can be made about computers. Computers were made to work by men who did not always understand the theretical implications of what they were doing. But so what! They still made them work! The theorists often were left to explain what had been created not to specify it. [[User:Virtual Traveler|Virtual Traveler]]

:::Thanks for your understanding about the need for a rewrite; while I happened to pick on the classification section, my whinge was about the profusion of the page had collected on many topics which had completely obscured the forest for the trees.

:::As far as the Church-Turing thesis goes, I was trying to get across the vital idea that computers are universal information processors; I tried to mention it in the most painless way possible.  Feel free to change it (of course); I'll have a think about a less hardcore version.

:::The history section was a nightmare to get to the state it is; in the end, I avoided mentioning too many names because computers were largely team efforts, and singling people out is simply too diffcult (and controversial with the case of Eckart, Mauchly, and von Neumann).  As far as the three names that got mentioned, maybe Shannon's name could be removed without interrupting the flow of the article, but there's no other way to describe Zuse's projects without mentioning Zuse himself (and if you didn't mention his work any random German who came across this page would go ballistic at the slight).  And, as far as Babbage is concerned, he was the first person to dream up (in anything other than a fanciful sense) the idea of a computer.  I think it's worth a sentence in a potted history, and if you mention his work you need to mention him.  
:::As to the anonymous engineers, there's a certain truth to that, but I did try in the history section to mention the importance of the many precursor technologies. --[[User:Robert Merkel|Robert Merkel]] 08:36, 21 August 2005 (UTC)

== New version drafted at [[Computer/Temp]] ==

I have drafted a heavily revised version of this article at [[Computer/Temp]].  I have deliberately narrowed the focus to the modern, digital computer, and removed a lot of the definitional stuff, and tried to give more comprehensive treatment of what a stored-program computer is and what it does.  It still needs a lot of work, but I think it's to a point where it can replace the present article as the basis for future work.  If anybody's got a violent objection to me replacing the present version with the new version, could they say so here? --[[User:Robert Merkel|Robert Merkel]] 13:31, 12 August 2005 (UTC)

== Halting problem ==
You know, I'm on crack today, it seems. Let me re-state: The halting problem is not [[NP (complexity)|NP]], as I had stated. It is not solvable, as stated in the edit, which I will restore, but make a bit clearer. -[[User:Harmil|Harmil]] 15:01, 15 August 2005 (UTC)

==Replaced with new version==

OK, In the light of some suggestions and mainly positive comments, I have bitten the bullet and replaced the old version with the version from [[Computer/Temp]].  Have at it. --[[User:Robert Merkel|Robert Merkel]] 05:54, 20 August 2005 (UTC)

== Illustrations... ==

One thing the rewrite hasn't really touched is the question of illustration.  While I'm sure we can find suitable photographs, is there anything in this article that really needs a diagram for further elucidation?  Maybe a conceptual diagram of the stored program architecture? --[[User:Robert Merkel|Robert Merkel]] 04:47, 23 August 2005 (UTC)

:It would be nice if we had some pictures for the "History" section. --[[User:Keramida|Keramida]] 05:04, 23 August 2005 (UTC)

== "The network is the computer" ==

The recent edit by Jjshapiro is wrong: The quote from Sun was as above, not "the computer is the network".  Also, most of what's been added is better covered in the [[Internet]] article.  Also, there's a big difference between networking and internetworking, which was left alone before (not particularly relevant to '''Computer''' article anyway) but is nicely muddied here now...   I just don't have time at the moment to try to sort all this out for him/her.  Maybe it should just be reverted, but 210.211.241.195's new para about the WWW wasn't much good either - "shopping or marketing a product"?  Sorry, guys.   --[[User:Nigelj|Nigelj]] 19:18, 11 September 2005 (UTC)

:Whoops!  Thanks for correcting my reversal of that quote.  But don't you agree that that principle involves a redefinition of the scope of the computer and should be in the Computer article?  I do agree that in principle internetworking doesn't necessarily belong in the Computer article except by extension.  On the other hand, under today's conditions, the internetwork is the network.  [[User:Jjshapiro|Jeremy J. Shapiro]] 15:18, 22 September 2005 (UTC)

== VANDALISM ALERT! ==

Some one inserted words like "gay" and "bumming" into the article and make some links unworking because of this! Please restore this article to a honest state. Thanks.

== Atanasoff-Berry Computer ==

I have reverted the history section to an earlier version.  This is an *overview* article, and giving so much prominence to a special-purpose machine like the [[AtanasoffBerry Computer]] is inappropriate.  While its contributions of using binary numbers and all-electronic computations were important, it wasn't a general-purpose machine.  It was another step along the path to the general-purpose, stored-program machines, perhaps comparable in importance to the ENIAC and Zuse's various machines and thus worthy of approximately the same amount of space.  Frankly, the ABC page needs some NPOV and cleanup work too (the article confuses the concepts of Turing completeness and the stored program architecture), but that's another issue.  --[[User:Robert Merkel|Robert Merkel]] 13:49, 5 October 2005 (UTC)

:I am familiar with the history of the ABC and agree with user Robert Merkel's edit.  Well argued.  --[[User:Coolcaesar|Coolcaesar]] 05:12, 6 October 2005 (UTC)

== Revert. ==

I recently reverted some edits that seemed to confuse [[computer]] with [[personal computer]] as explained [[User talk:Hallenrm|here]].  I thought I'd put this here since it could look like vandalism on my part to an outside party.  [[User:Indium|Indium]] 10:01, 16 November 2005 (UTC)


==revert of first paragraph==

I have removed the bit about networking from the lead paragraph; as networking is *not* an essential component of all computers, and it's a hell of a lot older than 10 years.  See [[embedded computer]] for an example of computers that often don't have any networking capabilities, and [[ARPANET]] for networking that long predates 1995.  Computers are more than just home PC's, people.  --[[User:Robert Merkel|Robert Merkel]] 12:32, 20 November 2005 (UTC)

==That is indeed very conservative==

It seems that Robert Merkel is extremely conservative and intolerant. When one talk about computers, whether server computers, embedded computers or personal computers, there ability to communicate is unquestionable. It appears that he belongs to an ancient tribe which believes that computers can only compute. In fact communication in modern world uses computers extensively. He is behaving like an osterich. The definition of the word computer has changed through decade, from a person who calculated, to the analytical engine of Charles babbage, to electronic calculator (Karl Zuse) to microproceesor controlled mainframes to the modern computers that are seen in several avtaars. So nothing is served by citing the definiion in Mariam Websters dictionary. Definitions of terms change with time and one has to learn to accept the changes.[[User:Hallenrm|Charlie]] 08:46, 21 November 2005 (UTC)

:Charlie, are you familiar with the [[Wikipedia:no personal attacks]] policy?  Please follow it.  And, if you'll excuse me a little credentialism for a moment, I have a PhD in software engineering, and am currently a postdoc; my specific field happens to be software testing.  I started using computers in 1980; I actually started writing real code in 1989.  At one time, I did some research into network fault diagnosis for a multinational telecommunications hardware provider.  After that, I worked for a company developing open source software; I was the only Australian-based employee of the company, and my primary communication with them was by email and private IRC.  While I missed the BBS era, I was certainly aware of it, and was a regular reader and occasional contributor to [[Usenet]] back when it was the primary group communications medium of the internet.  At the moment, I'm working on building a simple embedded system in my spare time; And, while it's not my field, I have friends who work on wireless networking stuff.  As to my understanding of the history of computers, as well as reading many books on the topic (one particularly relevant one to your claims is ''The Dream Machine'' by M. Mitchel Waldorp), one of my undergraduate lecturers, Peter Thorne, worked on [[CSIRAC]] (''The Last of the First'' by McCann and Thorne is an excellent account of the machine); he was there at the very beginnings of the modern computer.  So I think I have a pretty comprehensive perspective on what a computer is and isn't.

:Firstly, the claim that computers only became communications devices in the last decade is an incredibly PC-centric view of the world.  Perhaps the first wide-area computer network was [[SAGE]]; by the release of the IBM 360 series remote terminals were a comercially-available feature, see [http://www-03.ibm.com/ibm/history/exhibits/mainframe/mainframe_PR360.html this press release from 1964].  The [[ARPANET]], the direct ancestor of the modern internet, was switched on in 1969.  Even in the microcomputer world, [[Ethernet]] was common in offices in the 1980's, and BBS's were popular in the hobbyist underground (if you could afford the phone bills).  There is nothing fundamentally new or changed about networking in the last decade; it's just that what was largely restricted to large businesses, the military, academia, and the hacker underground went mainstream.  It's like claiming that cars were invented in the 1950's because that's when every family got one.  

:The second claim is that networking ability is somehow a fundamental characteristic of a computer.  While the ability to receive input and output from the outside world is indeed a fundamental component of computation, the exact method by which this is achieved is not particularly fundamental to making a computer a computer.  If I unplug the network cable from my PC it does not become any less of a computer!  Are the small pile of embedded computers in a BMW 7 series somehow not computers because they're not connected to the Internet?  My telephone is a incredibly useful networked communications device, but it is not a computer (though mobile phones are essentially embedded computers with radios attached).  The key thing that distinguishes computers from other devices is the ability to compute, not to communicate over a global network.  

:So, conservative or not, I think I'm right and you're wrong, and if you want to try to convince me you'll have to do better than gratuitous insults.  --[[User:Robert Merkel|Robert Merkel]] 12:40, 21 November 2005 (UTC)

Sorry, if I sounded like insulting you. I had indeed no such intention. The point I was trying to make that at present the most common perception of a computer is a device that is (and can be) used for communication. You must be aware of the old saying " exceptions prove the rule". True there can be computers that are bereft of any communication ability, but they are not the only computer machines. A vast majority of computers are now being deployed for communication purposes. So if a line is added to the definition that computers can also facilitate communication and are increasing used to do so, I see no harm. But, you seem to be extremely possesive of your lines and cannot tolerate any one changing them, so let it be. But remember we are communicating with each other only through computers, and that includes computers other than our PCs. [[User:Hallenrm|Charlie]] 07:04, 22 November 2005 (UTC)

: While I disagree with several aspects of his rewrite of the Computer article, I concur with Robert Merkel's point that computers are primarily devices for computing, and their modern use as communication devices is merely a secondary point---that is, merely only one of many possible applications of computing technology.  If you do not understand Merkel's analysis, it is probably because you are unfamiliar with basic computer science---in which one learns that a computer can be built on the basis of any well-understood physical principle which can be mapped to a Turing-complete set of basic operations.  Working computers have been built out of Tinker-Toys and toilet paper rolls.--[[User:Coolcaesar|Coolcaesar]] 07:14, 22 November 2005 (UTC)

::By the way, what are those?  This is the Wikipedia; the article ain't holy writ.  I only get narky when edits happen to make the article worse rather than better.  --[[User:Robert Merkel|Robert Merkel]] 11:53, 22 November 2005 (UTC)

:::If you go back to the earlier version of the Computer article, I had inserted an explanation of two of the most important concepts underlying computing---computers simply carry out mechanical operations which result in the drawing of shapes which humans assign meaning to.  Computers do not really think (at least yet).  Furthermore, computing is not theoretically linked to any specific implementing technology, and computers could be built out of anything, although we use electronic computers for now due to their reliability and because other technologies suffer from the "Turing tar pit" problem.

:::In my experience, these concepts are probably the most difficult for laypersons and non-mathematicians (myself included) to grasp.  It is because these ideas are so difficult that (1) computers have often been stereotyped by the media as "thinking machines" (and indeed there was a supercomputer maker of that name) and (2) electrical engineering is often confused with computer science.  --[[User:Coolcaesar|Coolcaesar]] 03:57, 23 November 2005 (UTC)

::::With regards to the implementation technology, that's a reasonable point; it's sort of implied but not perhaps made explicit enough.  However, you open a can of worms with your contention that "computers do not really think".  How do you know that I'm just not carrying out mechanical operations that result in keys being pressed on my keyboard that *you* are assigning meaning to? :) 

::::The [[Artificial intelligence]] article has also gone down the toilet over the years and I feel the urge to rewrite coming on (compare [http://en.wikipedia.org/w/index.php?title=Artificial_intelligence&oldid=85289 this old version] with the current article), but it (or the associated articles *should* explain this issue.  The two famous thought experiments in the area are the [[Turing test]] and the [[Chinese room]].  Personally, I tend to dismiss the Chinese room; but then, I don't assign any "intelligence" to a chess-playing computer, and plenty of people at least used to think chess-playing ability was the sign of intelligence.  

::::I agree that some brief mention of the question "can computers think" should be made on this page, though, but it needs to be done well.  
It may be very true that "computer scientists" learn through their education that "computers are devices that can process information only" , but the readers of wikipedia are not computer scientists alone. To press this point is rather puritanical not practical. [[User:Hallenrm|Charlie]] 08:25, 22 November 2005 (UTC)

:What's the problem, Charlie?  The communication aspects of modern computing are well covered in the section [[Computer#Networking and the Internet]].  It's not fundamental, but is important, so it has it's own subsection, that is well linked to other specialist articles.  No problem, as far as I can see. --[[User:Nigelj|Nigelj]] 10:38, 22 November 2005 (UTC)

::Computers are devices that process information.  One of their major applications is as communication devices to the extent that the information is sent to and comes from a remote location.  How hard is that?  I don't know why Charlie is having such a hard time with this.  

::Furthermore, the tendency on Wikipedia is to respect the general usage of a term by professionals in a field (for example, Internet v. internet).  This is due to Wikipedia's general "No original research" policy, which in turn gives rise to corollaries like a preference for objectivity over subjectivity.  The objective definition that most editors of this article support is already established in computer science textbooks all over the world, and is taught in computer science courses every day.  

::Look, Charlie, if you want to modify the definition, go do some research and get articles from major newspapers or peer-reviewed journals to establish a shift in the formal or informal meaning of the term.  Then come back here and post your evidence and maybe a consensus will develop in favor of a change.  For help on research, see [[Wikipedia:How to write a great article]].--[[User:Coolcaesar|Coolcaesar]] 03:57, 23 November 2005 (UTC)

That's it. Why can't this article be introduced with your definition. " Computers are devices that process information.  One of their major applications is as communication devices to the extent that the information is sent to and comes from a remote location." [[User:Hallenrm|Charlie]] 07:14, 23 November 2005 (UTC)
{{Aan}}

Oleg, no, it wasn't done with consultation.  I've just gone ahead and done it.  Over the next few days, I hope I (with assistance from others if they want) can work it into a state where it is clearly superior to the existing article, at which point it can replace that article and be the basis for further work.  

I decided on a fork because I couldn't achieve the ground-up reworking I wanted in one go, and I didn't want to leave the article in a screwed-up state.  --[[User:Robert Merkel|Robert Merkel]] 00:11, 30 July 2005 (UTC)

== Above-the-fold ==

The fact that the text that comes before the TOC is several paragraphs of detailed content seems to be a step backward. Certainly entymology and related info doesn't need to go there, does it? -[[User:Harmil|Harmil]] 01:05, 16 August 2005 (UTC)

== Some difficult text ==

===Intro===
:''Perhaps the key distinguishing feature between what we today call "computers" and the earlier mechanical devices is that these devices, ingenious though many were, could only be used to do a limited number of tasks. Modern digital computers are, by contrast, extremely versatile. In fact, they are universal information processing machines.''

This is a bit prosy, and doesn't convey information as clearly as it should. Perhaps:

:''The distinguishing feature of modern computers is that they are universal information processing machines.''

The subsequent Church-Turing info really belongs in a section of its own, later on.

:''Therefore, the same computer designs have been adapted for an infinite variety of tasks''

That's impossible. Infinite time would be required.

:''from playing music to controlling the flight of aeroplanes, and millions of others.''

Klunky....

:''This article concentrates mainly on the modern, universal, programmable computer. Discussion of earlier types can be found in History of computing hardware.''

A section called "History" with the usual:

:''Discussion of earlier types can be found in [[History of computing hardware]].''

lead-in.

:''Computers are present in a huge variety of physical packages.''

That should be re-worded. First off, "huge" is subjective and vague. Also, "physical packages" is redundant.

What, exactly was your concern with the original intro? It seems adequate.... -[[User:Harmil|Harmil]] 01:20, 16 August 2005 (UTC)

===Programs===
:''the Firefox web browser is created from roughly 2 million lines of computer code, and there are many projects of even bigger scope.''

That needs to be re-worded. As is, it just doesn't scan well at all. -[[User:Harmil|Harmil]] 01:25, 16 August 2005 (UTC)

== stored program architecture ==

The [[stored program architecture]] link in the section title is not necessary. Just make it a link in the paragraph text. -[[User:Harmil|Harmil]] 01:22, 16 August 2005 (UTC)

== Overall feel ==

Thanks for those comments, but what I'm really asking is whether you think the article as a whole is more coherent and presents a more useful summary than the current [[Computer]] article, and could serve as a replacement (after which it would of course be subject to further editing. --[[User:Robert Merkel|Robert Merkel]] 05:36, 16 August 2005 (UTC)

:In general I like it! There are parts to make better (see what they said above.. ;-) ), but it is quite better disposed then the old one, which was more or less just a bunch of somewhat related text pieces thrown together. This one, however, seems to have a thread from start to end. I tried to chip in some improvements here and there.

:Also I made a change to the sentence about Moore's law in the beginning. To be exact, it says that you can integrate more and more transistors in one package as time goes. Historically, this has meant faster computers (ie higher clockspeed, easy measure!), however, right now that development isn't going in that direction as much as it used to - the development goes at more and more distributed systems, dualcores etc, while the clock frequencies aren't really moving (that much at least). The term "more powerful" is for a layman essentially identical to "faster", but to an expert, it is more coherent with this recent development. [[User:TERdON|TERdON]] 14:04, 19 August 2005 (UTC)

::In the last edit you did (making a new, promising, history section!), that link to the article on [[analog computer]]s were removed. I do understand that digital computers are the usual thing today and the main topic of the article - however there is an argument to at least let a single sentence in the history link to the analogue counterpart - if not, this is an article that only deals with the [[digital computer]] (oops, that seems to redirect here, but still). I do know that the link is available in the computer history article, so this might be a question for debate though. Computers, IMHO, refers to both somewhat, and it would be a wise idea to be general and also point on computers not seen today... Overall, still a good initiative. [[User:TERdON|TERdON]] 03:45, 20 August 2005 (UTC)
{{Aan}}

== Thoughts on Lovelace's Contribution ==

IMHO, I think that it needs to be added that not only was Babbage responsible, but Lovelace also (Even if she was Babbage's figurehead, it cannot be disputed that her name is part of computer history.)

:[[Ada Lovelace]]'s actual contribution to Babbage's work is disputed, with evidence to suggest that it was relatively minor.  Even Babbage's work is only briefly mentioned in this article. We're trying to achieve a brief overview here.  --[[User:Robert Merkel|Robert Merkel]] 02:55, 31 March 2006 (UTC)

::I agree that Ada's work did not help Babbage very much - and one great difficulty is knowing how much of what she wrote came from Babbage and how much was her own, original research.
::* '''If you accept the view that she did much of the work herself''': then her contribution was essentially to invent the entire art of programming.  Her notes show things far more advanced than Babbage was ever planning to construct - so this is perhaps a tenable position.  Her notes describe (for the first time) concepts such as the subroutine and the need to have loops that could be terminated at the start or at the end, and a bunch of other subtle details that we all take for granted.  Anyway - she is generally credited with being the worlds first programmer - and as such needs to be described in some detail in the article - although arguably in her own right rather than as a footnote to Babbage's achievements.
::* '''There is a contrary view''': that she merely transcribed what others gave her - which is a far less romantic view - which probably has a lot to do with why this is an unpopular position to take.  In this view, she basically translated a paper into English, she took all of her programming ideas from Babbage - and was a complete klutz at mathematics.  The latter isn't a problem for me - I'm a total klutz at math too - yet I've been a successful software and hardware designer and a programmer for 30 years and earn a six figure salary doing it.  The kind of thinking it takes to come up with the ideas in her notes isn't of a mathematical nature.
:: Ada has a major programming language named after her - I believe that's an honor only ever bestowed on Ada Lovelace and Blaise Pascal!  So, if we believe that her work was original then I would argue that Ada's contribution was possibly even larger than Babbages - since her ideas are still in active use today - and so she '''must''' have some mention here.  If we don't believe she originated the ideas - then she is a mere footnote and already has more credit than she deserves.  I have no idea which of those views is correct - and even if I did, it would be POV not fact.  We probably need to adopt some of the language from the main [[Ada Lovelace]] article which discusses this in some detail with supporting evidence.
:: Ada's work (here: [http://www.fourmilab.ch/babbage/sketch.html] and here: [http://www.cs.yale.edu/homes/tap/Files/ada-lovelace-notes.html]) makes stunning reading for a modern programmer - the clarity with which she explains what programming '''is''' is quite wonderful given that computers didn't exist at the time she was writing it - but we'll never know whether she was merely good with words (as daughter of one of the worlds greatest poets - this would be unsuprising) or whether the ideas that she so clearly explains are also her own.  I don't see how we will ever know - and it's likely that she will tend to be given the benefit of the doubt because it makes a better story that way! [[User:SteveBaker|SteveBaker]] 12:59, 31 March 2006 (UTC)

== Spelling... ==

Let's not get into revert wars over American or British spelling, please. --[[User:Robert Merkel|Robert Merkel]] 04:42, 5 December 2005 (UTC)

== Opening text ==

It strikes me as somewhat disturbing that the first line of this article implies that a computer has to execute stored programs.  Plenty of devices that are nearly universally called computers couldn't execute ''any'' instructions (like ENIAC).  The text should be modified somewhat; especially since the next section does an adequate job of explaining that computers have been around far longer than the Von Neumann architecture. -- [[User:Uberpenguin|uberpenguin]] 02:00, 18 December 2005 (UTC)

: ENIAC ''could'' be programmed - it had a plugboard kind of affair to do that.  This has been described as 'rewiring the computer to reprogram it' - but that's a little unfair.  This is no different (in principle) from a ROM-based computer that can only obey a single program.  I don't think we'd exclude something like a mask-programmed ROM-based microcontroller just because it's difficult (or even impossible) to '''change''' its program - so long as ENIAC followed instructions, it's a computer.  It's a grey area though.  A Jaquard loom follows programmed instructions - but it's not considered a computer because it doesn't "process data" using its program. [[User:SteveBaker|SteveBaker]] 16:26, 28 February 2006 (UTC)

There are the historical devices, that were called computers, that never executed stored programs, that today might not be called computers, such as the tabulating equipment, where the equivalent of programs were in wired boards that would be plugged into the hardware as needed to be used.  Then there are contemporary alternatives to the [[general purpose computer]] such as [[supercomputer]] where the equivalent of [[software]] is moved to [[hardware]]. [[User:AlMac]]|[[User talk:AlMac|<sup>(talk)</sup>]] 10:28, 18 January 2006 (UTC)

== Eckhart and Mauchly == 

Check a computer history text; Eckhart and Mauchly are probably responsible for the majority of ideas which von Neumann sketched out in his report. --[[User:Robert Merkel|Robert Merkel]] 02:08, 18 December 2005 (UTC)

:I second that.  This is a basic point which is taught in all decent computer history classes.  Von Neumann's most important contribution was simply putting it all together in his famous report which then was circulated all over the place (and that's why we call them Von Neumann machines).  --[[User:Coolcaesar|Coolcaesar]] 19:10, 18 December 2005 (UTC)

== Picture of computer==

Is there some reason for the free publicity Dell and IBM are getting as PC makers with the photo on this page? (That was a rhetorical question)
--[[User:Eyal Rozenberg|Eyal Rozenberg]] 10:21, 27 May 2006 (UTC)

I remember a few months ago, I made a minor edit to the definition of computer to include its communication abilities. I was welcomed with a lot of derision from Merkel et. al. saying that I do not understand computer because I am not a computer scientist and that a computer is much more than a PC. I am now indeed surprised that the new introduction is accompanied by a picture of a PC. [[User:Hallenrm|Charlie]] 12:32, 31 December 2005 (UTC)

:Fair point - I've removed the picture.  Any suggestions for an appropriate replacement?  --[[User:Robert Merkel|Robert Merkel]] 06:31, 16 January 2006 (UTC)

:A PC is a computer.  A computer isn't necessarily a PC.  As a choice for a photo to put at the top of the article, a PC isn't necessarily the best choice - but it's not wrong either.  IMHO, anyone who comes to Wikipedia is using a computer - and VERY likely knows what a PC is and looks like.  In that regard, a photo of a PC isn't conveying much information beyond "Yes, this *is* the article about 'Computers' - you didn't mis-type it".  It might be better to come up with a more thought-provoking or informative picture to head up the article. [[User:SteveBaker|SteveBaker]] 21:54, 16 February 2006 (UTC)

::Well, I personally think a picture of a PC is the best way to lead the article because it's the form that most people are at least somewhat familiar with (they have to be using one to read Wikipedia).  Maybe an IBM/360 mainframe or a high-end workstation (like Sun or SGI) would also work.  In contrast, I'm not so sure about the exotic computers that run on toilet paper rolls or Tinker-Toys, since (1) we would have to get clearance for a picture and (2) they're not instantly recognizable to most people as computers. --[[User:Coolcaesar|Coolcaesar]] 08:38, 17 February 2006 (UTC)

:: A picture should add something to the article though.  If (as we seem to agree) anyone who is reading Wikipedia already has a computer (probably a PC) sitting in front of them then it's unlikely that including a picture of a generic PC would add anything.  How about something recognisable as a PC - but with historical interest - an *original* IBM PC or an Apple ][, a Commadore PET or a Radio Shack TRS-80 Model-I?  Maybe show the insides of a PC or pick some other kind of computer entirely - a washing machine controller or an ECU from a car?  There has to be something we can put up there that simultaneously says "Computer" and imparts some more information that makes the typical reader say "Wow!  I never knew that."   [[User:SteveBaker|SteveBaker]] 14:07, 17 February 2006 (UTC)

:: An introduction takes a topic, summarises it and relates it to what people know about the world. The rest of the article is what you use to expand people's minds. I think a PC is by far the best choice for the introduction picture as for many, many people a computer and a PC are the same thing. By putting a PC on the top it allows people to start the article at a comfortable level. The more novel pictures should be saved for the rest of the article, where the process of explaining the topic takes place. --[[User:FearedInLasVegas|FearedInLasVegas]] 19:05, 17 July 2006 (UTC)

:::I could not disagree more.  Someone comes to the article with the (incorrect) idea that a computer is a PC (a very common view) - and the very first thing they see '''CONFIRMS''' that exact failure of imagination!  That's ridiculous - we can do better.  Furthermore, a photo of a PC has precisely zero information content since everyone who can reach Wikipedia has one literally at their fingertips.  Finally, whenever we put up a photo of a current PC, someone replaces it with another PC from another manufacturer - so we are giving free advertising to Dell/Gateway/Apple/whoever.  We can (and do) give them that initial 'comfort' factor by showing a Lego computer in a kid-friendly yellow case.  At any rate - the main discussion of this is towards the bottom of this talk page - you should follow that discussion and post further remarks there. [[User:SteveBaker|SteveBaker]] 19:44, 17 July 2006 (UTC)

::::I thought this was the best place to talk about the front picture. Do you recommend that I create  a new heading at the bottom of the page about this? I think that when someone sees a picture of a PC on the computer page it tells them that a PC ''is'' a computer (note the order). The biggest reason I can see not to do this is because of advertising, although I can't see how that applies any more to this than pictures of Lego computers. On a slightly different point, there are no pictures of PCs on this page at all. I think this is necessary as, like you say, most people have PC on their minds when they come to this page. Not to acknowledge this association is missing something out of the article. --[[User:FearedInLasVegas|FearedInLasVegas]] 16:22, 18 July 2006 (UTC)

:Still 100% with Steve on this one.  It would take some mighty convincing arguments before I'd ever see a PC featured as the first image in this article.  (Hint: "PCs are common forms of computers that people readily identify as computers" is not a convincing argument) -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-07-17&nbsp;22:30Z</code>

: ''"I think that when someone sees a picture of a PC on the computer page it tells them that a PC ''is'' a computer (note the order)."'' - but people reading Wikipedia already ''know'' that a PC is a computer - they have one right within a couple of feet of them.   It's definitely not a high priority matter to tell them something they already know.  The first photo in the article is by far the biggest bang for the buck as regards passing on information.  We shouldn't waste that telling them something they already know and quite possibly make matters worse by reinforcing an unfortunate assumption.  That photo isn't decoration - and it isn't there to give people the confidence that they found the right page - it's there to convey useful, encyclopeadic information. [[User:SteveBaker|SteveBaker]] 20:16, 18 July 2006 (UTC)

== Language is Software ==
<Comment on the following phrase in [[Computer]]—''A computer is a machine capable of processing data according to a program — a list of instructions. The data to be processed may represent many types of information including numbers, text, pictures, or sound''.>


[http://www.yesselman.com/TwainSpn.htm#LanguageSoftware From] [[Daniel C. Dennett]]'s ''Consciousness Explained'' 1991; ISBN: [http://www.amazon.com/gp/product/0316180661/qid=1137101881/sr=1-1/ref=sr_1_1/104-8232826-7293530?s=books&v=glance&n=283155 0316180661] p. 302:

:The philosopher [http://www.hfac.uh.edu/phil/leiber/jleiber.htm Justin Leiber] sums up the role of [[language]] in shaping our mental lives: 

::Looking at ourselves from the computer viewpoint, we cannot avoid seeing that natural language is our most important "programming language." This means that a vast portion of our knowledge and activity is, for us, best communicated and understood in our natural language... One could say that natural language was our first great original artifact and, since, as we increasingly realize, languages are machines, so natural language, with our brains to run it, was our primal invention of the universal computer. One could say this except for the sneaking suspicion that [[language]] isn't something we invented but something we became, not something we constructed but something in which we created, and recreated, ourselves. [Leiber, 1991, page 8]

[[User:Yesselman|Yesselman]] 21:40, 12 January 2006 (UTC)

:That's no big deal.  Leiber is simply putting a different spin on old ideas that were better articulated by [[Julian Jaynes]], [[Richard Dawkins]], [[Benjamin Lee Whorf]], and [[Neal Stephenson]] (see ''[[Snow Crash]]'').  And yes, there are already some people in neuroscience who argue that the human brain is essentially an [[analog computer]].  Plus there's the sci-fi term [[wetware]] that's been around for a while. --[[User:Coolcaesar|Coolcaesar]] 08:28, 13 January 2006 (UTC)

I always liked "A Computer is a machine for following instructions" - but I don't recall who first said it. [[User:SteveBaker|SteveBaker]] 21:48, 16 February 2006 (UTC)

== External Links ==

the last [http://en.wikipedia.org/w/index.php?title=Computer&diff=39946574&oldid=39920383 edit] changed the external link http://www.tech-forums.net/ with http://www.compuforums.org/ leaving the same description of the link, these two forums are not the same, and so I find the edit questionable. Rv?

: It's probably someone promoting their favorite forum.  If there is no solid reason to use one or the other, consider having either both or neither. [[User:SteveBaker|SteveBaker]] 14:01, 17 February 2006 (UTC)

:: http://www.compuforums.org/ was first added on [http://en.wikipedia.org/w/index.php?title=Computer&oldid=23234948 14 Sept 2005] which is a long time ago, perhaps there's a reason for that link, so I will just rv. for now. Anyone could delete the link altogether, thanks for your answer. --[[User:ABShipper|A/B 'Shipper]] 女 <sup>[[User talk:ABShipper|(talk)]]</sup> 14:29, 17 February 2006 (UTC)

== Are these really computers? ==

We've defined a computer (correctly IMHO) as:

  "A computer is a machine for manipulating data according to a list of instructions - a program."

...yet we talk about the Abacus, the Antikythera mechanism, and clockwork calculating devices as early computers.  These are NOT computers according to our own definition - they are merely calculators.  They can't follow a list of instructions - they are not "programmable".  Even Babbages difference engine wasn't programmable - that honor goes to his analytical engine - which was never built and may not even have worked.

Furthermore, if we allow the Abacus to be counted as a computer (or even as a calculator) then human fingers get the credit for being the first computer since all an abacus is is a mechanical aid to counting.  It doesn't automate any part of the process.  Also, if the Abacus makes it, then so must the slide-rule - which is a VASTLY more sophisticated calculating machine because it can multiply and divide.

This is inconsistent.  Either we must accept calculators (for want of a better term) as computers and change our headline definition to allow non-programmable machines to be computers - or we must rewrite the section that includes such mechanical wonders as the Antikythera and the Difference engine and explain that whilst they were antecedents of the computer - they are not computers because they are not programmable.  I suggest the second course of action.

[[User:SteveBaker|SteveBaker]] 14:26, 28 February 2006 (UTC)

:Scroll up and you'll see me bringing up more or less the same issue.  I don't really agree with the headline definition of a computer since it only allows inclusion of modern programmable computers, not many older devices that could still be considered computers but were not programmable. -- [[User:Uberpenguin|uberpenguin]] 15:11, 28 February 2006 (UTC)

:Then you must also change the definition of 'Computer'...one or the other - you can't have both!  The article clearly defines what a computer is - then goes on to list a bunch of things that are clearly not computers under that definition.  We can argue about the definition - but no matter what, you have to admit that the article is inconsistent.  My change (which someone just Rv'ed) fixed that - if you don't agree with my change (or something similar to it) - then you MUST change the definition in the headline.   HOWEVER, if you do that then a difference engine is a computer, so is a slide-rule, an abacus and your fingers.  Come to think of it, a pile of rocks is a computer under that kind of definition because a pile of rocks is a primitive abacus.  I think the clear, sharp line that distinguishes a computer from a calculator is programmability - arguably it's a Church-Turing thing.  Draw that line and you must rule out the difference engine, the abacus and it's ilk.  We can still recognise the contribution of calculators to early computers - and perhaps even point out the grey areas - but there IS a bright line here. [[User:SteveBaker|SteveBaker]] 16:10, 28 February 2006 (UTC)

::We begin the history section with saying that "Originally, the term "computer" referred to" something different. Maybe we could include your argument later in that passage to pick up the thought that the definition of a computer has evolved to programmable computational devices, to clearly distinguish between early computational devices and a modern computer.--[[User:Johnnyw|Johnnyw]] 20:35, 28 February 2006 (UTC)

:::I think the easiest way out of the situation is to have the opening definition apply to only programmable computers, describe the evolution of the term in the history section, and mainly concentrate on programmable computers in the rest of the article.  That more or less follows the format of most other good computer hardware related articles here. -- [[User:Uberpenguin|uberpenguin]] 23:12, 28 February 2006 (UTC)

:::I disagree that the original meaning of the term "computer" referred to something different.  Human 'computers' were used in ways precisely the same as modern silicon computers.  The guy who ran the team "programmed" and "optimised" the use of the compters (people) in precisely the way a modern computer programmer does with his hardware.  To understand how these human computers were used, read some of the biographies of Richard Feynman who (amongst other things) ran the 'computer' group during the Manhatten project.  He organised the work-flow of his (human) computers - optimising their list of instuctions - making 'subroutines' that one group could calculate repetitively, using 'if' statements to skip over unneeded sections of his "program" depending on the results of previous calculations...it ''WAS'' programming.  The team of humans who did the work were both ''called'' computers and were "manipulating data according to a list of instructions" - which fits the modern definition of the word ''computer''.  I don't think the meaning of the term has actually changed all that much. [[User:SteveBaker|SteveBaker]] 00:51, 1 March 2006 (UTC)

::::I think you're blurring the line between definition and analogy  If you have to use quotations around "program" and "optimize" for your statement to hold water, then perhaps your suggestion to include human computers in the current article's definition is a bit too metaphoric.  Anyway, I'm not very interested in rewriting this article myself, so I won't really argue this much. -- [[User:Uberpenguin|uberpenguin]] 02:06, 3 March 2006 (UTC)

::::My use of quotations was erroneous - my apologies for misleading.  I intended no analogy.  Human computers had been used like this for lots of tasks in many situations - but the clearest explanation I've found is in the writings of Richard P.Feynman.  There are many versions of this in different biographies - the one that I picked off my bookshelf first was Feynman's "The Pleasure of Finding Things Out".  In the chapter entitled "Los Alamos from the Bottom" we hear of the times when he was running the computer center at Los Alamos - consisting of dozens of humal computers with Marchant mechanical calculators.  He organised a system of colored index cards that were passed from person to person containing the data - with each person having a set of algorithmic steps to perform on each data element depending on the color of the file card.  This included instructions on who to pass the results on to and on what color of index card.  Any modern programmer would recognise what Feynman was doing.  He had a parallel array of computers (human ones) - each of which had a written program, data packets flowing in and out, and an arithmetic unit for each human computer in the form of the Marchant calculator.  He discusses optimising his procedures for doing this - and then eventually replacing his human calculators with IBM tabulators (which were evidently just non-programmable multipliers and adders).  This isn't just an analogy - it's a process that is in essence indistiguishable from an electronic cluster computer - except that it uses human computers.  The human computers with their index cards and Marchant calculators would (as I understand it) have no problem qualifying under the Church-Turing constraints. [[User:SteveBaker|SteveBaker]] 04:32, 3 March 2006 (UTC)

::::Can I make a suggestion at this point?  There's plenty of scope for an article on human computers.  --[[User:Robert Merkel|Robert Merkel]] 04:49, 3 March 2006 (UTC)

== Computational ==

Either the "Computer" article needs to explain "computational" as seen through the eyes of Douglas Hofstatder, Donald E. Knuth and Alan Turing or a separate article created anew. Arturo Ortiz Tapia 13:06 hrs, +6 GMT., February 28th 2006

:Could you elaborate a little on what you mean by "computational"? --[[User:Robert Merkel|Robert Merkel]] 23:51, 28 February 2006 (UTC)

== Criticism of the ABC Picture ==

I just removed the ABC picture from the article.  Here are my reasons:
# When shrunk to thumbnail size, all the labels on the picture turn into little dots that look like the image has noise all over it.  At the ''very least'' we need a version without the labels and clutter to use as the thumbnail.
# When the diagram is brought to full size, those same labels are fairly meaningless without some explanatory text surrounding them.
# The image itself gives no idea of size or context.  Is this the whole computer?   Is it just a piece of it?
# A photo would work better here - diagrams are fine for explaining the inner workings and component parts - but that's not what we're doing here.  We are talking about the capabilities of the machine as a whole and for the image to earn it's price, it has to tell us something about this machine that the text doesn't do.  What it looks like, how big it is...stuff like that.
(According to the ''history'' comments, Uberpenguin agrees with me)
[[User:SteveBaker|SteveBaker]] 17:31, 5 March 2006 (UTC)

:Agree fully, but let me also give you some background on why I didn't delete it myself.  There's currently an anonymous editor who is pretty ardent about including information about the ABC and the related 1970s court case in several computer related articles, regardless of whether the information is relevant to the article or not (see the recent history of [[CPU]] and [[UNIVAC]] for examples of what I mean).  I've asked him to stop indiscriminately adding the text to articles and discuss it on the talk pages, so we'll see if he complies.  He's accused me in his edit summaries of trying to obscure history by removing what I believe to be irrelevant text and pictures, so I didn't particularly want to start trouble by removing the image here where it could conceivably be relevant. -- [[User:Uberpenguin|uberpenguin]] 17:52, 5 March 2006 (UTC)

::I also agree with both of you that the diagram should not be there.  I remember studying the ABC in college (as well as the work of competitors like Zuse, Stibitz, Eckert & Mauchly, Shannon, etc.) and I always thought that Atanasoff's courtroom quest was rather quixotic and silly.  I also agree that if the ABC should be shown at all, a photo would be more appropriate.  --[[User:Coolcaesar|Coolcaesar]] 19:00, 5 March 2006 (UTC)

::: This second picture was added after considering that the previus one was ugly, u were not educated enought to understand what was writen on it and there is no people on it so you can say how big it is ... now this one had met all your criteria but still you've removed it just becouse four people can deside the censensorship. Now I've met all your criteria be good boys and put back the second picture yourself !!! P.S. It's not my and the rest of the readers problem that you've been told wrong in class just because of the cold war which is over BTW !!![[User:71.99.137.20|71.99.137.20]] 00:21, 7 March 2006 (UTC)
:::: I almost forgot I have writen to several institutions and medias (like NY Post, Newsweek and etc.) about your censorship, so from now on think twice what your sparing about history. [[User:71.99.137.20|71.99.137.20]] 00:27, 7 March 2006 (UTC)

:::: Re: Your peculiar comment about the educational level of people you've never met...it's not a matter of OUR education.  I've designed computers '''vastly''' more complex than ABC from scratch at the ASIC chip level - I fully understand all of the points on the diagram.  (You might maybe want to apologise - but that's OK).  However, that's '''not''' the point.  The point is whether the average Wikipedia reader who looked up 'Computer' would understand the labels without supporting text.  IMHO, anyone who needs to '''look up''' computers in an encylopedia (as opposed to writing articles in it) should be expected to have a lower level of knowledge on the subject and therefore would NOT be expected to understand the diagram.  People who are computer experts may be the ones to write this article - but it's 8 year old kids with a class assignment who will be reading it.  As for censorship - if Wikipedia editors (that's all of us) can't edit the article once someone has put something into it without being accused of censorship then Wikipedia cannot exist because it's all about collaborative editing.  Right here under the edit box it says "If you don't want your writing to be edited mercilessly or redistributed by others, do not submit it." - that applies to photos too.  If I don't like what you put into the article - then I can take it out again.  That's not censorship - it's me fixing a problem that I (rightly or wrongly) perceive.  Going screaming to the media just because someone doesn't agree with your choice of photograph does not speak well of you - and if truth be told, I very much doubt that even one person who reads your inflamatory remarks believes for one moment that you actually did that - this does nothing for your reputation. [[User:SteveBaker|SteveBaker]] 04:40, 7 March 2006 (UTC)

:::::Please learn the difference between censorship and contextual relevance.  You have yet to give a single comprehensible reason as to why we should go out of our way to display the ABC over one of the other plethora of contemporary computers.  Perhaps I was a bit hasty to remove the picture because of your recent history of edits, so allow me to explain my position with a measure of lucidity. The abridged history section here only has enough text to merit perhaps one picture of a very old electronic computer.  I'm not particularly opposed to it being of the ABC as opposed to ENIAC, but a concern is that the ABC image you uploaded has unknown copyright status.  If that were fixed, I'd have no problems putting it to a vote as to whether the ENIAC image should be replaced with ABC.
:::::Now, please take this opportunity to reflect on your unnecessary behavior; labeling and name-calling.  I'm trying to be as accomodating as possible (especially since I see no particularly good reason to change an image that serves purely as an example), but your reverting and then attacking anybody who wishes to discuss your changes reasonably is unacceptable.  Please read [[WP:NPA]] and try to behave in a civil fashion. -- [[User:Uberpenguin|uberpenguin]] 00:52, 7 March 2006 (UTC)

::::::I would like to say, however, that if this WERE put to a vote I'd still oppose the ABC picture inclusion because it is of a replica, not the original.  For brief informative purposes illustrating an early electronic computer, I believe it's much more valuable to use an image of the original. -- [[User:Uberpenguin|uberpenguin]] 01:17, 7 March 2006 (UTC)
::::::: Yeah - but from the point of view of telling our readers what the computer looked like - if it's a good replica then a photograph of it is a reasonable stand-in.  The useful information it conveys is still there.  It would have been dishonest to protray it as a photo of the real thing - but so long as it's clearly labelled (which it is) - I think that's OK with me. [[User:SteveBaker|SteveBaker]] 04:43, 7 March 2006 (UTC)

Some of this debate is getting out of hand.  Writing letters to the press is a ridiculous over-reaction to Wiki editing.  If you can't stand this kind of thing - you shouldn't be here - your text and photo choices WILL be edited.  Sometimes by people who know better than you - sometimes by random idiots - in neither case could it remotely be called "Censorship".

Now - we should have a rational debate about this image.  Here are some facts:

# The first ABC picture was unsuitable - this is widely agreed and I set out a comprehensive set of REASONS why I deleted it.  I didn't do it on my own (although that would have been acceptable) - at least one other respected Wikipedian agreed with me - and I explained myself.
# The second ABC picture would probably be suitable from a quality standpoint - but it has NO COPYRIGHT ATTRIBUTION.  So we can't use it - period.  It should never have been uploaded in the first place.  We can't afford for Wikipedia to be sued by the copyright owner.  So, NEVER, EVER post a photo without cast iron copyright permissions. So, a slap on the wrist goes to the person who put it up here.
# Given the above, removing the second picture was 100% warranted - there should be no dispute over that - read the Wiki rules.
# HOWEVER:  When you do something like reverting someone else's work or a semi-reasonable photo, it's good manners to put a note here in talk to say why you did it.  This engenders debate - which hopefully comes to some conclusion without a major bustup.  So a slap on the wrist for the person who removed it '''because of the lack of a justification - not because of the removal of the image'''.

Those are the facts.  Now my opionions:

* '''If''' the person who posted the image can find another with SOLID copyright history (and given the grief about this, believe me, it'll be checked) - then we should have a debate over whether ENIAC or ABC is more suitable as the computer to illustrate this section - or whether both photos can be included.  But that debate can't happen until we see this hypothetical ABC photograph.
* The issue of this anonymous poster trying to get the ABC story into Wiki in a bunch of places is (IMHO) irrelevent here.  If it belongs here - it should be here - if it doesn't, it doesn't and it's truly irrelevent to the quality of this page what the motivations of the poster are.

So, kiss & make up and let's get a nice article together here!
[[User:SteveBaker|SteveBaker]] 02:29, 7 March 2006 (UTC)

:To be perfectly honest, I jumped the gun on reverting that image because I thought it was the same image as before.  That was my mistake, and thus the reason for my lack of justification for its removal.  That was, admittedly, wrong, but as we both have pointed out there are valid reasons not to use the new image either.  As I stated above, I have nothing fundamentally against using a picture of the ABC here, but I do have concerns with the specifics of both proposed images and the worry of making the article cluttered by weighing the text down with too many images.
:I'm perfectly willing to discuss the inclusion of an ABC picture here.  Please excuse me if I seem a bit tense or borderline on ad hom, but perhaps you'll understand my irritation at being openly insulted without provocation in this and other articles that the anon editor has recently participated in. -- [[User:Uberpenguin|uberpenguin]] 02:43, 7 March 2006 (UTC)

:: I understand.  But listen: In a recent study of online communications (email as it happens), 80% of writers believed that their readers would comprehend the 'tone' of their messages - where in fact, only 20% actually did.  This is worse than chance!  So we all have to take extra special precautions in all online dealings not to assume motives - one mans insult is another mans honest mistake.  So - we still can't do anything either way about a better ABC photo.  The one that was there last WILL be automatically deleted by the copyright police bot in just a few days.   We can't/mustn't/won't use it.  <shrug>  If [[User:71.99.137.20]] would care to provide either a different (and copyright-free) photo - or ascertain that in fact the photo that was posted is in fact legal - then we can discuss whether we want more photos or whatever.  Right now, it's a moot issue because we can't argue about a photo that doesn't exist.  QED [[User:SteveBaker|SteveBaker]] 04:20, 7 March 2006 (UTC)

:::I agree with SteveBaker.  Also, I believe the anonymous user is not editing in good faith, since all professional computer historians (Ceruzzi, Aspray, et al.) do not see the ABC as the only "first computer," regardless of what the Atanasoff family and their friends believe.  I suggest that the anonymous user should be blocked if he or she fails to cooperate with Wikipedia policy. --[[User:Coolcaesar|Coolcaesar]] 06:39, 7 March 2006 (UTC)

::::I would not advocate blocking. If the Atanasoff's have evidence then let evidence be presented - lets discuss it's validity and document the results.  We can't do original research here - but we most certainly can & should read all available sources and see things from all available points of view. If this debate has already been carried out on other Wikipedia pages - then let's hear about that too. [[User:SteveBaker|SteveBaker]] 12:42, 7 March 2006 (UTC)

::::: Steve this is verry good what you're saying but those are people who know the facts but are just prejudice and just whant to shut my mouth I've been blocked once already and denied the ability to create account so I can defend that I want to say. You don't need to believe me all I'm trying to write is a sworn testamony in US court, go to this adress and see for yourself [http://www.cs.iastate.edu/jva/jva-archive.shtml] [[User:71.99.137.20|71.99.137.20]] 21:11, 7 March 2006 (UTC)

::::::Sir, nobody has blocked you from registering an account, and your IP was '''temporarily''' blocked for violating the [[WP:3RR]] (''nobody'' &mdash; including registered users &mdash; may themselves revert any article more than three times in a 24 hour period).  Instead of waving the prejudice finger again, why don't you either find a picture that can be legally used on Wikipedia or make valuable edits? -- [[User:Uberpenguin|uberpenguin]] 21:01, 7 March 2006 (UTC)

::::::: The Iova State University will be more than happy to give me whatever the licence is needed. And that's realy stange because in order to mail Voice_of_All I needed to be loged on, and up until I changed my IP there was no "create an accont" on the log on page. [[User:71.99.137.20|71.99.137.20]] 21:11, 7 March 2006 (UTC)

:Assuming that Iowa State owns the copyrights to that picture, you will need them to release (and have written proof of such release) it to either GFDL or public domain.  -- [[User:Uberpenguin|uberpenguin]] 21:18, 7 March 2006 (UTC)

::Incidentally, if you're going to go through the trouble of getting the school to release a picture, get [http://www.cs.iastate.edu/jva/images/abc-1942.gif this one] since it's actually a picture of the ORIGINAL computer, and not a replica. -- [[User:Uberpenguin|uberpenguin]] 21:41, 7 March 2006 (UTC)

Hmmm - I've been reading up on this ABC gizmo.  Once again - from the first line of our article:

   "A computer is a machine for manipulating data '''according to a list of instructions''' - a program."

(my emphasis)

The ABC is not (according to that definition) a computer at all.  It's another fancy hardwired calculator.  It's pretty impressive for it's time though - very small compared to the roomfulls of stuff you needed for ENIAC.  But in the end, it's not a computer - it's an ALU and some memory - but it lacks any kind of sequencer/program-counter/program storage.  If you allow non-programmable devices to be computers, how do you distinguish them from calculators?   The single unique thing that makes a computer different is that it has a stored program that you can change in order to do anything your imagination can come up with.  Sorry - but I now agree 100% with uberpenguin - there is no need to say much about this thing anymore than there is a need to go into the workings of the [[Antikythera]].  The sentence we have about the ABC right now is PLENTY. [[User:SteveBaker|SteveBaker]] 04:59, 10 March 2006 (UTC)

:I agree with all of this.  It's a fascinating device, and a major step along the road to what we now call computers, but it was a special-purpose machine with no programmability.  --[[User:Robert Merkel|Robert Merkel]] 05:13, 10 March 2006 (UTC)

:: I'm sorry Steve, but ABC not only does instructions, but it does 30 instructions at one time (the first parallel processing). Talking about sequencer it has system clock that's actually mechanical (the motor on the right side). The memory modules are made from capacitors and vacuum tube(exactly the way DRAM is constructed), although the access is mechanical(drums) instead of electronic. I can see that you jump to conclusions pretty easy. You have to understand ABC is not a machine or special-purpose computer it's the first "digital" computer prototype and because it has laid the "digital" fundamentals in just one version it's a GIANT step. You can't expect to have all the issues smoothed out with just the first version, but at least the bases.
I'm going to put this here so you can tell me what do you agree or not about ABC or has to be paraphrased "The first Digital computer to use binary base, logical operators instead of counters by using vacuum tube (transistors), memory based (capacitors - today's DRAM), separation of memory and computing functions, Parallel processing (it actually did 30 instructions at once), and system clock]]".

(RESTORED ORIGINAL CONVERSATIONAL ORDER TO THIS TALK PAGE [[User:SteveBaker|SteveBaker]] 18:25, 10 March 2006 (UTC))

::: So are you now claiming that it '''was''' programmable?  Doing 30 instructions at once is not programmability if they can't be sequenced automatically.  Bottom line: Was the ABC Turing-complete?  I'm pretty sure the answer is "no".  If so, then this does not fit the common definition of what a 'computer' is.  So in terms of your proposed wording:  (a) Don't say it's a computer and (b) if it's not a computer do we really want to go into so much detail about Yet Another Calculator?  [[User:SteveBaker|SteveBaker]] 07:43, 10 March 2006 (UTC)

:: And Uberpenguin I can see that the histories has been sanitized very good, even where I have adding text has been cleared. On the question "Who am I to say that John Atanasoff is genius" - I didn'twrote anywhere that he is a genius, but for sure the President of the United States already did said that by giving him "National Medal of Technology" and the United States court had ruled that John Vincent Atanasoff and Clifford Berry hadconstructed the first electronic digital computer at Iowa State College. I what also to add here what John Mauchly confirmed under oath:
- He spent from 13 June 1941 to the morning of 18 June 1941 as a guest in Atanasoff's home in Ames.
- During this period as Atanasoff's guest he spent uncounted hours in discussions of the Atanasoff Berry Computer and computer theory with John Atanasoff and Clifford Berry.
- On three or four days he accompanied Atanasoff to his office in the Physics Building and observed the Atanasoff Berry Computer in the company of Atanasoff and Clifford Berry.
- He had seen demonstrations of the operations or some phases of the functions of the Atanasoff Berry Computer and might have engaged in manipulation of some parts of the machine with Clifford Berry.
- He was permitted to read Atanasoff's 35-page manuscript on the construction and operation of the Atanasoff Berry Computer from cover to cover and probably did read it. Atanasoff and Berry had willingly answered questions and entered intodiscussions with him about the machine and the booklet, but Atanasoff had refused to let him take a copy to Pennsylvania.
- Immediately after his visit to Iowa State in June, Mauchly had written letters to Atanasoff and to his meteorologist friend, Helms Clayton, expressing enthusiasm about the Atanasoff Berry Computer and had taken a crash course in electronics at the University of Pennsylvania.
- On 15 August 1941 he wrote a comprehensive memorandum on the difference between analog calculators and pulse devices that incorporated some ideas that were almost identical with those in Atanasoff's 35-page manuscript on the ABC.
- On 30 September 1941 he had written to Atanasoff suggesting a cooperative effort to develop an Atanasoff computer and had asked if Atanasoff had any objection to him using some of the Atanasoff concepts in a computer machine that he was considering building. [[User:213.222.54.133|213.222.54.133]] 08:25, 10 March 2006 (UTC)

::::(Cool! A new anonymous IP address has appeared!).  Look - for the purposes of THIS article, we really don't care who invented what, who was smarter than who, or who had the best lawyers.  The court case is irrelevent to THIS article.  What matters is whether we consider the ABC to be a true computer.  So far, all but one person here agrees that we don't.  I took an informal poll at work today and asked my co-workers: "What single feature distinguishes a computer from a calculator?" - out of 18 people, I got UNANIMOUS agreement that general programmability is the sole, single feature.  We agree that it doesn't matter whether it's an analog device, a base 10 or base 3 device, how it's memory works or whether it is a parallel machine or not - whilst all of those are important innovations along the route to making a practical computer, the one single defining feature is the ability to write a program, put it into the machine somehow and have the machine execute it as a sequence of steps.  I'm convinced that the ABC couldn't do that - so by all practical definitions, it's a CALCULATOR...albeit a highly advanced one with all sorts of neat modern features and albeit that Atanasoff invented them all and was a really clever guy.  I could care less about that at this point.  The sole issue for THIS article is whether we tell our readers that the ABC was the worlds first computer - it seems utterly clear to me that is was not.  Unless I see evidence to the contrary - I think the debate is over. [[User:SteveBaker|SteveBaker]] 18:25, 10 March 2006 (UTC)

:::::I agree that the argument should be over.  When we ask for the anonymous editor(s) to justify that the ABC was a computer by modern standards, we get back rhetoric describing the ABC's features and the 1970s court case.  I'm sorry, but US patent court is not any kind of recognized authority on technology or computer history.  The ABC was not programmable, therefore it was not a computer by the standards of this article or most modern definitions. -- [[User:Uberpenguin|uberpenguin]] 20:36, 10 March 2006 (UTC)

== What is a "computer"? ==

I've moved part of the discussion originally found on [[Talk:Atanasoff-Berry Computer]] here since it has become irrelevant to that article.
----

:::Here's the problem though. On this page of Wikipedia, we had said that the ABC is a computer - without adequately explaining that it is not a computer AT ALL in the modern sense of the word - it's essentially just an oversized pocket calculator.  That allows this page (and certain anon users) to start claiming (without qualification) that the ABC was the world first digital computer - which is certainly not true in the modern sense of the word.  IMHO, it wasn't even true in the 1940's sense of the word in which the human who OPERATED the ABC would be called a 'computer'.  If you allow the claim of 'First ever digital computer' here - how can you deny it elsewhere (and I'm thinking of course of the [[Computer]] and [[CPU]] articles) without Wikipedia being inconsistant.  So THIS article needs to explain - very carefully - the restrictions on that claim otherwise other pages will end up needing heavy revision in order to remove ENIAC and replace it with ABC as the first ever digital computer - and to do so would be exceedingly misleading.  By all means, rephrase (or even revert) what I wrote - but you must do something because what was there before is WRONG.  IMHO, you don't write a modern encyclopedia using words in the sense that they were meant in the 1940's (at least not without careful consideration and qualification)...otherwise we are going to end up calling a lost of merely happy people ''gay'' and I really don't think that's a good idea!! [[User:SteveBaker|SteveBaker]] 18:49, 10 March 2006 (UTC)

::::If we're using the modern (i.e. most of the last century and this one) definition of computer, yes, I'd agree that programmability is the defining feature and therefore ABC does not qualify as a computer.  I think the article shouldn't call it a computer, but should point out that by some definitions it would be called a computer, but by the modern definition it is not one. -- [[User:Uberpenguin|uberpenguin]] 19:06, 10 March 2006 (UTC)

Oh come on, a computer is one that computes.  Whether it's a person determining ballistic trajectories or a machine determining ballistic trajectories or an electronic computer determining ballistic trajectories...it's all the same.

The ABC is not turing complete and nor did it have stored programs, but it computed.  Use your qualifiers ("modern") if you wish but a stored program computer is a subset of all computers and such qualifiers are irrelevant.  Just like a square has four equal sides (a "modern rectangle" if you will and get my point), but that doesn't make it any less of a rectangle.  The ABC is a ''computer'' and to call it any less is pure rubbish just as if you said a square isn't a rectangle.

The ABC was the first '''''electronic''''' digital computer.  The difference engine was not electronic, so it didn't beat the ABC to that one. [[User:Cburnett|Cburnett]] 01:44, 11 March 2006 (UTC)

:Then we should change the definition of "Computer" on this page.  Shall we also rope in calculators and abaci into the "computer" definition?  I'm just playing devil's advocate here to point out that it's not easy to come up with a good definition of "computer" that will stand and be consistent across multiple articles. -- [[User:Uberpenguin|uberpenguin]] 01:58, 11 March 2006 (UTC)

::Yes, a calculator is a computer.  It is a specific purposed (possibly programmable) computer.  An abucus is not a computer because it is an instrument and is no more of a computer than beans and wire.  An abacus does no computation but the user performs computations by using it.  Do you call an AND gate a computer?  Certainly not, but it's definitely used by computers.  An abacus is not a computer, but it is used by one.

::I have no idea why this is so hard for you to comprehend.  A computer is one that computes.  It has several subcategories: human, mechanical, digital, electronic, and probably more.  Under electronic there's analog and digital.  Under digital there's programmable and non-programmable.  There's an entire taxonomy to computers, but all that is required to be a computer is membership in this taxonomy. And an electronic, digital, non-stored program machine that computes is definitely in said taxonomy. [[User:Cburnett|Cburnett]] 02:11, 11 March 2006 (UTC)

:::But what does 'to compute' mean?  How does it differ from 'to calculate'?  Assuming we are writing Wikipedia in modern english - and not some older dialect - we have to ask what constitutes a computer and what a calculator?  In every modern usage, a computer is a programmable machine and a calculator is not (except of course when we apply the qualifier "programmable calculator" - but that's WHY we say that rather than just "calculator").  We have desktop computers, there is an engine management computer in my car - both are programmable - but this little box with all the buttons on is a calculator we never call those little four function calculators "computers" - NEVER!  We all understand the usage - where is the confusion?  If you really truly believe that something that is not programmable is a computer - then where do we draw the line.  It is PERFECTLY possible to design a programmable device that is Turing-complete yet has no hardware for arithmetic or numeric capabilities. This would clearly be a computer - because we could program it and have it implement arithmetic and such in software.  But a calculator - without programmable features is just not something recognisable as a computer. 
::: Where would you draw the line between what is a computer and what isn't?  Is the Difference Engine a computer?  How about one 'adder' circuit?  Is an abacus a computer?  A slide rule?  A table of logarithms.  Ten human fingers?  A pile of rocks? ...because you can calculate with a pile of rocks. If the definition of computer is SO lax as to include anything that can perform any kind of arithmetic or logical operation - then a light switch is a digital electronic computer that pre-dates the ABC by a century or more. 
::: But if you insist that a non-programmable device is a computer then you'll have to go through articles such as 'Computer' (which states that a computer is programmable in the very first sentence of the article) and find some better definition.  To do otherwise would be to confuse the heck out of our readers.  They read the 'Computer' page - it says computers are programmable data processors - then they see the reference to the ABC, click through to this page and it says something to the effect that this device is a computer - but that it's not programmable....now what?  They've just read a flat out contradiction...how does this get resolved?
::: [[User:SteveBaker|SteveBaker]] 02:59, 11 March 2006 (UTC)

:::: Let me preface this by saying that your reply is so incoherent and jumpy that it's hard to form a good reply.

:::: You need to read [[Von Neumann architecture]].  I quote:

::::: ''The earliest computing machines had fixed programs. Some very simple computers still use this design, either for simplicity or training purposes. For example, a desk calculator (in principle) is a fixed program computer. It can do basic mathematics, but it cannot be used as a word processor or to run video games.''

:::: So the von neumann arch article recognizes fixed programs as computers. But ignoring semantics and etymology, just look at the number of recognitions that it's a computer.  It's even in the name.  Honestly, [http://0-www.ieee.org.csulib.ctstateu.edu/organizations/history_center/milestones_photos/atanasoff.html the IEEE] recognizes it as a computer.  So if you want to challenge the term, then wikipedia is not the forum for such a challenge.  Go write the IEEE, or perhaps [http://www.cs.iastate.edu/jva/court-papers/page49.gif the US district court of Minnesota] that named the ABC as the "first electronic digital computer".

:::: I think the IEEE and US courts say enough to null your argument.  But if you still feel you're right, then stop making WP your soap box and argue it in a proper forum.

:::: All that said since you seem quite confused: rocks, fingers, abacus, etc. are instruments or aids for calculation and are not computers.  No more than my shoes or rolls of toilet paper are computers.  I can use any of them as aids to compute, which makes me a computer.

:::: Semi-finally, you're using WP as an authoritative reference.  As much as I like WP and have contributed to it, the word of IEEE and the US courts says more than the introductory sentence to "computer" on a web page.


:::: Finally, a light switch?  For serious?  I can understand fleshing out an argument and seeing how far you can stretch something (and even being sarcastic to point out the flaws in the others argument).....but seriously: a light switch?  Rocks?  As computers.  Come on.  Surely you're more intelligent than that and can look up the word in the dictionary and understand how I'm using it to know that I wouldn't consider rocks or a light switch as a computer. I even stated an abacus is not a computer in that which you replied, so why would you go so far to saw a light switch.  Come on!  I haven't treated you with any disrespect, so why do you have to do so to me by using asinine examples that I already discounted?  I only say this so that any further replies don't go so, for lack of a better word, wrong. [[User:Cburnett|Cburnett]] 05:13, 11 March 2006 (UTC)

:::::His reply was in no way incoherent; please do not label a different point of view as invalid just because you don't agree with it.  We are simply asking, at what clear point do you draw the line between computer and calculator?  Steve is suggesting using the Church-Turing thesis as a benchmark; you are more or less expecting this to be evaluated on a per-case basis.  I agree with Steve that the definition of "computer" has changed a good deal; in modern usage you never see a non-programmable device called a computer (find some common counterexamples and you've made your point).  Forgiving Steve's hyperbole, would you consider an ALU to be a computer in its own right?  A series of op-amp integrators?  Both certainly are computing devices, but are they computers?  I know that neither of us would answer "yes" to that question, but I mention them as examples because your proposed definition of "computer" is so broad that you could conceivably include these devices.

:::::Please understand that we believe the definition of computer to no longer be as cut-and-dry as you understand it to be.  We aren't treating you with disrespect in any way, just attempting to make the point that we believe your definition of "computer" to be far too inclusive to be useful on an encyclopedia.  That being said, I'm not sure why the court case keeps coming up.  If you consider patent court an authority on defining technology or language, you'd have a hard time supporting your argument.  Furthermore, the IEEE has the luxury of not having to agree upon a formal definition of "Computer."  We do, and thus this discussion.  On your WP comments; nobody is claiming WP as a reliable resource, we only have noted the need for consistency here. -- [[User:Uberpenguin|uberpenguin]] 14:01, 11 March 2006 (UTC)

::::::The IEEE doesn't have to define computer, but they clearly include the ABC in it by calling it the first electronic, digital computer.  If they didn't believe in calling it a computer, then they wouldn't.  The shear number of people that call it a computer is sufficient enough to make your argument that it's not a computer basically original research.

::::::''Computer Organization & Design'' by Patterson & Hennessy ({{ISBN|1-55860-490-X}}) calls the ABC "a small-scale electronic computer" and "a special-purpose computer".  So the IEEE call it a computer, a district court calls it a computer, and a professor in comp arch from Berkeley and Stanford call it a computer.  I did a google search and came up with no one discrediting the ABC as a computer.

::::::Honestly, how many authoritative sources are needed?  Why should I, and the readers of WP, take the argument of an undergrad (uberpenguin; I couldn't find much about Steve except that he has a son so he's probably not an undergrad) over than of professors, texts, and the IEEE and pretty much every source out there?  I don't really care if you agree on my definition of computer &mdash; for another article &mdash; but ''everything'' I find calls it a computer.  You are merely confusing yourself with the modern ''flavor'' of computers with the definition of "computer."

::::::Again, if you desire to change the name of the ABC and the definition of "computer" then WP is far from the correct forum.  If you want to make the turing test the de facto standard for labeling something a computer, then you are also on the wrong forum for such a change.  I most certaily agree that a "modern computer" is one that does pass the test, but the modern computer is in a '''''sub'''''category of "computer".  Heck, to a lot of people "computer" means [[Microsoft Windows|Windows]] but I'm nowhere near ready to acquiesce that definition. [[User:Cburnett|Cburnett]] 15:38, 11 March 2006 (UTC)

::We really should be moving this discussion over to [[computer]], because the crux of this is "how do you define computer", not "what is ABC's name".  Incidentally, mentioning my educational status skirts the border of ad-hom.  Questioning other authors' knowledge isn't a valid way to make your point here. -- [[User:Uberpenguin|uberpenguin]] 18:13, 11 March 2006 (UTC)

:::Clarification: I want to continue this discussion, but would rather do so at [[computer]] since it's become almost totally irrelevant to this article. -- [[User:Uberpenguin|uberpenguin]] 18:20, 11 March 2006 (UTC)

:::::::Once you start asking for people's credentials - you know things have degenerated too far for a reasonable conclusion. FWIW, I'm a professional computer/software designer.  I've been programming computers since 1973.  I've designed simple CPU's and I currently am the technical lead for a group of a half dozen programmers. I really want to know what YOU (Cburnett) would define a computer to be.  Because if it's something that merely performs arithmetic - then we have a serious problem. The 'pile of rocks' and 'light switch' examples were (of course) merely an effort to have you tell me why they are excluded from the definition of a computer (I think we all agree that they are not) yet the ABC is.  If you can clearly articulate what that definition is - and quote some references to back it up - then I'll be first in line to change the first sentence of the [[Computer]] article.
:::::::Here's the problem though.  If doing arithmetic is sufficient then an abacus (presumably) counts as a computer.  Lots of people use them for doing arithmetic - and in some cases are faster at doing some classes of operation than is a pocket calculator.  So if you do accept an abacus - then I regret that a pile of rocks becomes hard to exclude.  If I want to add 11 to 17 then I move 11 rocks off into a separate pile, then move 17 rocks off into another pile - then push the two piles together and count how many rocks I have.  The pile of rocks just calculated 11+17=28.  But as I explained, mere arithmetic is not enough to define a computer.  Many years ago (when technology wasn't what it is now), I had to build a massively parallel (but pretty slow and stupid) computer array for doing graphics processing.  Because the chip technology I was using wasn't all that great - and I needed to pack 128 computers onto a single chip, I build a ONE BIT computer - with the only operations being (from memory) NAND, SHIFT (which means 'copy this bit into carry and replace with zero), copy, load-with-1, load-with-0 and jump on carry. (Well, there was a little more than that - but not much).  This computer could do NO ARITHMETIC in it's basic hardware.  But it's a Church-Turing device - so it could be programmed to emulate arithmetic.  Given that WAS a computer, you have to allow that performing arithmetic (in hardware) is not a requirement to make a computer.  In this case, purely boolean logic operations were implemented in hardware.
::::::: So, I repeat - what is your definition of a computer?  I don't think "Anything that some judge in a district court says is a computer" is a sufficient description. [[User:SteveBaker|SteveBaker]] 18:30, 11 March 2006 (UTC)

Okay, now that this is on the correct page for this discussion, I'd like to try to point out that the issue is that the usage of the term "computer" is somewhat inconsistant on WP and needs to be discussed.  [[User:Cburnett]], how would you propose we define "computer" here, and how should we apply it to various early computational devices?  [[User:SteveBaker]] has more or less suggested we use the Church-Turing thesis as the benchmark for what a computer is. While I personally believe this is pretty accurate for the modern usage of the word "computer," Cburnett has correctly pointed out that earlier "computers" fail this test but are still largely considered to be computers.  I would like to add to this discussion that in every dictionary and encyclopedia I've looked up "computer" in, the introductory simple definition '''always''' includes programmability or program execution or instruction sequence execution as part of the fundamental definition for "computer."

Here's a list of points that we should try to reconcile:

* The definition of "computer" has ''largely changed'' since the days of the ABC, ENIAC, etc; I believe we all agree on that.
* In modern usage the term "computer" is never applied to any non-stored-program device.  I can't think of any notable counterexamples, but if you have any they would be crucial to this discussion.
* The former usage of "computer" is more nebulous.  Programmability was not part of the definition and thus we have devices like the ABC that were called computers, but were neither general-purpose nor programmable by simple means (that is, those that don't involve rebuilding/redesigning the computer).
* We ought to have some consensus on how we specifically apply the term "computer" to these early devices to avoid this sort of discussion in the future.

Please respond to these points, add more if you feel the need, and invite other users to discuss who might be interested (and competent).  Please resist the urge to totally dismiss the other person's point of view, because we have (IMO) all made valid points and there is no need to slap labels on people. -- [[User:Uberpenguin|uberpenguin]] 18:32, 11 March 2006 (UTC)

:Hi guys.  Great discussion!  My two-penn'th: 
:*Language is a living thing that changes and evolves totally as societies change and evolve. Therefore we can clearly distinguish the 1940s common usage of a highly technical and socially relevant word like 'computer' from its modern common usage.  Clearly we need to do so in articles about 1940s 'computational' technology (like the [[Atanasoff-Berry Computer|ABC]] one).  We can also be wary of 1973 court judgements[http://www.cs.iastate.edu/jva/] over the meaning and applicability of this word now that another 33 years have passed, during which 'computers' (both the things and the word) have entered many more billion peoples' everyday lives, and evolved so much in themselves too.
:*In modern usage, there is no doubt - as per Uberpenguin's dictionaries and encycloediae, as well as the current article's first sentence - that today, both common and technical usage of the word implies, as he says, '''programmability''' or '''program execution''' or '''instruction sequence execution'''.  Maybe we should warn readers of the [[Computer]] article that this was not always the case - then briefly mention the AB'''C''', human computers etc. But that's as far as that goes - a warning to the reader not to let the evolution of our language confuse him or her.
:Job done? I hope so.  BTW, FWIW, I have quite a lot of academic qualification in this subject area and every day I develop software, self-employed, for a living.  On the other hand, I have no connection with either the Atanasoff or the Berry families, and have never taken anyone to court over anything. --[[User:Nigelj|Nigelj]] 19:34, 11 March 2006 (UTC)

I have never disagreed or countered that the common usage of computer is that-which-sits-on-my-desk-and-runs-solitare.  Not once.  Do I have a problem with [[computer]] talking about the von neumann arch as "a computer"?  Nope.  What I would change is that the word "computer" means the modern computer of a stored-program machine and the whole turing test bit.  I think it would be wholly negligent to ignore the historical defintions and usages of the word.  WP doesn't exist to redefine words and since applying new definitions to words is considered original research: the practice is not allowed on WP.

What I have countered all along is renaming or redefining the AB Computer as something other than a computer.  Every thing I have read that discusses anything about the ABC does not attempt to counter its name: AB machine, AB toaster, etc. etc.  They all refer to it as the AB ''Computer''.  I have not seen one verifiable source that discusses the definition of computer and that discusses why the ABC is not a computer.  A dictionary definition, in this case, does the discussion a disservice by using an extreme few words to define it.  [[Java (programming language)|Java]] defines the language as "an object-oriented high-level programming language" but I would wholly reject such a simplified definition to describe what the [[Java programming language]] really is.  It's too simplified, just like dictionary definitions of "computer".

Whatever this discussion leads to won't change the 1973 court result and it won't change many publications by the IEEE and various professors as calling the ABC a computer.  In the end, [[Wikipedia:No original research]] demands verifiability and I believe I have pulled sufficient number of sources to show that the ABC is a computer.  Only when a verifiable source can be shown (a dictionary is not a [[primary source]]) that the ABC is not a computer and that the turing test is required of a device to be labeled as a computer can WP redefine the definition to such.  So: where are these verifiable sources?  [[User:Cburnett|Cburnett]] 04:30, 12 March 2006 (UTC)

* The problem with Cburnett's position is that the AB'''C''' was called a computer in the 1940's and none of us debate that - but if it were created as a small electronic gizmo with the sole function of solving equations today, it would not under any circumstances be called a computer.  We'd treat it like we treat a fancy calculator.  So in what modern sense can we call it "the first digital/electronic/whatever computer" - when we don't think it's a computer at all ''by the modern meaning of the word computer''?  Does this mean that something that back in the 1940's might reasonably have been called "the worlds first computer" has to lose it's title?  Well, yes - I'm afraid it does.  The meaning of the word has changed.  We can't possibly keep on telling people something that is no longer true without heavy qualification.  I've been wracking my brains for another word that's changed it's meaning - but this is still the clearest I can come up with:  Back in the 1940's the word "Gay" meant "Happy" - but here in Wikipedia, we don't go around calling people of the 1940's who were happy heterosexuals "Gay" - because it would totally give the wrong impression.  No matter that a book written in 1940 can be found as a concrete reference that 'proves' that John Q. Hetero was "Gay" - he's STILL not gay by 2006 standards.  I think that's a reasonable analogy for the problem we have here.  The 1973 court case was NOT about "Is the ABC a computer or not?" - it was about "Did some guy infringe on some other guy's patents?" --and he did - but that proves nothing about what the meaning of the word "computer" is in 2006.  What further confuses matters is Cburnett's insistance that any device that 'computes' is a 'computer'.  Well, OK - but that's just a circular argument - what does the verb "to compute" mean?  Let's put up some new references - things on the web we can all read and compare:
** The 'Definitions' section of "www.computeruser.com" says: "Definition for: computer - An electronic device that has the ability to store, retrieve, and process data, and can be programmed with instructions that it remembers." 
** "searchwinit.techtarget.com" -- "computer - A computer is a device that accepts information (in the form of digitalized data) and manipulates it for some result based on a program or sequence of instructions on how the data is to be processed."
** http://www.webopedia.com/TERM/C/computer.html - "Computer: A programmable machine. The two principal characteristics of a computer are:
**# It responds to a specific set of instructions in a well-defined manner.
**# It can execute a prerecorded list of instructions (a program). 
** http://cancerweb.ncl.ac.uk/cgi-bin/omd?query=computer&action=Search+OMD: "computer - A programmable electronic device that can be used to store and manipulate data in order to carry out designated functions; the two fundamental components are hardware, i.e., the actual electronic device, and software, i.e., the instructions or program used to carry out the function."
** The Free On-line Dictionary of Computing - "<computer> A machine that can be programmed to manipulate symbols."
** The Computer desktop encyclopedia: "computer - A general-purpose machine that processes data according to a set of instructions that are stored internally either temporarily or permanently."
** University of Colombia Press Encyclopedia: "computer, device capable of performing a series of arithmetic or logical operations. A computer is distinguished from a calculating machine, such as an electronic calculator, by being able to store a computer program (so that it can repeat its operations and make logical decisions), by the number and complexity of the operations it can perform, and by its ability to process, store, and retrieve data without human intervention."
* That's just the first half dozen Google hits...there are a bazillion of them and they pretty much all agree that PROGRAMMABILITY is a key thing.
* Is a dictionary a primary source?  Well, I wouldn't quote a dictionary as a source of information about the ABC (for example) - but when I'm asking about the definition of a word (which is truly what this debate is about) then where else but a dictionary?
[[User:SteveBaker|SteveBaker]] 05:48, 12 March 2006 (UTC)

:Just to add my 2 cents, nobody here would disagree that coverage of special-purpose computing devices is an important part of the Wikipedia.  It is, and they are indeed covered in much detail.  It is quite appropriate to refer to these devices as part of the historical context of the history of computing, which the article does.  And the article discusses briefly (the [[history of computing hardware]] discusses in much more detail) about how the key feature - universal programmability - of the modern computer evolved gradually.  However, for modern readers, we should reflect modern usage, and primarily discuss the digital, Turing-complete, programmable computer in this article.  If that happens to mean the ABC falls outside that definition and is intead viewed as a fascinating step along the road to it, well so be it.  --[[User:Robert Merkel|Robert Merkel]] 09:51, 12 March 2006 (UTC)

Here are a few other words that have changed their meaning over time.  The word 'computer' is no different: In 1940, and maybe, in one judge's opinion, still in 1973,
the word could reasonably be used to include the ABC.  But not any more, apart from as a historical footnote to the modern concept, with a link to its own article.
;link : chain link, conceptual link, hypertext link
;server : one who serves (servant), waiter or waitress, centralised computing resource
;nice : you have to look back a few centuries to see this one
;automobile : steam, coal gas or petrol powered, sir? Very few cars from the 1900s and 1910s could legally be registered as such now.
;gay : as above
;net : fishing, internet
;web : spiders, www
;chat : natter to IRC
;text : now a verb and applied to mobile phones
;cool : chilly, good
;mistress : householder's wife, long for Mrs, woman married man is having affair with

I'm not a linguist, and I'm sure there're plenty of better examples.  I've also muddled words with ''changed'' meanings with words that currently have ''multiple'' meanings.  This doesn't harm the argument as some of those multiple meanings will undoubtably fade into disuse within another generation or two.

The word computer began life with its literal meaning, 'one who computes', then began to get applied to various electronic inventions by analogy, but has now settled down to a precise and more restricted meaning - the digital, programmable thing you have on your desk, or in your server room - not your calculator, not your mobile phone (although these may also have tiny computers inside them, running some software to help them work), but the computer - the thing that runs ''your'' software.  This is not pedantry, it's perfectly normal, everyday speech.  To try to deny it or alter it is impossible, or at least is an attempt at social engineering - [[WP:OR]] doesn't even come close to describing such an attempt. --[[User:Nigelj|Nigelj]] 12:18, 12 March 2006 (UTC)

:Tis funny, since my sources (the kind that give more than a definition) still label the ABC a computer.  Here I thought I was the one using resources to keep Steve from redefining computer and doing original research.  I haven't seen him produce anything much beyond a dictionary definition for his position (you know, professional organizations, professionals that deal with this stuff, federal courts, text books on the topic, etc. etc.).  Nothing.  And '''''I'm''''' the one doing social engineering and original reserach???  Holy crap it makes my brain hurt. [[User:Cburnett|Cburnett]] 15:11, 12 March 2006 (UTC)

::How can you say I'm redefining ''computer''?  I'm just giving you it's modern definition.  I didn't redefine anything - it redefined itself.  This should come as no suprise because words do this all the time.  I didn't do original research - I looked up the meaning of ''computer'' in a gazillion places and then I read everything I could find about the ABC.  From those readings, I find that the modern definition that every reasonable resource provides for the word ''computer'' says - typically in the very first sentence - that a computer is a PROGRAMMABLE device.  Now, I look at every reference for the ABC and find that it is most definitely not programmable.  What other conclusion can I possibly come to?   You have taken a different approach to answering the same question.  You find some references that say that the ABC is indeed a computer - but one of those is a court case - and I don't think we regard Judges as experts on modern technology - the other is an IEEE publication - ''Computer Organization & Design'' by Patterson & Hennessy ({{ISBN|1-55860-490-X}}) ...unfortunately, none of the online ISBN lookup services can find this book (check, for example http://isbn.nu/155860490X ) [[User:SteveBaker|SteveBaker]] 17:13, 12 March 2006 (UTC)
::: UPDATE:  You have the wrong ISBN number - it's ISBN: 1558606041  - when I'm next at the library, I'll check it out. [[User:SteveBaker|SteveBaker]] 17:54, 12 March 2006 (UTC)

::We've been talking about this court case in 1973.  1973 was a long time ago in terms of the life of computers - and perhaps some of us here don't have a feel for the environment that Judge was making his decision in.  I was in high school in 1973.  Let me relate how the general public (including Judges) saw computers back then.  We calculated using slide rules and thick books full of log tables. In the early 1970's (I don't know the exact year), my parents spent a LOT of money on buying me a TI pocket calculator - not only was I the first person in my entire school to have one - but nobody else had even seen a pocket calculator up close (I've always been an early adopter).  It was a total marvel - my math teacher practically had tears in her eyes when she found she could show a mathematical series converging by pushing four buttons per step and seeing the numbers get closer and closer to the ideal value.  We learned Fortran in 1973 (we were probably the first school in England to teach computers) - writing our programs out on 'coding forms', mailing them to the regional computer center where they punched them onto cards and slipped our programs in at the end of the payroll calculations on the midnight shift.  We got our printouts back two weeks later in the mail.  That was as close as 99.99% of the population ever got to a computer.  In 1974, the computer we used in college was a Singer mainframe - it had no concept of subroutine calls - there was no stack, it had conditional jump instructions - but they could only jump forwards down the instruction stream, it had a 24 bit word and packed characters into 6 bits (no lowercase!).  We stored programs on papertape and used Teletypes to enter them.  Things were primitive back then - everyday people had NO IDEA what computers were.  Language was different - Judges were old guys who had only gotten where they were by being on the legal circuits for decades.  To regard a judge (ruling in 1973) to be an expert witness about what a computer is considered to be in 2006 is sheer lunacy. This is why that court case is worth '''nothing''' in deciding what is or is not a computer in a modern context. [[User:SteveBaker|SteveBaker]] 17:49, 12 March 2006 (UTC)

:::OK, Steve.  I think we have a clear consensus here, including everyone except Cburnett, and I for one am starting to doubt his good faith and agenda.  Have you seen how long this discussion  has dragged on for?  Over 53 KB in the last two sections here!  I think if Cburnett keeps on, ignoring our points and repeating his same intransigent position, now with mounting aggression ('<nowiki>And '''''I'm''''' the one doing social engineering and original reserach???  Holy crap it makes my brain hurt</nowiki>'), we may begin to regard it as [[troll]]ing ([[WP:DFTT]]).  Notice how he totally ignores my posts, even my existence, and keeps going for you by name?  He must reckon you're more fun than me!  The WP policy is 'Don't feed the trolls', so let's just get the article sorted out, IMHO.    If we have trouble with endless reverts, we can escalate that.  There's more important things to do than endlessly argue this dead point, I think. --[[User:Nigelj|Nigelj]] 20:18, 12 March 2006 (UTC)

::::That's fair comment.  I will cease to feed the troll. [[User:SteveBaker|SteveBaker]] 20:59, 12 March 2006 (UTC)

I think the label "troll" is far to harsh and is unnecessary.  While I disagree with <s>Cbaker's</s> Cburnett's position and his defense of it, he certainly has some valid points.  I do have to agree that the consensus here is against Cburnett, however, and that there is little point in debating this further amongst ourselves.  I think we should move on to proposals for reworking the intro of this article somewhat to explain that the term's meaning has changed since the advent of the stored program computer. -- [[User:Uberpenguin|uberpenguin]] 00:56, 13 March 2006 (UTC)

: (Cbaker??!)  Actually, I read the Wikipedia page about "Don't feed the troll" - and you are quite correct, Cburnett's comments are not trolling according to the Wiki definition.  But still - I think we are better to just fix the problems and end the debate here. [[User:SteveBaker|SteveBaker]] 16:45, 13 March 2006 (UTC)

::Whoops.  My mind must've been somewhere else, I fixed my mistake. -- [[User:Uberpenguin|uberpenguin]] 16:57, 13 March 2006 (UTC)

I agree.  The general field of devices that perform computation was and still is broad.  After decades of use, we now know what "A Computer" is.  The ABC was not a computer, in spite of being named one.  Most people are unwilling to take a stand against the ABC marketing campaign.  I applaud your effort.
--[[User:Zebbie|Zebbie]] 05:43, 14 March 2006 (UTC)

== Featured Article Candidate?!? ==

I see someone has nominated this article for FAC.  It would have been nice to have discussed it here first.  I have added a strong oppose comment in the FAC discussion page - this article is a million miles away from being a good article on the general subject of "Computer".  I can pick almost any sentence out of it and find a reason why it's too specific, inaccurate or just downright wrong.

We have a VERY long way to go to reach the dizzy heights of FA.

Sorry.

[[User:SteveBaker|SteveBaker]] 03:16, 7 April 2006 (UTC)

Example: I'll take one of the smallest sections of the article: "Instructions" - let's go through claim by claim:

* The instructions interpreted by the control unit, and executed by the ALU, are not nearly as rich as a human language.
*:By what measure?  Are you comparing a single machine code instruction with a word? a sentence? a phoneme?  What on earth do you do to measure the "richness" of a language.  Saying "The cow in that field is happy" in x86 machine code is indeed very tricky - but transcribe any C++ function into English - and if it involves abstract base classes, templates, exceptions, etc, you'll end up with a 20 page English description of a one page function.  So which language is "richer"?   This sentence is bullshit.

* A computer responds only to a limited number of instructions, which are precisely defined, simple, and unambiguous.
*:Could the computer respond to an infinite set of instructions?  No - of course not.  Are they "simple" - well, by what standards?  Yeah - they are probably not ambiguous...but the rest of this sentence has zero information content.

* Typical sorts of instructions supported by most computers are "copy the contents of memory cell 5 and place the copy in cell 10", "add the contents of cell 7 to the contents of cell 13 and place the result in cell 20", "if the contents of cell 999 are 0, the next instruction is at cell 30".
*:This gives the strong impression that the instruction contains the addresses of the memory cells...which is true only of a very limited class of computers.  In many simpler computers, you can only do register-to-register addition - in others, you can't have two addresses in one instruction - so copy cell 5 into cell 10 needs two instructions.  Do "most" computers support those instructions...well, maybe.  I could maybe live with this sentence - but I don't like it.

* All computer instructions fall into one of four categories: 1) moving data from one location to another; 2) executing arithmetic and logical processes on data; 3) testing the condition of data; and 4) altering the sequence of operations.
*: Sorry - but what about resetting a watchdog timer or switching an interrupt priority, putting itself into a SLEEP mode to save power...there are LOTS of instructions that fall beyond that set of four things.  There is no clear distinction between doing arithmetic and doing logic - that's an arbitary distinction based on the fact that 'arithmetic' was invented before binary logic operations...and comparisons are just arithmetic by another name...not a separate class of operation.  This sounds like it was written by someone who has a week of machine code classes - not someone who actually knows what makes up a computer.

* Instructions are represented within the computer as binary code — a base two system of counting.
*: Nonsense - many older computers used base 3 or base 10.

* For example, the code for one kind of "copy" operation in the Intel line of microprocessors is 10110000.
*: "The Intel line of microprocessors" - Intel make MANY kinds of microprocessor.  The instruction for COPY on an 8048 is different from a Pentium.

* The particular instruction set that a specific computer supports is known as that computer's machine language.
*:...or it's "Microcode".

* To slightly oversimplify, if two computers have CPUs that respond to the same set of instructions identically, software from one can run on the other without modification.
*: '''slightly oversimplify?!?''' - try running your copy of WORD on your Linux x86 PC - or on an x86 Mac!   More fundamentally, the memory configuration, I/O locations, firmware...lots of other things determine software portability.

* This easy portability of existing software creates a great incentive to stick with existing designs, only switching for the most compelling of reasons, and has gradually narrowed the number of distinct instruction set architectures in the marketplace.
*: I disagree.  Many video games have been ported between Playstation, GameCube and Xbox.  Those three systems have TOTALLY different CPU architectures.  What makes the software portable is that the programs are written in a high level language.  If you write in JAVA (say) your program will run OK on a huge range of CPU's.  Portability isn't much to do with low level machinecode any more.

So - in just one section of this article, I can find very good reasons to disagree with every single sentence.  This is true of nearly all of this article.  That might be good enough for a run-of-the-mill Wiki article - but it's nowhere close to the standard of a Featured Article.

[[User:SteveBaker|SteveBaker]] 03:38, 7 April 2006 (UTC)

:Fft... You didn't even need to justify why this article isn't nearly ready for FA.  I had the exact same reaction as you upon learning this.  I'll add my oppose as well.  This article needs a lot of work before it can be called "good". -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-04-07&nbsp;03:40Z</code>
:This just underlies the bigger problem - not enough references. The section you use as an example above contains no references - if someone were to start adding references to it, they would likely see the problems you mention. '''<font color="8855DD">[[User:Pagrashtak|Pagra]]</font><font color="#6666AA">[[User talk:Pagrashtak|shtak]]</font>''' 03:44, 7 April 2006 (UTC)
::Which in turn underlies a yet bigger problem. This article is a 'big topic' article - like [[Physics]] or [[Mathematics]].  It can't be written by just talking about the PC on your desk at highschool level.  That stuff can go down in subservient articles (such as [[Central processing unit]].  This article needs to make statements that are equally true of ENIAC, the Intel 4004, the Pentium, the Cell processor in the PS3, the pneumatic computer made out of [[Lego]], the experimental DNA-based computers and the Quantum computers.  It has to think about gigantic supercomputers, multiple-CPU machines...all the way down to the lowliest microcontroller.  It can't '''assume''' that all computers are binary - or even that they all do arithmetic.  It has to be very careful when talking about memory - are we talking about registers, cache, RAM, ROM, Flash, Magnetic disks?  It mainly has to speak in extremely broad generalities and point the reader to more detailed articles on history, modern computers, quantum computing.  What we have here needs to be completely thrown away and we need a clean start. [[User:SteveBaker|SteveBaker]] 03:55, 7 April 2006 (UTC)

:Go at it, then.  [[Computer/Rewrite]].  Call us when you've got something better.  You'll find it a lot harder than you presently think.  
:Just to take up one specific point that was raised, you seem to be exhibiting the classic computer scientist's nitpickery.  Whether instructions are register-to-register, register-to-memory, memory-to-memory, stack based, or using some other kind of addressing scheme is really irrelevant to the point that was being made, which was to give an approximate insight into what individual machine language instructions are like for somebody who knows SFA about computers.  Which is, of course, who this article is for, not for demonstrating how much detail the writers happen to know about the inner workings of CPUs.--[[User:Robert Merkel|Robert Merkel]] 04:13, 7 April 2006 (UTC)

* Sorry - but at what point did I say it would be easy?  How can you possibly presume to know how hard ''I'' personally think it would be?  To the contrary - I've been lurking around (mostly fixing the eternal vandalism that plagues this article) trying to figure out what to do about it.  I think it's going to be very hard indeed to rewrite this in the manner it deserves...which is why I havn't done it (yet!).  However, difficulty-of-fixing does not a featured article make.   If it were merely adequate to write a half-assed article because a good one was too hard - then Wikipedia would be virtually useless.  Remember - I didn't start complaining until someone pushed it into the FAC limelight.

* As for nit-picking. There is a really good reason why computer scientists are nit pickers.  We live and die by writing things extremely concisely.  When you are in the middle of a million lines of C++ code (as I am in my daily job), you simply cannot afford vagueness - that one stoopid little 'nit' that didn't get picked is the one that'll crash the computer the very moment a customer steps in front of it!   However, having seen how the English majors here on Wikipedia can pick an article to death - I'd say that we geeks are not the worst culprits!  :-)  Precision is very important in an encyclopedia - and I make no apologies for demanding it. Besides, the point of my previous tirade was not to attempt to find specific things that need fixing.  I was attempting to convey the magnitude of the problem by picking one section more or less at random and examining it critically.  The vast majority of the statements made are false - or at least strongly debatable.  We could argue about that one specific point out of the NINE statements I analysed.  But even if I concede that I was wrong about it - what remains is that 8 out of 9 sentences I examined were faulty - you can probably talk me out of one or two more if you work at it (please don't!) - but still, that's a mighty pile of incorrect statements for an encyclopedia entry.  Heck, if even 10% of the statements were faulty we'd have a major problem!  [[User:SteveBaker|SteveBaker]] 04:55, 7 April 2006 (UTC)

::I agree that a couple of statements there are flat-out wrong; the Intel one is a good example.  Most, however, are hand-waves that could be debated endlessly in isolation but serve to illustrate a broader point in an accessible manner.  And, frankly, I don't believe anyone can write an acceptably short and accessible article about such a broad topic without using such handwaves.  It's exactly the same as an article on, say, "the history of the world" - there are ''inevitably'' going to be one-sentence summaries of things that somebody with expert knowledge of that area will vigorously dispute.  

::If you can come up with an article that uses a better set of handwaves, I will be very impressed; wiki barnstars all round.  But what I will *strongly* object to is what I fear you're likely to come up with; in your efforts to avoid handwaves you end up with an article that doesn't actually convey any useful information to somebody unfamiliar with the topic.  --[[User:Robert Merkel|Robert Merkel]] 05:08, 7 April 2006 (UTC)

:::This dialogue is irrelevant.  The point is that the article is nowhere near FA status.  End of story.  Continue this conversation again when someone gets around to actually rewriting the thing. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-04-07&nbsp;05:12Z</code>

:::Also, regarding the references concept.  Refs aren't quite as necessary when most of the people watching the article have a good idea of what they are talking about.  For the general structure and scope of the article it's fairly impossible to find a few references that magically justify what you are asserting.  References really become useful on contentious points or specific details. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-04-07&nbsp;05:15Z</code>

::::I'm merely trying to point out some considerations if SteveBaker wants to attempt a rewrite.  By the way, looking at that specific section of the present article, it's probably the worst of the lot.  That specific section could do with some attention.  --[[User:Robert Merkel|Robert Merkel]] 08:28, 7 April 2006 (UTC)

:::::The attention it just got is making matters worse.  Whoever wrote the sentences on Neural Networks and Quantum computing clearly doesn't know anything about either of them - and didn't even bother to read the Wikipedia articles before writing about these topics!  I can't imagine why you'd possibly want custom '''hardware''' neural nets for spam filtering - that's the kind of thing that you can do with neural network software (but that's not how it's currently done - Baysian filter is nothing to do with neural nets).  As for [[Quantum computing]] processing information that can only be described in higher [[mathematics]]?!?  Where did that come from?!  One excellent (theoretical) application of Quantum computation is in factoring large numbers efficiently.  The data is simple integers and the process is just factorization - no 'higher mathematics' in sight there! Quantum computers (if they ever become practical) will be mostly distinguished by the fact that they can use superposition and entanglement to perform a vast number of calculations in parallel.  Please - if you don't know a lot about a subject, '''don't write about it''' - at least until you've researched enough to know the basics. [[User:SteveBaker|SteveBaker]] 17:23, 7 April 2006 (UTC)


::::::If a contribution doesn't improve the article, just revert it.  And if you leave this article for a week and it's in a worse state than it was, revert to a version from a week ago if necessary.  Don't tolerate poor-quality contributions.  -[[User:Robert Merkel|Robert Merkel]] 00:42, 9 April 2006 (UTC)

==Removal of "good article"==
seeing as many of us have reached the conclusion this is no longer a "good article", perhaps we should remove the tag? [[User:Vulcanstar6|Vulcanstar6]] 04:09, 7 April 2006 (UTC)

i figure i should ask you all this time, :P 
[[User:Vulcanstar6|Vulcanstar6]] 04:14, 7 April 2006 (UTC)

Well, GA doesn't actually mean a whole lot - all it takes is one person to promote the article or one person to delete it.  The tag says that the article achived GA status - which it did - it's a fact and it bears asserting...even if the presence of the tag merely confirms some people's opinions of the worthlessness GA process.  So - I say leave the tag there as a warning to those who think it's worth something!...but YMMV. [[User:SteveBaker|SteveBaker]] 04:34, 7 April 2006 (UTC)

== ENIAC == decimal ==

I added a (differently worded) comment about ENIAC as decimal.  I am in the "ENIAC as first (insert definition) computer" camp, but I think that since the argument is often over "first computer" vs. "first reprogrammable computer" vs. "first electronic computer" vs. "first elecronic digital computer" vs. "first binary electronic reprogrammable digital computer", a brief mention (not as a limitation, but simply a distinction to the systems that used binary) is in order. -- [[User:Gnetwerker|Gnetwerker]] 17:29, 7 April 2006 (UTC)

* Without trying to stir up the ABC hornet's nest again, the problem here is that any computer you can think of can be awarded the prestigious title "First (insert definition) computer" for some definition of the word "computer".  However, in the context of this article, we have a definition of the word that describes what this article is about - and for that definition of the word (which I believe is the true, modern definition), it doesn't matter whether the computer is binary or decimal.  Given that, I think it's VERY important that we mention that the ENIAC was a decimal machine because it's something that Joe Public may not be expecting - many people (I'm sure) belive that all computers are binary and digital - they are suprised to hear that there were decimal computers - and computers which didn't use two's complement for negative numbers - or which had 6 bit bytes...so pointing out those weirdnesses is a vital thing to do.  I am coming to the view that we should perhaps move all of this "First computer" stuff off into another article.  In the Wikiproject Automobiles (which has a lot of similar "First" problems), we have [[List of automotive superlatives]].  That article has the space and organisation to list all of the things that might make a claim to be "First" or "Best" or "Most" at something - it also has "Honorable mention" catagories for thing like "First ever mass-produced car" which is generally considered to be the Model T Ford - but which honor probably belongs to an Oldsmobile.  If we had that for computers - and referred to it from the main articles whenever a "First" type claim needs to be explained - then we can get rid of all of this clutter and get down to the important issues of describing what a computer actually IS. [[User:SteveBaker|SteveBaker]] 14:45, 9 April 2006 (UTC)

:That's sounding good, Steve.  Now I don't want to stir things up either, but can we just forget analogue computers here? These use(d) op-amps and capacitors directly to model and solve differential equations.  They were very important in helping engineers understand all kinds of damped oscillatory systems like shock-absorbers in car suspension systems and automatic control loops for heavy things like gun turrets and slow things like furnaces.  Maybe, as is currently going on in [[solar panel]], [[solar cell]] etc, this, top level page could become more of a disambiguation page with a very general overview illustrating the range of meanings the word currently has, and has had in the past - then links to all the specific pages, like [[history of computers]], [[computer architectures]], [[personal computer]], [[mainframe computer]], [[supercomputer]], [[analog computer]], [[embedded computer]], [[microcomputer]] etc. (Interestingly, I just typed all those article names off the top of my head, and now I see they're all blue.  I haven't gone to read them all yet!) --[[User:Nigelj|Nigelj]] 15:40, 9 April 2006 (UTC)

** Yes - analog computers don't fit our definition because they aren't programmable in the normal sense of the term - although there were hybrid computers (used extensively in my field of Flight Simulation) that most definitely were programmable.  However, I would be happy to accept an argument that those were basically digital computers with an analog "computer" as a peripheral.  Those were still in service at NASA as recently as 8 years ago. [[User:SteveBaker|SteveBaker]] 15:44, 9 April 2006 (UTC)


==Move page==

Hi all. It seems to me this page is slightly off topic. It describes ''digital computers''. I propose to move this page to [[digital computer]], and to but a disambiguation page called ''computer'' pointing to stuff like:

* [[Analog computer]]
* [[Digital computer]]
* [[DNA computer]]
* [[Turing machine]] (or perhaps a page listing all main mathematicals models of computers)
* [[Molecular computer]]
* [[Quantum computer]]

[[User:Powo|Powo]]

:I concur that the article is too tilted towards the digital computer design, which then means that WP is contributing to the common misconception that digital computers are the only kind of computer.  But I also think a mere disambiguation page would be too intimidating for WP users who do not understand computer science, especially young children and senior citizens.  

:Looking at the article, it seems to me that the first third of it is already written in a hardware-neutral fashion, or could be easily adjusted to meet that standard. It is the second half that probably could be split off into a separate digital computer article.  Then the first one could end in a list of See also links, with digital computer at the top of the list as the most familiar kind of computer.  --[[User:Coolcaesar|Coolcaesar]] 15:35, 23 April 2006 (UTC)

::Not strictly so, I don't think, Coolcaesar.  As far as analog computers go it falls down in the first sentence and continues to do so all the way - there is no 'list of instructions'.  I don't know enough about DNA, molecular and quantum computers to comment, but I think 'stored programs' are generally a bit thin on the ground outide of the digital/Turing model.  On the other hand, I agree that, judging from the vandalism and graffiti, most kids and their grannies think that this article is (or should be) about the thing on their desks and nothing else.  But then isn't that the purpose of an encyclopedia, to help enlighten them?  Maybe the best answer is a new introduction about how complex the concept is, followed by brief but descriptive link-sections to all the options? --[[User:Nigelj|Nigelj]] 16:09, 23 April 2006 (UTC)

::I think that there are two issues here:
::# Any article with the name 'Computer' covers a VAST amount of terratory.  I think we should treat it like the page called '[[Physics]]'  (PLEASE go read that to see what I mean).  It should be a broad-brush introduction to the subject in general and should consist mostly of a description of the topic leading to a well-organised set pointers out into the rest of Wikipedia.  The present article is about a million miles from being that.
::# An article about digital/electronic computers such as are on people's desks is needed - and this one is a good starting point.
::So, yes - I guess I agree with moving this article out of the way and getting started on a proper "Computer" article.  We really need to ask why someone would come to Wikipedia and type 'Computer' into the search box.  Anyone who does this is (by definition) using a computer to do so - and knows (at least) how to use the most basic functions.  So they CERTAINLY aren't coming here to find out what a computer *is*.  In all likelyhood they are after more detailed information about a tiny, specialised area of computing and simply assumed they could find it by going to 'Computer' and following links from there.  That's why we need to do this like the [[Physics]] article does it. [[User:SteveBaker|SteveBaker]] 23:47, 23 April 2006 (UTC)


:::Analog computers are an interesting historical artifact.  Molecular computers (as I understand the concept) are just a reimplementation of Turing machines on a smaller, faster scale, and are research projects.  Quantum computers and DNA computers are moderately interesting research projects.  Why complicate matters unnecessarily?  A paragraph headed "alternative computing models" is all that's required for the last three.  --[[User:Robert Merkel|Robert Merkel]] 11:43, 24 April 2006 (UTC)

:::: '''Minor remark''': I am not a specialist, but molecular computers are not at all a reimplementation of a Turing Machine (are you thinking of nano-computers?). A molecular computer is a massiviely parallel computer taking advantage of the intrinsect computational power of individual molecules. A "computation" of such a molecular computer could, therefore, happen in a test tube! Not much of a Turing machine (although not more powerfull from the computability point of view, of course...) Regards, --[[User:Powo|Powo]] 13:03, 24 April 2006 (UTC)

::::I mostly agree with Steve and Robert.  I think that the label "analog computer" is somewhat odd and subjective in the first place.  One could easily argue that an (analog) oscilloscope is a type of "analog computer" when you consider exactly what devices explicitly called analog computers did.
::::While I certainly understand the difficulty with placing a precise and ''useful'' definition of computer (that is, not fun rhetoric like "a computer is that which computes"), I find myself in the position that the usage of the term computer has come to very nearly exclusively mean "stored program computer" in modern usage.  I think this article should reflect modern usage and primarily focus on this type of computer with just minor mention of historical computers and links to appropriate articles.
::::In all honesty, there is a lot more to be said about digital electronic computers any way.  Since the advent of the stored program computer in the 1940s, there hasn't been much significant effort to design computers outside this paradigm.  As Robert said, quantum, DNA, and molecular computers are all research topics and not yet things that inundate every facet of modern society as stored program (micro)computers are.  -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-04-24&nbsp;15:27Z</code>

::::Analog computers per-se have very little to do with our definition of 'computer' as something programmable (although they are typically reconfigurable...which is not quite the same thing).  What DOES deserve further discussion is the [[hybrid computer]] - which was a mix of digital and analog circuits.  The programmability of these machines is just like a modern digital computer - but the actual calculations would typically be done with analog circuitry.  These are genuinely programmable 'true' computers - but with analog ALU's. [[User:SteveBaker|SteveBaker]] 03:33, 25 April 2006 (UTC)

== Alternative computing models... ==

So does anybody want to write a paragraph or two about the alternative computing models discussed above?  --[[User:Robert Merkel|Robert Merkel]] 00:15, 27 April 2006 (UTC)

== computer ==

i have read this article and found the information very usefull thank you for this has it helped me with understanding the computer industry more

== Jacquard ==

I was surprised to see the Jacquard loom not mentioned in the history section of the computer article. I don't know if it was left out by design or overlooked. I've always had the impression that the Jacquard loom was considered a key development in digital computers. I added a quick mention just before Babbage. [[User:Rsduhamel|Rsduhamel]] 03:28, 7 May 2006 (UTC)

:Good addition; it's a bit of a judgement call on what things to include and what to leave on in that section, but you're right that the Jacquard loom was a pretty important example of precursor technology. --[[User:Robert Merkel|Robert Merkel]] 07:10, 7 May 2006 (UTC)

:The three pieces of the puzzle of modern computers are the arithmetic part, the programmability part and the storage part.  We have lots of stuff about calculators and other arithmetic devices. The Jaquard loom (which is undoubtedly an important step on the programmability side) is a valuable addition.  Thanks!  What do we need to say about storage?  We don't mention mercury-acoustic memory, drum storage, the Hollerith card punch system...there is a lot left to say! [[User:SteveBaker|SteveBaker]] 15:24, 7 May 2006 (UTC)

== Illustration ==

The "illustration of a modern personal computer" looks nice, but does not add any information to the article.  Either an actual picture or a diagram with more information (such as text labels that identify the different parts of the machine) would be more useful. -- [[User:Beland|Beland]] 04:59, 7 May 2006 (UTC)

:It's an introductory image.  It really doesn't have to do much other than provide a common example of the subject.  Besides, I much prefer a nice diagram to one of the umpteen pictures of peoples' PC desks that creep into this article from time to time. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-05-07&nbsp;21:27Z</code>

: I don't really like it either - it's a zero-information-content image.  We should find a photo of a more unusual computer - or perhaps a diagram with the parts labelled or something.  But I really don't see the point of showing a picture of a PC because it's almost 100% certain that the person reading the article is sitting right in front of a PC at the time they are reading it!  We need a photo that screams "Computer" without being a PC. [[User:SteveBaker|SteveBaker]] 01:12, 8 May 2006 (UTC)

::I'm definitely not liking having (copyrighted) images of two PCs in the intro.  If we MUST include a PC, at least show some other form of a computer as well.  How about a mainframe like a [[System/360|S/360]]? A midrange like a [[Programmed Data Processor|PDP]] or [[AS/400]]?  An embedded computer?  Perhaps something older like the [[Harvard Mark I]]?  There are far more interesting and unusual (and free) pictures of all of those systems available on Wikipedia.  I hate the idea of both intro images being PCs, partially because they are so ubiquitous and I think we could do better on the 'interesting' front than a picture of a box+monitor+keyboard. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-05-25&nbsp;17:20Z</code>

:::I agree.  It's certainly ridiculous to use a copyrighted image (and especially one that has Dell promotional stuff on the monitor screen) - when it's so easy to take a nice photo of a PC that doesn't carry any copyright or advertising.  But still (as I've said before) BY DEFINITION: EVERYONE WHO CAN READ WIKIPEDIA KNOWS WHAT A COMPUTER LOOKS LIKE BECAUSE THEY ARE USING ONE RIGHT NOW!!!  So this is a pointless choice of image.  It's not informative (to our readership) and it's not particularly interesting or beautiful - so we might as well pick something else.  I think we need to carefully explain that not all computers look like PC's.  We should pick the computer from a washing machine or a microwave oven or the one in your car or an old Sinclair Spectrum or a PDP-11 or a Lego RCX computer...anything other than a modern laptop or deskside computer because that is totally uniformative. [[User:SteveBaker|SteveBaker]] 18:55, 25 May 2006 (UTC)

::::There's always [[:Image:PDP-8i cpu.jpg|this interesting image]] of a PDP-8/I's guts.  I doubt that one will go over well, though, since I imagine we want the intro photo to be of the "outside" of a computer (course, using a picture of an embedded computer trumps that...).  I'm open to suggestions, but please, let's not keep the PC double-whammy in the intro. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-05-25&nbsp;19:35Z</code>

:::::I'd suggest a picture of a [[microcontroller]] on the basis that they are a complete computer, but not as the reader may know it. --[[User:Robert Merkel|Robert Merkel]] 07:21, 27 May 2006 (UTC)

::A microcontroller or SoC would be fine by me, but can we do better than just showing an IC's packaging?  I think that might be a tad bland for an intro picture (though not as bland as a desktop PC). -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-05-27&nbsp;07:38Z</code>

:::How about a [[Lego Mindstorms]] controller? --[[User:Robert Merkel|Robert Merkel]] 07:42, 27 May 2006 (UTC)

::::I'm for it... It's a fairly identifiable object that may not normally be thought of as a computer.  Seems appropriate.  Do you have a particular picture in mind?  I think it would be best to have a disassembled view of it so it becomes apparent on first glance that there are some microelectronic guts in the thing. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-06-04&nbsp;23:30Z</code>

:::::Your wish is my command!  I took a photo of my RCX 1.0 computer - then dismantled it and took another - mushed the two photo's together so you can see the insides and the outside.  Took me 10 times longer to get the darned thing back together again - but after three tries, it works again!  Hope this one is to everyone's final satisfaction. [[User:SteveBaker|SteveBaker]] 01:21, 5 June 2006 (UTC)

:Lovely.  I think that's a great picture for this article's intro. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-06-08&nbsp;05:23Z</code>

::I disagree. This article is about the computer: arguably the most important technological achievement of the 20th century. Is a Lego toy really an appropriate ambassador of this? (It's better than the Furby, granted). There are computers out there doing all sorts of amazing things, surely we can come up with something photogenic that does them justice? [[User:213.38.7.224|213.38.7.224]] 13:38, 3 July 2006 (UTC)

:::''"Is a Lego toy really an appropriate ambassador of this?"'' -- Yes, totally appropriate.  What better way to illustrate how computers have infiltrated almost every facet of our lives?  I think having a full microcomputer in a toy brick is every bit as impressive as any picture of a SoC die we could slap up there. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-07-03&nbsp;14:07Z</code>

::::The Lego computer is a complete computer - an SoC die really isn't so it should be excluded on that basis (also, the current photo has an SoC die slap in the middle of the circuit board).  If someone has a better photo of a complete computer ('''''NOT A PC OR A MAC!''''') that has as more to tell the reader than the Lego RCX photo - then I'm happy to consider it.  But three out of the four people who contributed to this discussion so far liked the idea of the RCX photo (one person proposed the idea, a second person agreed and took the photo and a third person said it was a great choice, the fourth person didn't say they don't like it) - so far, we seem to be keeping everyone but [[User:213.38.7.224|213.38.7.224]] happy. I believe it is a mark of just how ubiquitous computers have become that they show up in places like this.  We need a picture that informs people.  It has to say "Computers are everywhere - Computers are NOT just PC's - This isn't an article about PC's - Computers are in every corner of our lives - Things that you might not think of a computers are in fact computers - Computers look like this inside - Computers control things".  I think the Lego picture along with it's caption does exactly that.  A picture of a PC ''sucks'' in every way imaginable - it says nothing to the reader (who is almost certainly sitting at a PC as he/she reads the article!) - it promotes commercial rivalry as every PC maker seeks to have THEIR PC be the one in the photo - it's also '''obvious''' - and making a non-obvious choice here is what'll keep people interested in reading the article.  I'm happy to entertain alternatives  but I'm not seeing any offered.  If you have a better photo - bring it here and let's discuss it. [[User:SteveBaker|SteveBaker]] 14:54, 3 July 2006 (UTC)

:::::Agreed.  (as an aside, some SoCs are pretty much complete computers with CPU, memory, I/O and peripheral structures; they just require power supplies and a clock signal to work, and little more to be useful.  Still, the Lego brick is a solid choice for the intro photograph). -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-07-03&nbsp;16:21Z</code>

::::::Well, I guess you could count an SoC chip by itself - but without a power supply + clock...hmmm - dunno - is a car still a car if you take the wheels off?  I guess so.  But anyway, the H8 chip on the bottom-right corner of RCX board is an SoC (System-on-a-Chip) and showing it in an appropriate context (ie with clock, etc) seems worthwhile. [[User:SteveBaker|SteveBaker]] 18:50, 3 July 2006 (UTC)

:::::The Lego RCX illustrates the ubiquity of computers today, but I think it makes their use seem rather limited, failing to show the impact of computers on communication, science, manufacturing, culture. But I suppose what wouldn't? A cellphone, a supercomputer, a computerized production line, or a PC would be misleading too. So I don't really have any better suggestions! [[User:213.38.7.224|213.38.7.224]] 16:38, 4 July 2006 (UTC)

::::::We can always have more photos in order to cover all of those other things - the question here (I presume) is which of those things should be at the top of the article.  There is no doubt in my mind that pictures like the Furby and the RCX are important to telling the story - the issue for some people isn't whether these pictures belong in the article - it's a question of what should be at the top. [[User:SteveBaker|SteveBaker]] 17:04, 4 July 2006 (UTC)

== Dispute over example in "Programs" section ==

I've reverted back and forth now with [[User:81.179.195.75]] [http://en.wikipedia.org/w/index.php?title=Computer&diff=54049486&oldid=54041678 here], [http://en.wikipedia.org/w/index.php?title=Computer&diff=54050287&oldid=54049486 here], [http://en.wikipedia.org/w/index.php?title=Computer&diff=54051143&oldid=54050287 here], [http://en.wikipedia.org/w/index.php?title=Computer&diff=54051816&oldid=54051143 here] and [http://en.wikipedia.org/w/index.php?title=Computer&diff=54052420&oldid=54051816 here].  I can't figure out where the "40 million lines of code" figure is coming from, but the 2 million lines of code figure is coming from the Robert Lemos article.  What's going on, [[User:81.179.195.75]] ? -[[User:GTBacchus|GTBacchus]]<sup>([[User talk:GTBacchus|talk]])</sup> 16:53, 19 May 2006 (UTC)

:I support your reverting him for your given reasons.  If the anon user doesn't provide any evidence and reason to use his text but continues to revert, we ought to report him on [[WP:VIP]]. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-05-19&nbsp;17:54Z</code>

::It makes more sense now.  The 40M figure is coming from [[Source lines of code]], where it's quoted as coming from a book, referenced by ISBN.  I think it would actually be interesting to use both figures, to see the difference in scale between a web browser and a behemoth OS. -[[User:GTBacchus|GTBacchus]]<sup>([[User talk:GTBacchus|talk]])</sup> 18:40, 19 May 2006 (UTC)

:::Allright, though if you want striking code base comparisons, try an office suite or a RDBMS like DB2 or Oracle...  40 million lines pales in comparison.  -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-05-19&nbsp;20:04Z</code>

== Edit to Program section requested ==

"A typical example is the Firefox web browser, created from roughly 2 million lines of computer code in the C++ programming language;[7]"

Firefox is not a typical example, typical would infer common or usual Firefox is not yet either of these.  2 million lines of code appears invalid, the foot note link forwards to an external site that does not mention that Firefox is written in C++.  The external link does mention 2 million lines of code however the roughly statement makes it appear to be that authors best guess.  

There is no proof that 2 million lines of code was actually counted using SLOC (source lines of code) method like the lines of computer code link is alluring to.  The SLOC article does contain several examples all validated in that article, Windows XP would provide a better typical example because of the readers familiarity with Windows XP (perhaps unfortunately everyone knows XP).  Placing a Windows XP example in the programs section is perhaps out of place, placing it within Operating systems section would be more appropriate.

:Equally, we could point to the SLOCs in the Linux kernel, which is trivially verifiable because it's open source; or, as an example that people might not be familiar with, the fact that the [[F-35 Lightning II|Joint Strike Fighter]] will apparently have roughly [http://www.mathworks.com/industries/aerospace/aerodef_conf06/abstracts.html 11 million lines of source code] in its associated software.  But we're kind of missing the point here, which is to demonstrate that software applications represent a huge intellectual effort. --[[User:Robert Merkel|Robert Merkel]] 14:46, 20 May 2006 (UTC)

::Linux and to some extent Firefox have, by being open source have a potentially accurate but also more variable (depending on distrib, optional extras etc) SLOC figure.  Windows being closed source ought to have a less variable but, more difficult to prove figure.  The size of the intellectual effort is important, a contrast between an applications lines of code and a Operating Systems lines of code could be informative to the readership.  Part of the point is also that edits are occuring because the facts are disputed by both parties, the SLOC article contains verifiable data provided by published authors about the subject matter.  The “roughly 2 million” in the cnet article to me at any rate carries less gravitas.  Windows XP cited as an example is one that the majority of readers could relate to.

:::The trouble with picking Windows XP is in knowing what exactly is being counted.  Windows is not just one program - it's DOZENS of programs kinda mushed in together.  Firefox is a good example because it's a single, standalone chunk of software and you can verify by counting the lines yourself. [[User:SteveBaker|SteveBaker]] 18:59, 25 May 2006 (UTC)

::::I think "dozens" is a severe understatement.  I agree, however, that the Windows XP source code reference is very very weak.  I'd suggest following Robert's suggestion by counting the lines of code in some readily recognizable piece of open source software. Perhaps a Linux or BSD kernel or even OpenOffice.org (if someone wants to have a script churning away at their hard disk for an hour :).  I'd be glad to do the count myself if someone would indicate which piece of software might be suitable to count... -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-05-27&nbsp;03:45Z</code>

:::::OOO might be a good illustration, as the average non-technical reader might have a good idea what functionality is provided through that 20 bajillion lines of code...---[[User:Robert Merkel|Robert Merkel]] 07:20, 27 May 2006 (UTC)

:::::Technically, we can't just count the lines in some software package and report the result since that would be a violation of [[WP:NOR]].  (I personally find WP:NOR to be a real pain to adhere to - but it's a rule) [[User:SteveBaker|SteveBaker]] 12:09, 27 May 2006 (UTC)

::::::Personally I don't consider that to be original research; it's just a quick calculation to find an undeniable, easily verifiable fact.  Is it original research, for example, to perform calculations for the maximum addressing space under LBA in the [[Advanced Technology Attachment|ATA]] article?  I'm sure with enough searching you could find a citation for the given calculations, but why bother if the majority of the editors of the article know the numbers to be correct and can verify them with a calculator?  The NOR rule was designed to prevent inclusion of information that is questionable (which of course is ridiculous since plenty of published papers contain questionable to downright false assertation).  If you were to apply NOR unilaterally, most good articles would be downright impossible to keep around since it's impossible to cite every single factual statement made in a text.  One's own wording of historical events or the usefulness of a certain device could very well be interpreted as 'original research' if you take things far enough.  Anyway, I wouldn't worry about NOR in this case unless someone here actually has an objection to the proposed citation and method.  All that I see as necessary is a footnote to the effect of exactly how the lines of code were counted. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-05-27&nbsp;16:44Z</code>

== Russian computers section ==

I've removed this section because it contained numerous overtly POV statements and several rather wild claims that were unsourced.  What's more, the information probably doesn't require an entirely separate section within this article when any significant developments in Russian computer technology should be seamlessly integrated with the rest of the article.  I ask the author of that paragraph to reference some of his claims and give more detail about which developments he feels are most important to mention in this article. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-05-27&nbsp;03:40Z</code>

:This has nothing to do with POV statements or wild claims. I was simply quoting an article from a program known for its journatlistic integrity. The reason why so little is known is because of the cold war. Back then everything that was going on in the Sovjet Union was a secret for the west, and when the war was over the whole thing started to fade into oblivion when it was shut down. That's why there is so little information about it. If you can find a way to integrate it with the rest og the article, that's fine. If you don't read Norwegian, maybe someone can translate it for you; http://www.nrk.no/kanal/nrk_p2/verdt_a_vite/1956658.html And a short interview partly in English; http://www.nrk.no/dynasx?p_lenke_id=244350&p_artikkel_id=1956658&mswmext=.asx


::Your contribution was also far too detailed for an overview article; it probably belongs more to  [[history of computing hardware]].   Frankly, I'm not sure it's even significant enough to mention here.  None of the really big developments in computing happened first in the Warsaw Pact; they managed to reverse engineer or independently invent most things the West came up with, but they didn't beat the West to the punch on anything major.  In fact, as our article on the [[ES EVM]] suggests, it's even possible that the political leadership's emphasis on cloning Western designs rather than developing their own technology screwed up their own computer industry.  --[[User:Robert Merkel|Robert Merkel]] 05:16, 29 May 2006 (UTC)

:::It's good stuff - it just doesn't belong in this article.  I suggest you create a new article and put it in there.  An article on Computing in Russia could certainly be linked to from here. [[User:SteveBaker|SteveBaker]] 18:08, 30 May 2006 (UTC)

::::Thanks. In my opinion, it is irrelevant if it had any impact in the present technology or where the original inspiration came from. It is still a part of computer history. And yes, it was a bad idea to include mix western and russian computer technology. Maybe someone could write an article about, but that would have to be someone who knows how to write articles about computers.

For those who are interested in what was deleted:

Russian computers

"Parallell with what was going on in the west, USSR had their own computer evolution and technology. The first was made in Akademgorodok at the end of the 50's and named M20, and the last one at the end of the 60's which was called Big Electronic Computational Machine, or BESM 6. Measured in qaulity, these were just as good or even better than those in the west. Later generations were also made, but these were influenced by western technology, or by IBM 360 to be more precise. USSR even created their own personal computer in 1983, called Chronos. Its operative system got the name Exelcior. This program was later improved and ended up reminding about Windows in some areas, which was an example of parallel development. This personal computers were more modern and had the same speed as the PCs that existed in the west at the same time. The russian scientists were working on and making prototypes of advanced integrated circuits for their Chronos, such as special circuits for signal processing which had the same function as present graphics cards and sound cards, but the impact of perestroika would put a permanent end of this alterantive direction in computer science and technology. If the evolution had continuted, it would probably have ended up as a good alterantive to the present and dominant standards."  [[User:Rhynchosaur|Rhynchosaur]] 16:39, 27 August 2006 (UTC)

==Oldest computer in the world==
I leave it up to far more knowledgable editors than myself to judge whether this item [http://www.physorg.com/news68796309.html] is appropriate for inclusion in the history section, or with some other related article. [[User:Politis|Politis]] 18:11, 7 June 2006 (UTC)

: Thanks for the link.  That article refers to the Antikythera Mechanism - which is mentioned in the second sentence of the history section - so we've got it covered.  The difficulty in assigning the term "Oldest Computer" relates to the very definition of ''computer''.  We have chosen to apply a modern definition which distinguishes between 'calculators' and 'computers' by requiring that a computer be '''programmable'''.  By this definition, the Antikythera Mechanism is not a computer - it's a calculator.  You could use it to calculate the position of stars, sun, moon and planets - but that's all it could do.  It could not be reprogrammed to balance your checkbook or to do any operation other than that one specific thing.  If we '''were''' to consider primitive calculating engines as 'computers' (which we don't) then the first device like that is probably the [[South Pointing Chariot]] which effectively performed the calculation of subtracting one number from another and multiplying by a third number using gear wheels.  It's not impressive - but it's just as much a ''calculator'' as Antikythera,  Truly, the concept of using artificial aids to doing arithmetic is as old as the tally stick or counting on ones fingers...I doubt you could definitively come up with a "first ever single-purpose arithmetic calculator". Antikythera is an important milestone along the way - but I still think ENIAC wins the title "first computer". [[User:SteveBaker|SteveBaker]] 19:47, 7 June 2006 (UTC)

::I concur with SteveBaker.  ENIAC was obviously the first programmable electronic computer and programmability (as well as the concept of the state of a variable) are what make computers different from calculators.  --[[User:Coolcaesar|Coolcaesar]] 20:13, 7 June 2006 (UTC)

:::I don't know that it's so obvious.  If it were, then we probably wouldn't have to give due respect to [[Konrad Zuse]].  Anyway, I'm fully supportive of our continuing to define a computer by the trait of programmability.  It makes defining the scope of these articles much much easier. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-06-07&nbsp;22:25Z</code>

::::Not wanting to stir up that particular hornet's nest again - I think we give due credit to people who made things that helped to make computers possibe.  We wouldn't have computers without all the work on what we are currently calling ''calculators'' - and all of the things like the [[Jacquard loom]] that contributed so much to programmability - and the guys who designed stuff like punched cards, random access memory, the transistor, the Teletype, the cathode ray tube, boolean logic, binary numbers...the people who worked on those things (including Konrad Zuse) deserve mention for the work they did - even if the things they built don't fit our definition of ''computer''.  [[User:SteveBaker|SteveBaker]] 18:28, 8 June 2006 (UTC)



== Computer != PC (again!) ==

<rant>
Far too much of the content of this article makes the horrific mistake of equating the term "Computer" with "Personal Computer".  We really, really need to stop doing that.  From the guy who repeatedly insists on putting a photo of a PC at the top of the page because ''that's what computers look like'' to people who say stuff like the arrival of the mouse and the GUI was what made computers what they are today...this is WRONG, WRONG, WRONG.

There are something like 100,000,000 PC's in the USA today.  How many computers are there?  Well, in my house there are 5 PC's - and my son did a school project which entailed counting how many computers there are.   You wanna guess?  The answer was 87 - and I think that was an underestimate because he decided that things like the chip that plays "Happy Birthday" inside one of those horrible birthday cards was probably a custom music chip and not some kind of tiny computer...I'm not so sure.  However, I think it's pretty safe to say that there are at LEAST a dozen computers for every PC (heck - there's likely to be a dozen computers inside a PC).   So PC's are a relatively small fraction of the computers out there - almost too insignificant to mention - let alone dominate the article.

Let's talk about washing machine controllers, telephones, the computer inside your TV remote, the ones inside toys, the industrial controllers, the car engine management computers, your wristwatch...the PC comes *WAY* down the list.  It's worthy of a mention - but it shouldn't be the majority of the article.

<end of rant>
[[User:SteveBaker|SteveBaker]] 21:16, 8 June 2006 (UTC)

:Calm down.  It's okay.  The most verbal editors to this article all agree with you and recognize the problem.  Just remove incorrect text; rants aren't needed. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-06-08&nbsp;21:53Z</code>

== Comparable dates ==

The article includes the following:

"Notable achievements include the Atanasoff-Berry Computer (1937)... the secret British Colossus computer (1944)... the Harvard Mark I (1944)... the decimal-based American ENIAC (1946) and Konrad Zuse's electromechanical Z3 (1941)..."

To the new reader the dates are presented as though they are directly comparable. I don't have enough information to be sure but I have the impression that this is not so.

These are dates I've found in various articles. (Please correct where they're wrong.)

{| class=wikitable
!Machine!!Concept!!Demonstrated working!!Operational use
|-
|ABC||'''1937'''-38||1941?||N/A
|-
|Colossus||1943?||1943||'''1944'''
|-
|Harvard Mark I||1939||1944?||'''1944'''
|-
|ENIAC||1943?||'''1944'''||1946
|-
|Z3||1939||'''1941'''||?
|-
|}

It looks from this table as if the ABC's concept date is being presented as comparable to ENIAC's "shown working" date or Harvard Mark I's operational date.

It would be more useful for the reader to be given dates which show the same stage of development for each machine. The concept date doesn't look very useful for this purpose. Equally, the operational date would be unfair to the ABC and possibly the Z3 if they were never in operational use. The date when the machine was first shown working looks like the one that is best for this purpose.

Can anyone contribute further dates or any other thoughts? [[User:Adrian Robson|Adrian Robson]] 19:19, 22 June 2006 (UTC)

:I am a bit rusty on the history of the early computers but I think you are right that the dates need to be fixed for consistency.  We have a big problem on Wikipedia with Iowa State University graduates trying to elevate the prestige of their lousy fourth-tier university (and awful fifth-tier state) by pushing a POV favoring Atanasoff's work over all others in many computer-related articles.  What does everyone else think? --[[User:Coolcaesar|Coolcaesar]] 20:42, 22 June 2006 (UTC)

::I think the attack on IASU folks is unnecessary.  The persons responsible numbered in the two or three range.  I also don't think that all those dates are even necessarily needed in this article; wikilinks to the respective computers' pages should be adequate. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-06-22&nbsp;21:26Z</code>

:::Let sleeping trolls lie. [[User:SteveBaker|SteveBaker]] 23:53, 22 June 2006 (UTC)

::::Many thanks for the comments. As there don't seem to be any improvements on the dates at the moment, I'll update the article with the dates in the Demonstrated Working column, unless someone has any better ideas in the next few days. [[User:Adrian Robson|Adrian Robson]] 17:11, 4 July 2006 (UTC)

:::::Depending on which side of [[Honeywell v. Sperry Rand]] you're on, you might argue the dates for ENIAC were 1943-1944-1945.  (The middle date was especially important for determining the critical date of its patentability; the judge in the case said that the two-accumulator version of the ENIAC, ready in 1944, was as good as the whole thing, citing this as a reason for invalidating the patent.)  ENIAC was certainly in "operational use" throughout 1946 prior to its move to the BRL. [[User:Robert K S|Robert K S]] 14:36, 31 August 2006 (UTC)

::::OK, I've amended the table above. I'll amend the text of this article and I would think the template table of dates should be amended in line with this, too. [[User:Adrian Robson|Adrian Robson]] 08:40, 13 October 2006 (UTC)

== Featured article ==

It's my desire to get this article to featured status, but that's not something I feel I can accomplish on my own.  My expertise lies in digital computers and their architecture, so while I can address the bulk of the article myself, I'll fall short on some of the other critical areas (such as history, computational theory, and alternative computing models).  Here are my notes upon surveying the article regarding some of the things it needs before it can attain featured status:

*'''Intro''' - The intro is pretty solid.  It could probably use a little refinement once the bulk of the article is brought up to a high standard, but in general I'm pretty happy with how it reads.
*'''History''' - Also a good general overview.  It would probably be good to carefully check all the dates and facts just to make sure it's accurate.  I think the paragraph on stored program machines could stand for a little bit of improvement.  There certainly should be a little more text on modern computer developments.  The history section should at least briefly mention the advent of the PC and how it has helped computers become ubiquitous in industrialized society.  I think we should also include a few pivotal architectural improvements that were made feasible by transistors.  Increasing levels of parallelism and complex superscalar CPUs are worth brief note, I think.
*'''Stored program architecture''' - Needs improvement.  I think in the first few sentences it is guilty of missing the point of what is significant about stored programs in the first place.  Furthermore, the logical separation of control units, execution units, memory, and I/O isn't mandated by the stored program design, nor is it always the case (I'm thinking of some DEC machines in which the line between memory and I/O was very thin because I/O was abstracted AS addressable memory).  This section rambles a bit, goes into some unnecessary diversions, and probably needs to just be rewritten.  It's extremely important to talk about the stored program architecture in clear simple terms without getting distracted by implementation details or an explanation of the mechanics of memory storage systems, etc.  I think this is also the place to talk briefly about usage of the term "computer" and how stored program architecture is an important, if not the important, defining point of what a modern "computer" is.
*'''Digital circuits''' - Okay, but could be trimmed down.  I think there's too many specifics in there for a general article on computers.
*'''I/O devices''' - Hmm... Well something should be said about I/O in this article, but where this section is situated seems very out of place.  I'm open to suggestions here.
*'''Programs''' - Crucial section to have.  Probably should be renamed to "Software".  Needs to have a short paragraph about ISAs since they are the interface point between software and hardware.
*'''Libraries and operating systems''' - Probably should stay, but needs a little reorganization and flow improvement.
*'''Computer applications''' - Okay, maybe reorganize a bit and come up some major lines across which to divide various computer uses into classes.
*'''Alternative computing models''' - Needs to be expanded a little.  For some reason I feel that it isn't representing various theoretical and experimental computing models well enough.  However, my knowledge on the subject is limited, so perhaps someone else could take an interest here?
*'''Computing professions and disciplines''' - Do we really need this?  It's definitely related, but I don't find it of significant direct importance to this article.  Maybe it can be moved to another article or removed altogether and an appropriate article linked instead.
*'''See also''' - Trim this way down.  It's overwhelming as is.

What's missing:
*'''Computers in society/popular culture''' - Someone (preferably a sociologist :) really ought to write something about how computers have gradually become an integral part of society and the way people live and communicate.  Smatterings of this topic are dispersed through the article, but I think it's important enough to merit its own section.

Please discuss what you think I have right/wrong about the direction this article needs to go in.  I'll start improving along these lines where I can.  If you'd like to work on some of the above topics, please let me know here.  Recruiting other knowledgable editors would also be a big plus since this is a broad topic that requires the viewpoints of multiple people. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-07-18&nbsp;22:52Z</code>

: I still believe that the entire article is The Wrong Thing - not that it's inaccurate or badly written or anything - it's just not what should be here.  'Computer' should be a very abstract top level article like [[Physics]] that surveys the field and points you off to other articles for almost all of the content.  Beyond the introduction, [[Physics]] has almost one word in four linked off to other articles and tables to help organise the material..that's what we need here. 
: [[Computer]] should organise the field and impart structure onto the articles we already have.  We should do a comprehensive sweep of all of the Wikipedia for articles on stuff like computer games, logic design, programming libraries - and organise it into tables like the ones in [[Physics]].  The history stuff should be in a history of computing article - the alternative computing models belongs in an article of its own...a "How Computers Work" article would be worthwhile.  Trying to wedge too many fragments of information on this wide field into a single article just isn't working...IMHO of course.  [[User:SteveBaker|SteveBaker]] 23:15, 18 July 2006 (UTC)

::To an extent I agree, though I don't believe Computers to be nearly as broad a topic as Physics.  Even if we do turn this into a hub article (which I'm not at all opposed to), it still should (and can) have a very solid intro and probably a brief history section.  So do you want to propose a structure for this article?  It's been nagging me for awhile that the entry article for everything computer related on Wikipedia is in a relatively poor state, and I'd like to get something done about it that everyone will be happy with. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-07-18&nbsp;23:39Z</code>

:::Well, I really do like the way [[Physics]] does this - so I think we should organise like this:
:::*Intro paragraph - what we have now is OK.
:::*Introduction section - explain that we have hardware and we have software - we have operating systems and applications - we have the basic hardware CPU/RAM/ALU/whatever and we have peripherals.
:::*Tables - structure the subject - list the Wikipedia articles that fall into each catagory.  Do this heirarchically - hardware breaks down into architecture, electronics, other technologies (quantum, bio, nanotech, etc).  Software breaks down into OS's, applications, libraries, languages.  Then break those down - languages breaks into C, C++, Fortran, Pascal...etc.   OS's breaks into DOS, Windows, Linux, UNIX, MacOS.   Libraries breaks into DirectX, OpenGL, OpenAL, etc.  Applications breaks into video games, office applications, etc. 
:::*Summarize the key articles - history, computer architecture, software.  Have those be rich with links.
:::*A really comprehensive 'Further Reading' section.
::: [[User:SteveBaker|SteveBaker]] 03:04, 19 July 2006 (UTC)

::::I like it.  Do you want to work on any particular part of that?  I'll take whatever you don't want.  I think the tables section will take a lot of thought since beyond the major classes of OS (DOS, Win32/NT, Unix/BSD-derivative), there are a plethora of other miscellaneous designs.  Same goes for programming languages.  Anyway, let me know what section(s) you want to take and we can start working on this on [[Computer/Temp]] or the like. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-07-19&nbsp;03:18Z</code>

:::::I guess I'll take the tables part if you like.  But before we dive in and start work, let's at least get some kind of consensus from the community here.   I'd hate to spend a month honing and polishing a replacement article only to end up with open warfare about whether it's the kind of thing we should have here.  Let's leave it a few days to see what comments we get from other contributors. [[User:SteveBaker|SteveBaker]] 04:04, 19 July 2006 (UTC)

::::::You could spend a month waiting for anyone else to comment...  However, a few days is reasonable to wait, I suppose. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-07-19&nbsp;13:43Z</code>

:::::::I think this is a potentially reasonable plan. My only concern with this plan is that the subsidiary articles that it would link to sometimes offer relatively few concessions to readability for the non-expert reader, and do little to place these topics in context, which was my goal with the present version of the article.  So this means that it will be important to place the links in context within this article, but also to improve the readability of some of the other computing related articles.  In fact, surprisingly enough, the computing-related articles now generally lag in quality behind some other aspects of the Wikipedia.   

:::::::Secondly, we should really think about how such a top-end article belongs in the context of other such articles, such as "computing", and [[computer science]].

:::::::I would further request that any reorganization take place on a temporary page until it is to a satisfactory standard.  

:::::::I'll try my best to help out when I have time, but I'm travelling at the moment so I don't have a lot of it.  --[[User:Robert Merkel|Robert Merkel]] 13:58, 19 July 2006 (UTC)

::::::::I agree that a few days waiting for comments is reasonable - we don't need to wait a month.  I was merely commenting that if we worked on it for a month and then dropped the present article in favor of the new one - then the howls of complaint might be terrifying!  If we can at least get a reasonable amount of consensus before we start then hopefully we can say "You had the chance to discuss this and you didn't" and have people such as yourself on record as agreeing that this is a good idea so it's not just a minority going off at a tangent.  I recommend we wait until Monday in order that 'weekend Wikipedians' get a chance to comment - but if we don't see too many howls of derision we could at least get started sooner than that.
::::::::I also agree that some of the subsidiary articles aren't up to scratch yet - but the solution is to fix them and '''not''' to try to cover up their deficiencies with an uber-article that trys to cover everything (after all - not everyone will come here first).  I agree that a significant part of what's proposed here is to move sections of the current text out into the subsidiary articles and to improve them as needed.
::::::::We certainly need to coordinate what we propose with [[computing]] and [[computer science]] (and perhaps also [[electronics]] and [[software]]).  There is a strong case for merging [[computing]] with this article (or vice-versa).
::::::::We ''obviously'' need to do this work off in a subsidiary page. [[User:Uberpenguin|uberpenguin]] already put a copy of this article into [[Computer/Temp]] - and we should certainly work there until we can say that the new page is better than this one and then throw the big red switch. [[User:SteveBaker|SteveBaker]] 17:12, 19 July 2006 (UTC)

:You're preaching to the choir here, Robert.  Seems like nearly every time I look up a digital electronics or computer architecture related article here, it's either a mess, too convoluted to understand, or downright wrong.  Your concern is valid, but the more I think about it, the more I agree with Steve that this article should be a hub like Physics.  Once this article is up to snuff perhaps we can slowly focus our attention on other major articles that need some love.  Regarding merging this article with [[Computing]], I'm not sure how well that will go over since a convincing argument can be made (even based upon what we've discussed in the past) that a computer is a specific kind of computing machine, and "Computing" can include all kinds of devices that we would no longer term "computers".  Whether that alone is meritous of separate articles I don't know, but I'm inclined to keep them separate just for simplicity's sake.  The scope of one hub article about computing and computers would be fairly enormous. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-07-19&nbsp;21:38Z</code>

::I went ahead and posted messages on the talk pages for the [[Wikipedia talk:WikiProject Electronics#Computer|electronics]] and [[Wikipedia talk:WikiProject Computer science#Computer|computer science]] wiki projects asking for their insights, suggestions, and help.  Also, [[Computer/Temp]] currently just redirects to [[Computer]].  It was used in an earlier revamping effort, so whenever we get underway with the new article, we can just start putting content there.  -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-07-19&nbsp;21:49Z</code>

:::So, after a week and a half with no comment from third parties, do you think it's safe to start rewriting this article on [[Computer/Temp]]?  I'll copy the current article text there to work from. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-07-31&nbsp;18:53Z</code>
::::Though I am not involved in this project, after reading your proposals and discussion, it sounds reasonable to create somewhat of a hub article. It would be a shame though, if only tables and links remained, a decent prosa embedding and/or introducing the links and tables would be lovely (and necessary, too.) Best wishes, I'll provide help if needed and if time permits. --[[User:Johnnyw|<font color=454545>Johnny</font><font color=999999>w</font>]] [[User talk:Johnnyw|<font color="black">''talk''</font>]] 21:40, 31 July 2006 (UTC)

:::: Yeah - let's get started.  We aren't talking about ''only'' tables and links - but rather putting a strong emphasis on pointing people to other articles than trying to answer everything in-situ.  This will give us the freedom to provide a nice overview without the need to dive into details that are covered elsewhere.  Once again - I'd like to recommend the [[Physics]] article as the model we should shoot for. [[User:SteveBaker|SteveBaker]] 22:10, 31 July 2006 (UTC)

:::::Yes, and as it is I think we should retain most of the intro of this article and a fixed-up history section in the redesigned hub article.  Anyway, let's keep all further discussion of the content of the new page on [[Talk:Computer/Temp]]. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-07-31&nbsp;22:53Z</code>

== [[User:Coolcaesar]] and [[Wikipedia:Requests for arbitration]] ==

Since [[User:Coolcaesar]] frequents this page, I request that any of the editors on this page that have an opinion of [[User:Coolcaesar]] to express it on a recently filed [[Wikipedia:Requests for arbitration]]. You may also add further evidence there to support your view, as well as explain all situations/attitudes/etc. about the user. I urge anyone that has any sort of opinion about this user leave a comment, and comments cannot be used against you in any way on the arbitration page in the future. Thanks for your time. --[[User:Mr.Executive|Mr.Executive]] 08:46, 20 July 2006 (UTC)

(I restored this edit because it is un-cool to remove other people's stuff from Talk pages) [[User:SteveBaker|SteveBaker]] 22:18, 20 July 2006 (UTC)

:Though in all honesty Coolcaesar does NOT really edit this page much and this is a rather inappropriate place to advertise his RFAr. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-07-21&nbsp;02:26Z</code>

:: I agree - but removing posts (particularly controversial ones) from Talk pages is a worse thing - so I put it back. [[User:SteveBaker|SteveBaker]] 13:35, 21 July 2006 (UTC)

wtf...why is the article like repeating?

== computer networks ==

The article mentioned the SAGE computor. A link showed a 86000 based computer from a colorado company.

The SAGE AN/FSQ7 Tube Computer At NYADS McGuire AFB
N.J. was made by IBM at Kingston New York. I was a Blue Suiter who attended the Computer School at 
Kingston New York in 1961. Air Force Blue Suiter. It was a lot of fun working on that old beast. Later I worked at 
McChord AFB Washington, on the CC/DC Sage complex.I rarely ever see the name SAGE mentioned.[[User:75.20.220.32|75.20.220.32]] 03:42, 31 August 2006 (UTC)David Stewart 

Additional note
===Networking and the Internet===
Computers have been used to coordinate information in multiple locations since the 1950s, with the US military's [[SAGE Computer Technology|SAGE]] system the first large-scale example of such a system, which led to a number of special-purpose commercial systems like [[Sabre (computer system)|Sabre]].

I am a newbie. I didn't wish to disturb the written text refering to the sage link. The link goes to a computer co in Colorado.
My comment refers to the Military (USAF) AN/FSQ7 Sage
Computer. A second generation Tube (valve) Machine 
manufactured by IBM at Kingston, New York.[[User:Dcstu41|Dcstu41]] 20:17, 31 August 2006 (UTC) dcstu41

:Dcstu, next time, just be bold and fix the link.  I've made the correction. --[[User:Robert Merkel|Robert Merkel]] 01:55, 6 September 2006 (UTC)

== Image ==

Is realy the lego thing the best we can find for the first image of the article?Why not an open casing?--[[User:Pixel ;-)|Pixel ;-)]] 17:24, 19 September 2006 (UTC)

:Open casing of what?  We've had several discussions of the image before, and it's always been agreed that a picture of a PC is about the most boring and useless thing that could be in the lead of this article. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-19T19:18Z</code>

== questions ==

what bit patterns are represented by the following hexadecimal notations?
i)BC                        ii)9A


How many cells can be in a computers main memory if each address can be represented by three hexadecimal digits?

:This isn't really the place to seek help with questions like that. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-28T14:21Z</code>

: BC=10111100, 9A=10011010,  Three hex digits is 12 bits so 4096 addresses are available.
: ...and yes, this is the wrong place to ask. [[User:SteveBaker|SteveBaker]] 19:56, 29 September 2006 (UTC)

== Who made? ==

Hey who was the first inventor of the computer?

:That's practically impossible to provide a single answer for since there are varying definitions of "computer".  Read [[history of computing hardware]]. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-29T15:39Z</code>

:Yes - the definition of '''''what a computer is defined as''''' is the tricky part of that question. [[Charles Babbage]] is generally credited with the first design for a programmable machine (The '[[Analytical engine]]') - but he never managed to build it.  Arguably, before that, the [[Jacquard loom]] was a programmable machine - and it worked and was widely used - but it didn't do any arithmetic - so I guess that's not a computer either.  Then you get into various machines that could perform calculations - but which weren't programmable.  If you count those as computers then can you count a mechanical adding machine as a computer?  What about a slide-rule - or an abacus?  If you allow an abacus then does counting on one's fingers count as 'computation'?  So nowadays we generally call those things 'calculators' and reserve the term 'computer' for programmable things.  So that leads us to obscure machines such as the cryptographic contraptions that the British built during WWII to crack German codes - but those machines were only just barely programmable - and there is some debate about whether they can be counted as the first computers.  For my money:
:* The later model of the [[ENIAC]] was the first working, practical, useful, programmable computer that could do math - which would give the credit to John William Mauchly and J. Presper Eckert of the University of Pennsylvania.
:* If you allow for theoretical designs that were never built then Charles Babbage easily gets the credit.
:* If you allow ''non-programmable'' calculating machines then the first human to count using his/her fingers gets the credit - the unknown inventor of the abacus might arguably have built the first thing that could add and subtract (but only multiply and divide indirectly) - Edmund Gunter invented the slide rule which could multiply and divide (but not add or subtract)  but [[Blaise Pascal]], and [[Gottfried Leibniz]] built the first mechanical calculators that could do all four arithmetic operations without much human intervention - so one or other of them wins in my opinion (they invented these machines at about the same time - it's hard to say who was first).
:* If you allow programmable but non-calculating machines then [[Joseph Marie Jacquard]] gets the prize for his programmable weaving machine that could weave complex patterns from 'programs' set up on punched cards.
:I worked on the team that built the first ever CD-ROM (which contained all of the dictionary entries for the letter 'O' with text, pictures and sounds) - does that count?  No - I thought not. [[User:SteveBaker|SteveBaker]] 19:51, 29 September 2006 (UTC)

True [[User:Gottoupload|Gottoupload]] 22:31, 4 October 2006 (UTC)

New note by David Stewart
Just a comment on this undertaking. I am one of those dinosauers who worked on computers in the early sixties. The SAGE AN/FSQ7. The work here on sage was pretty good. The work on the overall effort is .
Nuts work of this scope just doesn't fit proper discriptors. Great Job.I read it with interest.
Mr Baker sir, My computer is beige.I am now retired. Work of this nature is vital to the future generations. I salute you folks.It will take years.People ask me about this stuff. How do you explain a lifetime playing with these things. You folks are doing just that. Don't worry about things just keep putting in the information. There is tonnes of it. For starters what are you going to use as archival devises. The state of the art outruns the archival process. Plann ahead.for when memory goes holographic or goes organic whatever.
10-23-2006 [[User:Dcstu41|Dcstu41]] 03:36, 24 October 2006 (UTC)Dcstu41

: Firstly David, I would strongly encourage you to contribute to Wikipedia.  If you were around in the early days of computers, every single thing you can remember will be of huge interest to future generations.   The best way to preserve that information for future generations is to stuff it into Wikipedia.

: I think the concerns over archival matters largely goes away with the advent of OpenSourced documents and the Internet.  Wikipedia can't get 'stuck' on obsolete media so long as people are still using it.  It gets mirrored onto lots and lots of other sites - and as new media comes along, it'll just naturally get copied from one to another.  Concerns that ancient historical versions might get 'left behind' is unwarranted too because Wikipedia has integral version control that means that you can easily get back to the state of any article at any time in the past.  The thing to be concerned about is closed-source material. [[User:SteveBaker|SteveBaker]] 04:20, 24 October 2006 (UTC)

== [[Computer/Temp]] ==

The major rework of this article has been proceeding slowly (see [[Computer/Temp]] for the progress so far.  I think we are close to the point where we can remove the present [[Computer]] article and move [[Computer/Temp]] up to replace it.

The changes are ''drastic'' - so don't be surprised to see vast chunks of the current article simply vanishing.  The intention is most definitely NOT to try to say everything there is to say about computers in one gargantuan text.  The intent is to provide a simple introduction to each of the major subject areas and to defer to the many excellent daughter articles that are out there.

I'd like the change over to go smoothly and without major upset from people who might feel that a lot of their work is being destroyed.  So I'd ask for the following:
# Just so we keep things together, please discuss any problems you have with [[Computer/Temp]] on it's own talk page - not here.
# There is definitely information in the current [[Computer]] article that isn't in [[Computer/Temp]] - that is 100% deliberate.  However - if there is information in [[Computer]] that isn't in [[Computer/Temp]] '''''and''''' it's not in any of the articles mentioned as a "''Main article: ...''" in [[Computer/Temp]] - then we need to know about that.  Probably, that means expanding the referenced article because even [[Computer/Temp]] is awfully long already.  Anyway - if you find something like that, please let's discuss it on [[Computer/Temp]]'s talk page.
# Obviously we need facts to be checked, prose to be polished and references to be added.
# The new [[Computer]] article needs to be a featured article - so as soon as we move [[Computer/Temp]] up there, we need to start seriously attacking it as FA reviewers will.
Thanks in advance. [[User:SteveBaker|SteveBaker]] 05:57, 1 November 2006 (UTC)

===Last Warning!!!===
We are very close to moving [[Computer/Temp]] to [[Computer]] - the article as it is now will abruptly cease to exist and be replaced by the new version... [[User:SteveBaker|SteveBaker]] 22:57, 10 November 2006 (UTC)
{{Aan}}

== Article organization ==

Since we want to make this a hub article ala [[Physics]], I think it would be good if we first tried to agree upon an outline.  I think it's a lot easier to expand upon an outline than just start writing.  Here's what I have in mind, feel free to modify it as you see fit:

* Intro paragraph/section -- Can be partially adapted from what we already have, but this should be done last after the rest of the article is written.
* History -- Also can be adapted from what we have.  Keep this brief and link to the various articles on computing history.
* Computing hardware -- Mostly tables of links
** Very early computers
** Early electronic computing devices
** SSI/MSI/LSI computers
** Microcomputers
*** Embedded computers
*** Personal computers
*** Server class computers -- ehh... maybe
** Theoretical designs -- quantum, bio/chemical, etc
* Software topics -- Also tables, need some help here
** Compilers/core libraries
** Operating systems
** Applications -- this is obviously super-broad, someone suggest some way to reasonably divide this up further and avoid making this section too huge
* Computer-related professions
** Engineering
** IT
** Programming
* Further reading -- books and such, actual WP articles should be covered above

We also probably should figure out where to fit in some links to articles on computing theory, dunno exactly where to put that.  Please critique what I have above.  I know I've missed things, but I intend this to only be a starting point. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-08-01&nbsp;20:21Z</code>

: I like that organisation - I have several suggestions.
:# The 'Software topics/Applications' section would certainly explode if we actually named applications in there - and we'd probably get linkspammed into oblivion if we did.  So it needs to name ''classes'' of application (Word processing, Spreadsheets, Games, Browsers,...) - and we need to keep in mind the non-obvious ones (Industrial control, surveillance, simulation, scientific visualisation...).  That's going to be a hard list to maintain - but we can at least find some way to segment the space ('Desktop application types', 'Embedded application types'...etc).  The problem is going to be deciding how finely we slice it - do we say "Office applications" or do we have "Spreadsheets, Word processing, Scheduling..." ?   I suggest we start to put it together and see how out of hand it gets.  We can provide a careful balance between brevity and completeness when we 'go live' with the new article - and perhaps it'll stay well balanced when the masses start tweaking it.
:# I don't agree with "Compilers/core libraries" as a division.  Compilers belong in 'Software development tools' - which in turn belongs with 'debuggers' and such as a section down under 'Applications'.  Libraries are clearly different though - they are not applications and they aren't operating systems - so they need their own section.
:# We should probably add a topic under 'Software' that covers data formats and transmission protocols - XML, HTML, WAV, JPEG, that kind of thing - and also TCP/IP, NetBIOS, etc.  There is a very thin line between what's data and what's code these days - having a section on file formats allows us to talk about things like XML and PHP that skate along the edge without causing major riots about whether they are 'Software' or not.
:# I wonder if we need a top-level section about standards and standards organisations?
:# The hardware section needs something about 'Peripherals' so we can talk about printers, scanners, hard drives, etc.
:[[User:SteveBaker|SteveBaker]] 22:36, 2 August 2006 (UTC)

::Regarding 1, I say we stay broad and vague and only cite very notable examples in few high level categories.  We can always use the "not our problem" trick by referencing the main article as [[Application software]] or the like.  I'm creating a new section on this talk page with our proposed outline, revised per your suggestions.  Feel free to edit it directly without concern for trampling on my comments. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-08-03&nbsp;00:47Z</code>

:::I'm strongly opposed to naming any actual examples of application programs - no matter how notable.  Once you name one - you either name all half million of them or you spend the rest of your natural life reverting linkspam as every two-bit software house puts it's favorite widget into the list.  This article is one of the most vandalised in the entire Wikipedia - let's not invite more problems.  [[User:SteveBaker|SteveBaker]] 00:52, 3 August 2006 (UTC)

::::That's mostly fine by me, however I don't think we'll be able to avoid naming some things.  Think about it, in the category of OSes, you more or less HAVE to cite some examples to define major "types"; Windows NT family, UNIX/BSD families, the TRON family, etc.  Any software mention on this article should definitely be very sparing, but I think it would be very hard to create a comprehensive (lofty aspiration, I know) article without name-dropping some software.  Anyway, take a look at the outline below and change anything you feel is out of line. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-08-05&nbsp;03:56Z</code>

:::::I have no problem with naming OS's - firstly there aren't so many of them that we can't name them all - secondly, most of the really obscure ones are OpenSourced and not likely to cause LinkSpamming.  The problem with applications is that we could never come even close to naming all of the notable ones - let alone the ones who THINK they are notable. Libraries are a little more tricky - but notable libraries are typically not commercial - or they are bundled with something else...so maybe they aren't a problem either.  We can name OpenGL, DirectX and stuff like that without getting into trouble I think. [[User:SteveBaker|SteveBaker]] 14:36, 5 August 2006 (UTC)

::::::Okay.  The application section will definitely be the hardest to reduce to something sane, so I say we don't worry about it too much until we have to.  I'll get started with this outline this weekend and the week after next (I'll be out of town this coming week). -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-08-05&nbsp;15:35Z</code>

== Proposed outline ==

* Intro paragraph/section -- Can be partially adapted from what we already have, but this should be done last after the rest of the article is written.
* History -- Also can be adapted from what we have.  Keep this brief and link to the various articles on computing history.
* Computing hardware -- Mostly tables of links
** Very early computers
** Early electronic computing devices
** SSI/MSI/LSI computers
** Microcomputers
*** Embedded computers
*** Personal computers
*** Server class computers -- ehh... maybe
** Peripheral devices
** Theoretical designs -- quantum, bio/chemical, etc
* Software topics -- Also tables, need some help here
** Libraries
** Operating systems
** Data exchange
*** Protocols
*** File formats
** Applications
*** ''To be determined...''
''Perhaps...''
*** Office & Productivity
**** Word Processing
**** Desktop Publishing
**** Presentation
**** Database management
**** Scheduling & Time management
**** Spreadsheet
**** Accounting
*** Internet Access
**** Browser
**** Email/News/Chat Client
**** Web Server
**** Email server
*** Manufacturing
**** CAD
**** CAM
**** Plant management
**** Robotic manufacturing
**** Supply chain management
*** Graphics
**** 2D Paint
**** 2D Vector Drawing
**** 3D Modelling
**** 3D Rendering
**** Video editing
**** Image processing
*** Audio
**** Music editing
**** Music playback
**** Mixing
**** Audio Synthesis
*** Software Engineering
**** Compiler
**** Assembler
**** Interpreter
**** Debugger
**** Text Editor
**** Performance analysis
**** Version control & Source management
**** Shells and Command-line tools
*** Educational
**** Edutainment
**** K thru 9 education.
**** Commercial training systems
**** Flight simulation
*** Computer Games
**** Strategy
**** Arcade
**** Puzzle
**** Simulation
**** 1st Person Shooter
**** Platform
**** Massively Multiplayer
**** Text Adventures
*** Misc
**** Artificial Intelligence
**** Malware scanners & checkers.
**** Installation tools
**** File management
* Computer-related professions
** Engineering
** IT
** Programming
** Standards organizations
* Further reading -- books and such, actual WP articles should be covered above

== The '''''BIG''''' table. ==
I took the proposed table (above) an dumped it into the article - making links wherever possible (until I ran out of time/stamina!).  I think we probably need to lose the rightmost column of the applications section and make the rightmost links just be a comma-separated list under the second-to-the-right boxes.  [[User:SteveBaker|SteveBaker]] 20:50, 5 August 2006 (UTC)

:Agreed.  We definitely need to do something along those lines to keep the software table from dominating the rest of it.  We can always split the major topics into individual tables if need be.  In fact, that's probably better from an organizational standpoint since it will allow us to add headings which will be added to the TOC.  Good job so far, though.

:Some things to discuss:
**We could probably do for some concrete examples in the '''computer hardware''' section.  Especially in the first two sub-sections which don't in themselves have articles that can be linked to.  In other words, let's list some notable early computers and notable pre-microcomputer devices as well.  I realize this leaves us potentially open to another notability argument like the ABC debacle, but I don't think that's a valid reason to leave the information out.  I'll add a few.
**The '''Operating system''' subsection probably could be modified a little bit.  I think Solaris can be taken out since it falls under the UNIX category just as easily as do AIX, HP-UX, etc.  I'll change the sub-sub category heading to "[[UNIX]]/[[BSD]]" (BSD is just as important to UNIX as UNIX is) and perhaps mention some major related OSes in a sub-sub-sub-section (heh).  I'm also somewhat inclined to stick Linux under UNIX, but I know this will cause a lot of angst, so I'll leave it.  Additionally, I think the [[TRON project]] may be worthy of mentioning here.  ITRON variants, after all, collectively power more devices than any other single embedded OS and probably any single desktop/server OS.
**'''Library''' seems to be lacking...  Surely we can think of a better categorization?  The three listed are all media libraries and we don't even bother to mention the [[C standard library]].
**We probably should work in mention of major programming languages.
**IEEE and IEC should definitely be mentioned as relevant standards organizations.

Actually, the more I think about it, the more I feel we should separate the big table into separate tables along major topic lines. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-08-05&nbsp;22:34Z</code>

::I did some reorganization of the table.  However, I've come to the conclusion that we really should separate the table into the major categories and create sub-sections for them.  Each subsection should briefly introduce and explain the topic and then display the table.  While tables will help this article, I think it's bad form if we totally rely on them as the sole content.  The reader should be given SOME help in interpreting what they are looking at. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-08-05&nbsp;23:41Z</code>

:::I didn't intend to suggest that this was remotely close to the final form this table must take - I just wanted to put a representative sample of stuff in there so we could get a feel for the scope of the problem - and where the subdivisions could meaningfully be made.  It certainly makes it clear that we don't want to split the Application software section up as finely as I did - and that we need a lot more hardware granularity.  I agree that splitting the table up with subsection title text between the sections would help.  That serves a couple of useful purposes:  Firstly it gets the entries into the index at the top of the article - so shortcuts to each sub-table are then possible.  It also removes the left-most column from the table which is definitely a good thing.
::: In the hardware section, I think we should talk about the classic 1st Generation (mechanical), 2nd Generation (Vacuum Tubes), 3rd Generation (Transisitors) and 4th Generation (Silicon chips) of computers.  The earlier generation sections can be split further into things that are strictly calculators (anticathera mechanism, difference engine, ABCD, etc) and things that are hard-programmable (Digicomp-I, ENIAC) and things that are truely programmable.  We should then subdivide the 4th generation into Mainframe --> Minicomputer --> Microcomputer phases and we can yet further subdivide the microcomputer section up into the 8bit (8080, Z80), the 16 bit (MC6800, Intel 8088), the 32 bit (MC68000, Intel 80486/Pentium) and the latest 64 bit eras.  We somewhat need to be guided by the existance or otherwise of suitable articles to link to - but from the noodling around I did today, it's pretty clear that there are articles of some sort on just about every minute aspect of computers so I don't think we have to worry on that score (although once this is done, I think we're going to want to start looking at all of these myriads of poor articles with a view to brushing them up a little).
::: For the 'Libraries' and 'Standards organisations' sections - I agree that we need to put in a wider set - I was getting table-entry-fatigue by that point - so I just stuck in ''something''.  If we get the overall structure right, we can dink around with the actual contents later...even after the new article goes 'live'. [[User:SteveBaker|SteveBaker]] 01:06, 6 August 2006 (UTC)

::OK - the '''''BIG''''' table is now a handful of almost sanely sized tables.  It's much nicer that way. [[User:SteveBaker|SteveBaker]] 01:20, 6 August 2006 (UTC)

:::I like your idea for the organization of the history of computer hardware.  Let's go down that route. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-08-06&nbsp;01:23Z</code>

:::: Excellent!  Your wish is my command...(I'm *so* sick of editing tables!). We need to fill out some of the entries - I'm a bit hazy on the really early relay-based machines and I've kinda zoned out on 64 bit systems (It's a CPU!  What more do I need to know?  I don't even bother to ask their names anymore.)  [[User:SteveBaker|SteveBaker]] 03:08, 6 August 2006 (UTC)

::::: Frankly the only major relay based computer that I can think of is the [[Harvard Mark I]] (from which we get the "Harvard architecture").  Tubes were much more popular than relays for obvious reasons.  Anyway, [http://ed-thelen.org/comp-hist/BRL61.html this report] by BRL is a good resource for listing major American computers of the early electronic era. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-08-06&nbsp;03:22Z</code>

::::: Also, the 32/64 bit thing is a little difficult since a lot of common ISAs started out as 32-bit and later had 64-bit extensions or implementations.  You could even argue that Intel x86 has been around in 8, 16, 32, and 64 bit forms (though I think most people wouldn't go that far; starting with the 32-bit 80386).  I'll add a footnote to this effect. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-08-06&nbsp;03:36Z</code>

:::::: I just noticed that nowhere does the software table mention graphical user environments (or user interfaces in general).  I think that's an oversight that we should correct! -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-08-06&nbsp;04:02Z</code>

::::::: Yeah - I know.  The trouble is that in MS Windows, the GUI is just a part of the OS - but in UNIX'en, it's a totally separate package...and indeed is split into the X11 layer and a window manager layered on top of that.  So it's a little difficult - it's also arguable that most of the code in a GUI system is in the libraries that programs link to - gtk for example. [[User:SteveBaker|SteveBaker]] 05:45, 6 August 2006 (UTC)

::Okay, back from vacation and ready to work on this some more.  I think the user interface category is a pretty important subtopic to have under software.  I also think it can more or less be split into [[WIMP (computing)|WIMP]], [[text user interface]]s (weak article, but you'll understand what I'm getting at), and Other, for experimental stuff that doesn't fit either category.  I have no qualms with listing MS Windows under both the OS and the UI categories since there is no common name I'm aware of that differentiates the core Windows OS from the GUI.  We can just interject a footnote that explains why Windows appears in both sections and be done with it.  Sound good? -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-08-13&nbsp;02:20Z</code>

:::Yep - that works for me. So we can have a section on Window managers so we can point to X11, Carbon, MS Windows (as-a-windowing-system), KDE, Gnome, etc. [[User:SteveBaker|SteveBaker]] 03:01, 13 August 2006 (UTC)

::::Careful there... You're treading territory where we might have to make further subdivisions.  Sometimes there's a fine line between "window manager" and "graphical desktop environment", especially in the Free software world.  I'd rather not get into that particular semantics debate, so I think it's better to just lump all WIMPs into one category and not make mention of the various components therein.  For example, X11 is a graphics server (standard), GTK is a widget toolkit, Metacity is a window manager, add all three and some various extra software and you get GNOME, a desktop environment.  This article shouldn't get into that level of detail, especially since the lines between those components are often not as easily defined in many other GUIs (like MS Windows and QNX Photon).  Unless you have further suggestions, I think the farthest we should go here are the subdivisions of WIMP, text interface, and other.  We'd probably be well suited to just mention "complete" graphical environments from the *nix world (KDE, GNOME, Xfce) rather than the components that make them up like X11.  As much as I hate to more or less ignore X11 for the purposes of this article, I can't think of a good way to draw category lines that can include X11 and stay terse. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-08-13&nbsp;03:14Z</code>

== Actual article content ==

So we have (and will continue to) talked a lot about table organization so far, but we need to start thinking about what this article should actually discuss content wise.  Tables are awesome for an article like this but they cannot totally carry it, so we need some actual text.  Obviously the text should be very lean and only express some fundamentals about computers and let the tables take over from there.  I'll throw out a few things I think this article should actually discuss:

*Summary of computer hardware history
*Explanation of what a "computer" is and how the meaning of the term has changed and been assimilated into popular culture.  Tie this in with stored program architecture (making a good segue into software).
*Brief discussion of software "from the ground up".  That is, explain low-level and high level languages and how we get from, say, C to assembly to machine code.  I'm probably not the one to write this in its entirety; it would be better suited for an actual programmer.
*Something about networking... Yet to be determined since I'm not sure what level of detail we should give this one.

Comments? -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-08-14&nbsp;00:23Z</code>

: I can certainly explain binary through assembler through low level languages to high level languages from the ground up (I am a programmer).  What concerns me more is a way to more firmly get across the message about what a program does with the hardware.  In my experience, people outside the industry are pretty terrible at understanding what's going on inside the big beige box.  (It doesn't help this article at all - but in the past, I've taught children using a 'kid powered computer' that I put together:  http://www.sjbaker.org/steve/software/hiccup.html )

::I dunno how specific we want to get with exactly how a program influences hardware.  That gets heavily into implementation and is covered somewhat in other articles like [[CPU]].  Unless of course we're talking across terms and I'm missing what your concern is... -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-08-14&nbsp;02:40Z</code>

== Historical data in '''''BIG''''' table'o'links ==
There was some editing relating to DOS appearing in the big table of software links and an edit summary that said that DOS didn't belong there because it's pretty much obsolete.

I don't agree with that because an encyclopedia has to talk about historical stuff as well as current things - but that got me to thinking...If we should mention old operating systems as well as new ones...we should talk about IBM timeshare and batch systems and the infamous GEORGE-III - also VAX/VMS, VM/CMS, CP/M, MS-BASIC...and DOS/PCDOS/MSDOS/DrDOS.  But would it be better to separate current software links from historical ones?   This is a tricky organisational decision.

I think my current preference would be to split some of the categories in the current table into 'Current' and 'Obsolete' sub-categories so that people who aren't interested in dusty old history and just want to see what OS's there are out there right now don't have to click through 50 links to obsolete stuff in order to find what they want.

What about programming languages and such?   In the window manager category, we'd need 'GEM' for example. <small><span class="autosigned">—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:SteveBaker|SteveBaker]] ([[User talk:SteveBaker|talk]] • [[Special:Contributions/SteveBaker|contribs]]) </span></small><!-- Template:Unsigned -->

:Yeah, but if we keep going down that road it will be difficult to prevent the tables from becoming huge and duplicating the other huge list pages that already exist.  What I really meant to get across is that DOS and Windows don't have much to do with one another any more, and even saying that old Win 9x was "DOS-based" is a real stretch.  I just don't think DOS and Windows should be grouped together if we keep the DOS category around.  As much as possible we should just list pretty notable examples in each category.  I mostly added a bunch of items with the view that I might as well put them down while I have them in mind and we can always trim the fat later. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-08-14&nbsp;16:25Z</code>
:P.S. - Don't worry about discussing minor additions here unless you think there'd be objections to them.  It will save time.  If you think GEM is significant, by all means add it. -- [[User:Uberpenguin|uberpenguin]] <code>@ 2006-08-14&nbsp;16:26Z</code>

== Comments ==
I've had [[Computer]] on my watchlist for a good while, since I edited it at some point or other, but just now took a closer look at a Talk reply and discovered this rewrite discussion.  Here are my comments (not necessarily in order):

1. Technically, viruses and other malware are software, so....

2. Might note a couple of examples of firmware that users are most likely to deal with, such as in BIOS "ROM"s, optical drives (CD/DVD writers) and routers.

3. Might check some of the software distribution sites to see how they categorize software and what categories they have.  One of the oldest such sites is [www.simtel.net Simtel].  You'll also find some of the more obscure categories.

4. The intro says "A computer is a machine...".  "Machine" has connotations which may not apply well in the future, such as with organic and/or cellular computers.  "Device" is better, but not much.

:I'd say, stick with machine.  Regardless of the mechanical connotations of the term, "Turing machine" is one of the benchmarks by which we define computers, so I don't see a problem in keeping the term. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

5. Isn't there already a [[Software]] article?  If so, then let it be the main article for software, and just summarize it here.  This will take care of any complaints about the software topic being large.

:Yeah, but we should have at least some software categories here since it's such a major topic.  The trick is figuring out how to keep it concise. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

6. Might want to add that memory can also be I/O (in addition to programs and data) -- the best example, which is used by most modern computers, is video, which is generally memory-mapped.

:That's a good point, I'm just not sure if its appropriate for a very high level article like this one.  Beyond the stored-program architecture, I think it's a good idea to stay away from computer architecture details here. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

7. Don't forget supercomputers and grid/distributed computing (such as [http://www.beowulf.org Beowulf] and [http://setiathome.berkeley.edu SETI@Home].

:Good catch; we should definitely have some information about massively parallel systems. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

8. Re: Unix/BSD/Linux, I'd say just call them all "Unix-like" or something similar.  Separating them would imply more to the masses than is actually the case, and the wording is sufficiently generic that only fanatics would be offended (they'll be offended no matter what anyway, so &$%^ them).

:"Unix-like" is fine by me. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

9. May want a mention that the von Neumann architecture has also indirectly contributed to our current malware woes.

:I sort of think that's a loose connection to make.  Many modern ISAs include support for marking memory segments as read-only.  I suppose that it is fair to say that the architecture was never designed with security in mind and that the lack of separation between software and data can be a security issue if it's not addressed.  We'll see, overall I think this is a very minor point in the context of what we should be covering in this article. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

10. On the subject of the Windows GUI; it is indeed all built in, but there are several different ones, such as GDI (in Win3x, as I recall), GDIPLUS (in Win95 through XP, I think), and DirectX 10 (in Vista).  As for *nix, I seem to recall hearing of some work to merge KDE and Gnome, or something like that.

11. People seem to easily misunderstand what software is and can do.  Anyone who reads science fiction (especially the cyberpunk type) can see that some of the authors have a non-technical "knowledge" of what software can do, etc.; from the viewpoint of someone in the field, some of this "science fiction" more closely resembles fantasy.

:Agreed... I think Steve commented on this earlier.  I'll try to take a stab at explaining how software works in basic terms when I redo the stored program architecture section.  I think the hardware/software interaction is probably the most important thing we can convey in this article, so it will probably require a lot of tweaking. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

12. I think historical stuff (like DOS) should be in the historical section, and mentioned elsewhere only when it is relevant to discussion of modern computers.  Note that third-party versions of DOS are still supported, although nowadays mostly aimed at embedded systems.

:So you think we should remove legacy OSes from the software table and only mention them where they fit in with other sections like history?  I guess I could go along with that. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

13. I'm not sure that I'd say that Win3x was DOS-based.  A better term might be "DOS roots", which understates the matter, but is closer or "uses DOS as a foundation".

14. Instead of (or in addition to) comparing modern embedded computers to a deck of cards, I'd suggest mentioning modern cellphones.

15. Please define or rephrase "regenerative memory".

:Heh... "Capacitor memory"... I agree, that's terribly phrased. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

16. In theoretical future technologies, it seems to me that the distinction between quantum and nanotechnology, and biological and nanotechnology, are likely to be considered nit-picking or irrelevant in the future; may want to clarify that there is considerable overlap here already and it's likely to overlap more in the future.

:Sounds good, feel free to make appropriate changes. I know very little about theoretical computing models, so I'm not the one to write this information. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

17. Fourth Generation probably ought to be in subgroups, with the bit sizes and the computer categories being separate.  There are mainframes with bit sizes from 4 to 64, and ditto for embedded, personal, laptop, and server.

:The integer range thing was, I think, just used as a method of subdividing microprocessors.  I'd be very keen on using a different scheme, though, because I see little reason to even mention bit width in this article.  -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

18. What is "Death ray of Ming the Merciless"?  I presume it's humor (and yes it would certainly be an output device of some kind), but I don't get the reference; was his death ray computer-controlled?  If so, this is probably more appropriate (and the humor more obvious) in a section on automation and/or robotic control.

:Joke. Placeholder. Article under (re)construction. Smile. :) -- ~~

19. I would replace ", and the next instruction is fetched." with something like "and the process reapeats with the next cycle.".  I would drop the "halt" comment entirely, since you never see it in most programs, as far as I know (I haven't messed with assembler in ages), and it might raise more questions with some people.

:That section is going to be totally redone. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

20. Note that every model of CPU generally has its' own machine language, and there are significant differences betwwen manufacturers, such as Intel and AMD, but they have a large number of instructions which work identically or nearly so on all CPUs in the class.

21. I tend to dislike the word "program", since it tends to be overloaded, as in "TV program", "TV station programming", and so forth.  I try to use "Software" where I can, since it is less ambiguous.  Consider a program to record TV programs, or something like that; I would expect people to get confused.  My suggestion is to say that when most people talk about programs, they really refer to the more generic "software".  Many people also misuse it, as in "can you program my computer?".  I'd suggest saying that "program" is more of a technical term than anything else.

22. Might want to mention that programming (software engineering) is still an art in many ways.

:Heh... I'm not going anywhere near that one.  Let's just stick to the facts. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

23. The "computational" paragraph is unclear.  I understand what it means, but it may not be clear to the uninitiated.

:I think that's a relic from the old article.  Free free to make broad sweeping changes to the software section because we had planned to anyway. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

24. "few technical reasons why a GUI has to be tied to the rest of an operating system" -- I'm not entirely sure that I agree with this.  At the very least, it needs to be tied to the shell, as is done in Unix-like OSes.  The problem then is that you get applications which run with one shell but not another, which is why it is made part of the OS in the first place.  See my comment above (10) about KDE and Gnome.

25. Human-like robots are being sold in Japan, which is apparently desperate for them due to their aging population; see [http://world.honda.com/ASIMO ASIMO].

26. Only a single main article is listed; should be able to have main articles listed for most of the sections.

:Yup. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

--[[User:Scott McNay|Scott McNay]] 05:35, 6 September 2006 (UTC)

:Scott, basically we copied what was on the Computer article with the intention of rewriting most of it.  So what you see here still contains a lot of stuff that needs to be redone.  Unfortunately I haven't been working on this very much lately, but I'd still like to finish our plans here.  The thing is, more editors always help, especially for an article with such a broad scope.  Please feel free to change whatever you like on this page, just make a note on this talk page explaining what you've done. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

::Boy - that's a lot of great feedback!
:: ''1. Technically, viruses and other malware are software, so....''
:: Yep - I agree - those should go inside the software category.
:: ''2. Might note a couple of examples of firmware that users are most likely to deal with, such as in BIOS "ROM"s, optical drives (CD/DVD writers) and routers.''
:: Yes - but let's try to avoid becoming too 'PC-centric' and talk about firmware in embedded situations such as cell-phones.
:: ''3. Might check some of the software distribution sites to see how they categorize software and what categories they have. One of the oldest such sites is [www.simtel.net Simtel]. You'll also find some of the more obscure categories.''
:: I'm pretty happy with the categories we have.  Those sites tend to be a bit PC-centric too.
:: ''4. The intro says "A computer is a machine...". "Machine" has connotations which may not apply well in the future, such as with organic and/or cellular computers. "Device" is better, but not much.''
:: In Physics, a machine is defined as a thing for converting energy from one form to another or transferring energy from one place to another.  A computer certainly does that (and so would an organic or cellular computer) - but in truth, the important thing is not the 'machine' aspect of converting energy, that's how computers are implemented.  I don't like ''device'' either. I agree with the sentiment that we talk about ''Turing machines'' and ''Babbages' Engine'' - and the old name for a CPU was ''The Mill''.  The use of mechanical metaphors is not inappropriate.  I vote to keep ''machine''.
:: ''5. Isn't there already a Software article? If so, then let it be the main article for software, and just summarize it here. This will take care of any complaints about the software topic being large.''
:: Yes - that is the intention - we just kinda fizzled out on the effort.  We need each of the large sections to be reduced to a couple of paragraphs preceeded by one or two <nowiki>{{main|xxxxx}}</nowiki> templates.  Any important information from this article that isn't in the subservient article needs to be carefully transferred first.
:: ''6. Might want to add that memory can also be I/O (in addition to programs and data) -- the best example, which is used by most modern computers, is video, which is generally memory-mapped.''
:: Well, that *was* true in the past.  Just try mapping the display of your fancy new nVidia 7900 into CPU space and see where it gets you!  The fact that I/O might have a separate address space (8080-style with IN/OUT instructions) - or mapped into the main memory space (68000-style) is somewhat arbitary.  In the end, there are addressable locations to which you read and write data.  I think the distinction between memory-mapped and I/O-mapped I/O is a strange one in the modern world.
:: ''7. Don't forget supercomputers and grid/distributed computing (such as Beowulf and SETI@Home.''
:: Yes - good point.
:: ''8. Re: Unix/BSD/Linux, I'd say just call them all "Unix-like" or something similar. Separating them would imply more to the masses than is actually the case, and the wording is sufficiently generic that only fanatics would be offended (they'll be offended no matter what anyway, so &$%^ them).''
:: The percentage of Linux/BSD fanatics in the Wikipedia community is VASTLY higher than in the general public.  Offend them at your own risk!  In truth, BSD '''''is''''' Unix - they come from the same source code tree - and you could justifiably smoosh them together.  Linux, however, is '''''not''''' Unix - although it has similar internal interfaces.   So to be completely encyclopeadic about it, we should keep them separate.  To not do so would entail smooshing almost every single operating system into either "Windows" or "Not-Windows" - and that would be doing a terrible misservice to our readership. The only OS's that are both not-Windows and not-Unix-like are very, very obscure indeed.  If you gave equal prominance to (say) Windows, Unix and BeOS - but left out Linux and MacOSX (Remember - MacOSX is based around BSD) - that would give a very skewed view of the world.  So no - I object in the strongest possible terms to this suggestion.
:: ''9. May want a mention that the von Neumann architecture has also indirectly contributed to our current malware woes.''
:: No - that's not true.  To separate out code and data in main memory (which is what I think you are referring to) might help a little bit - but you wouldn't want to have separate hard drives for code and data - so there is still plenty of scope for malware.  Also, much of the software we use is interpreted - Java programs (for example) are data that is read by the Java interpreter - which is code.  So Java malware would still be possible.  The ability to treat code as data and data as code is '''''key''''' to the success of modern networking.  Without the von-Neumann architecture, Wikipedia couldn't exist!
:: ''10. On the subject of the Windows GUI; it is indeed all built in, but there are several different ones, such as GDI (in Win3x, as I recall), GDIPLUS (in Win95 through XP, I think), and DirectX 10 (in Vista). As for *nix, I seem to recall hearing of some work to merge KDE and Gnome, or something like that.''
:: I'm not a Windows expert - if there are distinguishable Windows GUI's then lets list them.  But the KDE/Gnome merge that (IIRC) RedHat attempted would only merge two out of the dozen or so window managers that are in common use in the *nix world.
:: ''11. People seem to easily misunderstand what software is and can do.''
:: Yes indeed.
:: ''Anyone who reads science fiction (especially the cyberpunk type) can see that some of the authors have a non-technical "knowledge" of what software can do, etc.; from the viewpoint of someone in the field, some of this "science fiction" more closely resembles fantasy.''
:: Right - and attacking those misconceptions are at the heart of what I want to achieve here.
:::  ''Agreed... I think Steve commented on this earlier. I'll try to take a stab at explaining how software works in basic terms when I redo the stored program architecture section. I think the hardware/software interaction is probably the most important thing we can convey in this article, so it will probably require a lot of tweaking. -- mattb @ 2006-09-06 13:12Z''
::: Yes.  I want to get this down right.  I don't think it belongs in the [[Computer]] article - it needs to go into one of the subservient articles - with the usual two paragraph summary placed here.
:: ''12. I think historical stuff (like DOS) should be in the historical section, and mentioned elsewhere only when it is relevant to discussion of modern computers. Note that third-party versions of DOS are still supported, although nowadays mostly aimed at embedded systems.''
:: What is '''''history'''''?  The problem with computers is that something we used last year is history this year.  I think perhaps a better approach is to divide the history into defined 'eras' - the computer generations forming the natural boundaries.  This approach is taken in (for example) the [[History of automobiles]] article.  That way, we have a recent history section that says "Fourth generation to present day" and avoids the need to artificially decide when (for example) DOS became obsolete.  I should point out that a major new version of the OpenSourced DOS was released just last week - so there must still be a fairly vigerous user community.  We also heard on Slashdot the other day that some criminal had his Commadore 64 confiscated by police - who found that they couldn't understand the darned thing and were unable to investigate the files that were on it!  When the Y2K thing was a big worry, companies who were still doing payroll on 20 year old machines came out of the woodwork demanding fixes to their antique software.  So beware - things may not be as obsolete as you think!
:: ''13. I'm not sure that I'd say that Win3x was DOS-based. A better term might be "DOS roots", which understates the matter, but is closer or "uses DOS as a foundation".''
:: In order to run Windows 3.1, you first booted into DOS and then typed 'WINDOWS' to boot Win3.1 - so yes, it most definitely was DOS-based.
:: ''14. Instead of (or in addition to) comparing modern embedded computers to a deck of cards, I'd suggest mentioning modern cellphones.''
:: Right - the 'Deck of cards' analogy is long outdated...so is 'the size of a cellphone'.  I just bought a computer that's inside a USB dongle less than 1" long which includes Linux and a complete web-server.  The 2cm x 2cm die of an nVidia 7900 graphics chip contains 16 vertex processors and 48 fragment processors - each of which is an essentially separate computer. 64 computers in a 2cm die makes each one about 2.5mm across...so maybe "The size of a grain of rice" is the best size description.  Embedded computers can be almost arbitarily small.
:: ''15. Please define or rephrase "regenerative memory".''
:: I agree.
::: ''Heh... "Capacitor memory"... I agree, that's terribly phrased. -- mattb @ 2006-09-06 13:12Z''
::: Urgh!  You're right.  Do you mean "Memory which is erased by the act of reading it" - in which case magnetic core stores fit that bill too.  However, you can argue that when the read-then-rewrite cycle is performed automatically - then it's not regenerated anymore.  On some very old core-store computers, you had to explicitly rewrite memory after you read it using software instructions.  The idea being that if you didn't need the value any more after you'd read it, then the computer could run faster by not regenerating the memory automatically.   However, I'd be hard pressed to name a computer that was like that.
:: ''16. In theoretical future technologies, it seems to me that the distinction between quantum and nanotechnology, and biological and nanotechnology, are likely to be considered nit-picking or irrelevant in the future; may want to clarify that there is considerable overlap here already and it's likely to overlap more in the future.''
:: I don't think we know that.  Quantum computing is very, very different than anything else because of 'superposition' trickery in which the qubits can simultaneously hold every possible solution to a problem.  A nanotechnological/mechanical 'pushrod' memory would be just like conventional RAM and hold a definite 1 or a 0.  So those are utterly distinct.  It may turn out that biological and quantum technologies may one day merge - but right now, they are entirely different.  The people who are using DNA replication to perform massively parallel 'travelling salesman' type calculations in a bucket full of slime are doing something quite different from the nanotechnologists like Drexler who envisage Babbage-machine types of technology shrunk down to atomic scales.  So no - I disagree.  Those are all very distinct field right now.   If they ever do merge, we can change the article - but there is absolutely zero evidence that this has in fact already happened - not that it is likely to do so in the near future.
:: ''17. Fourth Generation probably ought to be in subgroups, with the bit sizes and the computer categories being separate. There are mainframes with bit sizes from 4 to 64, and ditto for embedded, personal, laptop, and server.''
:: Dangerous.  We are talking about history here.  The history of the technology doesn't go in nice linear bus width increments like you think.  We had Amdahl machines with 64 bits before we had 4 bit microprocessors.  So no - I strongly disagree.  The generations are about technology leaps - mechanical/relays/vacuumtubes/transistors/MSI/LSI - not about bus widths.
:: ''18. What is "Death ray of Ming the Merciless"? I presume it's humor (and yes it would certainly be an output device of some kind), but I don't get the reference; was his death ray computer-controlled? If so, this is probably more appropriate (and the humor more obvious) in a section on automation and/or robotic control.''
:: It was late - I was tired - this is a temporary article.  I wondered how long it would take someone to notice it!  Feel free to remove it!
:: ''19. I would replace ", and the next instruction is fetched." with something like "and the process reapeats with the next cycle.". I would drop the "halt" comment entirely, since you never see it in most programs, as far as I know (I haven't messed with assembler in ages), and it might raise more questions with some people.''
:: The 'HALT' instruction is very common in embedded situations since it puts the processor into a powered down state.  Please stop thinking in PC terms.  Also, the concept of a program halting is key to many theoretical computational issues such as (obviously) "The Halting Problem".
:: ''20. Note that every model of CPU generally has its' own machine language, and there are significant differences betwwen manufacturers, such as Intel and AMD, but they have a large number of instructions which work identically or nearly so on all CPUs in the class.''
:: Again, you are looking only at PC clones.  Look at embedded systems - they most certainly are not remotely cross-compatible.
:: ''21. I tend to dislike the word "program", since it tends to be overloaded, as in "TV program", "TV station programming", and so forth. I try to use "Software" where I can, since it is less ambiguous. Consider a program to record TV programs, or something like that; I would expect people to get confused. My suggestion is to say that when most people talk about programs, they really refer to the more generic "software". Many people also misuse it, as in "can you program my computer?". I'd suggest saying that "program" is more of a technical term than anything else.''
:: The term is certainly overloaded in other fields - but ''software'' is too vague.  Software includes data files.  A "Software package" might include many programs - or no programs at all (just libraries).  Within the field of computing, the term has a very precise meaning.
:: ''22. Might want to mention that programming (software engineering) is still an art in many ways.''
:: Yes.  Something I wish I could get my boss to understand!  It truly is an art form.  Any experienced programmer will look at two pieces of code - both of which solve the problem at hand - both of which are equally fast and space-efficient and he'll say "Wow!  That's a beautiful piece of code - but this other one is ugly!" - that's art.  Furthermore, show those same two pieces of code to another programmer and he'll probably come up with the opposite view.   I run a team of 5 or so programmers who all work on the same 1 million lines-of-code application - and I can tell who wrote what bits just from the coding style alone.  It's like looking at the brush strokes of a grand master oil painting and saying "I can tell that so-and-so didn't paint this - it was his understudy".  Yes - it's most definitely an art. 
:: ''23. The "computational" paragraph is unclear. I understand what it means, but it may not be clear to the uninitiated.''
:: Yeah - that's got to change.
:: ''24. "few technical reasons why a GUI has to be tied to the rest of an operating system" -- I'm not entirely sure that I agree with this. At the very least, it needs to be tied to the shell, as is done in Unix-like OSes. The problem then is that you get applications which run with one shell but not another, which is why it is made part of the OS in the first place. See my comment above (10) about KDE and Gnome.''
:: Yep - that's got to go.  The GUI patently obviously DOESN'T need to be tied to the OS because there are plenty of examples (KDE, etc) of GUI's that are perfectly usable that aren't tied to the kernel.  They aren't tied to the shell either.   The Window manager ("GUI" is a vague term here) is at the same level in the software heirarchy as the shell - it is in fact possible to talk of "graphical shells" and "command line shells".  But you can launch a graphical shell from a command line shell and vice-versa (at least under *nix).  So one is not 'above' the other in the hierarchy.
:: ''25. Human-like robots are being sold in Japan, which is apparently desperate for them due to their aging population; see ASIMO.''
:: Yes - but a robot is just a machine that happens to contain a computer.  In fact, if you watch shows like "Robot wars" in which computers are hardly ever present, it is aparrent that the term "Robot" has lost it's connotation of "A mobile machine driven by a computer".  Robotics is a separate field from Computing and I don't think we need to say very much about it here. Robotics are an application of computers - just like cellphones, greetings cards that play "Happy Birthday" when you open them, space craft, cars, PC's, TV remotes, Furbies, dish washers....why should we single out robots for special mention?
:: ''26. Only a single main article is listed; should be able to have main articles listed for most of the sections.''
:: Yes - we aren't done yet.
:: [[User:SteveBaker|SteveBaker]] 15:46, 6 September 2006 (UTC)

== Comments, new section ==
(Getting long)
(ouch, I'm still copying most of it.  Oh well, at least it's now spaced out more, to separate individual items)

:: ''4. The intro says "A computer is a machine...". "Machine" has connotations which may not apply well in the future, such as with organic and/or cellular computers. "Device" is better, but not much.''
:: In Physics, a machine is defined as a thing for converting energy from one form to another or transferring energy from one place to another.  A computer certainly does that (and so would an organic or cellular computer) - but in truth, the important thing is not the 'machine' aspect of converting energy, that's how computers are implemented.  I don't like ''device'' either. I agree with the sentiment that we talk about ''Turing machines'' and ''Babbages' Engine'' - and the old name for a CPU was ''The Mill''.  The use of mechanical metaphors is not inappropriate.  I vote to keep ''machine''.
::: Telling people that their brain is actually a machine may not go over to well with some. :) --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)
:::: An encyclopedia is not for telling people what they want to hear - it's about telling them the truth - which is often uncomfortable. The brain is not only a machine - but it's also a computer.  The marvel of the thing is how insanely dense it is.  Several years ago I read that the entire DRAM production of the world for one entire year had just reached the equivelent of one human brain.  We're a bit beyond that now - but a modern PC has about the processing power of an earthworm. [[User:SteveBaker|SteveBaker]] 03:56, 7 September 2006 (UTC)
::::: In this context, I'll agree. --[[User:Scott McNay|Scott McNay]] 05:44, 7 September 2006 (UTC)

:: ''6. Might want to add that memory can also be I/O (in addition to programs and data) -- the best example, which is used by most modern computers, is video, which is generally memory-mapped.''
:: Well, that *was* true in the past.  Just try mapping the display of your fancy new nVidia 7900 into CPU space and see where it gets you!  The fact that I/O might have a separate address space (8080-style with IN/OUT instructions) - or mapped into the main memory space (68000-style) is somewhat arbitary.  In the end, there are addressable locations to which you read and write data.  I think the distinction between memory-mapped and I/O-mapped I/O is a strange one in the modern world.
::: I used to do a lot of video fiddling.  That was many years ago, though.  I have no experience with modern video cards, though, with their advanced features, aside from putting one in and watching the impressive graphics. --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)
:::: Graphics is my livelyhood - you can take that one to the bank.  The ability to direct access display memory is really only still there so a PC can boot.  Once it's up and running, you're locked behind a device driver that DMA's command packets to the firmware.  The interface is more like a network connection than a peripheral in the classic sense. [[User:SteveBaker|SteveBaker]] 03:56, 7 September 2006 (UTC)
::::: A serial-like connection like that is probably easier for the GPU to handle; it doesn't need to second-guess what the application is doing with its' memory space. --[[User:Scott McNay|Scott McNay]] 05:44, 7 September 2006 (UTC)
:::::: Right.  You don't want contention between the CPU and GPU for RAM access - and you also don't want potential screwups when the CPU and GPU are accessing the same locations.  It's just cleaner to require all accesses to go through the GPU. [[User:SteveBaker|SteveBaker]] 05:40, 1 November 2006 (UTC)

:: ''8. Re: Unix/BSD/Linux, I'd say just call them all "Unix-like" or something similar. Separating them would imply more to the masses than is actually the case, and the wording is sufficiently generic that only fanatics would be offended (they'll be offended no matter what anyway, so &$%^ them).''
:: The percentage of Linux/BSD fanatics in the Wikipedia community is VASTLY higher than in the general public.  Offend them at your own risk!  In truth, BSD '''''is''''' Unix - they come from the same source code tree - and you could justifiably smoosh them together.  Linux, however, is '''''not''''' Unix - although it has similar internal interfaces.   So to be completely encyclopeadic about it, we should keep them separate.  To not do so would entail smooshing almost every single operating system into either "Windows" or "Not-Windows" - and that would be doing a terrible misservice to our readership. The only OS's that are both not-Windows and not-Unix-like are very, very obscure indeed.  If you gave equal prominance to (say) Windows, Unix and BeOS - but left out Linux and MacOSX (Remember - MacOSX is based around BSD) - that would give a very skewed view of the world.  So no - I object in the strongest possible terms to this suggestion.
::: Considering the topic of this article, I'd expect the majority audience to be "the masses", not fans.  I suspect that you'd have trouble explaining the difference between Linux and the various Unix variants in such a way that most readers wouldn't then say "ok, so, again, what's the difference?".  However, I grant that there's probably enough turf war already. :)  Anyway, my point pretty much is that it's likely to be a waste of space listing anything other than OSes that people are likely to have heard of; anything else should be in the "Operating Systems" article. --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)
::::: Once again - our purpose here is to educate.  If we only list the things people know about, we're getting nowhere! [[User:SteveBaker|SteveBaker]] 03:56, 7 September 2006 (UTC)
:::::: Ok, let me re-do again: this is an article on "computers"; there should be no more than a handful or so of OSes listed; readers desiring more detail should go to the OSes article.  OSes listed here may include one or two that people may not have heard of, BUT are used in devices or services that they may know about, such as Symbian OS in cell phones, or Beowulf clusters doing weather analysis (or whatever; I'm sure Beowulf is used for something which is known to many people). --[[User:Scott McNay|Scott McNay]] 05:44, 7 September 2006 (UTC)
::::::: Beowulf clusters are used for lots of things - but not many of them are noticable to the general public.  That doesn't matter though - our task is to educate - so anything the general public doesn't know about is especially deserving of our efforts.  I guess the ultimate example of a Beowulf cluster would be the 'Googleplex' - the cluster that runs the Google application.  Also the large 'render farms' that the movie studios use for making stuff like Toy Story.  I actually work with Beowulf clusters in my job - which is designing graphics software for serious military flight simulators. [[User:SteveBaker|SteveBaker]] 05:40, 1 November 2006 (UTC)

:: ''9. May want a mention that the von Neumann architecture has also indirectly contributed to our current malware woes.''
:: No - that's not true.  To separate out code and data in main memory (which is what I think you are referring to) might help a little bit - but you wouldn't want to have separate hard drives for code and data - so there is still plenty of scope for malware.  Also, much of the software we use is interpreted - Java programs (for example) are data that is read by the Java interpreter - which is code.  So Java malware would still be possible.  The ability to treat code as data and data as code is '''''key''''' to the success of modern networking.  Without the von-Neumann architecture, Wikipedia couldn't exist!
::: I didn't say that von Neumann architecture was bad; the benefits would seem to massively outweigh the problems.  Nevertheless, it would be a trivial point to make in this type of article, so might as well drop this item. --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)


:: ''10. On the subject of the Windows GUI; it is indeed all built in, but there are several different ones, such as GDI (in Win3x, as I recall), GDIPLUS (in Win95 through XP, I think), and DirectX 10 (in Vista). As for *nix, I seem to recall hearing of some work to merge KDE and Gnome, or something like that.''
:: I'm not a Windows expert - if there are distinguishable Windows GUI's then lets list them.  But the KDE/Gnome merge that (IIRC) RedHat attempted would only merge two out of the dozen or so window managers that are in common use in the *nix world.
::: Ignore this; mostly nitpicking on my part. :)  Applications need to be updated to run on the newer GUI versions, but the GUIs are not interchangeable; one merely supersedes the older ones.  --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)


:: ''11. People seem to easily misunderstand what software is and can do.''
:: Yes indeed.
:: ''Anyone who reads science fiction (especially the cyberpunk type) can see that some of the authors have a non-technical "knowledge" of what software can do, etc.; from the viewpoint of someone in the field, some of this "science fiction" more closely resembles fantasy.''
:: Right - and attacking those misconceptions are at the heart of what I want to achieve here.
:::  ''Agreed... I think Steve commented on this earlier. I'll try to take a stab at explaining how software works in basic terms when I redo the stored program architecture section. I think the hardware/software interaction is probably the most important thing we can convey in this article, so it will probably require a lot of tweaking. -- mattb @ 2006-09-06 13:12Z''
::: Yes.  I want to get this down right.  I don't think it belongs in the [[Computer]] article - it needs to go into one of the subservient articles - with the usual two paragraph summary placed here.
:::: The other day, I read "Spin State", by Chris Moriarty.  There are several scenes in which the protagonist is using virtual reality (in modern terms) to explore remote virtual scenarios, and is trapped by antagonists, unable to return to her body.  The book is straight cyberpunk, no psychic stuff.
:::: This may be related to a problem that many people seem to have distinguishing between memory and permanent (disk drive) storage, and understanding that loading something from the hard drive into memory doesn't cause it to disappear from the hard drive.  Oh, and as long as I'm ranting, please save me from people who think "computer" refers to the monitor, and say "hard drive" when they talk about the computer ("can you put a DVD writer in my hard drive?").  I don't mind "CPU" as an abbreviation for "computer", though. --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)
::::: Well, I flinch everytime I hear someone make that mistake - but again, our mission here it to prevent that kind of thing. [[User:SteveBaker|SteveBaker]] 03:56, 7 September 2006 (UTC)
:::::: Which mistake?  CPU instead of computer?  I sometimes use it as an abbreviation when writing, but never when talking.  --[[User:Scott McNay|Scott McNay]] 05:44, 7 September 2006 (UTC)
:::::::CPU instead of computer is what makes me flinch.  I open up the case of what is undoubtedly called a computer - and inside I can point to the little piece of it that is the CPU.  Now, some people call the big beige box with the drive slots on the front and the connectors on the back "The CPU box" - and that's something I mind less because it distinguishes it as that part of the totality of the computer that happens to contain the CPU...much as people might refer to "The Bedroom" when that room in fact contains quite a lot of other furniture. [[User:SteveBaker|SteveBaker]] 05:40, 1 November 2006 (UTC)

:: ''12. I think historical stuff (like DOS) should be in the historical section, and mentioned elsewhere only when it is relevant to discussion of modern computers. Note that third-party versions of DOS are still supported, although nowadays mostly aimed at embedded systems.''
:: What is '''''history'''''?  The problem with computers is that something we used last year is history this year.  I think perhaps a better approach is to divide the history into defined 'eras' - the computer generations forming the natural boundaries.  This approach is taken in (for example) the [[History of automobiles]] article.  That way, we have a recent history section that says "Fourth generation to present day" and avoids the need to artificially decide when (for example) DOS became obsolete.  I should point out that a major new version of the OpenSourced DOS was released just last week - so there must still be a fairly vigerous user community.  We also heard on Slashdot the other day that some criminal had his Commadore 64 confiscated by police - who found that they couldn't understand the darned thing and were unable to investigate the files that were on it!  When the Y2K thing was a big worry, companies who were still doing payroll on 20 year old machines came out of the woodwork demanding fixes to their antique software.  So beware - things may not be as obsolete as you think!
::: Ok, agreed.  BTW, I've yet to see a good explanation of why the US (most dependent upon technology at the time) did not have severe problems when Y2K came around.  It pretty much fizzled (which is definitely a good thing!), and the suggestions that it was mostly hype doesn't seem adequate, and suggestions that the public warnings about it did the trick also doesn' seem adequate, both based on what I'd been hearing. --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)
:::: I think it fizzled because enough people were terrified of the consequences that they knuckled down and fixed it.  It's not like it was a surprise or was hard to plan for!  It's interesting though that the 'billenium' bug (when all UNIX machines had their dates wrap throught the 1,000,000,000th second) went entirely unreported - even though it had the exact same chance of blowing up in our faces.  There have been other critical numeric overflow issues too - when the Dow Jones index hit 10,000 for example. [[User:SteveBaker|SteveBaker]] 03:56, 7 September 2006 (UTC)
::::: I'm honestly surprised that there haven't been more events due to such problems.  --[[User:Scott McNay|Scott McNay]] 05:44, 7 September 2006 (UTC)

:: ''13. I'm not sure that I'd say that Win3x was DOS-based. A better term might be "DOS roots", which understates the matter, but is closer or "uses DOS as a foundation".''
:: In order to run Windows 3.1, you first booted into DOS and then typed 'WINDOWS' to boot Win3.1 - so yes, it most definitely was DOS-based.
::: It was the same with Windows 95, 98, and 98SE (but not ME); Windows simply got loaded automatically.  --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)
:::: That's what I thought - but the last version of Windows I actually used was 3.1.1 - so I thought I'd stick with what I knew! [[User:SteveBaker|SteveBaker]] 03:56, 7 September 2006 (UTC)

:: ''15. Please define or rephrase "regenerative memory".''
:: I agree.
::: ''Heh... "Capacitor memory"... I agree, that's terribly phrased. -- mattb @ 2006-09-06 13:12Z''
::: Urgh!  You're right.  Do you mean "Memory which is erased by the act of reading it" - in which case magnetic core stores fit that bill too.  However, you can argue that when the read-then-rewrite cycle is performed automatically - then it's not regenerated anymore.  On some very old core-store computers, you had to explicitly rewrite memory after you read it using software instructions.  The idea being that if you didn't need the value any more after you'd read it, then the computer could run faster by not regenerating the memory automatically.   However, I'd be hard pressed to name a computer that was like that.
::: I'd suspect that if you find such a device, it's probably embedded. --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)
:::: Right - so it's pretty academic to talk about the memory needing to be regenerated unless you have to do it explicitly in software.  Ironically, push-rod memory (such as might come about in Nanotech processors) might need to be regenerated - so this isn't just some kind of historical anachronism. [[User:SteveBaker|SteveBaker]] 03:56, 7 September 2006 (UTC)

:: ''16. In theoretical future technologies, it seems to me that the distinction between quantum and nanotechnology, and biological and nanotechnology, are likely to be considered nit-picking or irrelevant in the future; may want to clarify that there is considerable overlap here already and it's likely to overlap more in the future.''
:: I don't think we know that.  Quantum computing is very, very different than anything else because of 'superposition' trickery in which the qubits can simultaneously hold every possible solution to a problem.  A nanotechnological/mechanical 'pushrod' memory would be just like conventional RAM and hold a definite 1 or a 0.  So those are utterly distinct.  It may turn out that biological and quantum technologies may one day merge - but right now, they are entirely different.  The people who are using DNA replication to perform massively parallel 'travelling salesman' type calculations in a bucket full of slime are doing something quite different from the nanotechnologists like Drexler who envisage Babbage-machine types of technology shrunk down to atomic scales.  So no - I disagree.  Those are all very distinct field right now.   If they ever do merge, we can change the article - but there is absolutely zero evidence that this has in fact already happened - not that it is likely to do so in the near future.
::: I've seen theories (Roger Penrose, as I recall) that the brain is already a quantum computer.  Many of the quantum computing work that I've read about seems to involve nano-scale items. --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)
:::: Yeah - but those are definitely "out there" in terms of the mainstream.  However, even if the brain were quantum-based, there is still a major difference between the way that present day researchers are looking at quantum, biological and nano-scale mechanical computers. [[User:SteveBaker|SteveBaker]] 03:56, 7 September 2006 (UTC)

:: ''17. Fourth Generation probably ought to be in subgroups, with the bit sizes and the computer categories being separate. There are mainframes with bit sizes from 4 to 64, and ditto for embedded, personal, laptop, and server.''
:: Dangerous.  We are talking about history here.  The history of the technology doesn't go in nice linear bus width increments like you think.  We had Amdahl machines with 64 bits before we had 4 bit microprocessors.  So no - I strongly disagree.  The generations are about technology leaps - mechanical/relays/vacuumtubes/transistors/MSI/LSI - not about bus widths.
::: Take a closer look at the table; it lists both bits and types. --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)

:: ''18. What is "Death ray of Ming the Merciless"? I presume it's humor (and yes it would certainly be an output device of some kind), but I don't get the reference; was his death ray computer-controlled? If so, this is probably more appropriate (and the humor more obvious) in a section on automation and/or robotic control.''
:: It was late - I was tired - this is a temporary article.  I wondered how long it would take someone to notice it!  Feel free to remove it!
::: I don't have a problem with humorous items, but I think that they should also be correct (considering that this is an encyclopedia), instead of being a flop (or gigaflop). :) --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)
:::: I honestly would never have left it in there once the article goes "live". [[User:SteveBaker|SteveBaker]] 03:56, 7 September 2006 (UTC)

:: ''19. I would replace ", and the next instruction is fetched." with something like "and the process reapeats with the next cycle.". I would drop the "halt" comment entirely, since you never see it in most programs, as far as I know (I haven't messed with assembler in ages), and it might raise more questions with some people.''
:: The 'HALT' instruction is very common in embedded situations since it puts the processor into a powered down state.  Please stop thinking in PC terms.  Also, the concept of a program halting is key to many theoretical computational issues such as (obviously) "The Halting Problem".
::: Nevertheless, it still feels to me like excessive detail for this type of article.  I think it would be appropriate an an article on assembly or machine language, or on embedded programming, or OS-level programming (waiting for the next interrupt), all of which are well beyond this scope of this article. --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)
:::: OK - whatever. [[User:SteveBaker|SteveBaker]] 03:56, 7 September 2006 (UTC)

:: ''20. Note that every model of CPU generally has its' own machine language, and there are significant differences betwwen manufacturers, such as Intel and AMD, but they have a large number of instructions which work identically or nearly so on all CPUs in the class.''
:: Again, you are looking only at PC clones.  Look at embedded systems - they most certainly are not remotely cross-compatible.
::: I did say "class", deliberately, although that may not be the best term.  I would rephrase it as: "...but CPUs in the same class generally have a large number of instructions which work identically or nearly so on all CPUs in the class." --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)
:::: I guess it depends on what you mean by 'class'.  An Intel Pentium and an AMD of whatever generation are almost identical.  A PowerPC CPU and a Pentium are pretty close - but take a look at an ARM or a MIPS CPU - those are RISC machines and have totally stripped down instruction sets that bear almost zero resemblance to the x86 style of machine with all of it's baroque curlicues and twiddly bits.  I mean yes, on a very naive level you have ADD and SUB and MUL and MOV - but that's not what it's all about.  Look at the addressing modes.  Those RISC machines have a mere handful - x86 have more addressing modes than you can count!  We can debate this - but for the purposes of this article, it's very dubious to say that these machines are even broadly similar. [[User:SteveBaker|SteveBaker]] 03:56, 7 September 2006 (UTC)
::::: For example, the "x86" class (or whatever you'd prefer to call it).  A modern multicore 64-bit AMD consumer CPU (intended for a standard PC) should still be able to run generic machine language programs written for Intel 8088 CPUs, even though they are vastly different in many ways.  --[[User:Scott McNay|Scott McNay]] 05:44, 7 September 2006 (UTC)
:::::: But not vice-versa.  Hence the distinction. [[User:SteveBaker|SteveBaker]] 05:40, 1 November 2006 (UTC)

:: ''21. I tend to dislike the word "program", since it tends to be overloaded, as in "TV program", "TV station programming", and so forth. I try to use "Software" where I can, since it is less ambiguous. Consider a program to record TV programs, or something like that; I would expect people to get confused. My suggestion is to say that when most people talk about programs, they really refer to the more generic "software". Many people also misuse it, as in "can you program my computer?". I'd suggest saying that "program" is more of a technical term than anything else.''
:: The term is certainly overloaded in other fields - but ''software'' is too vague.  Software includes data files.  A "Software package" might include many programs - or no programs at all (just libraries).  Within the field of computing, the term has a very precise meaning.
::: Perhaps should have a note about the specific meaning of "program", then.  On a side note, I must say that I really dislike "program" used as asomething other than a noun; most mis-use of it seems to happen when it is used as a non-noun. --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)
:::: The verb "to program" bothers you?  Surely not. [[User:SteveBaker|SteveBaker]] 03:56, 7 September 2006 (UTC)
::::: The verb "to wood" bothers you in reference to carving wood?  So many people mis-use it that I think it's probably best to discourage non-specialists from using it as a verb.  "I want you to program my computer" (person typically waggles fingers while saying this) may be technically correct usage, but tells you virtually nothing about what the person actually wants.  The usual meaning of something like this is generally "I want you to install xyz for me". --[[User:Scott McNay|Scott McNay]] 05:44, 7 September 2006 (UTC)
:::::: I can't help that people sometimes use the term incorrectly.  However, amongst programmers, the verb is cleanly specified, frequently used and universally understood. [[User:SteveBaker|SteveBaker]] 05:40, 1 November 2006 (UTC)

:: ''22. Might want to mention that programming (software engineering) is still an art in many ways.''
::Heh... I'm not going anywhere near that one.  Let's just stick to the facts. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>
::: Sorry, but anyone who claims that it is not a fact is either lying or has no experience with programming, or is the world's best programmer and never has trouble making his programs print "Hello, World!". :) --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)
:: Yes.  Something I wish I could get my boss to understand!  It truly is an art form.  Any experienced programmer will look at two pieces of code - both of which solve the problem at hand - both of which are equally fast and space-efficient and he'll say "Wow!  That's a beautiful piece of code - but this other one is ugly!" - that's art.  Furthermore, show those same two pieces of code to another programmer and he'll probably come up with the opposite view.   I run a team of 5 or so programmers who all work on the same 1 million lines-of-code application - and I can tell who wrote what bits just from the coding style alone.  It's like looking at the brush strokes of a grand master oil painting and saying "I can tell that so-and-so didn't paint this - it was his understudy".  Yes - it's most definitely an art. 
::: What I meant was that there is still a lot of b'guess and b'gosh involved; the best-written programs can still crash, and it's not like building a house of cards with cards and glue; some days it's more like building a house of cards with cards and an electric fan.  --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)
:::: That's nothing to do with it being an art.  People complain about how buggy software is but they never see what a MILLION lines of gibberish really looks like and gaze in awe that not one single comma can be out of place or it'll go horribly wrong.  I'm reminded of something a Lockheed engineer once told me.  A typical Boeing 747 has a HALF TON of 'shims' in its structure taking up engineering tolerances and mechanical design errors.  Any mechanical machine of the complexity of a million line software application would never work.  The nearest machines in logical complexity are NASA spacecraft such as the Shuttle - which has close to a million parts.  It's failure rate is a lot worse than most million line software applications.
::::: Oh, you mean you're talking about reality. :)
::::: Speaking of reality, it's hellish when you're trying to debug a program, and spend days getting nowhere... because the bug turns out to be in a library. :( --[[User:Scott McNay|Scott McNay]] 05:44, 7 September 2006 (UTC)
:::: No what makes software an art is not that it's so often unreliable - "art" isn't a derogatory term here.  It is that beauty and style plays a part in what could otherwise be considered a purely rigerous structure.  There is true beauty in some code.  These five lines reverse the order of the bits in a 32 bit word.  Hell - that's BEAUTIFUL!  It's art - but only a programmer will ever understand why:

   n = ((n >>  1) & 0x55555555) | ((n <<  1) & 0xaaaaaaaa) ;
   n = ((n >>  2) & 0x33333333) | ((n <<  2) & 0xcccccccc) ;
   n = ((n >>  4) & 0x0f0f0f0f) | ((n <<  4) & 0xf0f0f0f0) ;
   n = ((n >>  8) & 0x00ff00ff) | ((n <<  8) & 0xff00ff00) ;
   n = ((n >> 16) & 0x0000ffff) | ((n << 16) & 0xffff0000) ;

[[User:SteveBaker|SteveBaker]] 03:56, 7 September 2006 (UTC)

:::::Perhaps I meant "black magic" or some such thing. :)
:::::I never knew you could reverse bits by shifting, ANDing, and ORing; I'll have to look at that sometime.  It does look beautiful, and it looks lke it can be done in parallel to some extent.  Is it faster than right-shifting one register into Carry, and left-shifting from Carry into another register, 32 times? --[[User:Scott McNay|Scott McNay]] 05:44, 7 September 2006 (UTC)
::::::That's what makes it art.  It's certainly not especially efficient - I've never seen anyone use it in an actual, functioning program - and if they did, I'd probably change it to the 'conventional' method.  However, it's beautiful...as art.  I wouldn't want to wallpaper my bedroom using a repeating Mona Lisa pattern either.  Another one I like is swapping two integer variables...normally, we'd say:
  int tmp = x ;
  x = y ;
  y = tmp ;
:::::which seems really, really, ugly to me.  It offends me in the same way that pictures of Elvis painted on velvet offend me.  But (for integers at least) you don't need to use a temporary variable if you say:
  x = x ^ y ;
  y = x ^ y ;
  x = x ^ y ;
:::::...or even (in some programming languages):
  x ^= y ^= x ^= y ;
:::::But that's art - it's not "a good way to program" - it's some peculiar thing that good programmers somehow instinctively recognise as 'cool'.  Mathematicians and to some extent Physicists have a similar sense of what is elegant in an equation or a theory. [[User:SteveBaker|SteveBaker]] 05:40, 1 November 2006 (UTC)

:: ''24. "few technical reasons why a GUI has to be tied to the rest of an operating system" -- I'm not entirely sure that I agree with this. At the very least, it needs to be tied to the shell, as is done in Unix-like OSes. The problem then is that you get applications which run with one shell but not another, which is why it is made part of the OS in the first place. See my comment above (10) about KDE and Gnome.''
:: Yep - that's got to go.  The GUI patently obviously DOESN'T need to be tied to the OS because there are plenty of examples (KDE, etc) of GUI's that are perfectly usable that aren't tied to the kernel.  They aren't tied to the shell either.   The Window manager ("GUI" is a vague term here) is at the same level in the software heirarchy as the shell - it is in fact possible to talk of "graphical shells" and "command line shells".  But you can launch a graphical shell from a command line shell and vice-versa (at least under *nix).  So one is not 'above' the other in the hierarchy.
::: I'm not familiar with how *ix OSes handle this, so perhaps I should shut up now. :)  --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)
:::: The OS has the 'kernel' which deals with stuff like process scheduling, file I/O, memory management, program loading and unloading.  There is a graphical server (the X-windows system in the case of most *nix) - which is "just another program" as far as the kernel is concerned.  It isn't really special in any way.  X provides a fairly raw experience by itself and it's designed to operate in a client/server mode where programs that need GUI facilities send messages to X...possibly over a network if you are running the program on one computer and viewing the graphics elsewhere.  Hence, we run 'Window Managers' on top of X that deal with the cute look and feel stuff and some of the nice behaviors.  Application programs run within that.  But all of these things (other than the kernel) are just programs.   None of them are 'special'.  You can replace any of them at will and that's OK.  Windows isn't like that.  The 'kernel' and the graphical environment and the window manager layers are all tangled up and near impossible to extricate.  This is the cause of many of the ills of Windows as regards malware, poor reliability and such - and it has taken truly gargantuan efforts on the part of Microsoft to try to keep up with the robustness that a clean architecture would have bought them. [[User:SteveBaker|SteveBaker]] 03:56, 7 September 2006 (UTC)
::::: They are trying to get rid of some older stuff; DOS applications are no longer supported in Windows Vista; you need an emulator for that now, and they apparently have research projects on creating a new architecture.  However, I do agree that it's top-heavy.  --[[User:Scott McNay|Scott McNay]] 05:44, 7 September 2006 (UTC)

:: ''25. Human-like robots are being sold in Japan, which is apparently desperate for them due to their aging population; see ASIMO.''
:: Yes - but a robot is just a machine that happens to contain a computer.  In fact, if you watch shows like "Robot wars" in which computers are hardly ever present, it is aparrent that the term "Robot" has lost it's connotation of "A mobile machine driven by a computer".  Robotics is a separate field from Computing and I don't think we need to say very much about it here. Robotics are an application of computers - just like cellphones, greetings cards that play "Happy Birthday" when you open them, space craft, cars, PC's, TV remotes, Furbies, dish washers....why should we single out robots for special mention?
::: Perhaps because the article mentions robots, which is what I was responding to. :) --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)
:::: Yeah - I don't think there should be much emphasis there - although people will perhaps look here for information about robots - so we need to put in something minimal to link them off to the right places. [[User:SteveBaker|SteveBaker]] 03:56, 7 September 2006 (UTC)

:Scott, basically we copied what was on the Computer article with the intention of rewriting most of it.  So what you see here still contains a lot of stuff that needs to be redone.  Unfortunately I haven't been working on this very much lately, but I'd still like to finish our plans here.  The thing is, more editors always help, especially for an article with such a broad scope.  Please feel free to change whatever you like on this page, just make a note on this talk page explaining what you've done. -- [[User:Matt Britt|mattb]] <code>@ 2006-09-06&nbsp;13:12Z</code>

::: I did indeed make some changes, but I thought most of it should be discussed first.  I didn't reply much to your comments, Matt, since you're so agreeable. :)  --[[User:Scott McNay|Scott McNay]] 03:06, 7 September 2006 (UTC)

:::: Yes - this is a productive discussion. [[User:SteveBaker|SteveBaker]] 03:56, 7 September 2006 (UTC)

== Major Rework ==
I've spent a couple of hours doing a top-to-bottom rework of this 'Temp' version.  I think we are getting close to the point where I'd like to swap it out for the main article - but before we can do that with a clean conscience, we need to make 100% sure that all of the good information in [[Computer]] is definitely present either in this new article or (preferably) mentioned in one of the articles listed as 'main' at the top of each section.  The new article - abbreviated though it is - is still vastly too big for a Wikipedia article.  The goal is 32kbytes - we're still up in the 50kbyte region. [[User:SteveBaker|SteveBaker]] 05:40, 1 November 2006 (UTC)

:We're getting there.  As I said below, I've blown away some sections that we shouldn't need for the final article.  I'm in the process of copy editing (fixing little things like em dashes), rewording a little for clarity and grammar (remove second-person portions and get rid of archaic words like "whilst"), and adding some footnotes and such.  Footnotes are great in an article like this where you want to keep the text simple and understandable, but would also like to make little side points to explain things in more depth.  I used them a lot in [[CPU]]&emdash;successfully, I think.  One stylistic thing I noticed that I'd like to think about changing is the italics and bolded words.  I understand totally what you're trying to do by highlighting key words and phrases because I did exactly the same thing when I wrote [[CPU]].  Unfortunately the climate on FAC is that any non-essential use of bolding and italics is bad, and this will probably come up later if we don't address it now.  So if it's alright with you, I'd like to cut down these styles to the barest minimum. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-01T13:01Z</code>

::OK - that's good.  The more stuff we can dump out of the article, the better...so long as it's adequately explained elsewhere.  I was a little horrified to discover that this new article is actually longer than the original one - so there is certainly a need to trim the fat out of it wherever possible.  My bolding and italicing habit is a bad one - feel free to cleanup!  'Whilst' is not archaic to British english speakers.  It's a useful word.  I can certainly recode the little C snippet into some kind of pseudo-assembler, I think that's a reasonable idea.   The traffic light example (or something like it) seems very necessary to me.  For people who truly don't understand what programs are or have any clue as to the inner workings, I find that these kinds of simple example really bring home to people both the power of programmability and it's putfalls.  I'm certainly open to discuss changing the example - or the way it's explained - but I'm reluctant to remove it completely. [[User:SteveBaker|SteveBaker]] 17:00, 1 November 2006 (UTC)

== Touched up intro ==

I tried to simplify the language of the intro a little bit and have it be a decent broad overview of computers.  It still may need a little tweaking, but I hope everyone is more or less pleased with its general content at this point.  Hopefully I'll be putting in some good work on this article within the next few days. -- [[User:Matt Britt|mattb]] <code>@ 2006-10-24T20:12Z</code>

== What remains ==

I've gone through and edited the History section to my liking.  Please make any revisions as you see fit.  I believe all that remains for this article is to give the Stored program architecture section a good rewrite.  I feel that we probably ought to say something briefly about networking and the internet (not sure exactly where to fit this in; suggestions are welcome).  Beyond that, I believe all the sections after the big 'ol links section can be eliminated as unnecessary overlap with other articles.  After that's done, we should give the big 'ol table a thorough review and post a suggestion at [[Talk:Computer]] for anyone interested to review this article before we move it to main space.  Afterwards, we probably should request this article to be [[WP:AFD|deleted]] since temporary pages in mainspace is now disallowed per [[WP:SP]].

I'll also be converting the references over to the Harvard reference style and separate the footnotes and references section.  I feel that combining footnotes and references into one section is messy and altogether horrible, and it makes much more sense to separate them.  If you have any major objections, feel free to discuss it here. -- [[User:Matt Britt|mattb]] <code>@ 2006-10-24T23:36Z</code>

== Stored program architecture section ==

This is probably the most important section in this article, so I wanted to get it right.  In trying to come up with a pretty basic criteria by which to explain it, I decided to go ahead and read through von Neumann's famous paper (a good read, by the way, it's very obvious why it is considered one of the pillars of modern computer design).  I took some notes which I'll copy here.  I've tried to note modern terms for many of the structures von Neumann described. (Sorry that this is messy, I just threw it down in a text editor whilst reading)

----

'''Core parts of the device (EDVAC):'''

CA - Central arithmetical
* Arithmetic
* Contains three storage elements; two "input" operands, Ica and Jca; and one "output" operand, Oca (registers in modern terms)
CC - Central control
* Separation of instructions and computational organs used to carry out instructions
* Instructions must be stored somehow
M - Memory
* Intermediate steps for complex internal operations (e.g. multiplication, division)
* Program storage
* Data storage
**Examples: tables for approximating complex functions like algorithms, initial & boundary conditions for diffeqs, intermediate results for iterative methods, sorting problems, statistical problems
* No separation of data and program storage!  Good quote: "While it appeared that various parts of this memory have to perform functions which differ somewhat in their nature and considerably in their purpose, it is nevertheless tempting to treat the entire memory as one organ, and to have its parts even as interchangable as possible for the various functions enumerated above."

CC and CM collectively make up C.  All data transfers between the three core components must be effected by structures contained entirely within the core components.

'''Extra parts not considered "core":'''

R - Storage (recording) medium; may somewhat blend with M

I - Input
* Organs that facilitate the transfer of information from R into C and M
O - Output
* Organs that facilitate the transfer of information from C and M into R

'''Details:'''

Interestingly, the EDVAC design used an additional bit for all numbers to distinguish instructions from data values (the number itself was 30 bits plus one bit for sign). I guess you could call this an early form of memory execution protection. :)

Memory was logically divided into "minor cycles", corresponding to one instruction or data value (a word)

Well-defined instruction set... types of "orders" (instructions):

1. Orders for CC to instruct CA to carry out an arithmetic operation
2. Orders for CC to cause the transfer of a number from one place to another
* can occur between CA and M, within M, or within CA. the second kind can be replaced by two operations of the first kind, so for simplicity, all M-M transfers are handled by two CA-M transfers (load/store instructions in modern terms)
* CA-CA transfers are equivalent to modern register-register operations
3. Orders for CC to transfer its own connection with M to another point in M with the purpose of getting its next order from there (a jump in modern terms)
* For simplicity, CC is designed to follow orders in their "natural sequence" in M, but transfer instructions exist
* Two types of "transfers": transient and permanent (the latter is the one that is most commonly called a "jump" today; the former is basically a combined "call/return" which remembered its place in memory and executed one instruction at the transfer point before returning.. von Neumann saw little use for this latter type, but later systems obviously added a proper call stack and a return instruction to support modern subroutines)

There end up only being four actual instructions for CC; one instruction includes all of the arithmetic operations for CA (as well as I/O and the CA to M transfer operations ''only'').  Therefore CA does some instruction decoding of its own, which is a little different from modern designs where all decoding is usually performed in the control unit.  The other three instructions are for jump, transfer of a constant into the CA's Ica (immediate value for a register, if you will), and a load from M to CA (CA to M was an instruction passed to and decoded by CA).

----

[[Image:Von Neumann architecture.png|thumb|right]]

The "Extra" stuff is just various details and such that I found interesting.  For the purposes of this article, I think we should key in on is the simple organization of the EDVAC.  That is, CC, CA, M, I, and O.  R probably can be omitted because in modern usage, R has come to encompass any sort of I/O device, not just the storage described for EDVAC.  The diagram to the right is nearly suitable for showing this organization.  The only problem with it is that, strictly following von Neumann's description, there should be no arrow from the control unit to the memory since EDVAC's CC never had to write to memory.  The question is, how should we explain von Neumann's model simply?  I'm inclined to stick with the diagram on the right with some slight modifications:

* Remove the arrow from the control unit to memory since there really isn't a need for the control unit to write back to memory in the von Neumann model.
* Change "accumulator" to "temporary value storage" since the latter is more self-explanatory.
* Perhaps change the names of the control unit and ALU to match von Neumann's terms.  This has the disadvantage of making it a little more work to connect them with their modern analogues, but has the advantage of not suggesting that this is an ALU in the modern sense (since a modern ALU doesn't contain registers, direct connections to memory, or direct connections to I/O)
* Make it prettier.

Of course, there's always the other option of just dumping von Neumann's EDVAC model and using a simplified model of what a modern microprocessor is more likely to look like.  Please do weigh in! -- [[User:Matt Britt|mattb]] <code>@ 2006-10-25T02:30Z</code>

Whilst this is a great description of EDVAC - it's a totally incomprehensible explanation for someone who came here to find out how his computer works.  We need to dumb-down the language considerably and provide actual examples.   Anyway - I completely rewrote the Stored Program section - splitting it into a discussion of what a program '''''is''''' - and a discussion of how a typical computer goes about executing it.  The result is a bit long - and one could argue for moving it out into a subservient article - but if there is any part of the story which needs to be on the main [[Computer]] page - it's this.  Please let me know what you think. [[User:SteveBaker|SteveBaker]] 05:46, 1 November 2006 (UTC)

:Nice work.  I like how things look in general.  I'm making some small changes; mostly stylistic with a few footnotes and extra bits of information I'd like to see.  Two things that I'd like to discuss:  First, the short C-ish syntax program in the Stored program architecture section.  I'd sort of prefer to see that rewritten in a more assembly-like pseudocode because I think that's better indicative of the kind of instructions computers follow.  C syntax is obviously more high level and takes a bit of translation before it can become something that might be executed by an average CPU.  This is a rather minor issue, though.  The other issue is the traffic light example program.  I'm really not sure whether this is necessary to have.  It's definitely a nice, easy to understand example, but I don't particularly think it adds much to what we're trying to get across in this article.  I'd like to see a little discussion on that.

:I think we're nearly good to go, though.  I blew away several sections that are either already covered in our rewritten content or are covered in other articles (the digital circuits section, I believe, has little place here).  I've left the networking and the internet section because I believe that it's a subject important enough to briefly mention here.  -- [[User:Matt Britt|mattb]] <code>@ 2006-11-01T12:52Z</code>

== GPU a computer ==

In the process of proofreading, I've come across an example that calls a CPU a computer in its own right.  Is this actually true (I don't know)?  We're taking a strong "computers are stored program machines" stance in this article, so if GPUs aren't full stored program machines, then we probably ought not call them computers.  Personally I thought even the most advanced ones were highly complex SIMD/DSP units at best.  Anyway, we're doing pretty good if this is the biggest content issue I've found so far. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-02T03:19Z</code>

: A GPU is certainly a programmable calculating machine - it can emulate a Turing machine - so according to Church-Turing it's able to do whatever any other computer can do (assuming you have enough memory and time).  They aren't von Neumann though because the programs are kept rigidly separate from the data - so we're talking 'Harvard architecture' here.  But GPU's are certainly stored program machines.  I can write a program (either in machine code, or in the '[[Cg]]'/'[[HLSL]]' or '[[GLSL]]' high level languages), compile it on the CPU - download it to the GPU and cause it to execute.  'if' statements are available - although in some machines 'for' loops are not (with loops being 'unwrapped' by the compiler).  However, the very latest GPU's can run pretty much anything you could write in (say) the C language so long as you don't want recursion or suchlike.  They are indeed SIMD's (one single instruction stream is fed to as many as 48 little processors inside the GPU in parallel - which explains the problem with loops) - and I guess they are also a little like DSP's in some ways.  But both SIMD and DSP machines are perfectly valid 'computers'...just a little weird compared to something like a Pentium. [[User:SteveBaker|SteveBaker]] 16:50, 2 November 2006 (UTC)

:Also, we might want to give the multitasking section a little more love.  Certainly interrupts are an important part of multitasking, but there are more methods to accomplish that end than solely with interrupt handling.  Maybe just a few words on this matter?  I'd write it myself, but I'm really not a software person, so I'd probably do a poor job. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-02T03:29Z</code>

:: I wanted to defer that to the main article.  All we need to do here is give a flavor of how things are done. "Less is more". [[User:SteveBaker|SteveBaker]] 16:50, 2 November 2006 (UTC)

== Pictures ==

I couldn't decide... I'm sort of between the [[:Image:Us-nasa-columbia.jpg|Columbia]] or a photo of a classy looking Cray ([[:Image:Cray-XMP48-p1010241.jpg|Cray-XMP48]], [[:Image:Cray 2 Arts et Metiers dsc03940.jpg|Cray-2]]).  What do you think?  I've also taken the liberty to add pictures here and there as I saw fit.  Feel free to change them if you find a more appropriate one. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-02T04:56Z</code>

:Also, can we please try to decide on just one photo for the intro paragraph?  I know it's hard, but I really hate stacking photos on top of one another like that.  Anyway, I'm done for the night.  I hope you're not horrified at all the changes I made (my proofreading can sometimes tend towards my own personal writing style), but I think you'll mostly approve.  I'll give this article another critical read-through and copyedit tomorrow and try to rewrite the networking section. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-02T05:22Z</code>

::I intended to have the wristwatch be backup to the statement that a modern computer could fit into a watch.  A lot of people would be skeptical about that - and adding an actual photo of a real, live, full-blown computer  makes that a complete reality.  But I didn't want a photo of a wristwatch to confuse a complete neophyte into thinking that computers were present in every wristwatch - or that computers and wristwatches are really the same thing.  I agree that two photos at the top of the article is not ideal - but I think it's justified in this case.  If this is really bothersome then I would prefer to move the photo - along with the "how big are computers" discussion somewhere further down the article. But to be perfectly honest - I'd prefer to keep it where it is now. [[User:SteveBaker|SteveBaker]] 16:56, 2 November 2006 (UTC)

:::I honestly don't think it's necessary to provide visual verification of what we're asserting.  Another thing to consider is that we're only representing embedded computers in the lead.  If we're only going to have one photo there, it's fine, but if we are to have two, I'd sort of rather see one be a different class of machine.  Anyway, I won't make a big deal about this... If you strongly want to keep it, that's fine. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-02T17:05Z</code>

::::Another way to look at this would be to dump the Lego RCX computer and look for a photo of the biggest computer we can find - ASCII-Blue or something.  Then keep two photos at the top of the article as a way of visually illustrating the spread. [[User:SteveBaker|SteveBaker]] 18:38, 2 November 2006 (UTC)

:::::That's cool with me...  I'll find out whether content produced by the US National Laboratories is free for us to use, because LLNL has a good [http://www.llnl.gov/asc/computing_resources/bluegenel/photogallery.html photo gallery] of [[BlueGene/L]], the current number one spot on the Top500 list. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-02T20:31Z</code>

:I've added several pictures to the article.  I think the text/picture ratio is pretty good now, though we could stand for a couple more images.  The only one I'm unhappy about is [[:Image:Java source2.png]], which is (I think) a very boring way to show a modern program.  However, I couldn't really think of a better way off the top of my head.  If you have any ideas on the matter, let me know.  I was toying with the idea of an experimental or novelty programming language that is unconventional, but I sort of think it might be better to stay with something highly representative of common high-level languages. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-04T20:40Z</code>

::I have a personal dislike of pictures that contain only text - and for pictures that are only there for decoration.  We can do nicer (and coloured) text within regular Wiki markup - then it's easier to edit too.  But we already have examples of programs - let's make them useful examples and explain what they are rather than dumping some random piece of code in just for decoration. [[User:SteveBaker|SteveBaker]] 22:56, 4 November 2006 (UTC)

:::Agreed, I put it there mostly because I felt we should find something to illustrate a modern program, even if it is just filler.  Any ideas? -- [[User:Matt Britt|mattb]] <code>@ 2006-11-04T23:32Z</code>

== Alternative "add 1000 numbers" program ==

Per my suggestion above, here's the add 1000 numbers program in MIPS32 assembly:

       lui   $2, 0         ; use $r2 for the sum
       lui   $3, 0         ; use $r3 for the current number
       lui   $4, 1000      ; use $r4 for terminating condition of Loop
 
 Loop: addi  $3, $3, 1     ; increment current number
       add   $2, $2, $3    ; add current number to sum
       beq   $3, $4, End   ; if current number is 1000, jump to End
       j     Loop          ; jump back to Loop
 
 End:  nop                 ; end of program; do nothing

Of course, this doesn't define any memory segments (better to leave all that out in a discussion of pure computers), but this code would work just dandy if translated to machine code and run on a MIPS R3000.  Tell me what you think.  We can change the comments or remove them altogether, if you'd like.  -- [[User:Matt Britt|mattb]] <code>@ 2006-11-04T21:13Z</code>

:Also, here's what this would look like assembled and in-memory (big-endian):
  Address       Instruction
 ----------     -----------
 0x00000000     3C 02 00 00
 0x00000004     3C 03 00 00
 0x00000008     3C 04 03 E8
 0x0000000C     20 63 00 01
 0x00000010     00 42 18 20
 0x00000014     10 64 00 01
 0x00000018     08 00 00 0C
 0x0000001C     00 00 00 00

:I'm fairly sure this is correct.  I assembled it by hand, so there may be an error, but I think it may be useful just for illustrative purposes to cement the idea that programs really do just get turned into numbers.  If we should end up sticking this into the article, it might be useful to label line numbers in the assembly with (abbreviated) hexadecimal as well. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-04T22:14Z</code>

::Here's another thought.  Maybe we could use something like this instead of the stop light example.  I think my dislike of the stop light program stems from the fact that it is in really vague pseudocode and doesn't (in my humble estimation), tell the reader what sort of things a real computer is likely to do step-by-step.  I think its abstractness may imply to some readers a level of intelligence that can't be attributed to computers.  On the other hand, within the MIPS R3000 example there are all kinds of limitations of computers (and MIPS R3000s :) that we can point to...  Very simple operations, the need for expressing an algorithm in absolute step-by-step terms, etc.
::Anyway, give it some thought.  I realize that it may be a daunting idea to dump assembly language into an example intended for the lay man, but I honestly think that with careful explanation it could be very useful in giving the reader a solid concept of exactly the kind of thing that a computer does.  As far as I can tell, an good concrete explanation of a simple (real) low-level computer program in lay terms doesn't exist on wikipedia (even [[assembly language]] fails to deliver).  Why not have it in the computer article where the reader might learn something pretty useful about computers? -- [[User:Matt Britt|mattb]] <code>@ 2006-11-04T22:29Z</code>

:::I think we need something that's easy to understand - it doesn't have to be any real assembler for a real machine - it's illustrative value is as an example.  So how about:

        mov     0,sum    # set sum to 0
        mov     1,i      # set i to 1
  loop: add     i,sum    # add i to sum
        add     1,i      # add 1 to i
        cmp     i,1000   # compare i to 1000
        jle     loop     # if i <= 1000 jump back to 'loop'
                         # all done, result is in 'sum'

:::<s>I'd rather use a real assembly language to be honest (though your point is valid and I agree with it).  Let me rewrite that thing in MIPS32 to simplify it a lot... -- [[User:Matt Britt|mattb]] <code>@ 2006-11-04T23:11Z</code></s>

:::On second thought, using MIPS32 won't really make anything simpler as such; just allow usage of a different branch condition.  I don't really see what's significantly more complicated about my R3000 example than your example.  Is it the register names?  I don't see why that would be difficult to explain...  We could easily add a store word instruction to store the result in a named memory location after the loop, too.  Anyway, what are your concerns with using the R3000 example?  I'd really rather use a real example if possible, but I won't be stubborn about it if you honestly think it would be detrimental to do so. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-04T23:29Z</code>

::::The MIPS code is totally impenetrable to people who don't know what all the mnemonics mean - heck, I've spent a significant chunk of my life writing assembler programs and '''''I can't understand it''''' - without getting the assembler manual.  It's full of distracting dollar signs that add nothing to the descriptive nature of the example - and what the heck is 'lui' anyway?!  My example is more like PDP-11 assembly - or most 8 bit microcontrollers...very simple - very easy to understand. [[User:SteveBaker|SteveBaker]] 03:30, 11 November 2006 (UTC)

:::::I don't particularly think that any assembly is going to be quickly digested by the uninitiated, but the point of using it is to show a real example of a program that directly translates into computer instructions.  I could make the argument that the average reader will have just as good a chance of understanding what the <code>for ( ; ; )</code> syntax means as they would of figuring out what lui or jle do.  We can leave comments in to explain things.  The reader doesn't have to know exactly what the mnemonics mean or what the instructions do; understanding what they do in the context of the example should be sufficient.  However, you may have a point about the distracting symbols and such, so I'll concede to using your syntax.  Can we just dispense with the "end" instruction?  -- [[User:Matt Britt|mattb]] <code>@ 2006-11-11T05:58Z</code>

::::Oh, we could just replace that last jump with a nop if you think that the infinite loop thing is distracting (went ahead and did it). -- [[User:Matt Britt|mattb]] <code>@ 2006-11-04T23:35Z</code>
:::::Let's dump the 'end' instruction from my version and use that then. [[User:SteveBaker|SteveBaker]] 12:34, 11 November 2006 (UTC)

:I hate to keep revisiting this, (please don't hate me!) but I have one final alternative suggestion for your consideration that may make us both happy.  I dug up a copy of the [http://bitsavers.vt100.net/dec/www.computer.museum.uq.edu.au_mirror/D-09-30_PDP11-40_Processor_Handbook.pdf PDP-11/40 processor handbook] (good 'ol Bitsavers) and wrote the program in PDP-11 assembly.  Turns out that it's extremely intuitive (thanks to the PDPs' rather flexible addressing system) and not much different from your example:

       mov      #0,sum     ; set sum to 0
       mov      #1,num     ; set num to 1
 loop: add      i,sum      ; add num to sum
       add      #1,num     ; add 1 to num
       cmp      num,#1000  ; compare num to 1000
       ble      loop       ; if num <= 1000, go back to 'loop'
       halt                ; end of program. stop running.

:As you can see, this is nearly identical to your example, but is actually perfectly valid PDP-11 assembly.  The only differences are the # marks to denote immediate addressing for the numerical operands and the mnemonic ble (branch less-than-equal-to) instead of jle.  In this case I think the pound signs don't detract at all.  It's very common for people to use # as shorthand for "number", which is exactly what is happening in this program.  This version is both simple and valid assembly (for a very common computer, no less) and has the extra bonus of me being able to throw in a good reference for the article (the PDP-11/40 handbook I used).  I think that this version is a great way to make us both happy (admittedly, this is a lot nicer example than the simple-but-painful MIPS).  The halt is optional for our purposes.  In basic operation it will literally halt the processor and in user mode it will generate a trap.  I don't care if we leave it or take it out.  Let me know what you think. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-11T17:15Z</code>

:Yep - that's fine.  Let's go with that then.  It's rather nice to have some history in there. [[User:SteveBaker|SteveBaker]] 22:16, 11 November 2006 (UTC)

== Big link table ==

Few things yet to be resolved with the big links table:

* Enthusiasts Clubs - anything to add here, or should we nix it?
* Output peripherals - any more major examples?
* Software - add section on programming languages?  I was thinking that programming languages is a pretty big software topic, but should we represent this topic in the table or just leave that to the programming language article?

We're getting close! -- [[User:Matt Britt|mattb]] <code>@ 2006-11-05T00:01Z</code>

: Since the (new) purpose of this article is to be a gateway to the other articles about computers, we need the Big Table'O'Links to link to as many 'second tier' articles as possible.  We need to identify the high level articles that best cover the subject - and which point down to the 'third tier' articles.  So, for example, we should definitely link to [[Computer keyboard]] - which talks about keyboards in general and is what I'd call a 'second tier' article - but not to [[IBM PC keyboard]] because it is the task of [[Computer keyboard]] to enumerate all of the tiny distinctions between keyboard types and to link down to those.
:; Enthusiasts clubs : I think we need links to articles about enthusiasts clubs.  Ideally, we should find an article about computer clubs in general - but if there isn't one, we'll just have to list a bunch of clubs.
:; Output peripherals : (or peripherals in general) is a little tricky because things like stepper motors, solenoids, LED's...all of those things are peripherals to (for example) the computers inside robots. But I don't think it's right to list them.
:; Programming languages : Yes - we need a programming languages section - but I think it needs to be separate from software.  A "compiler" is a piece of software - but a programming language isn't necessarily a piece of software.  You could write in a language for which there is no compiler (as for example [[Ada Lovelace]] did (because she had to!) - but also [[Edsger Dijkstra]] who described his algorithms in a language of his own invention - for which a compiler was never written).  So I think we should simply add another table for programming languages.  I'll start that now.
[[User:SteveBaker|SteveBaker]] 13:54, 5 November 2006 (UTC)

::As far as peripherals are concerned, I think we should more or less ignore embedded computers since it makes the selection far to vast to effectively list.  -- [[User:Matt Britt|mattb]] <code>@ 2006-11-10T22:28Z</code>

== British vs US english ==

In my copyediting I've noticed that the article is inconsistantly using British and US variants of words.  This isn't surprising since I use US English and Steve uses British English.  We need to decide on one variant or the other and go with it.  I'd tend towards US English since the majority of the text seems to use US spellings and I'm doing a lot of the copyediting, but I wouldn't care if the article was written with UK word variants as long as everything is consistant.  Let me know. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-10T21:50Z</code>

: Yeah - I agree - the article was started in US english - so it should stay that way.  Please feel free to fixup anything that looks wrong. [[User:SteveBaker|SteveBaker]] 22:46, 10 November 2006 (UTC)

This is an interesting point. It is interesting because the page does indeed seem to be written by an American. I know this not because of the language used, but by the outrageous bias towards American claims of the invention of programmable computing. It is understandable how the general public, especially the Americans, would not be aware of Bletchley Park's achievements. Station X was Top Secret, and its existence has only been disclosed within the last 30 or so years, under a freedom of information act. Also, the time of an invention and when a prototype was manufactured, are two seperate dates. Either way, Alan Turing, who by all rights is why the West "won" World War II in the first place, invented the Universal Turing Machine no later than 1936. 
**********        
Now unlike Alan Turing, Im not a mathematical genius, but I can calculate that 1936 is a good 5 years before 1941 (which is a German claim in any case). Not only that, but the British Charles Babbage, technically, invented the first computer as early as 1822. It was simply called the Difference Engine, and used Boolean Logic, and some time before then and his death in 1871, he had invented, if not built (funding problems) a Turing-complete computer. 
 
Its a shame for those true British heroes that their countrymen cannot edit the actual page that researchers will read, and be misinformed by, to the point that in the future, America will be yet again given the credit for British achievement. 

ps little tip: Great Britain invented almost Everything. go on, put it to the test.

22:46, 12 November 2006 <small><span class="autosigned">—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:84.192.146.246|84.192.146.246]] ([[User talk:84.192.146.246|talk]] • [[Special:Contributions/84.192.146.246|contribs]]) </span></small><!-- Template:Unsigned -->

:Entire books have been written on the subject of just what the first "computer" is.  I've read plenty of history on the subject and am mildly annoyed that you seem to think that we have given unilateral credit to Americans.  Please take the time to read the history section before making wild claims like this.  If you still have issues, please bring them up here and we'll try to address them.  Turing's work is indeed important, but regards computability more than actual computer implementation.  Von Neumann gets a lot of credit simply because he was one of the first to pen a concrete design for an electronic stored program computer that saw implementation.  We never claim that von Neumann invented the concept of stored programs; nobody singlehandedly did.  We never even arbitrarily claim what the first computer was, preferring to list a few devices that may or may not be considered the first computers.  Zuse certainly did important work (and we mention him), Colossus was a very significant achievement from the Brits (which we mention), the British even beat von Neumann to implementation with Baby and EDSAC (which we also mention), we even speak first of Babbage.  Where is this American bias you are talking about?  If the mere mention of von Neumann and the ENIAC team is an American slant, then I'm afraid we can't help you.

:P.S. - That claim neither helps your argument nor has any shred of truth.  The very nature of the word "almost" should indicate how useless the statement is.  Want an example?  Ohhh... [[Transistor]] (possibly implemented first by a German, but developed for commercial use by Americans). -- [[User:Matt Britt|mattb]] <code>@ 2006-11-12T22:06Z</code>

::Also, the difference engine was not programmable.  You're thinking of the analytical engine.  We mention both of these in the article. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-12T22:31Z</code>

:: Without being partial, I must say more people in the world prefer British English.--[[User:Darrendeng|Darrendeng]] 09:01, 30 November 2006 (UTC)

::: You could use that 'popularity' argument to argue that the entire Wikipedia should be in <insert your favority flavor of English here>.  But that's not how the rules work.  The rule is that if there is a compelling reason to choose based on the subject matter then use whatever is most appropriate and stick to it.  So an article about [[Sussex]] should be in Brit-English, an article about [[Texas]] should be in US-English, an article about [[Brisbane]] should be in Aussi-English.  But if there is no compelling regional content in the article then whichever flavor of the language the article was initially written in should continue to be used.  There is nothing uniquely British, American, Australian - or whatever about [[Computer]] - it's a totally international phenomenom.  Hence, since this article happened to start out in US-English and must therefore stay that way.  This is a good rule because we really don't want edit wars about colour/color, tyre/tire, and -ise/-ize.  (PS, I am a Brit) [[User:SteveBaker|SteveBaker]] 13:09, 30 November 2006 (UTC)

== Ready to move? ==

I just gave the [[Computer]] article a look-over and have come to the conclusion that there's nothing left there that isn't covered or appropriately linked here.  I think this article is nearly finished save for a few minor items and some copyediting.  The only thing this one lacks compared with the current main article is that table comparing some of the machines leading up to stored program computers.  I like that table, but I don't know if its really necessary or how to integrate it into this article.  Any thoughts on that?  If you don't think we should keep the aforementioned table, let's go ahead and move the article. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-10T22:02Z</code>

: That table turns out to be a Template - so I stuck it into the History section in about the same place it is over at [[Computer]].  It fits into the flow of the text surprisingly well.  I'm happy to move the article anytime - .  Should we literally move it using the 'move' button - or copy/paste 100% of the text?   I'm not sure of the consequences for version history, the semi-protection status, etc, etc.  [[User:SteveBaker|SteveBaker]] 22:58, 10 November 2006 (UTC)

::That's a good question...  I'll figure out the answer in a few minutes and you'll see the result. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-10T23:03Z</code>

:::Woah...wait a minute.  We only have 3 references in the new article - the old one has 17 - we need to transfer as many of them as we can. [[User:SteveBaker|SteveBaker]] 23:17, 10 November 2006 (UTC)

::::Most of the references in the old article were either very weak web references or don't really apply to the new article.  I'll give them all another look-over, but I'm not a big fan of throwing in a bunch of weak weblink refs just for the heck of it. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-11T05:45Z</code>

=== References from the old Computer main article ===
We need to put these references back into the article someplace.
<pre><nowiki>

 <ref name="antikythera">{{cite web | author=Phillips, Tony | publisher=American Mathematical Society | year=2000 | title=The Antikythera Mechanism I | url=http://www.math.sunysb.edu/~tony/whatsnew/column/antikytheraI-0400/kyth1.html|accessdate=2006-04-05}}</ref>
 <ref name="Schickard">{{cite web | year=Unknown | publisher=computerhistory.org | title=Visible Storage | url=http://www.computerhistory.org/VirtualVisibleStorage/artifact_main.php?tax_id=01.01.06.00|accessdate=2006-04-05}}</ref>
 <ref name="shannon">Shannon, Claude Elwood (1940). [http://hdl.handle.net/1721.1/11173 A symbolic analysis of relay and switching circuits]. Massachusetts Institute of Technology: Thesis (M.S.)</ref>
 <ref>{http://scienceworld.wolfram.com/biography/Shannon.html Biography of Claude Elwood Shannon] - URL retrieved [[September 26]], [[2006]]</ref>.
 <ref>{{cite web | author=Unknown|title=IA-32 architecture one byte opcodes|publisher= sandpile.org| year=Unknown | url=http://www.sandpile.org/ia32/opc_1.htm | accessdate=2006-04-09}}</ref>
 <ref>{{cite web | author=Kanellos, Michael | title=Intel: 15 dual-core projects under way | publisher= CNET Networks, Inc.| year=2005 | url=http://news.com.com/Intel+15+dual-core+projects+under+way/2100-1006_3-5594773.html | accessdate=2006-07-15}}</ref>
 <ref>{{cite web | author=Chen, Anne | title=Laptops Leap Forward in Power and Battery Life | publisher= Ziff Davis Publishing Holdings Inc. | year=2006 | url=http://www.eweek.com/article2/0,1895,1948898,00.asp | accessdate=2006-07-15}}</ref>
 <ref>The last of the first : CSIRAC : Australia's first computer, Doug McCann and Peter Thorne, ISBN 0-7340-2024-4.</ref>
 <ref name="toms-tcount">{{cite web | author=Thon, Harald and Töpel, Bert | publisher=Tom's Hardware |title=Will Core Duo Notebooks Trade Battery Life For Quicker Response? | year=January 16, 2006 | url=http://www.tomshardware.com/2006/01/16/will_core_duo_notebooks_trade_battery_life_for_quicker_response/ | accessdate=2006-04-09}}</ref>
 <ref name="WindowsXP-size">Tanenbaum, Andrew S. ''Modern Operating Systems'' (2nd ed.). Prentice Hall. ISBN 0-13-092641-8.</ref>
 <ref name="ibm-pr">{{cite press release | publisher = IBM Data Processing Division | date = April 7, 1964 | title = System/360 Announcement | url=http://www-03.ibm.com/ibm/history/exhibits/mainframe/mainframe_PR360.html}}</ref>
 <ref>{{cite web | title=Classical Super / Runaway Super | year=Unknown | publisher=Globalsecurity.org|url=http://www.globalsecurity.org/wmd/intro/classical-super.htm|accessdate=2006-04-05}}</ref>
 <ref>The last of the first : CSIRAC : Australia's first computer, Doug McCann and Peter Thorne, ISBN 0-7340-2024-4.</ref>
 <ref>{{cite web | author=Brown, Alexander | title=Integrated Circuits in the Apollo Guidance Computer | year=August 22, 2002 | url=http://hrst.mit.edu/hrs/apollo/ic | accessdate=2006-04-05}}</ref>
 <ref>{{cite web | year=Unknown | title=Technological Innovation and the ICBM | publisher=Smithsonian Institution|url=http://www.hrw.com/science/si-science/earth/spacetravel/spacerace/SpaceRace/sec200/sec270.html|accessdate=2006-04-05}}</ref>
 <ref>{{cite web | title=North America Internet Usage Stats | publisher=Internet World Stats | year=April 3, 2006|url=http://www.internetworldstats.com/america.htm#us|accessdate=2006-04-05}}</ref>

</nowiki></pre>


== A computer program on a punch card? ==
I suppose one could create an entire 80-byte program, but saying that an entire program could fit on a computer card is misleading. Most programmers will tell you that most computer programs took hundreds, if not thousands, of cards. [[User:74.100.135.139|74.100.135.139]] eric

:True, but the punch card was just an example.  It's not really critical that we go into detail here since, as you said, it's not at all inconceivable to have a program that small. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-12T18:50Z</code>

::The very important 1401 bootstrap program took less than a full card.  [http://www.examplehttp://www.chac.org/engine-ascii/engv1n1.txt]  Search that page for "1401 bootstrap program".  The 1401 itself was vastly important, so much so that most IBM S/360 Model 30 computers sold were ordered with the 1401 emulation feature. <small><span class="autosigned">—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:70.231.146.45|70.231.146.45]] ([[User talk:70.231.146.45|talk]] • [[Special:Contributions/70.231.146.45|contribs]]) </span></small><!-- Template:Unsigned -->

== Remarks 11/12/06 on the new "Computer" article ==

I chatted with uberpenguin and here are some of my comments.  They're all over the map, from simple typos to suggestions to additional important topics to cover to sections that should be completely rewritten (History) to have a contemporary emphasis (lots has happened since 1960 but you wouldn't know it from the History section).

1. Take the first sentence's links and make paragraphs from them.  Make a paragraph for "machine".  Make a paragraph for "data".  Make a paragraph for "instructions".  You already have the last, but you need to have all three, in order.  (Perhaps this exercise will make you ponder whether a better word than "machine" could be used.)

: The intent is that this be a gateway article that mainly exists to point to the deeper articles out there.  That first sentence is deliberately short and to the point - expanding it out into a bunch of paragraphs would remove it's meaning.  This is an electronic encyclopedia - and we have links...let's use them. The word 'machine' is carefully chosen - and I'm well aware of it's meaning. [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)

2. Herman Hollerith's name is not spelled with two n's.  (oops, you fixed it)

:Fixed. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:34Z</code>

3. The huge computer room picture should be captioned Columbia, not Colombia, I believe.  But a still cooler picture, and more contemporary at that, is the one at http://en.wikipedia.org/wiki/Computer_cluster

:Fixed the "Colombia" typo. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:34Z</code>
::Rather than write complaints about simple typos here in the talk page - please just fix them. [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)

4. The article does not even mention Apple or Mac computers and gives too little emphasis on the evolution of the modern-day computer from the punched-card accounting machine.  (But then wikipedia doesn't even *have* an article on accounting machines!)

:We only briefly mention PCs.  Macs fall under the broad category of PCs.  Personally I think our history section is plenty long enough for the purposes of this article.  Of course, we're taking a strong "computers are stored program machines" stance in the development of this article's text, which explains why we quickly skim over the history of computers after the development of the stored program computer. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>
:: There are any number of computers that we haven't mentioned - what about Clive Sinclair's contribution?  What about the Commadore PET, the Tandy TRS-80, the Atari ST and the Amiga?  There are vastly too many to mention - this article is already nearly twice the size that Wikipedia recommends articles should be.  That's why we have the huge links section at the end.  I agree that Wikipedia doesn't have an article about accounting machines - and it probably should.  Just as soon as someone writes it, we'll add a link to it here. [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)
:::The Osborne is not a mainstream computer, of course.  I don't suggest you document obscure machines.  Intels, Powers, Sparcs, IBMs are mainstream computers.  Richard Hitt

5. The History section gives information almost entirely before 1960; but the history of the modern General-Purpose Digital Computer actually dates from the IBM System/360 announcement, in 1964.  A huge amount of evolution has occured since then.  The IBM Personal Computer announced in 1980 began the mind-boggling progress to the present day, coupled with Bill Gates's Windows.

:See my last comment.  What you're saying is valid, but this article focuses on computers as stored program machines in order to help clearly define what should and shouldn't go here.  Even though this may cause some bits of computer history to be omitted, I think it was the right choice (plus the [[history of computing hardware]] article is very good and covers all this). -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>
::Once again, this is a gateway article.  We link to the full 'History of Computers' article.  We only have space for a sentence or so about each major era. [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)
:::I still think you put too much emphasis on old history and skimp on new history.  Richard Hitt

6. You could mention what happens when a computer makes a mistake.  There is memory-checking logic and CPU-checking logic to cause a special program to take control and analyze the failure.

:Something to this effect could be added to a footnote.  I don't really think it should go in the main article since it may interrupt flow a bit. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>
:: Firstly, the computer doesn't make mistakes (or at least so amazingly rarely as to be irrelevent) - the mistakes are in the programming.  I'm not sure what 'memory checking and CPU checking logic' this is - but in the 35 years I've been working with computers, I've never come across this magical thing. [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)

:::The IBM 7094 with its 32,768 36-bit words amazingly didn't even have parity checking for its memory.  The 360 started out with parity checking for its memory, as did the 1401.  Later 360s used CRC (Cyclic Redundancy Check) which is capable of detecting two-bit errors in a 64-bit memory word and correcting single-bit errors.  Furthermore all IBM S/360, S/370, on up to the present day, have a so-called Machine-Check interrupt.  When the machine detects an error in itself, it triggers a machine check to give control to the machine-check interrupt handler.  The IBM PC started out with parity-checked memory; I haven't kept up but I'm sure it is at least that good.  My computer experience is at a low-enough level that I had to know about these features, starting in 1966.  The reason you think computers don't make mistakes is because of these hardware features and because of software that people like me wrote.  Holler for references; they should be all over the place.  Richard Hitt

7. The concept of subroutine calls is essential to modern programming.  The concept of stacks is essential to all unix-like operating systems, such as Windows and Linux.  You should speak of these hardware features.

:I agree, we need to at least mention subroutines, good catch.  Stacks I'm not so sure about. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>
:: Once again - we have limited space.  We need to discuss what else we can toss out of the article - not what additional fat we can add.  Detailed discussion of these details belong in daughter articles. [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)

8. The modern computer is called technically a General-Purpose Digital Computer, and you should introduce this term.  As to Special-Purpose Computers, such as embedded computers, you should spend scant time but provide a link to a Sepcial-Purpose Computers page.  As to Analog Computers, likewise.

:General purpose?  Special purpose?  Anymore those terms are pretty fluid and can be evaluated on a per-case basis, I think.  Most supercomputers are special purpose, but they often use general purpose hardware.  Most embedded computers are special purpose, but they often use the same types of hardware used in "general purpose" computers.  I think there is a distinction, but that it's too weakly defined for us to be able to tersely state it in this article.  Just my opinion. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>
:: If there are more articles that we havn't linked to - please add them into the links tables at the end.  There is already a link to analog computers there.  But if there is no article - we can't link to it. [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)

9. You should probably mention today's hot topic of virtualization, which will become increasingly important:  http://en.wikipedia.org/wiki/Virtualization

:Hmm... Yeah, I'd support adding brief mention of it.  Maybe into the link tables? -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>
:: Yeah - it could go into the links table if there is an article about it. [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)

10. Instead of a photo of ferrite-core memory, choose a photo of contemporary computer memory, either of a typical memory-expansion card or of a memory chip.  Search images.google.com for:  memory chip.

:Heh... My choice of core memory was intentional.  Pictures of IC packaging are exceedingly boring, pictures of SRAM/DRAM organization are way too technical to just throw into the article, and I don't have a good, high-res photo of a DRAM die (which might actually be interesting to use).  If we can find a free high-res photo of some DRAM (or I can find the time to take one), that might be nice to put in the memory section, but until then I'd rather have an eye-catching picture than one that favors modern technology.  The image is simply for illustrative purposes, anyway. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>
:: Yep - I agree with Matt.  Everyone knows what a chip looks like - let's pick things people havn't seen. [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)
:::I understand your point of view and at first blush I agree with it.  But consider this.  Most people nowadays think of add-on memory as that long bar of chips that they recognize if they dare to open a computer and have a book illustrating what's stuck on the motherboard.  These only slightly sophisticated users would be done a disservice by showing them a picture of a thing they have NOT seen nor are likely to see.  That was my complaint about the picture of ferrite cores (doing justice to that picture would of course require explaining x and y and sense lines).  I favor the viewpoint that we should show them pictures of parts they HAVE seen, so they can go, "Aha!  Now I know what that thing is!"  Richard Hitt

11. If you're going to give an example of a binary machine instruction, don't give a MIPS32 example but give an Intel x86 example.  Save old or obsolescent illustrations for the History section.

:MIPS32 isn't old or obsolete.  Furthermore, instruction decoding is much more straightforward than x86 since it's a fixed-length instruction word architecture.  As with the core memory, it's just an illustrative example. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>
::Why Intel?  I don't know what the current figures are - but three years ago there were more MIPS processors in the world than x86 processors.  You are making the severe (and all too common) assumption that all computers are little boxes that run Windows or MacOS.  Embedded computers are VASTLY more numerous and actually have a larger impact on society than PC's. [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)
:::Actually I'd pick a S/360 assembly program; it's much simpler than the x86, on the order of a MIPS.  But I do like the simplicity and the self-explanation of the assembly code for the MIPS32 that you picked.  Richard Hitt

12. Does a GPU actually contain multiple, possibly 50, cores, that is, processors?  The linked article doesn't verify this.

:That might be a good one to add a ref on. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>
:: The present nVidia 6800 ULTRA has 48 fragment processors and 16 vertex processors for a total of 64 processors.  I wouldn't call them "cores" though since that's a little misleading.  We could probably reference an nVidia technical site for that. [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)

13. Note http://en.wikipedia.org/wiki/Computer_cluster -- you could modernize some of your supercomputer hyperbole by noting that many of the duties that supercomputers did are now done much more economically by large clusters of commodity PCs running Linux.

:Yeah, it wouldn't hurt to add a sentence about clusters into the multiprocessing section. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>
:: No!  Many of the jobs that supercomputers did 10 years ago can now be done by desktop machines - but the jobs that present supercomputers do today are just as far out of reach of todays desktop as 10 year old supercomputer tasks were from the reach of PC's of 10 years ago.  All machines (both PC's and supercomputers) are growing in speed at about the same rate - and we'll never run out of problems that require supercomputers.  [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)
:::That's not really true, sad to say for supercomputers.  With the possibly exception of stuff like weather forecasting.  Render farms for the horrendous but very parallel job of computer animation are huge combinations of guess what?  x86 commodity boxes.  Google has half a million computers in their various datacenters, and they each and every one are guess what?  x86 commodity boxes running Linux.  Probably the Render Farms run Linux too.  Look at that picture from that wiki page I cited; you will see a HUGE complex of computers all guess what?  x86 commodity boxes running Linux, well, those look too well packaged and racked to be exactly commodity boxes but I'd bet they're x86.  I bet that NSA uses more and more clusters of etc, and they are cryptographic crunchers.  The NSA machines in the AT&T building at Folsom in SF are either Sun boxes or that Mountain View company whose name I forget, which contains the real winnowing code, I think.  Yes, I know that Cray, owned now by SGI?, is still making something.  Richard Hitt

14. For your program example of summing 1 .. 1000, why not show an example of computing the formula (n ( n + 1)) / 2?  And then mention how this better program takes one one-thousandth of the time of the first program.

:I dunno.  I think simply adding that in to the article as it stands would be unnecessary bulk.  Personally I'd rather use something like that for the program "Example" section rather than the stoplight program, but I'll defer to other editors here.  -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>
::The stoplight program shows the that computers don't just crunch numbers.  The 1...1000 example is merely there to show how much quicker it is to write the program than to use a calculator.  The n*(n+1)/2 part is there to point out that computers can take longer to do some jobs than humans because they don't think about what they are doing. [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)
:::My take is that it's poor programming, not stupid computers, that's the reason computers might take longer to do some jobs than they should.  I would make this point by showing the assembly code for the one-step algorithm as well as for the thousand-step one.  Or of course, show no assembly code for either; there's not much need to show any assembly code in a beginners' computer article.  Richard Hitt

15. Footnote 9  proclaims a software bug when it should merely note a possibly unexpected behavior.  The program may indeed have meant not to start a blink sequence until a full normal cycle has occurred.  There are safety reasons for doing exactly this in the real world with real traffic signals.

:I wanted to show how easy it is for bugs to occur - even in the most simple programs imaginable.  Yes, I suppose it could have been intended for it to work like that - but the same could be said of any program that misbehaves.   It's a nice simple example that anyone can understand - we can't be putting deep, complex stuff in here.  Remember, no qualified programmers would be expected to gain any useful information from this section.  It's there for people who have so little clue as to how a computer works that they'd type "Computer" into Wikipedia to find out.  [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)
::Yes, and it could be explained further; I'm not sure a naïve reader would do anything but scratch his head.  I've forborne suggesting tightening-up of the whole article, but a good editor, a technical editor, could do a lot of it.  It is not much subtler if at all than other parts of the page to say that a "bug" amounts to "unexpected behavior".  Richard Hitt

16.  Footnote 12.  Does flash memory wear out after repeated uses?  I don't think so.  See http://en.wikipedia.org/wiki/Flash_memory.  You should provide a reference that flash memory wears out or delete the statement.

:Yes, flash memory has a finite and comparatively (to DRAM) small number of write cycles before the floating gates stop working correctly.  I'll find a ref. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>

::[http://www.samsung.com/Products/Semiconductor/NANDFlash/SLC_SmallBlock/1Gbit/K9K1G08B0B/ds_k9k1g08x0b_rev10.pdf Here you go].  Page 2: ''"Endurance : 100K Program/Erase Cycles"''.  I'll add this as a ref to the article. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T03:13Z</code>
:::That sure is a technical pdf!  Maybe you could simply drop the issue of this flash memory problem?  I don't doubt that technology will improve flash memory to the point that it's much better.  Richard Hitt

:::Actually, here's a much much better reference that explains the failure mechanism (in case you're interested).  I'll use this one instead.  {{cite paper | author = Cappelletti P., Bez R., Cantarelli D., Fratin L. | title = Failure mechanisms of Flash cell in
program/erase cycling | publisher = IEDM Technical Digest | date = 1994}} (page 291). -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T03:17Z</code>
::::Is there a URL for that?  Richard Hitt

17.  Footnote 13.  Your link to instruction-set architectures only lists two 64-bit architectures, and one of them, the IA64, is a radical departure from the Intel x86 architecture.  Thus when you say, "All of the architectures mentioned here existed in 32-bit forms before their 64-bit incarnations were introduced.", if by "here" you mean the supplied link, you're wrong.  By the way, in general a compatible 64-bit architecture differs from its 32-bit predecessor not in the substance of its instructions but only in the size of its address space and its registers.

:You're right, the "here" was ambiguous.  I've tried to fix that. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>

18. Speaking of bugs, an important topic these days is "exploits".  Clever if unprincipled programmers create special programs to gain unauthorized control of a computer, perhaps via the internet, by exploiting a known weakness (bug) in a program (say, a web browser).  See http://en.wikipedia.org/wiki/Exploit_%28computer_security%29  You should mention this in your discussion of bugs, especially around the text "Errors in computer programs are called bugs. Sometimes bugs are benign and do not affect the usefulness of the program, in other cases they might cause the program to completely fail (crash), in yet other cases there may be subtle problems."  That is to say, sometimes benign bugs are exploitable and turn malign.

:I think that's a reasonable suggestion.  Couldn't hurt to add a sentence to tie in exploits. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>
:: Yep - I agree.  Holes that can be exploited can be regarded as bugs.  We should (briefly) explain that. [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)

19.  it's should be its in the Memory section in the text, "... slower than conventional ROM and RAM so it's use is restricted ...".

:Fixed. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>
:: Again - PLEASE just fix simply typos, spelling and grammar in the article rather than complaining about it here.  [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)
:::I would have if I could have.  I haven't signed up yet, and so the document wasn't editable for me.  Sorree.  Richard Hitt

20. moder should be modern in the text, "The computer in the Engine Control Unit of a moder automobile ...".

:Fixed. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>

21. Far more important than the von Neumann machine to the history of modern computing is the concept of upward compatibility.  You treat it only slightly, but the fact that a computer program doesn't have to be rewritten to run on a bigger newer machine of the same type (IBM S/360 through zSeries; Intel 8088 through Pentium) is far far far more important than the concept of the stored-program computer.  This historical Tipping Point occurred in 1964 with the announcement of the S/360, and IBM demanded of Intel with the advent of the PC that it do the same.  Otherwise, 386 programs would not work on a 486, 586, ....

:While it's definitely important, I wouldn't call it more important than the stored program concept.  The latter is the very definition of a computer.  However, I'm not opposed to treating upward compatibility a little more than we have. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T03:08Z</code>

22. Sorry to harp on your History section, but see http://en.wikipedia.org/wiki/History_of_computing_hardware_%281960s-present%29  -- and even THAT page misses the vast impact of upward compatibility on the computers of today.  Your history section should deal mostly with that page and not with the ancient history of http://en.wikipedia.org/wiki/History_of_computing_hardware, or only very very briefly.

:No need to apologize.  Your point is valid, I just disagree with it in the interest of keeping this article terse and deferring to the history of computing hardware for a fuller story. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T03:08Z</code>
: Again, we are at 56Kbytes of text - and the goal is <32Kbytes - we have to find ways to shorten the article - not expand it.  If more description is needed in History of computing hardware - then fine - add it there. [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)
::I certainly agree about the shortness issue.  I'm a newbie wikiwise but I can imagine that once it's out and public the trend will be to lengthen it, with various edits from the public.  I personally think much can be removed from this document without compromising its introductory goal, e.g., the assembler-language examples.  Richard Hitt

23. The description of a CPU as constituting a Control Unit and an Arithmetic-Logic Unit is what I learned decades ago.  Terminology has changed to things like "instruction fetch" and "execution" units.  The pipelining page, http://en.wikipedia.org/wiki/Instruction_pipeline, gives a good example of both pipelining and (take just the top line of the diagram) unpipelined processing.  IF fetches an instruction.  ID decodes that instruction.  And so on, as illustrated at points 1 through 5 on that page.  The wiki page http://en.wikipedia.org/wiki/Arithmetic_logic_unit appears to be based on the seventh edition of the Stallings book, and I suspect he's let it get rusty since the first edition.

:Yeah, and von Neumann called them the Central Control and Central Arithmetic units, respectively.  We just chose common terminology for both.  I think it's fine as is; there are still architectures that have very simple control units.  -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T03:08Z</code>

24. Numbers are what computers work with, but programmers must work with them too, often in close harmony with computers themselves.  The binary number system uses only 0s and 1s (for instance, 69 decimal is 1000101 binary) but more friendly systems are in use.  The hexadecimal system is widely used and uses the digits 0, 1, ..., 9, a, ... f, where 'a' represents decimal 10 and 'f' represents decimal 15.  And of course 0x10 represents decimal 16.  Thus 69 is 0x45 = 4 times 16 plus 5.

:I'm on the fence about this one.  If we mention the usage of various radix numeral systems it hsould be very brief. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T03:08Z</code>
::Might I humbly suggest you take my very sentences above and stick them in somewhere, almost verbatim?  I intentionally wrote them for that possibility; I should have said so.  Richard Hitt

:: It's probably worth a mention when we are talking about the progression from machine language to assembler.  That's where programmers start using hex.  [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)

Well, that's it for me for the moment.  Let me know what you think.  I haven't gone back and reread for typos as I usually do, because I have laundry to fold, so please excuse all the typos.

Richard Hitt rbh00@netcom.com

:Thanks a lot for your comments!  You've made several very helpful suggestions. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-13T02:59Z</code>

::Yep - thanks!  [[User:SteveBaker|SteveBaker]] 06:06, 13 November 2006 (UTC)

:::And thanks for putting up with my devil's-advocacy.  Sitting ducks are always the easiest to shoot at.  I think several sections of the document could be deleted with simple links for the curious:  the assembly code; much of the (early) history section; How many bits were in the bus for 8088, ..., x86-64 CPUs.  You could give a short jazzy section saying how much speed and capacity have increased for CPU, RAM, disks between 1980 and today for the ubiquitous PC, with links for further research; the combined order of magnitude of increase is somewhere around 10^12, truly astounding.  You could better use existing Wikipedia pages, and I have noted several throughout my initial commentary points.  Many many more exist to be exploited, I'm sure.

Richard Hitt rbh00@netcom.com

::::Richard: Thanks for your further clarifications - I hadn't noticed that you do not yet have a Wiki account and are therefore barred from editing this article.  May I strongly urge you to get a Wikipedia account - it takes you one minute - and after you've had it for a few of days you'll be able to edit this (and other) semi-protected articles and to sign your comments with four '~' characters.  You'll also have your very own Wiki user page with associated 'Talk' section.  Sadly, this article was being vandalised dozens of times per day by people with no Wiki accounts - and we were being overwhelmed by the effort it took to continually fix up the damage.  So we had to prevent them from editing by the 'semi-protection' mechanism which has the unfortunate side effect of locking out people such as yourself who have valuable contributions but have not yet created accounts.   Serious contributors (such as you are obviously becoming) are expected to get accounts - but even if you plan on just an occasional edit, an account is worth having.
::::Also, you talk about "once (...the article...) is out and public" and "edits from the public".  Let me make it abundantly clear: This article is already 'out and public' - all Wikipedia articles are, all the time - and I '''''am''''' ''the public'' - as is everyone else who ever edited any Wikipedia article anywhere! You and I and everyone else who contributes are on exactly equal footing - so if you disagree with what I've written, get in there and fix it (but don't be surprised if I subsequently 'unfix' it!) - we work by consensus  and your voice is as valuable as anyone else here (well, it will be if you '''''create a user account right now!!!!''''') [[User:SteveBaker|SteveBaker]] 15:42, 13 November 2006 (UTC)

25. I would like to add a link to Prof. Sir. Maurice Wilkes (http://en.wikipedia.org/wiki/EDSAC) work for the Lyons Tea Shop, to produce the LEO (Lyon Electronic Office) (http://en.wikipedia.org/wiki/LEO_I) - which i think is the first real computer. 28Jan07 Alex Wells

:On what grounds would you say that the LEO I is the "first real computer"?  The wikipedia article indicates it ran its first program in 1951, several years after the "firsts" we enumerate in this article.  -- [[User:Matt Britt|mattb]] <code>@ 2007-01-28T22:11Z</code>

== Future technologies & Crystal Balls ==

I agree that we shouldn't be talking about technologies that don't exist - but we can (and should) talk about the directions that reesarch is leading.  So I changed the table entry to 'Research technologies' rather than 'Possible future technologies' to better represent what we're talking about here.

All four types of computer have in fact been demonstrated at some very basic level in the lab: [[Quantum computer]] with just one or two q-bits have been shown to be capable of doing calculations (and remember that you need very few q-bits to make an insanely powerful computer).  DNA computers have been used to solve the Travelling Salesman problem and calculations of prime numbers - and someone recently demonstrated a tic-tac-toe playing program using some variety of chemical computer.   Optical switches and gates are actually used in some extreme niche applications (I actually use such a system from time to time - so I know this for a fact) - but none of them have been used to create an entire computer yet.  But all of those are definitely viable computer technologies - and we would be missing valid information to pretend that they simply don't exist just because they are in the research phase.

But there is a deeper matter at stake here.  '''This article is here mostly to link to other Wikipedia articles''' and to organise those links into a coherent context.   There '''are''' pretty good articles written about Quantum, DNA, Chemical and Optical computers - and it is the job of this article to point them out to our readership.   If it were decided that any of those articles were too forward-looking for Wikipedia (I don't think they are) - then those articles should be put up for AfD - and if/when they are deleted then the link to them should be expunged from this article.  However, it is not our job to second-guess the AfD folks or to offer summary judgement that an article is unworthy - to do so would be seocnd-guess AfD which would be a breach of Wikipedia process.

[[User:SteveBaker|SteveBaker]] 05:32, 14 November 2006 (UTC)

:Yup yup...  Agree with your actions and rationale.  As an aside, it seems apparent that quantum computers will be great at factoring numbers, but I'm not sure if that in itself leads to a better computer.  Certainly a cryptographer's (or ne'er-do-well's) dream, but as far as advantages for general computing...  That remains to be seen.  Solving the TSP with a DNA computer sure sounds interesting, though, and definitely has applications across the board. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-14T05:57Z</code>

::The key capability of both Quantum computers and Chemical/DNA computers is the ability to operate in almost infinite parallelism.  The qubits of a Quantum machine can hold all possible 'solution states' in parallel and a bucketful of DNA holds an insanely large number of states simultaneously (ditto for chemical/nanotechnological computers).  An optical computer can (theoretically) be made to switch different frequencies of light separately - so once again, there could be millions of calculations performed in parallel.  So pretty much all of these technologies are going to be applicable to vastly parallel problems - which certainly limits the areas where they provide a significant speedup.  Algorithms that don't exhibit massively parallel sections will probably run dog-slow on these machines - so you'd want a conventional electronic computer sitting there doing the non-parallel parts.  But code breaking, travelling salesmen problems (which are of great use for all kinds of applications), AI, finite element analysis, weather forcasting, virtual wind tunnels, graphics...all of those things are potentially '''vastly''' speeded up by any of those 'future' technologies. [[User:SteveBaker|SteveBaker]] 18:38, 14 November 2006 (UTC)

:It's appropriate for the article to point out new computer technologies being actively researched by linking to the articles on them.  The problem with this article as it stands is that these links are being presented as the ''Fifth Generation'' in the "History of Computing Hardware".  Which of these technologies, if any, might become the basis for the generation which follows VLSI cannot be determined without a crystal ball.  Not good.  The links need to be moved elsewhere, perhaps under "Other Hardware Topics". -[[User:R. S. Shaw|R. S. Shaw]] 07:07, 15 November 2006 (UTC)

::That's easy enough to fix.  I dispensed with "Fifth generation" in favor of "Theoretical/experimental".  This new table heading should be clear enough. -- [[User:Matt Britt|mattb]] <code>@ 2006-11-15T07:26Z</code>

:::That's OK with me.  I would have gone with something like "Technologies competing for the title 'Fifth Generation'" - but that's kinda wordy.  Yeah "Theoretical/experimental" works OK here. [[User:SteveBaker|SteveBaker]] 13:47, 15 November 2006 (UTC)

== Request for link to my page 'How Computers Work' ==

I suggest a link to my page 'How Computers Work' at http://www.fastchip.net/howcomputerswork/p1.html 
because much of the article is about '3. How Computers Work.' This is my first experience with Wikipedia. [[User:Thinkorrr|Thinkorrr]] 20:09, 3 December 2006 (UTC)

:Thank you very much for requesting that your link be added here rather than adding it to the article yourself.  That is generally considered proper procedure for external links (especially those with which you're affiliated) and is the correct way to proceed.  I browsed over the book briefly and it looks pretty good, though I'd venture to say that the subject matter is more on digital logic and simple microprocessors than computers in general.  Perhaps it would be a more appropriate link to include on the [[microprocessor]] article.  Give me a little time to think about it and look over the book in more detail, and let some other editors comment as well.  -- [[User:Matt Britt|mattb]] <code>@ 2006-12-03T21:06Z</code>

You are right.  I'll wait, though, as you suggest, before suggesting it.  It really is about the processor only, not computers in general, so it doesn't belong here. Maybe I could talk someone into putting a link there.  I wonder if I will be able to erase this. Thank you for your suggestion. [[User:Thinkorrr|Thinkorrr]] 22:53, 3 December 2006 (UTC)

== FAC already?! ==
I'm a little horrified to see this article put up for FAC - it's nowhere near ready.  It's going to get shot down in flames - and justifiably so.   We need to get it up to FA standards - yes - but an early nomination makes it harder to get through the second (and in this case, third) time. [[User:SteveBaker|SteveBaker]] 15:53, 25 December 2006 (UTC)

:I nominated it partially to get some exposure and constructive criticism since the peer review process is pretty much dead in the water.  Unfortunately, exactly the opposite happened and nobody is really offering constructive advice; only "the article's prose sucks, fix it" and "OH GOSH NEEDS REFERENCES".  Here I was thinking we'd get some discussion on the article's layout and presentation... -- [[User:Matt Britt|mattb]] <code>@ 2006-12-25T22:03Z</code>

== Career computer ==

Once upon a time there were people who were called 'computers', as that was their job. Is this fact addressed in the article? [[User:Vranak|Vranak]]

:It is treated in the linked [[history of computing]] article.  However, this usage is archaic and more or less irrelevant to what the word "computer" encompasses today.  Therefore it is left out of this article for brevity and topicality. -- [[User:Matt Britt|mattb]] <code>@ 2007-01-04T00:55Z</code>

== What's up with [[3 C's]] redirecting here? ==

Look and you'll notice the redirection. But there is no specific explanation on mention of the term in Computer. What is the relevance of having [[3 C's]] redirect here. I though 3 C's stood for [[Cool, Calm and Collective]] (which is an expression, and an article that probably doesn't exist). Thank you! --[[User:CyclePat|CyclePat]] 01:27, 18 January 2007 (UTC)
:P.s.: Alternatively there is also [[3C's]] a business term. --[[User:CyclePat|CyclePat]] 01:29, 18 January 2007 (UTC)

== ACM special interest groups ==

There has been one ACM special interest group ([[SIGGRAPH]]) mentioned in the article. I have added two others. Still, it is not clear why exactly these and no others are mentioned. In total, there are 12 such groups, and there is a category "ACM Special Interest Groups" for them. However, I do not know a way to insert a link to a category page (the only effect of such a link is that the current page is inserted into the category, which is of course not what is wanted here). --[[User:Tillmo|Tillmo]] 09:25, 20 January 2007 (UTC)
: OK, have found out now how to point to category pages. --[[User:Tillmo|Tillmo]] 16:53, 10 February 2007 (UTC)

== The add 1000 numbers program has an error. ==

The add the first 1000 numbers program has an error, it actually adds the first 1001 numbers. This is a very common mistake. it is often refered to as the off by one mistake. The OP code should be BLT.
Curtis Garrett [[User:Garrett Curtis|Garrett Curtis]] 23:50, 5 February 2007 (UTC)

:There's no mistake in the program.  Each number is summed before it is incremented, so the number '1001' is correctly caught by the ble instruction. -- [[User:Matt Britt|mattb]] <code>@ 2007-02-22T20:36Z</code>

I stand corrected. I see it now. Sorry about that. [[User:Garrett Curtis|Garrett Curtis]] 22:04, 7 March 2007 (UTC)

== Computer ==

[[I think that computers are an important part in everyone's life now. Most of own computers and most of use them on daily basis. I think that it is cool that life brought us this since it is also the fastest and the easiest way to communicate with your love ones]] <small>—The preceding [[Wikipedia:Sign your posts on talk pages|unsigned]] by high school student (ralston  high school)

== Advertisement in article ==

In the "hardware" overview table, there is reference to "Digi-Comp I, Digi-Comp II, Geniac" which is pointless in a historical overview. I guess it's advertisement for the Digi-Comp.

Besides, I diasgree that "computer" has several meanings: by now it is pretty clearly a universally programmable digital machine. Being electronic, binary or having stored program is not required. --[[Vincent C M]]  27 February 2007 <small>—The preceding [[Wikipedia:Sign your posts on talk pages|unsigned]] comment was added by [[Special:Contributions/194.219.37.70|194.219.37.70]] ([[User talk:194.219.37.70|talk]]) 18:20, 27 February 2007 (UTC).</small><!-- HagermanBot Auto-Unsigned -->

:I've removed the Digi-Comp and its ilk.  These were just left over from an early version of this article and weren't an advertisement.  As for your second concern, it's a difficult thing to reconcile the historical meaning of "computer" with its modern meaning and maintain brevity and consistency.  I think taking a formal, purely mathematical approach to defining a computer would be totally remiss since it ignores what most people, expert and lay man alike, think of as a "computer".  I also think that the stored program concept is totally central to the notion of a modern computer.  Nearly nothing that could be identified as a computer built within the past fifty years is not a stored program machine, and I think that fact is rather noteworthy.  I'm also curious why you selected the "digital" criteria for a computer over the others.  This article doesn't imply that a computer must be digital, electronic, or binary, though it does spend a lot of time on the all-important stored program concept.  If you have any concrete suggestions as to phrasing you'd like to see changed, by all means post them here for discussion. -- [[User:Matt Britt|mattb]] <code>@ 2007-02-28T00:26Z</code>

== Hard disk is an I/O device? ==
I thought this was a storage device, not an I/O device. I am probabally hugely mistaken, but...
:It is both. -- [[User:Matt Britt|mattb]] <code>@ 2007-03-22T14:50Z</code>
{{Aan}}

== US bias in table ==

The use of red and green colours in table "Defining characteristics of five early digital computers" seems to be intended to suggest that the American ENIAC was the first "real" computer, which it was not.

[[User:Zipdude|Zipdude]] ([[User talk:Zipdude|talk]]) 08:19, 19 May 2008 (UTC)

== VMS ==

VMS is an important enough operating system to be listed under operating systems. It had significant impact on the development of NT, it is still widely used today, has design features (logicals) found in few other systems, and has been supported across a number of platforms, including DEC Alphas, HP Integrity, a couple of Sun devices, as well as x86.   <small>—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/69.126.40.68|69.126.40.68]] ([[User talk:69.126.40.68|talk]]) 16:29, 23 February 2008 (UTC)</small>

== Shannon's involvement ==

The statement "largely invented by Claude Shannon in 1937" (in reference to digital electronics) is inaccurate and should be changed to something more like "enabled by the theoretical framework set forth by Claude Shannon in 1937" [[Special:Contributions/67.177.184.127|67.177.184.127]] ([[User talk:67.177.184.127|talk]]) 09:29, 19 February 2008 (UTC)


== Heron ==

Shouldn't Heron's mechanical play be mentioned in the history section? Was it not one of the first programmable devices?

:Per my understanding, the device to which you refer, while extremely remarkable, was not programmable. -- [[User:Matt Britt|mattb]] <code>@ 2007-04-12T16:25Z</code>

::Well my understanding is certainly limited. I saw it on the history channel some months ago. I suppose the 'programming' was rather built into the device and not really modifiable.

:::The trouble with Heron's mechanical theatre is that we just don't know enough about it.  It's hard to say to what degree it was programmable.  Certainly it wouldn't count as a "computer" because it didn't manipulate data - but perhaps there is a reasonable claim to be the first programmable machine.  I wish there were some really clear explanations of how it worked - but I haven't seen any.  I'll have a bash at improving the history section with this information. [[User:SteveBaker|SteveBaker]] ([[User talk:SteveBaker|talk]]) 02:10, 16 January 2008 (UTC)


Yesterday I wrote a section on [http://citruscomputers.com computer] components, which got reverted, then rereverted and then reverted again. I know that it didn't add much info, but it ''did'' add another perspective. At the moment, the article focuses almost entirely on the logical side of a computer, an important aspect, but it doesn't explain <u>what you see when you open a computer case</u>. Such down to earth information should also be present. At the moment, there is not even a link to the [[computer hardware]] article. This is (or should be) an umbrella article, serving different readers, giving an overview of all computer-related aspects and pointing to other articles. I suppose the biggest problem (as usual) is that it is written by experts, which is good, but also usually means the style is rather inaccessible to most encyclopedia readers (this is not a tech corner). Specialised info can go into specialised articles, but this one should also give a grassroots explanation.

As an illustration of what I mean, I was making a 'roadmapway' for my future computer requirements (I build them myself), for which I wanted to make a list of more and less vital components (as a visual aid), but decided it was easier to look it up in Wikipedia. To my surprise, I found no such list, so I made it myself. It should not necessarily be here, but it should be somewhere and then there should be a link to it in this article (preferably accompanied by a very short overview - the Wikipedia way) because this will be a first stop for people looking for such info. 

As for the structure, I worked from the inside out, explaining that the mb connects everything together and then 'hook everything up' to that - can it be part of the mb, does it fit directly onto it, is a cable used and is or can it be inside the computer case - all stuff that isn't evident from the article right now. And also, which components are essential (a graphics card is, unless the mb has that functionality) and which aren't (a sound card isn't). Let me put it this way - such info should be in Wikipedia. Where should it go? [[User:DirkvdM|DirkvdM]] 09:34, 19 June 2007 (UTC)

:I was rereverting because [[User:Matt Britt]] is an established contributor and shouldn't have his changes reverted by a bot. As for the merits of one version over the other, I will side with him; Computers extend beyond PCs, and in my opinion, the new section was too list-y. —[[User:Disavian|Disavian]] ([[User talk:Disavian|<sup>talk</sup>]]/[[Special:Contributions/Disavian|<sub>contribs</sub>]]) 13:23, 19 June 2007 (UTC)

::Alright then, my last question was where it should go, and you say [[personal computer]], which makes a lot of sense, so I'll put it there. And I'll add a link to that article (and [[computer hardware]]) in the 'see also' section, because you may know the distinction between the two, but people look something up in an encyclopedia because they ''don't'' know much about it. It's in the intro, but somewhat inconspicuously. Maybe there should be a listing of the types of computer in a table next to the intro. Also, you say the list is too 'listy', but that's what it's supposed to be. What's wrong with lists? [[User:DirkvdM|DirkvdM]] 07:04, 20 June 2007 (UTC)

:::About the list-y bit: according to [[Wikipedia:Embedded list]], 
{{cquote|Most Wikipedia articles should consist of prose, and not just a list of links. Prose allows the presentation of detail and clarification of context, while a list of links does not. Prose flows, like one person speaking to another, and is best suited to articles, because their purpose is to explain. Therefore, lists of links, which are most useful for browsing subject areas, should usually have their own entries: see Wikipedia:Lists (stand-alone lists) for detail. In an article, significant items should be mentioned naturally within the text rather than merely listed.}}
:::Of course, there are exceptions; ex: [[Georgia Institute of Technology#Colleges]]. —[[User:Disavian|Disavian]] ([[User talk:Disavian|<sup>talk</sup>]]/[[Special:Contributions/Disavian|<sub>contribs</sub>]]) 01:00, 22 June 2007 (UTC)

::::Note that the first word is 'most' and that it speaks of entire articles consisting of a list. This was (or rather ''is'', because it's now in the [[personal computer]] article) just a list inside an article. And it isn't even a list, it just has some lists in it. The alternative would be to enumerate them after each other (in-line), which is much less clear. That's a personal preference, I suppose (I like things to be as ordered as possible - blame it on my German background :) ), but I certainly won't be the only person who feels like this. I doubt if that text is meant to be applicable here. [[User:DirkvdM|DirkvdM]] 10:24, 24 June 2007 (UTC)

== Removal of text ==

I removed this text from the article:

Five Generations of Computers:
Over the 20th century, there have been 5 different generations of computers. These include:
# 1st Gen => 1940-1956: Vacuum Tubes
# 2nd Gen => 1956 – 1963: Transistors
# 3rd Gen => 1964 – 1971: Integrated Circuits
# 4th Gen => 1971 – Present: Microprocessors
# 5th Gen => Present & Beyond: AI

Because it appears to be [[WP:OR|original research]]. If someone has a source, there shouldn't be much of a problem with re-adding it, so long as it is communicated who proposed this model. [[User:Gracenotes|<span style="color:#960;">Grace</span><span style="color:#000;">notes</span>]]<sup>[[User talk:Gracenotes|<span style="color:#960;">T</span>]]</sup> <span title="Talk:Computer">§</span> 18:39, 16 July 2007 (UTC)
: I've just re-removed this text. I agree on the OR claim, but also disagree to an extent with the classification: especially with the "Present and Beyond: AI". Firstly, this is casting speculation on what will drive the industry in the future and secondly 'AI' has been 'the next big thing' since the 1960's. ''[[User:Angus Lepper|Angus Lepper]]<sup>([[User talk:Angus Lepper|T]], [[Special:Contributions/Angus Lepper|C]], [[User:Angus Lepper/Desktop|D]])</sup>'' 21:19, 16 July 2007 (UTC)
::I concur with Gracenotes and Angus Lepper that the text is original research, speculation, ''and'' factually incorrect, all of which violate numerous Wikipedia policies and guidelines, and should stay out of the article. --[[User:Coolcaesar|Coolcaesar]] 19:36, 18 July 2007 (UTC)
:::The list is correct for the first four generations and is discussed in [[History of computing hardware]], with more detail on the third and fourth generations in [[History of computing hardware (1960s-present)]].  However there is to date no consensus as to what the fifth generation is (or will be) but for one possibility see [[Fifth generation computer]]. --[[User:Nibios|Nibios]] 04:06, 30 July 2007 (UTC)
{{talkarchive}}
{{archive-nav|5}}


== Image Improvement ==

I believe a better picture of the NASA Super Computer should be shown, as this one is quite an illusion.  <small>—The preceding [[Wikipedia:Signatures|unsigned]] comment was added by [[User:Unknown Interval|Unknown Interval]] ([[User talk:Unknown Interval|talk]] • [[Special:Contributions/Unknown Interval|contribs]]){{#if:00:40, August 20, 2007 (UTC)|&#32;00:40, August 20, 2007 (UTC)}}.

In this topic I have to write that this article is not going to be complete without image of, I think today most widely used, simple, PC. I scrolled down the article in hope I will find it, but there isn't. Is it unlogic? For many people around the world, metion of word "computer" at first means PC. But they will not find it here. --[[User:Cikicdragan|Čikić Dragan]] ([[User talk:Cikicdragan|talk]]) 20:27, 13 January 2008 (UTC)

:This is an encyclopedia - not a picture gallery.  Ask yourself this:  What additional meaning could come to the article by showing computers that are essentially the same as the machine the person is sitting in front of as they sit reading Wikipedia.  We need to show images that ADD information.  Most people think of a computer as a laptop or a desktop PC or something - we can expand on their perceptions by showing images of massive supercomputers, computers made of gearwheels, tiny computers that fit into a wristwatch.  To try to keep this article down to some kind of reasonable size, we have to use our available screen space wisely - an photo of a common kind of PC is really largely irrelevant.  There would be a case for showing (say) an absolutely original early-model IBM PC - that's of historical value...but a common modern desk-side is just pointless. [[User:SteveBaker|SteveBaker]] ([[User talk:SteveBaker|talk]]) 01:52, 16 January 2008 (UTC)

== Homebrew computer ==

{{wikibooks|Microprocessor Design/Wire Wrap}}

Perhaps the article can add a section about homebrew (or DIY) computers. People as [[Dennis Kuschel]]<ref>[http://www.mycpu.eu/ hardware from electronic components]</ref> are starting to make a complete computer almost from scratch (by combining electronic components). Also a Dutch guy called Henk van de Kamer is making for example a complete [[CPU]] from basic transistors<ref>[http://www.hetlab.tk/index.php?paged=67 Henk Van de Kamer's transistor-cpu]</ref>.

:People have done this for years; it's almost a homage to the days of discrete component and SSI/MSI IC-based computers.  Anyway, this information is more of the trivial sort than of fundamental importance to computers in general. [[User:74.160.109.8|74.160.109.8]] 00:04, 6 November 2007 (UTC)

Yes but I believe now, powerful computers (with up to 1200+ CPU's) may be created while the spying of the trusted computer alliance may be avoided. Further searching led me to the [http://www.opencores.org/projects.cgi/web/or1k/openrisc_1200 OpenCores RISC 1200 CPU ]), and the AVR Webserver project (made diy) from Ulrich Radig (see [http://www.elektor.nl/artikelen-als-pdf/2007/december/avr-webserver.297065.lynkx?tab=2 Elektor article]) based on the ATMega644 AVR and the Ethernet-based-appliance control (see [http://tuxgraphics.org/electronics/200612/article06121.shtml Ethernet appliances]).  <small>—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/81.246.178.53|81.246.178.53]] ([[User talk:81.246.178.53|talk]]) 12:34, 23 November 2007 (UTC)</small>

Update: it seems it is already possbile and available at wiki ! See [[ECB_AT91]]  <small>—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/81.246.158.33|81.246.158.33]] ([[User talk:81.246.158.33|talk]]) 12:57, 23 November 2007 (UTC)</small>


== Deletion and redirect of [[Computer system]] ==

The [[computer system]] article was recently deleted and redirected to [[computer]]. A problem with this is that in many of the articles linking to [[computer system]] (see [[Special:Whatlinkshere/Computer_system|here]]), the term "computer system" is used to mean a "combination of hardware and software", rather than a "computer". The links in those articles need tidying up if WP is not going to have a "computer system" article. Maybe the instances that mean hardware and software could link to [[computing]] instead, if there is not going to be a "computer system" article. [[User:Nurg|Nurg]] 03:49, 11 November 2007 (UTC)

I realize that I may be horribly late to the party, but I also would promote that these two articles would lead to two entirely different discussions.
[[User:Mjquin id|Mjquin_id]] ([[User talk:Mjquin id|talk]]) 00:01, 18 July 2008 (UTC)

== CPU section ==

Very good article, but just from the perspective of a beginner reading the article such as myself, it would've been easier to understand if the CPU had been given an overall paragraph of information on how it functions as a whole first, instead of just talking about the ALU and control unit straight away individually.  <small>—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/60.234.157.64|60.234.157.64]] ([[User talk:60.234.157.64|talk]]) 05:33, 11 November 2007 (UTC)</small>

== Input/Output ==

Inside the Input/Output topic, the author mentions that there are fifty or so computers within a single computer. I considered this highly inaccurate and changed this to something more simplified, but have been reverted with the editor saying it was better written and added the processor and VRAM making it more logical to keep. I don't agree with that decision simply because a computer really is just the processor and memory working together to have an output, and even though there are embedded systems with various configurations and limited programmability, it wouldn't have fifty boards. I'll try to clean up the article a little again, and if reverted once again will try to find a consensus, because this simply doesn't make sense. --[[User:Bookinvestor|Bookinvestor]] 01:41, 2 December 2007 (UTC)
:I concur with Bookinvestor that the sentence is highly inaccurate and should be modified or deleted.  Whomever wrote that has either never read a basic textbook on computer architecture (e.g., [[John L. Hennessy]]'s classic textbook, which I've read twice over the years) or has not completed a freshman course in formal written English at a decent university.  --[[User:Coolcaesar|Coolcaesar]] 05:57, 2 December 2007 (UTC)

:The sentence doesn't seem all that misleading to me.  GPU's do much parallel processing and have many ALUs operating in parallel.  [http://pcquest.ciol.com/content/technology/2007/107031003.asp This article], for instance, describes a GPU using 128 processors.  Of course this doesn't mean 128 ''boards'' - the processors are sections of a single IC chip. -[[User:R. S. Shaw|R. S. Shaw]] 06:49, 2 December 2007 (UTC)

::I've read the article and have also found the wikipedia article [[stream processing]]. Although I really want to delete the whole paragraph, perhaps I should study a little more into what the author's trying to say because it opens a whole new realm of learning. I'll add this link to the paragraph to see exactly what the author's talking about with [[GPU]]s, and hope someone would be able to research with me. Thank you for your contribution Shaw. --[[User:Bookinvestor|Bookinvestor]] 19:24, 3 December 2007 (UTC)

== what is a computer? ==

The first sentence should be something like:
"A computer is an information processing machine which is capable of simulating any other information processing machine that can fit into its memory."
There are three salient parts to the definition:
  -information processing machine
  -capable of simulating any other ( like a Universal Turing Machine )
  -can be built in real life, so can't have infinite memory ( unlike a Universal Turing Machine )
Currently the first sentence of the article says "A computer is a machine that manipulates data according to a list of instructions", which is only one particular (but dominant) category of computer architecture -- Instruction Set Architecture.
Other real life categories which are actually sold are:
  parallel/multi-core -- one computer processes many lists of instructions in parallel
  reconfigurable/FPGA -- the computer is specialized for simulating any logic network
And some other classes which have only had prototypes:
  stream processors
  cellular automata simulators (probably only one of these was ever made (Margolis))
  dna computer  <small>—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/70.20.219.7|70.20.219.7]] ([[User talk:70.20.219.7|talk]]) 03:31, 25 January 2008 (UTC)</small><!-- Template:UnsignedIP --> 

:While your opening would probably stand up better as a formal definition, I'm not sure that's the best route to follow for an article which is intended to be an accessible introduction to computers.  Remember, this article isn't about theories of computation or computability, but about "computers".  The term "computer" has a vernacular meaning to society which is every bit as significant as the formal basis.  The writing of the article tries to balance these two worlds by introducing computers in fairly familiar terms and then branching out slightly into instruction set (or Von Neumann) architecture.  I believe this is a good approach because, as you state, this is the dominant realization of computers and will cover most anything that a typical person will think of as a computer (thread-level parallelism really doesn't break significantly from the "list of instructions" concept, either).  With some of your examples (stream processors, special-purpose logic and DSP, DNA computing, etc), there's probably a good bit that can be said about whether these even are computers (the classical "where is the line between a computer and a calculator" question).  We run into the issue of a heavily overloaded term in "computer", so we took the tact of making an article as accessible yet informative as possible to a general reader.   I really think it would only complicate matters to try to introduce much computing theory in this article, especially since there are a lot of other articles dedicated to the theoretical aspects of computers. -- [[Special:Contributions/74.160.99.252|74.160.99.252]] ([[User talk:74.160.99.252|talk]]) 19:07, 11 February 2008 (UTC)

A computer can't be a machine, because the Wikipedia entry for [[machine]] excludes computers.[[User:Heikediguoren|Heikediguoren]] ([[User talk:Heikediguoren|talk]]) 21:14, 10 July 2008 (UTC)

:The Wiki article for [[machine]] is in grave error and should be corrected; analog and digital computers both fall under the broader category of "machines". See [[Association for Computing Machinery]], "the world's first scientific and educational computing society" (and still going strong).

:The 3rd edition of the ''Concise Encyclopedia of Science and Technology'' (Sybil P. Parker ed., 1994, McGraw-Hill) defines "computer" as "A device that receives, processes and presents information. The two basic types of computers are analog and digital." (Note: A computer doesn't require "a list of instructions".) A broad definition indeed, but the introductory paragraph can quickly set aside the analog variety, which are still in heavy use, although we don't commonly think of them as "computers". Any analog meter, gage, motor driven clock, speedometer, odometer, etc. qualifies as a computer.

:I'm not sure "Modern computers are ''based on'' tiny integrated circuits…" is the proper phrasing. How about, "Modern electronic computers ''rely on'' integrated circuits…" The CPUs in PCs and Macs aren't so "tiny" – it's a relative term anyway.

:Btw, the caption that reads, "Microprocessors are miniaturized devices that often implement stored program CPUs" is all wrong; it doesn't even make sense. Microprocessors are literally integrated circuit (as opposed to discrete circuit) electronic computers – computers on a chip. And "central processing unit" should appear earlier in the text. As is, we see "CPU" before we're told what the letters stand for.

:Otherwise, nice work guys!
:Cheers, [[User:Rico402|Rico402]] ([[User talk:Rico402|talk]]) 10:16, 11 July 2008 (UTC)

:I STRONGLY agree. In the article itself, [[analog computer]]s are mentioned. By the given first sentence definition, they don't belong in the article... In general, this article is amazingly shallow for Wikipedia (which is usually very strong on computing subjects). [[Special:Contributions/213.112.81.33|213.112.81.33]] ([[User talk:213.112.81.33|talk]]) 23:47, 15 October 2008 

== Wristwatch computer ==

I removed the image - from the image description page, it appears it is a painted image, not a real product. If the image is re-inserted, we need a reliable reference. --[[User:Janke|Janke]] | [[User talk:Janke|Talk]] 11:05, 3 February 2008 (UTC)

== Defining trait(s) of a computer ==

The article states:

:"Nearly all modern computers implement some form of the stored program architecture, making it the single trait by which the word "computer" is now defined."

This is unreferenced, and to me this seems wrong -- I would say that Turing Completeness is also an essential trait of a "real computer".  Is there some basis for saying that stored program control is sufficient?

The article also uses the term "Turing Complete", without defining it.  I would suggest a brief explanation of this important concept, and then include it in the "trait by which the word "computer" is now defined." bit.  What do people think? — [[User:Johantheghost|Johan&nbsp;the&nbsp;Ghost]]&nbsp;[[User talk:Johantheghost|<sub>seance</sub>]] 16:55, 10 February 2008 (UTC)

Gottfried Wilhelm Leibniz should be mentioned as he invented the binary system and built mechanical calculators as well.  <small>—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/84.151.59.187|84.151.59.187]] ([[User talk:84.151.59.187|talk]]) 02:19, 17 April 2008 (UTC)</small>

== "[[Form factor]]" ==

The expression "form factor" is used in many Wikipedia articles on computers, and linked to [[Form factor]] (which is a disambiguation page though not labelled as such). This term apparently has several different meanings. One usage seems to be the "footprint" or overall physical size of the computer. This usage seems to be implied in many of our articles, however it is not given on the page [[Form factor]] (In other words, where this usage is intended the link is wrong).<br> 
Could we please:
*(A) Provide this definition at [[Form factor]], and an article for this meaning if appropriate
*(B) In all articles that mention "form factor", clarify which meaning is intended, as appropriate
-- [[User:Writtenonsand|Writtenonsand]] ([[User talk:Writtenonsand|talk]]) 05:24, 21 May 2008 (UTC)

== Computer <> Digital computer ==

A internal link could profitably, for the curious, be made to [[Analog computer]], and perhaps - for completude - to [[:fr:Calculateur stochastique|Stochastic computer]]<ref>http://pages.cpsc.ucalgary.ca/~gaines/reports/SYS/IdentSC/index.html</ref>
[[Special:Contributions/89.224.147.179|89.224.147.179]] ([[User talk:89.224.147.179|talk]]) 09:56, 22 June 2008 (UTC)


== Not all stored program computers have a program counter. ==

The control unit section of this article contains this text "A key component common to all CPUs is the program counter". This is almost correct but I think it should be changed to say A key component common to almost all CPUs is the program counter". The one exception I know of is the ICT 1300 series which have three control registers CR1,CR2 and CR3. After a normal single length instruction is executed in CR1  then CR3 is moved into CR2 and CR2 moved to CR1 and CR1 has one added to it and is moved to CR3. In normal operation (though nothing in hardware forces this) there is at least one unconditional jump instruction in one of the control registers. When one is executed in CR1 then CR1 and CR2 get loaded from core memory and CR3 receives a copy of the original jump instruction with one added. The previous contents of CR2 and CR3 are stored in another register where they are available as a return link for subroutines. When a conditional jump is executed, much the same happens except CR3 receives an incremented version of the jump but with the condition made into always true, that it it becomes an unconditional jump.

When executing a program with two subroutine jumps immediately following one another, all three control registers contain unconditional jump instructions. In this state, it is impossible to point at one of the control registers and say that is the program counter. It sort of has a program counter but it flits between registers and I suppose sometimes you could say there are three program counters. This avoids having any separate logic to implement the fetch cycle and also avoids having to have an extra bit in a program counter to allow for 24 bit instructions in a 48 bit word machine which has no addressable portions of a word.

If you prefer, an alternative wording would be "A key component common to all modern CPUs is the program counter" as the five ton ICT 1301 were built in 1962-5 and are in no way modern, though as over 155 were built, they were quite a large proportion of the computers of their day, certainly in the UK. I should add that the number 155 is because I have part of machine number 155 as well as all of machines numbered 6 and 75, so there could be considerable more than 155.

[[Special:Contributions/86.146.160.196|86.146.160.196]] ([[User talk:86.146.160.196|talk]]) 18:48, 27 August 2008 (UTC)Roger Holmes



== Example Mistake? ==

{{tl|editsemiprotected}}

The text currently in the article says:
{| class="wikitable" border="1"
|
Suppose a computer is being employed to drive a traffic light. A simple stored program might say:

   1. Turn off all of the lights
   2. Turn on the red light
   3. Wait for sixty seconds
   4. Turn off the red light
   5. Turn on the green light
   6. Wait for sixty seconds
   7. Turn off the green light
   8. Turn on the yellow light
   9. Wait for two seconds
  10. Turn off the yellow light
  11. Jump to instruction number (2)

With this set of instructions, the computer would cycle the light continually through red, green, yellow and back to red again until told to stop running the program.

However, suppose there is a simple on/off switch connected to the computer that is intended to be used to make the light flash red while some maintenance operation is being performed. The program might then instruct the computer to:

   1. Turn off all of the lights
   2. Turn on the red light
   3. Wait for sixty seconds
   4. Turn off the red light
   5. Turn on the green light
   6. Wait for sixty seconds
   7. Turn off the green light
   8. Turn on the yellow light
   9. Wait for two seconds
  10. Turn off the yellow light
  11. If the maintenance switch is NOT turned on then jump to instruction number 2
  12. Turn on the red light
  13. Wait for one second
  14. Turn off the red light
  15. Wait for one second
  16. Jump to instruction number 11

In this manner, the computer is either running the instructions from number (2) to (11) over and over or its running the instructions from (11) down to (16) over and over, depending on the position of the switch.
|}

However, I think it should be (changes in italic):

{| class="wikitable" border="1"
|Suppose a computer is being employed to drive a traffic light. A simple stored program might say:

   1. Turn off all of the lights
   2. Turn on the red light
   3. Wait for sixty seconds
   4. Turn off the red light
   5. Turn on the green light
   6. Wait for sixty seconds
   7. Turn off the green light
   8. Turn on the yellow light
   9. Wait for two seconds
  10. Turn off the yellow light
  11. Jump to instruction number (2)

With this set of instructions, the computer would cycle the light continually through red, green, yellow and back to red again until told to stop running the program.

However, suppose there is a simple on/off switch connected to the computer that is intended to be used to make the light flash red while some maintenance operation is being performed. The program might then instruct the computer to:

   1. Turn off all of the lights
   2. Turn on the red light
   3. Wait for sixty seconds
   4. Turn off the red light
   5. Turn on the green light
   6. Wait for sixty seconds
   7. Turn off the green light
   8. Turn on the yellow light
   9. Wait for two seconds
  10. Turn off the yellow light
  11. If the maintenance switch is NOT turned on then jump to instruction number 2
  12. Turn on the red light
  13. Wait for one second
  14. Turn off the red light
  15. Wait for one second
  16. ''If the maintenance switch is NOT turned on then jump to instruction number 2''
  17. Jump to instruction number 11

In this manner, the computer is either running the instructions from number (2) to (11) over and over or its running the instructions from (11) down to (16) over and over, depending on the position of the switch.
|}

This is because if the original one was used (and the switch was turned on), the program would loop from 11 through 16 continuously regardless of the switch's position. [[User:Numberonestarwarsfanatic|starwarsfanaticnumberone]] ([[User talk:Numberonestarwarsfanatic|talk]]) 22:49, 2 October 2008 (UTC)
:{{notdone}}The original example executes just fine: If the switch is on, the instructions loop continuously from #11 to #16.  Once the switch is turned off, the instructions continue until #16, then jump to #11, and then jump to #2, starting to loop again from #2 to #11.  The suggested change merely adds a redundant instruction.--[[User:Aervanath|Aervanath]] [[User talk:Aervanath|lives]] [[Special:Contributions/Aervanath|in]] '''''<font color="green">[[WP:O|the Orphanage]]</font>''''' 23:12, 2 October 2008 (UTC)

== Ask ==

*Who is created the first computer around the world ? 
*Who is created the first internet connection around the world ? 
*what is the first computer program ?
== Unable to edit ==

I wanted to correct a mistake, but I cannot edit the article. Why? [[User:Tohuvabohuo|Tohuvabohuo]] ([[User talk:Tohuvabohuo|talk]]) 10:45, 1 December 2008 (UTC)

Probably because the article is protected.

--[[User:Sci-Fi Dude|Sci-Fi Dude]] ([[User talk:Sci-Fi Dude|talk]]) 18:40, 5 June 2009 (UTC)



== Nokia overtaking HP ==

Following the statement about Nokia overtaking HP in sales of computers; if we consider smartphones as computers, it is true that Nokia overtook HP in Q1 2008 in sales:
Nokia sold 14,588,600 smartphone while HP sold 12,979, 000 computers.
Source: Gartner http://www.gartner.com/it/page.jsp?id=688116 and http://www.gartner.com/it/page.jsp?id=648619.  <small><span class="autosigned">—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:Thecouze|Thecouze]] ([[User talk:Thecouze|talk]] • [[Special:Contributions/Thecouze|contribs]]) 20:38, 9 January 2009 (UTC)</span></small>

Thank you for the references, Thecouze.
I think we should also make a more general statement about mobile phones, smart phones, and PCs as a whole (rather than specific companies).

Nearly all mobile phones (not just smartphones) use at least one general-purpose CPU (possibly in addition to a DSP):
reference: [http://news.cnet.com/Intel-has-ARM-in-its-crosshairs/2100-1006_3-6210033.html "Intel has ARM in its crosshairs"]
by Tom Krazit 2007.

If we consider mobile phones as computers, mobile phones ("Mobile Terminals") overtook PCs a long time ago:

* 294,283,000 Worldwide Mobile Terminal Sales to End-Users in 1Q08 -- Gartner [http://www.gartner.com/it/page.jsp?id=680207]
* 71,057,000 Worldwide PC Vendor Unit Shipment Estimates for 1Q08 -- Gartner [http://www.gartner.com/it/page.jsp?id=648619]
* 32,249,904 Worldwide Smartphone Sales to End-Users in 1Q08 -- Gartner [http://www.gartner.com/it/page.jsp?id=688116]

--[[Special:Contributions/68.0.124.33|68.0.124.33]] ([[User talk:68.0.124.33|talk]]) 06:02, 15 March 2009 (UTC)

== Error in computer logic in first computer program. ==

It would appear that the first computer program example (counting to 1000) has an error introduced when the increment value is itself also incremented each loop.  Thus the values held by the increment holder are 1, 2, 3, 4, 5,.., n.  The result counter does not increment by one but follows the following pattern:  1, 3, 6, 10, ... 

The value of the increment should stay steady during the course of the program execution.  <small><span class="autosigned">—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:Campej|Campej]] ([[User talk:Campej|talk]] • [[Special:Contributions/Campej|contribs]]) 04:59, 14 January 2009 (UTC)</span></small>

== Further topics section ==

The section is way too not encyclopedic - whould be converted to prose or deleted.--[[User:Kozuch|Kozuch]] ([[User talk:Kozuch|talk]]) 19:13, 22 March 2009 (UTC)



== Operating System/Application Software ==

Why is it necessary to have an operating system installed on a computer before installing application software  <span style="font-size: smaller;" class="autosigned">—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/219.89.124.31|219.89.124.31]] ([[User talk:219.89.124.31|talk]]) 23:29, 14 April 2009 (UTC)</span>

:To install and execute a specific software application your system must meet the requirements (both hardware and software) stated for that application. If the application's requirements state that operating system XYZ is required then the application is in some way dependent on XYZ and will not work without it.  Most, but not all, applications require an operating system. [[Special:Contributions/71.135.172.126|71.135.172.126]] ([[User talk:71.135.172.126|talk]]) 15:21, 15 April 2009 (UTC)

Need to add CPM to operating systems.  <small><span class="autosigned">—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:Mycroft 514|Mycroft 514]] ([[User talk:Mycroft 514|talk]] • [[Special:Contributions/Mycroft 514|contribs]]) 20:11, 18 November 2010 (UTC)</span></small><!-- Template:Unsigned --> <!--Autosigned by SineBot-->
Under scripting languages, REXX is one of the most widely used.  Also TEX is another one that springs to mind, and if you want to continue to most common then you need to add .BAT files on the PCs.  <small><span class="autosigned">—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:Mycroft 514|Mycroft 514]] ([[User talk:Mycroft 514|talk]] • [[Special:Contributions/Mycroft 514|contribs]]) 20:14, 18 November 2010 (UTC)</span></small><!-- Template:Unsigned --> <!--Autosigned by SineBot-->

== Somebody Please Fix: Small Spelling Error ==

From the 1st paragraph in "History of Computing":

"From the end of the 19th century onwards though, the word began to take on its more familiar meaning, decribing a machine that carries out computations."

- describing is spelled as decribing in this paragraph. It's semi-protected so I cannot edit this small error as a new user.
:Done. [[User:Dan D. Ric|Dan D. Ric]] ([[User talk:Dan D. Ric|talk]]) 01:21, 26 April 2009 (UTC)

== Copyvio? ==

At a glance, this example looks suspiciously like one of the first exercises in [[Uplink (video game)]].  It should be checked out, and credit given if that's its source. [[User:Digwuren|Дигвурен Дигвурович]]<sub>[[User talk:Digwuren|Аллё?]]</sub> 21:11, 16 June 2009 (UTC)

== Intro needs editing ==

The intro to this important article contains a number of sentences and phrases which could benefit from further editing. The prime candidate here is "Personal computers in their various forms are icons of the Information Age, what most people think of as a "computer", but the embedded computers found in devices ranging from fighter aircraft to industrial robots, digital cameras, and toys are the most numerous." The clause 'what most people think of as a "computer"' does not fit at all well into the sentence and could, for instance, be accommodated by rephrasing as follows:

It is notable that Oscar Wilde and Professor Britannica agreed on the same thing
"Computers will someday take up half a room!"  <span style="font-size: smaller;" class="autosigned">—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/98.162.199.142|98.162.199.142]] ([[User talk:98.162.199.142|talk]]) 00:39, 10 June 2010 (UTC)</span><!-- Template:UnsignedIP --> <!--Autosigned by SineBot-->

"Personal computers in their various forms are icons of the Information Age and are what most people think of as "computers". The embedded computers found in devices ranging from fighter aircraft to industrial robots, digital cameras, and toys are however the most numerous."

Perhaps someone could come up with something even better.-[[User:Ipigott|Ipigott]] ([[User talk:Ipigott|talk]]) 15:33, 18 June 2009 (UTC)

:I've just done a general update on the lead section, using your suggested wording. I hope it's a bit better now. --[[User:Nigelj|Nigelj]] ([[User talk:Nigelj|talk]]) 14:23, 20 June 2009 (UTC)

::The first sentance of the lead reads 'A computer is a machine that manipulates data according to a set of instructions.' Would it not be appropriate to change this to read 'A computer is a machine that manipulates data according to a set of modifiable instructions'? Purely mechanical dvices can be regarded as containing instructions that lead to manipulations of data. Surely it is the fact that the instructions - the program - can be modified that is a defining charactersitic of a computer.--[[User:TedColes|TedColes]] ([[User talk:TedColes|talk]]) 17:40, 20 June 2009 (UTC)

:::I don't think so. A single chip microprocessor with a program in read only memory is still a computer even though its instructions cannot be modified. Early computers were programmed with patch panels etc, later on we got 'Stored program computers' which were a big step forward but to rule the early machines to be excluded goes against accepted history.  <small><span class="autosigned">—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:RogerHolmes|RogerHolmes]] ([[User talk:RogerHolmes|talk]] • [[Special:Contributions/RogerHolmes|contribs]]) 18:19, 16 July 2009 (UTC)

::::Have checked anybody what appears on the intro, there's something like a troll around that put Computers are awesome! (especially Macs) on the intro of the article, can anybody check??

</span></small>

== Definition of computer ==

the definiion of computer given in this article may be better suited if it was "''instructions and give output in a useful format.''" instead of "''A computer is a machine that manipulates data according to a set of instructions.''" - --[[User:Nafsadh|nafSadh]] ([[User talk:Nafsadh|talk]]) 05:52, 13 July 2009 (UTC)

:I agree that the current definition is too vague. Slide rules and calculators are machines that manipulate data according to instructions, but most people would not refer to them as computers. --[[User:Jcbutler|Jcbutler]] ([[User talk:Jcbutler|talk]]) 19:54, 9 February 2010 (UTC)

::Wouldn't there also be a case for remove the word 'programmable' from the definition as I would say that it's rather dubious as to whether many of the examples of computers have been programmed at all rather than just 'happened', life being the obvious example. [[User:Stijn x|StijnX]] 11:01, 25 July 2010 (UTC)

:::I agree with StijnX that the word computer is unduly associated with the word "program". A device does need a program to be a computer. A program is a list of instructions or steps - yes, computers have to change inputs into outputs in a ''meaningful'' way, but that specific method of transforming methods can be inherent to the computer itself, like individual neurons or AND gates, both of which can be reasonably considered as rudimentary computers. I vouch for changing the definition from "A computer is a program machine that receives input, stores and manipulates data, and provides output in a useful format." to "A computer is a machine that receives input and manipulates it into a meaningful or useful output. It may also store data, and it may be programmable so that it can manipulate inputs or data in multiple ways."  [[User:Rablanken|rablanken]] ([[User talk:Rablanken|talk]]) 01:38, 25 December 2011 (UTC)

::::I'm sorry, I meant to post my last on the "disambiguation" talk page. I'm a little new. But I think I think it's still applicable to the introductory paragraphs of this article. [[User:Rablanken|rablanken]] ([[User talk:Rablanken|talk]]) 01:46, 25 December 2011 (UTC)

=== Definition of Computer ===

I deleted the [[IBM 608]] Transistorized Calculator from the [[List of transistorized computers]] as Bashe, ''IBM's Early Computers'' p.386, had noted “... properly called a calculator, not a computer in today's terminology because its small ferrite-core memory was not used for stored-program operation.” That edit was reverted: ''Wtshymanski (talk | contribs) (4,331 bytes) (and ENIAC was a stored program computer? Revert'' (I see, above, that Wtshymanski is an editor of this article!). Not agreeing with that revert, I accessed the [[Computer]] article, looking for a definition of ''computer'' (I was interested in the 608; someone else could worry about the unrelated ENIAC). 

Found ''A computer is a programmable machine designed to sequentially and automatically carry out a sequence of arithmetic or logical operations. The particular sequence of operations can be changed readily, allowing the computer to solve more than one kind of problem.  Conventionally''

The definition must stop at ''conventionally'' as that use admits of ''non-conventional'' computers.  The second sentence "The particular sequence of operations can be changed readily..." is wrong, see [[Embedded systems]] ''An embedded system is a computer system designed to do one or a few...''

The Computer article's definition is, then, ''A computer is a programmable machine designed to sequentially and automatically carry out a sequence of arithmetic or logical operations.''  Compare that to [[Programmable calculator]] ''Programmable calculators are calculators that can automatically carry out a sequence of operations under control of a stored program, much like a computer.''

Somewhat (a lot!) to my surprise Wtshymanski's revert was correct, calculators are computers!  At least so far as these Wikipedia definitions are concerned.  (Even considering the ''... sequence of operations can be changed readily...'' doesn't help as programmable calculators meet that criteria as well.)

The obvious suggestion: The [[Computer]] article should have a definition of ''computer'' sufficient to distinguish computers from programmable calculators. [[Special:Contributions/69.106.237.145|69.106.237.145]] ([[User talk:69.106.237.145|talk]]) 09:07, 21 June 2011 (UTC)
: Is this a useful distinction?  Is our hypothetical knowledge-seeking reader (Jimbo help him!) going to be confused between the racks of blinking lights filling a dinosaur pen and that thing he uses at tax time to tot up his deductions? Is not a pocket calculator implemented as a microprocesor with a stored progream? Was the IBM 608 Turing-complete?  What is Turing-complete anyway, and must a computer be Turing-complete to be a computer? Myself, I think it's useful to distinguish between machines used for doing lots of arithmetic vs. more general puprpose bit-twiddling (no-one did text editing on ENIAC, for instance) but it's hardly a crisp -or vital - distinction. ENIAC crunched numbers only and is plainly a computer, an HP 48 can spell out text prompts to you and is plainly a calculator - it's a complex world. --[[User:Wtshymanski|Wtshymanski]] ([[User talk:Wtshymanski|talk]]) 13:40, 21 June 2011 (UTC)

:: I believe that "Turing Complete" is the definition of a "Computer" that we should be using here.  It's more than just a statement of what the machine does.  The "Church-Turing" thesis points out that all machines that are Turing complete are functionally equivalent (given sufficient time and memory) to all other Turing-complete machines.

:: A calculator may have a computer inside it - but when used from the user interface, it's not a computer.  My car has a computer inside - but that doesn't make my car '''be''' a computer.   A programmable calculator, however, exposes its Turing-completeness to the user and most certainly is a computer under this definition.

:: The Turing completeness thing is important - any machine that is Turing complete could (with enough memory and time) emulate any machine that we would most certainly describe as "a computer" (eg an Apple II)...so how can we deny that a machine that could emulate an Apple II the title "computer"?  We obviously cannot.

:: So it is certainly the case that any machine that's Turing complete '''must''' be called a computer.  The only point of contention is whether there are other machines that while '''not''' being Turing complete also warrant being called "a computer"?   The difficulty is that most of those machines are old and no longer in use - and whether they were once called "computer" or not doesn't matter because our language has moved on.  After all, the original meaning of the word referred to human beings who did menial calculations for mathematicians, scientists, etc.  The meaning of the word has definitely shifted - and we're supposed to be using the modern meaning.

:: So I strongly believe that this article should use Turing-completeness as the touch-stone for what is and what is not a computer.  However, I think we'd be doing a disservice to our readership to use those words to describe what a computer is - because Turing Completeness is a tough concept to get ones' head around - and the people who come to read this 'top level' article are likely to be seeing a more approachable definition.

::  [[User:SteveBaker|SteveBaker]] ([[User talk:SteveBaker|talk]]) 14:12, 21 June 2011 (UTC)
:::Was an IBM 608 programmable calculator "Turing-complete"? I've never found an intelligible definiton of Turing-completeness  aside from the one that goes "A machine is Turing-complete if it can emulate a universal Turing machine (except for finite memory and speed)". I have a vauge picture of jamming tape into my TI58 calcuator and seeing if it can read marks off it and make marks on it...and then the utility of the comparision vanishes. Turing-complete seems to have something to do with "conditional branching" - from what I've read you must be very clever to come up with a Turing machine that doesn't have conditional branches.  A touch-stone is only useful if you can tell if something is touching it, and the "Turing complete" phrase for me has obscured more than it explains. 
::: I don't think the categories of "computer" and "calculator" are disjoint - my old TI 58 could automatically execute stored instructions and could change the order of execution depending on the results of operations on data, and so by any reasonable definition was a "computer". Just as plainly, it was specialized for doing arithmetic, so it was a "calculator". --[[User:Wtshymanski|Wtshymanski]] ([[User talk:Wtshymanski|talk]]) 15:20, 21 June 2011 (UTC)

=== Requested edit ===
While you are thinking about the ''Definition of Computer'' (the immediately above discussion topic), please delete the articles second sentence ''The particular sequence of operations can be changed readily, allowing the computer to solve more than one kind of problem.''.  This edit requested because 1) many (most?) computers are in [[Embedded systems]] and 2) some computers have the ''particular sequence of operations'' burned into ROM (the [[Dulmont Magnum]] was an early example) thus ''changed readily'' is not, in fact, a defining characteristic of ''Computer''.  Thanks, [[Special:Contributions/69.106.237.145|69.106.237.145]] ([[User talk:69.106.237.145|talk]]) 20:32, 26 June 2011 (UTC)
: Depends on what "readily" means.  If you built a machine from logic gates or Tinkertoy to solve the [[Knight's tour]] problem, it would be completely useless at solving anything else without redesigning it.  But any computer system solves problems with the same hardware, only changing the software.  The same processor that controls ignition timing on a car might equally well be used in a printer, same  hardware, different firmware -though you probably could not alter the mask ROM in either application. "Readily" is shorthand here which could be expanded in the article. --[[User:Wtshymanski|Wtshymanski]] ([[User talk:Wtshymanski|talk]]) 02:00, 27 June 2011 (UTC)

== Hardware table limitations ==

It is notable that the table's entries for third generation computers is limited to those from US companies. Clearly the list can't be comprehensive, but it should surely aim to be reasonably representative of important advances? I am particuarly thinking of Manchester University's [[Atlas Computer (Manchester)|Atlas]] with its introduction of virtual memory by paging.
--[[User:TedColes|TedColes]] ([[User talk:TedColes|talk]]) 06:51, 7 August 2009 (UTC)

== See also section ==

Some of the links in this section of the article have questionable relevance. For example, [[List of fictional computers]]; [[Electronic waste]]; [[Living computer theory]], a defunct link; and especially [[The Secret Guide to Computers]], which seems to be nothing more than an endorsement/promotion article.  Someone with editing ability please cull this section.  <span style="font-size: smaller;" class="autosigned">—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/98.26.191.8|98.26.191.8]] ([[User talk:98.26.191.8|talk]]) 01:51, 20 August 2009 (UTC)</span>

Why was the date 2012 changed back to 1613? It is correct.--[[User:Laptop65|Laptop65]] ([[User talk:Laptop65|talk]]) 12:03, 1 December 2009 (UTC)

== Transistor ==

Shouldn't the invention of the semiconductor transistor be given a lot more attention? Although I am not an expert on computers, I thought that developing a semiconductor was crucial in moving away from hefty vacuum tubes. [[Special:Contributions/77.250.86.100|77.250.86.100]] ([[User talk:77.250.86.100|talk]]) 20:55, 15 March 2010 (UTC)
:At the time this comment was posted, this was in the [[computer]] article, and it's still there today:
=== Semiconductors and microprocessors ===
::"Computers using [[vacuum tube]]s as their electronic elements were in use throughout the 1950s, but by the 1960s had been largely replaced by [[transistor]]-based machines, which were smaller, faster, cheaper to produce, required less power, and were more reliable. The first transistorised computer was demonstrated at the [[University of Manchester]] in 1953. In the 1970s, [[integrated circuit]] technology and the subsequent creation of [[microprocessor]]s, such as the [[Intel 4004]], further decreased size and cost and further increased speed and reliability of computers."
:That seems sufficient for this article.  Readers desiring further information can click on the [[transistor]] link, and there they can go to [[history of the transistor]] for even more detail. [[User:Wbm1058|Wbm1058]] ([[User talk:Wbm1058|talk]]) 01:16, 21 January 2012 (UTC)
::On second thought, ''[[semiconductor]] transistor'' should be the lead of that section [[User:Wbm1058|Wbm1058]] ([[User talk:Wbm1058|talk]]) 01:43, 21 January 2012 (UTC)

== about computer ==

i want to know whay computer come to be so powerful  <span style="font-size: smaller;" class="autosigned">—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/41.218.229.207|41.218.229.207]] ([[User talk:41.218.229.207|talk]]) 19:31, 5 April 2010 (UTC)</span><!-- Template:UnsignedIP --> <!--Autosigned by SineBot-->

:This may be a pointer to a real opportunity to improve the article. By the 21st of 22 paragraphs under 'History of computing', we are only up to vacuum tubes; the 1980s go by in one sentence and we never mention the 90s, the 00s, or the 10s. Maybe we can shift our field of view forward just a little to mention the last 20 - 30 years in passing? People have argued strongly in the past that we should not focus on the modern PC in the lede here, but it seems that now, we hardly mention its existence, let along its importance in the modern world. Under software, the assembler and traffic-light examples would be familiar to readers beamed forward to now from the mid 1970s. 
:If I had to answers the anon's question above I would talk about the increase in CPU complexity and word width, the increases in typical RAM and disk space, and find the points where modern highstreet laptops overtook yesteryear's military secret supercomputers. I would talk about the dramatic increases in software complexity made possible by high-level programming languages, shared libraries, specialist domain-specific languages, object-orientation, test-driven development, scripting languages etc. I would talk about the internet and the way it has led to collaborative open-source development, the way that examples, answers and code tricks previously buried in books and manuals are now just Google search away without the developer leaving their desk. I would, but I don't have sources to hand, and anyway there's no room unless we shift some of the ancient history out to sub-articles too. --[[User:Nigelj|Nigelj]] ([[User talk:Nigelj|talk]]) 20:08, 5 April 2010 (UTC)

== Could wiki admin please add the topic of .... ==

[[Medical device]], which is related to compliance issue
--[[Special:Contributions/124.78.215.0|124.78.215.0]] ([[User talk:124.78.215.0|talk]]) 07:18, 18 April 2010 (UTC)

== An Hungarian invention? ==

What sources are listed that the computer is, in fact, an Hungarian invention?  <span style="font-size: smaller;" class="autosigned">—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/86.101.12.46|86.101.12.46]] ([[User talk:86.101.12.46|talk]]) 16:45, 4 May 2010 (UTC)</span><!-- Template:UnsignedIP --> <!--Autosigned by SineBot-->
:Noted; I took out both national categories. Everyone knows Tesla invented the computer anyway, so it should be a Serbian Croatian Serbian Croatian Scottish British....duck season...rabbit season... --[[User:Wtshymanski|Wtshymanski]] ([[User talk:Wtshymanski|talk]]) 18:15, 4 May 2010 (UTC)

== computer word come ==

Some people says computer has a full form means all word C O M P U T E R  has a different meaning
but computer has no full form because before computer,calculator is came into the market which 
can accept number(0-9) and it can take some process like addition,subtraction,multiply,etc. after some 
years when a new calculator came which can accept number(0-9)as well as character is called compute
after some years when a new advance compute machine is came which can accept number(0-9),character
all data types it has extra ability of process it can store the data for long time '''is called computer.'''  <span style="font-size: smaller;" class="autosigned">—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/117.96.82.247|117.96.82.247]] ([[User talk:117.96.82.247|talk]]) 02:57, 18 May 2010 (UTC)</span><!-- Template:UnsignedIP --> <!--Autosigned by SineBot-->

== Edit request from Bmgross, 6 June 2010 ==

{{tl|editsemiprotected}}
<!-- Begin request -->
For the example of some code that might control traffic lights, the made up keyword "THEN" is spelled in both lowercase and uppercase.  It appears that the text was meant to be typed in uppercase.

P.S.  This would be the second of the code examples, where they have decided to add an IF statement.

<!-- End request -->
[[User:Bmgross|Bmgross]] ([[User talk:Bmgross|talk]]) 03:20, 6 June 2010 (UTC)
:{{done}} <font face="Kristen ITC">[[User:Ctjf83|<font color="#ff0000">C</font><font color="#ff6600">T</font><font color="#ffff00">J</font><font color="#009900">F</font><font color="#0000ff">8</font><font color="#6600cc">3</font>]] [[User Talk:Ctjf83|pride]]</font> 03:34, 6 June 2010 (UTC)

by:joshua domingo  <span style="font-size: smaller;" class="autosigned">—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/110.55.182.2|110.55.182.2]] ([[User talk:110.55.182.2|talk]]) 08:41, 26 July 2010 (UTC)</span><!-- Template:UnsignedIP --> <!--Autosigned by SineBot-->

== This article is in need of major reorganization ==

I can’t believe this article was once a featured article; for such a basic, fundamental article, this article is terribly organized.  The individual items of content seem solid, but what’s really lacking is any overall structure.  If one takes a look at say the Russian or Hebrew versions of this article, one gets some idea how a proper article would look like.  Anyone have the initiative to do a major reorganization of the article?  As a first crack, I would suggest a ''very'' rough structure such as the following:
* Lead should be rewritten; suggest using Google Translate on the Russian or Hebrew or other versions to get some idea how this should be done.
# History
## Computers in antiquity
## Calculating machines
## Mechanical/electromechanical computers
## First general-purpose computers
## Stored-program computers
## The integrated circuit (IC)
## Semiconductors and transistors
## Microprocessors and microcomputers
## Parallel computing and multiprocessing
## Multicore processors
# Types of computers (in lieu of “Misconceptions”)
## Personal computers
## Workstations
## Servers
## Supercomputers
## Mobile computing [e. g. cell phones, tablet computers]
## Embedded computing
# Theory of computing (?)
## Boolean logic and binary computing
## Turing machine
# Computer architecture
## Von Neumann architecture
## Harvard architecture
# Typical computer organization and hardware
## Central processing unit (CPU)
## Arithmetic logic unit (ALU)
## Memory
## Input/output (I/O)
## Storage
## Computer networking
### The Internet
### World-Wide Web
# Computer programming and software
## Machine and assembly language
## High-level programming languages
## Firmware
## Operating systems
## Multitasking and multiprogramming
## Protocols and/or platforms
## Applications
# Economics/industry
# Future trends
## Artificial intelligence [but should be ''very'' brief]
### [Also mention notable milestones such as Deep Blue vs. Kasparov]
### The “Singularity”
## Networked computing
## Embedded and pervasive computing
## Quantum computing
## Biological and molecular computing
# Computers and society [or some b.s. section like that]
## Computer literacy
## Technological divide/technology gap
## Societal implications (?)
# See also, notes, references, external links, etc.
Btw, there is so much room for improvement here, that shouldn’t the article be unprotected to give more editors the opportunity to contribute/fix/reorganize this article?  This current “frozen” version is just awful.  The article could be so vastly improved, ''without'' adding to the length (in fact, it could cover everything above and still be shorter).  Any takers?  &mdash;[[User:Technion|Technion]] ([[User talk:Technion|talk]]) 13:06, 27 July 2010 (UTC)

If you think that it is need of a revamp, why not be bold? [[User:Sir Stupidity|Sir Stupidity]] ([[User talk:Sir Stupidity|talk]]) 02:26, 29 July 2010 (UTC)
:One word:{{nbsp}} finals.{{nbsp}} Btw, &sect;1.6 and &sect;1.7 above should obviously be swapped.{{nbsp}} My bad.{{nbsp}} {{mdash}}[[User:Technion|Technion]] ([[User talk:Technion|talk]]) 06:30, 29 July 2010 (UTC)

The number of sections/subsections as it stands is fine, and is akin to other similar articles in Wikipedia. History section (as in Televison, Electricity, Telephone articles) is a general overview and chronologically encompasses the major advances/innovations/people that led to the modern computer. Subarticles for less significant content would be ideal. [[User:Michael Jones jnr|Michael Jones jnr]] ([[User talk:Michael Jones jnr|talk]]) 22:30, 31 July 2010 (UTC)

== Overpopulation of the internet ==

In the internet section, shouldn't it  be noted that the number of available IPv4 addresses is diminishing rapidly due to overpopulation, and that without widespread use of IPv6, we will run out of IP addresses? [[User:Jhugh95|Jhugh95]] ([[User talk:Jhugh95|talk]]) 17:34, 16 August 2010 (UTC)
:Read [[IPv4 address exhaustion]] and then reply again.[[User:Jasper Deng|Jasper Deng]] ([[User talk:Jasper Deng|talk]]) 06:17, 17 February 2011 (UTC)

== Misuse of sources ==

This article has been edited by a user who is known to have misused sources to unduly promote certain views (see [[WP:Jagged 85 cleanup]]). Examination of the sources used by this editor often reveals that the sources have been selectively interpreted or blatantly misrepresented, going beyond any reasonable interpretation of the authors' intent.

Please help by viewing the entry for this article shown at the [[WT:Requests for comment/Jagged 85/Cleanup5|cleanup page]], and check the edits to ensure that any claims are valid, and that any references do in fact verify what is claimed.  [[User:Tobby72|Tobby72]] ([[User talk:Tobby72|talk]]) 16:57, 6 September 2010 (UTC)

== Typo (extra "a") ==

In the “Limited-function computers” section, the article says “…designed to make a copies of themselves…”

Can someone take out the extra particle? --[[Special:Contributions/91.156.254.185|91.156.254.185]] ([[User talk:91.156.254.185|talk]]) 18:43, 8 September 2010 (UTC)
:Done! Thanks for pointing it out. [[User:Franamax|Franamax]] ([[User talk:Franamax|talk]]) 18:53, 8 September 2010 (UTC)

== the Z3 ==

it is marked as the first fully functional computer in 1998, but the book it was published in was from 1998, and the computer was from 1941.  <small><span class="autosigned">—Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:Cpubuilder|Cpubuilder]] ([[User talk:Cpubuilder|talk]] • [[Special:Contributions/Cpubuilder|contribs]]) 03:00, 24 November 2010 (UTC)</span></small><!-- Template:Unsigned --> <!--Autosigned by SineBot-->

== Changing priority ==

I'm  curious, in the entire history of the Wikipedia, has changing the "importance" setting in a talk-page banner ever made a detectable difference in the article quality? Or is it like those extra thermostats on the walls of office buildings - a placebo you can fiddle with instead of doing any work? --[[User:Wtshymanski|Wtshymanski]] ([[User talk:Wtshymanski|talk]]) 05:26, 5 December 2010 (UTC)

:I can't answer your question directly - it's hard to know what motivates editors to work on particular things - and even harder to guess whether more editors makes for a better quality article or a worse one!  However, there are some things that it really does have an effect on.  For example, when people talk about making a "hard copy" Wikipedia - or a DVD version for shipping to 3rd world schools who may not have an internet connection...they start off by picking the high-importance articles and work their way down the importance scale until they run out of space (or whatever).  There are also people who monitor the importance-versus-quality indicators on these articles.  If all important articles have high quality ratings - then we have a good encyclopedia...if too many important articles are crap - then we don't.  So the fact that this article is off-the-charts-important does actually have some effect...just not (necessarily) in improving it.  [[User:SteveBaker|SteveBaker]] ([[User talk:SteveBaker|talk]]) 03:08, 6 December 2010 (UTC)

Hi... wellcome to computer world...  <small><span class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:Iniyanit|Iniyanit]] ([[User talk:Iniyanit|talk]] • [[Special:Contributions/Iniyanit|contribs]]) 07:03, 2 January 2011 (UTC)</span></small><!-- Template:Unsigned --> <!--Autosigned by SineBot-->

== Edit request from 87.228.229.206, 6 January 2011 ==

{{tl|edit semi-protected}}
<!-- Begin request -->

please change HPLaptopzv6000series.jpg to Acer Aspire 8920 Gemstone by Georgy.JPG as the first picture shows an obsolette PC with windows XP that are not so often used like windows seven which is the last picture
<!-- End request -->
[[Special:Contributions/87.228.229.206|87.228.229.206]] ([[User talk:87.228.229.206|talk]]) 15:32, 6 January 2011 (UTC)
:[[File:Red information icon with gradient background.svg|20px|link=|alt=]] '''Not done:'''<!-- Template:ESp --> too much copyrighted content on screen.  How about [[:File:2009 Taipei IT Month Day1 Viewsonic Viewbook 140.jpg]], [[:File:Computer-aj aj ashton 01.svg]], [[:File:Computer-blue.svg]], [[:File:Computer n screen.svg]], or [[:File:Personal Computer Pentium I 586.JPG]]?  &nbsp; — '''<font class="texhtml">[[User:Jeff G.|Jeff]] [[User:Jeff G./talk|G.&nbsp; ツ]]</font>''' 16:38, 6 January 2011 (UTC)

== Programming Languages ==

There shouldn't be so much detail describing and listing programming languages in this article. And according to the reviews of this article, others agree with me. It's fine to mention them but so much detail shouldn't be there.  <small><span class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:GuitarWizard90|GuitarWizard90]] ([[User talk:GuitarWizard90|talk]] • [[Special:Contributions/GuitarWizard90|contribs]]) 23:34, 7 January 2011 (UTC)</span></small><!-- Template:Unsigned --> <!--Autosigned by SineBot-->

== Data integrity ==

> A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation.

Hardware is nowhere near that reliable. Hence
* Memtest86
* RAID
* SMART for HDDs
* ECC RAM
* HDD remapping
and so on and so on.


> The 1980s witnessed home computers and the now ubiquitous personal computer. 

Apple II, 1977
[[User:Tabby|Tabby]] ([[User talk:Tabby|talk]]) 09:52, 1 March 2011 (UTC)
:Regarding the first item; it is talking about processing operations (I think) and in that it is accurate, but somewhat pointless. --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 10:30, 1 March 2011 (UTC)

:: Indeed.  You can't say "The computer makes mistakes - and that's why it needs ECC RAM" (or whatever).  The ECC RAM is a part of the computer - so the best you can say is "The computer doesn't often make mistakes - in part because it has ECC RAM".  I think it's worth mentioning this because computers are (by far) the most reliable machines humans have ever devised - when you measure average error rate per cycle of operation.  My car's engine has four cylinders and cycles at (let's say) 4,000 RPM making about a million "operations" per hour.  It's unlikely to run for more than 100,000 miles - a few thousand hours - without catastrophic hardware failure - making perhaps a few billion cycles in the process.  My computer does that many operations in a second!  Actually, ECC RAM doesn't do a whole lot to save that.
:: It's a '''SERIOUS''' mistake to think "PC" when we say "Computer".  The little Arduino computer that I've programmed to drive my robotic milling machine doesn't have ECC RAM, RAID, SMART or remapping software...and the fault rate is still zero over uncountable trillions of operations.  When was the last time your wristwatch crashed?
:: We need to say this in the article because there is a perception that computers crash and their programs fail quite often - however, 99.9% of the time (at least), that's because of human error - the software has bugs.  The computer hardware is doing precisely what the stupid human told it to do.  I doubt a day goes by without some piece of software misbehaving - but unrecovered hardware failure happens maybe once every few years.
:: [[User:SteveBaker|SteveBaker]] ([[User talk:SteveBaker|talk]]) 05:25, 7 March 2011 (UTC)

== ALU Operations ==

An arithmetic and logic unit does not perform multiplication or division, or any trigonometric functions.  This is incorrectly stated in the article.  ALUs can do basic arithmetic operations (addition and subtraction) and logic operations (AND,OR,NOT,XOR,NAND,NOR,XNOR) (p. 114 Digital Logic and Microprocessor Design with VHDL by Enoch O. Hwang).  Integer multiplication and division are handled by separate functional units within the pipeline.  Computers generally do not support trigonometric functions on integers but rather on floating point numbers so these are handled in the FPU.  Multiplication and division of floating point numbers are also handled in the FPU.

If a circuit is built to perform an arithmetic operation such as division or multiplication it is called a datapath, not an ALU.

[[User:Chrisfeilbach|Chrisfeilbach]] ([[User talk:Chrisfeilbach|talk]]) 05:28, 19 April 2011 (UTC)

== Unsubstantiated statement about the castle rock ==

The statement that the ''castle rock is considered to be the earliest programmable analog computer'' is '''unsubstantiated''' and therefore this paragraph '''should be removed until a reference is found'''.  <br>
The heading of this very talk page requests: '''No original research''' and '''verifiability'''.  In [[Wikipedia:Referencing for beginners]] the introduction states that '''Any editor can remove unreferenced material''' this is all I did, furthermore there is no such thing as a ''programmable analog computer''.  This paragraph needs to go until a reference is found.<br>  
--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 13:23, 22 April 2011 (UTC)
:First of all there are cited references and it has been kept in the article for so long. So, just removing the content is not wise; ''it is like I'm doing, coz' I can do''. Although I do not know nothing about the [[Castle clock]] it did not seem like false information. It is not a ''programmable analog computer'' in code-based programming. It says it can be programmed (calibrated) to adjust day length. Calling it programmable is hypothetically not incorrect. So, it is better to have a discussion. 
:I'll suggest a rewrite of the paragraph with much short note (1 or 2 sentence) will be a better choice of edit. --[[User:Nafsadh|<span style="color:#0077CC">নাফী ম. সাধ</span>&nbsp;]]'''<span style="color:#17afee">nafSadh</span>'''<sup>[[User talk:Nafsadh|talk]] | [[special:contributions/Nafsadh|contribs]]</sup> 18:07, 22 April 2011 (UTC)
::The references are pretty bad for that section. But the claim it makes is valid & accurate. A "programmable analog computer" is a reasonable description (although I appreciate it is a fine line between purely adjustable and programmable), of course a narrow definition of "computer" might make it an inaccurate description. But we use the broad definition here :) -'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 19:42, 22 April 2011 (UTC)
:::In the [[Wikipedia:Five pillars|five pillars of Wikipedia]] the second pillar says that '''all articles must strive for verifiable accuracy: unreferenced material may be removed, so please provide references'''.  The sentence: ''the castle rock is considered to be the earliest programmable analog computer'' is unreferenced and a programmable analog computer '''is yet to be found''' ; this paragraph is in the computer article because it has the computer word in it.  I'm sorry ErrantX but only a reference can decide if something is ''valid & accurate'' or NOT! '''Do the right thing, add a verifiable reference, that's all!'''--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 20:21, 22 April 2011 (UTC)
::::I can not verify the offline references. But the section and citation is '''convincing'''. Although, ''is considered to be the earliest programmable analog computer'' might be replaced with ''is claimed to be the earliest programmable analog computer''. Wikipedia cares about verifiability not truth though. Your argument "programmable analog computer '''is yet to be found'''" is denied by the paragraph & citations we are discussing on!  --[[User:Nafsadh|<span style="color:#0077CC">নাফী ম. সাধ</span>&nbsp;]]'''<span style="color:#17afee">nafSadh</span>'''<sup>[[User talk:Nafsadh|talk]] | [[special:contributions/Nafsadh|contribs]]</sup> 20:55, 22 April 2011 (UTC)
:::::-nafSadh: ''I can not verify the offline references''.  Cool, then we agree, this has to be removed until proven right ! 
:::::-nafSadh: ''Wikipedia cares about verifiability not truth though'': You've got to be kidding, right ? 
:::::--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 21:11, 22 April 2011 (UTC)
::::::Plz, read [[Wikipedia:Verifiability]]; also read [[Wikipedia:Citing_sources]]. ''I can not verify the offline references'' means I do not have means to reach the reference. It does not meen the source in unverifiable. Wikipedia cites a lot of offline materials - we do not ask for verifiability of each and every source just because we can not reach the source. If editors have question about some context which is uncited, then the context shall be removed; but when it is cited editor must have to consult with the reference. Only if the context is deviated from cite-source or the source is not citable then the context must be removed. Removal of cited context is not what we shall do. I tried to verify the source, the source itself seems authentic, but not all pages are available online. This source is also cited in many Wikipedia articles and other articles. So, If someone manages the source and finds that, the context is not in its p184 then it can be removed. You removed the context saying according to discussion, but we '''did not have agreed yet to remove'''. Please '''revert the removal''' and wait for some days until we can come to an agreement. 
::::::In addition '''the context you added do not match with your citations'''(cite-6). In cite-7 no page number is given. --[[User:Nafsadh|<span style="color:#0077CC">নাফী ম. সাধ</span>&nbsp;]]'''<span style="color:#17afee">nafSadh</span>'''<sup>[[User talk:Nafsadh|talk]] | [[special:contributions/Nafsadh|contribs]]</sup> 05:41, 23 April 2011 (UTC)
:::::::So. The information is ''not'' unreferenced. It is simply a "offline" source that is provided; please avoid treating this as a worse source than online ones (the contempt people sometimes show for books as sources is... mad! :D). So there is no need to remove the text outright (though I support cutting it down). If you have to go to some effort to verify the reference then you simply have to go to that effort! I'll look for some sources tonight, I found a couple yesterday, I need to dig into my bookshelves to verify something. It is generally considered bad manners to simply remove referenced material outright - instead the proper approach is to ask for verification and discuss the material. I agree if this cannot be verified then it needs to go. However I think we can manage it without too much difficulty :) --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 09:30, 23 April 2011 (UTC)
::::::::Please re-include the reference for Castle clock. It'll be better to keep citations. as the source is verifiable, it is ridiculous to think that some editor added the citation without verifying it. --[[User:Nafsadh|<span style="color:#0077CC">নাফী ম. সাধ</span>&nbsp;]]'''<span style="color:#17afee">nafSadh</span>'''<sup>[[User talk:Nafsadh|talk]] | [[special:contributions/Nafsadh|contribs]]</sup> 19:25, 23 April 2011 (UTC)

I too think the castle-clock-as-computer is too far fetched to be plausible [[User:William M. Connolley|William M. Connolley]] ([[User talk:William M. Connolley|talk]]) 21:00, 29 May 2011 (UTC)

I note also the above assertions that there is a reference, but no-one has read it. Please be aware (N certainly should be, since he has participated in the debates) that this bears all the hallmarks of Jagged85-ism, who was known to misrepresent references. See [[Wikipedia talk:Requests for comment/Jagged 85/Cleanup]] [[User:William M. Connolley|William M. Connolley]] ([[User talk:William M. Connolley|talk]]) 21:06, 29 May 2011 (UTC)
:I'm not sure if it's in this discussion or note (I got rather lost TBH) but I did track down the source and check it out - the result was really "meh" and it doesn't really support the claim that was made. The current content (Ezrdr tweaked it I think) is better, accurate and cited. --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 21:23, 29 May 2011 (UTC)
:: Thanks; "meh" sounds like about what one would expect when checking up on the Jagged stuff. If the current text really is verified you may or may not want to revert my complete removal of ref to the Castle clock wot I just did, though my feeling is still that the claim is still quite tenuous [[User:William M. Connolley|William M. Connolley]] ([[User talk:William M. Connolley|talk]]) 21:30, 29 May 2011 (UTC)

===Unreferenced===
However, talking of unreferenced. The paragraph about mechanical calculators (the part that begins ''In 1642, the Renaissance saw the invention of the mechanical calculator, a device...'') is ''completely'' without sources :S Ezrdr, do you have a source to support the assertions made in that paragraph? --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 09:38, 23 April 2011 (UTC)

:Hi Errant, A couple of ''online references'' so that you can quickly double check them for accuracy.  First, a reference from [[Dorr Felt|Dorr E. Felt]] the inventor of the [[Comptometer]] that you can read on page 10 of his book ([http://www.archive.org/details/mechanicalarithm00feltrich Mechanical arithmetic, or The history of the counting machine], Dorr E. Felt, Washington Institute, Chicago, 1916): "The most famous was by Pascal in 1642. '''He is credited with being the first to make a mechanical calculator'''. But I do not think he fully deserved it. He didn't make one that would calculate accurately, even if you handled it with the greatest care and took hold of the wheels and cogs after taking the top off the machine, trying to help them along. I have tried them myself on several of his machines which are preserved, and making due allowance for age they never could have been in any sense accurate mechanical calculators".  Here is a quote from [http://books.google.fr/books?id=Rf0IAAAAIAAJ&pg=PA100&dq=arithmometer&as_brr=1#v=onepage&q=arithmometer&f=false The Gentleman's magazine, Volume 202, p.100]: "Pascal and Leibnitz, in the seventeenth century, and Diderot at a later period, '''endeavored to construct a machine which might serve as a substitute for human intelligence in the combination of figures''';". 
:It is the duty of any Wikipedian to remove unsound and unreferenced statements
:--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 12:56, 23 April 2011 (UTC)
::Cool. However I am more concerned with the second part of the paragraph which makes assertions about this being the basis for various modern computing parts. Especially the part about the Microprocessor & Intel; is there a source that states this link? --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 12:59, 23 April 2011 (UTC)
:::No problem: [http://www.intel.com/about/companyinfo/museum/exhibits/4004/index.htm Intel Museum - The 4004, Big deal then, Big deal now]
:::--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 13:06, 23 April 2011 (UTC)
:::or that: [[http://www.sciencemag.org/content/261/5123/864.abstract Microprocessors: From Desktops to Supercomputers]]
:::--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 13:09, 23 April 2011 (UTC)
::::Am I missing something on the first link? I can't see anything to support hat the micro-processor ''was invented serendipitously by Intel during the development of an electronic calculator, a direct descendant to the mechanical calculator.'' (i.e. I don't see mention of mechanical calculators :S). I'll have to download the other PDF when I get home, I don't have my Athens login here. Also; do we have a source for: '' initially, it is in trying to develop more powerful and more flexible calculators that the computer was first theorized'' - that seems fairly logical, but best to tie it up with a source (as you pointed out above) --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 13:28, 23 April 2011 (UTC)
:::::Inclusion of the word ''serendipitously'' is disputable. I strongly prefer the removal of it. --[[User:Nafsadh|<span style="color:#0077CC">নাফী ম. সাধ</span>&nbsp;]]'''<span style="color:#17afee">nafSadh</span>'''<sup>[[User talk:Nafsadh|talk]] | [[special:contributions/Nafsadh|contribs]]</sup> 19:25, 23 April 2011 (UTC)
===From sublime to ridiculous===
to ErrantX,<br>
First of all I want to recount the facts:
* I removed an unreferenced edit (I mentioned it in the ''edit summary'')
* My edit is revoked telling me to explain (instead of providing a reference)
* I explained that what I had removed didn't make sense, but that a valid reference would prove me wrong
* I rewrite the paragraph, still mentioning the castle clock, just removing the claim to first ...
* And now I have to reference every single word of my edits, I have to explain serendipity, I have to explain that the [[electronic calculator]] comes from the [[mechanical calculator]] ! <br>
[[Serendipity]] is when you find something that you were not expecting to find.  This is 1969, Intel, a young startup at the time, was developing some chips for an electronic calculator for the Japanese firm [[Busicom]].  Intel simplifies the 12 chip set requested down to 4 and delivers the calculator in early 1971.  But very quickly they realized that they had created a lot more than a set of calculator chips (this is the '''not expecting to find''' part of serendipity), but they do not own the design, fortunately Busicom renegotiates the deal for cheaper prices and they are able to share the ownership of the chip set (MSC-4) with Intel as long as they do not use it for electronic calculators (another serendipity) people use it from Prom Programmers, printers, teletype machines... Intel markets the 4004 a few months later.  The 4004 was never patented just because of that. 4 years later the first PC [[Altair 8800]] was released using a fourth generation Intel design (the 8080 after the 8008, the 4040 and the 4004)
<br>
Now, before I continue, you say that you need to go home in order to read some of my references, so please read them and we'll talk about them tomorrow.
--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 16:06, 23 April 2011 (UTC)
:Yes.. I am with you, and have knowledge of everything you have written above. ''However'' whilst that is sourceable information that we can deal with in the context of a reasonable timeline.... do you have a source that deliberately deals with the relationship to ''mechanical calculators''. I mean, sure, there are obvious ties in terms of the historical development of computing. But you've gone beyond that with the strong claim that these early mechanical calculators influenced modern computers. It's no longer just a timeline of development, it is analysis/opinion/commentary. That definitely needs a specific source :) I might be missing something, again, in that other source (I will have to read it when more awake) but I, again, do not see the part that supports the assertion being made. --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 18:19, 23 April 2011 (UTC)
::[[User:Ezrdr|Ezrdr]] discussing some of your points. please don't be ridiculed. :
::* you removed an <s>un</s>'''referenced''' edit.
::* Your edit was revoked because removal of cited content should be explained (it is unnecessary to ''provide'' a reference for an already cited text)
::* you explained that what you had removed didn't make sense. but all we can see is references talk against what you sense. I might have not mentioned castle clock by myself, but as it has been added, I disagree removal of it.
::* You rewrite the paragraph, although mentioning the castle clock, ''including new claims''
::* adding reference is good to keep things unchallenged. IN CONTRARY, you already have challenged cited sources and context. Wikipedia is not what you or I think, Wikipedia is what books, texts, pages, sources etc think.
::I feel that, removing cited text might ridicule the editor who added it. --[[User:Nafsadh|<span style="color:#0077CC">নাফী ম. সাধ</span>&nbsp;]]'''<span style="color:#17afee">nafSadh</span>'''<sup>[[User talk:Nafsadh|talk]] | [[special:contributions/Nafsadh|contribs]]</sup> 19:25, 23 April 2011 (UTC)
===Reference showing that Electronic calculators come from Mechanical calculators===
to ErrantX,<br>
Please read the Wikipedia article [[Sumlock ANITA calculator#History of ANITA calculators]].  Just stay clear of unreferenced claims though. ;-)--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 19:32, 23 April 2011 (UTC)
:I'm sorry but it would be a lot lot easier if you could just cite the claim. So far I have been unable to verify the analysis in that paragraph. I suspect we'll have to move it down a bit. Look; my problem is that the section is really making a big deal of the influence. There is no obvious connection between Intel's creation of the Microprocessor and the older mechanical calculators, it was part of the historical development of computers. Do you see why I think it is necessary to cite this piece of analysis of the history? At the moment my feeling is the material is best moved to the correct place in the time-line. To be clear: at the moment it looks like OR and SYNTH.
:Also; the first part of that analysis is still uncited (that being a more direct influence we can probably cite it and leave it be).
:FYI I can't find a decent source for the claim you removed; looks like you might be right. Though I am trying to get hold of the Discovery documentary to see what it is like. 
:You probably don't need to create more and more section headers. --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 20:53, 23 April 2011 (UTC)
::I agree with. [[user:ErrantX|Errant]] --[[User:Nafsadh|<span style="color:#0077CC">নাফী ম. সাধ</span>&nbsp;]]'''<span style="color:#17afee">nafSadh</span>'''<sup>[[User talk:Nafsadh|talk]] | [[special:contributions/Nafsadh|contribs]]</sup> 20:58, 23 April 2011 (UTC)
to ErrantX,<br>
First of all thank you for doing the research on the castle clock, I came to the same conclusion you did so far, but please bring it back if you find a verifiable reference.<br>
At this point I don't understand what you don't understand about the paragraph on mechanical calculators.  It is in the chapter '''Limited-function early computers''' because there is no other place for the description of something that is not a computer but which is one of its most direct ancestor ... in two ways.  Please cite what doesn't make sense and I'll make sure it's referenced.  I have about 50 books on the subject, in French and in English, I taught microprocessor classes, as a student, in the late 70s and I have an MSEE. I've programmed on mainframes, minis and PCs and designed disk drive firmware from scratch.  So please, shoot, I'm ready for you ;-)<br>
--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 22:40, 23 April 2011 (UTC)
:Um, well ,I thought I had explained the issue above :S I am trying to! But to put it another way. That section says several things (to paraphrase slightly):
:* Mechanical calculators evolved into digital calculators
:* While working on digital calculators Intel discovered the microprocessor
:* Therefore mechanical calculators influenced the discovery/development of the microprocessor
:The first two are facts. The last is an editorial/opinions/analysis for which ''we must have a source''. Otherwise it is something of OR. The problem is not that you have stated these two facts. But that you have tied them together, across hundreds of years of development, without a source to back it up. This is the principle of OR and SYNTH.
:There is also another piece of analysis: ''in trying to develop more powerful and more flexible calculators that the computer was first theorized''. Right now I cannot find a good source for it, so I would really appreciate a clear source for this analysis. The reason I am questioning it is because it lists Babbage as one person who was influenced in this way. From my knowledge of Babbage, his work was not related to developing existing calculators but related to his mathematical work (well, obsession really :)). Turing's main work on computers was primarily theoretical and so the link is even more tenuous! Hence, I am not sure that the assertion holds.. they have influence for sure. But I think that it is incorrect to say that it was due to people working on developing better calculators that the field was progressed. I think you'd struggle to find a source that makes such a broad claim! --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 23:07, 23 April 2011 (UTC)
::I asked to re-include the reference for Castle clock on original section or the talk; yet did not get any reply. --[[User:Nafsadh|<span style="color:#0077CC">নাফী ম. সাধ</span>&nbsp;]]'''<span style="color:#17afee">nafSadh</span>'''<sup>[[User talk:Nafsadh|talk]] | [[special:contributions/Nafsadh|contribs]]</sup>
::Ezrdr said, "''please bring it back if you find a verifiable reference"'', but the reference is already verifiable; you could not just reach the source :S   --[[User:Nafsadh|<span style="color:#0077CC">নাফী ম. সাধ</span>&nbsp;]]'''<span style="color:#17afee">nafSadh</span>'''<sup>[[User talk:Nafsadh|talk]] | [[special:contributions/Nafsadh|contribs]]</sup> 03:48, 24 April 2011 (UTC)
:::Do we really need this context, "''leading to the development of mainframe computers in the 1960s, but also the microprocessor, which started the personal computer revolution, and which is now at the heart of all computer systems regardless of size or purpose[16], was invented serendipitously by Intel[17] during the development of an electronic calculator, a direct descendant to the mechanical calculator[18].''" in ''Limited-function early computers'' section? It is a opinion or opinionated description. We do NOT need to place OPINIONs in Wikipedia. --[[User:Nafsadh|<span style="color:#0077CC">নাফী ম. সাধ</span>&nbsp;]]'''<span style="color:#17afee">nafSadh</span>'''<sup>[[User talk:Nafsadh|talk]] | [[special:contributions/Nafsadh|contribs]]</sup> 11:55, 24 April 2011 (UTC)

===Definition of Harassment===
Now, before any more is written, both of you need to check the Wikipedia definition of [[harassment]].<br>
To refresh your memory:<br>
I removed a statement which didn't make sense (there is no such thing as a programmable analog computer) and which reference pointed to nowhere and I am now subject to the nonsense displayed above this paragraph, having to explain every single words of my edits, being ordered around, ridiculed !<br>
Errant, you are overstepping your administrator's authority and ruining the spirit of Wikipedia:
*No, I do not have to seek your approval for the edits I make.  
*Edits don't have to make sense to you in order to be kept in this article.  
Errant, as the overseer of the Computer article (or at least you are behaving like it), I cannot believe the ignorance you showed about Babbage's work and his contribution to our modern digital society.  <br>As a Wikipedian I must be able to participate without being subjected to this kind of virtual verbal [[mugging]].
<br>
--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 07:38, 25 April 2011 (UTC)
:No <s>Now</s>, I <s>also</s> feel harassed! 
:instead of going to an edit war we tried to resolve some dispute in talk. 
:*No, you do not have to seek someones approval for the edits you make.  
:*Edits don't have to make sense to someone in order to be kept in this article.  
:likewise, 
:*I can delete/undo/revert/edit any of your or someones edit. Undoing and redoing edits coz edit war.
:*You removed a statement which didn't make sense to you, but made sense to some many editors.
:I felt offended by some of your talk revealing your own level of expertise which seemed like you looked down on us.
:--[[User:Nafsadh|<span style="color:#0077CC">নাফী ম. সাধ</span>&nbsp;]]'''<span style="color:#17afee">nafSadh</span>'''<sup>[[User talk:Nafsadh|talk]] | [[special:contributions/Nafsadh|contribs]]</sup> 09:02, 25 April 2011 (UTC)
:: I am not here as an admin, but an editor, sorry if there has been confusion on that - but I don't see where I implied as such, or used the tools :S I appreciate this may well be your field of specialism, however it is important to be able to verify all material added to the article. I am sorry if you feel ridiculed, but I cannot yet see an accurate source for the material you have added to the article. As you pointed out above, verification is crucial! Multiple problems exist with that paragraph that you simply have not been able to address. (I wrote a whole section on the problems... but am not posting it because I went into detail on the problems, which you mostly ignored, above).
:: You ''correctly'' questioned the other material, but you could have done it better :S Now I am questioning the material you have added, that you added without a source. Perhaps I could have done it better. But it still needs a source.
:: Programmable analog computers; please explain what is wrong with such a concept? Heathkit's were programmable, for example.
::Finally; perhaps it is a good idea to get a [[WP:3O]] on this. --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 10:20, 25 April 2011 (UTC)
:::I '''support''' the idea of getting a [[WP:3O]]. --[[User:Nafsadh|<span style="color:#0077CC">নাফী ম. সাধ</span>&nbsp;]]'''<span style="color:#17afee">nafSadh</span>'''<sup>[[User talk:Nafsadh|talk]] | [[special:contributions/Nafsadh|contribs]]</sup> 10:48, 25 April 2011 (UTC)
::::There are too many discussions in that, well, discussion.  So I have created new discussions for anything not related to the one described by this heading.  Babbage has nothing to do with the use of a reference that points to a dead reference.
--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 13:41, 25 April 2011 (UTC)

== Was the computer first Theorized By Babbage while trying to develop more powerful mechanical calculators ==

My understanding, from everything I have read, is that it is in trying to develop more powerful and more flexible mechanical calculators (differential and analytical engines) that the computer was first theorized.  I have two references to defend that position, I've used them both in my contribution to the Computer article:
*"It is reasonable to inquire, therefore, whether it is possible to devise a machine which will do for mathematical computation what the automatic lathe has done for engineering.  The first suggestion that such  a machine could be made came more than a hundred years ago from the mathematician Charles Babbage.  Babbage's ideas have only been properly appreciated in the last ten years, but we now realize that he understood clearly all the fundamental principles which are embodied in modern digital computers" ''Faster than thought'', edited by B. V. Bowden, 1953, Pitman publishing corporation
*"...Among this extraordinary galaxy of talent Charles Babbage appears to be one of the most remarkable of all.  Most of his life he spent in an entirely unsuccessful attempt to make a machine which was regarded by his contemporaries as utterly preposterous, and his efforts were regarded as futile, time-consuming and absurd.  In the last decade or so we have learnt how his ideas can be embodied in a modern digital computer.  He understood more about the logic of these machines than anyone else in the world had learned until after the end of the last war" Foreword, ''Irascible Genius, Charles Babbage, inventor'' by Maboth Moseley, 1964, London, Hutchinson
--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 13:26, 25 April 2011 (UTC)
:Well, quote one is an excellent reference regarding Babbage's vision. Second quote is much the same, as well as noting that his work was laughed at by his contemporaries. However, neither appears to support the statement that Babbage was developing more powerful mechanical calculators... :S Unless I am missing something obvious --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 13:58, 25 April 2011 (UTC)
Since ErrantX, the main protagonist, has left this discussion [[Talk:Computer#Is the Electronic calculator a direct descendant of the Mechanical calculator ?|(in this sub-section)]], I would like to bring it to a close with a quote from the [http://www.cbi.umn.edu/about/babbage.html Charles Babbage Institute]: "The calculating engines of English mathematician Charles Babbage (1791-1871) are among the most celebrated icons in the prehistory of computing. Babbage’s Difference Engine No.1 was the first successful automatic calculator and remains one of the finest examples of precision engineering of the time. Babbage is sometimes referred to as "father of computing." The International Charles Babbage Society (later the Charles Babbage Institute) took his name to honor his intellectual contributions and their relation to modern computers."<br>--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 11:12, 26 April 2011 (UTC)
===Was Babbage developing more powerful mechanical calculators ?===
It is important to separate all new questions from already answered ones, therefore this sub-heading.<br>  
In the proposal that [[Howard Aiken]] gave [[IBM]] in 1937 while requesting funding for the [[Harvard Mark I]] which became IBM's entry machine in the computer industry, we can read: "Few '''calculating machines''' have been designed strictly for application to scientific investigations, the notable exceptions being those of Charles Babbage and others who followed him.  In 1812 Babbage conceived the idea of a calculating machine of a '''higher type than those previously constructed''' to be used for calculating and printing tables of mathematical functions. ....After abandoning the difference engine, Babbage devoted his energy '''to the design and construction of an analytical engine of far higher powers than the difference engine'''..." Howard Aiken, ''Proposed automatic calculating machine'', reprinted in: The origins of Digital computers, Selected Papers, Edited by Brian Randell, 1973, {{ISBN|3-540-06169-X}}
--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 14:27, 25 April 2011 (UTC)

== Can the invention of the microprocessor by Intel while developing a calculator engine be called Serendipity ==

Serendipity comes to mind when reading the content of the Intel museum web site: [http://www.intel.com/about/companyinfo/museum/exhibits/4004/index.htm - The 4004, Big deal then, Big deal now]<br>
Serendipity is when you find something that you were not expecting to find. This is 1969, Intel, a young startup at the time, was developing some chips for an electronic calculator for the Japanese firm Busicom. Intel simplifies the 12 chip set requested down to 4 and delivers the calculator in early 1971. But very quickly they realized that they had created a lot more than a set of calculator chips (this is the '''not expecting to find''' part of serendipity), but they do not own the design, fortunately Busicom renegotiates the deal for cheaper prices and they accept to share the ownership of the chip set (MSC-4) with Intel as long as Intel does not use it for electronic calculators. Intel markets the 4004 a few months later.  Designers use it from Prom Programmers, printers, teletype machines...  4 years later the first PC Altair 8800 was released using a fourth generation Intel design (the 8080 after the 8008, the 4040 and the 4004), 6 years later the Apple II was born (using a different brand of microprocessor though)
--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 13:35, 25 April 2011 (UTC)
:I will say this very clearly, one last time because I don't think it has got in yet :( The problem is not largely with the facts, it is the presentation. Calling it serendipitous might be true, but it is still OR without a source that says it was fortune/accidental. Something the source does not say. My other major issue is that all of this is shoved at the end of the section about mechanical calculators in the 1600's - it is a strong claim of influence across the ages of development of computing. And so needs a strong specific source. You are placing a strong focus on the importance of mechanical calculators without actually supporting it in a source (instead trying to shore it up with some tangentially related sources) --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 14:02, 25 April 2011 (UTC)
ErrantX, the main protagonist, has left this discussion [[Talk:Computer#Is the Electronic calculator a direct descendant of the Mechanical calculator ?|(in this sub-section)]], so I guess this discussion is over, but I'd like to bring about a different question which stemmed from this ''debate'': '''In Wikipedia, can an appropriate neologism be used to describe an event that happened and was described in the media before it became popular?'''.  I don't think that this question should be discussed here, but it might be worth thinking about it. For anyone wanting to research the events surrounding the invention of the first microprocessor further, I'm joining the references of the two books that I have on the subject:
*''Bit by Bit, <small>an illustrated history of computers</small>'' by Stan Augarten, Ticknor & Fields, 1984
*''The untold story of the computer revolution, <small>Bits, Bytes, Bauds & Brains</small>'' by G. Stine, Arbor house, 1985 
In this last book, you will read on page 164,165 that "In spite of a management structure that favored and encouraged innovation, the introduction of the first microprocessor chip met with some hesitation on the part of Intel's board of directors....Intel's marketing people estimated that the entire worldwide market for microprocessors would be only a few thousand units per year!".  <br>Also the movie [[Serendipity (film)|Serendipity ]] that made me discover this word and its meaning was released in 2001.<br>
--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 15:29, 26 April 2011 (UTC)
:I haven't left. I've let you have your way for the moment, because it was like beating my head against a brick wall communicating the problem to you :) You have a moratorium while I go through the big pile of source material I dug out to research this, but I do still strongly suggest you a) find a source (which still has not appeared) and b) try to improve the wording (which leaves a lot to be desired). Will be back with sourced suggestions in a week or so. r.e. your question above.. it quite obviously depends on the context, subject and the sources. So the answer is Yes. And No. --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 15:48, 26 April 2011 (UTC)
::Welcome back, just realize that we are only talking about what's in the title of this discussion.--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 16:29, 26 April 2011 (UTC)
:::Are you referring to "serendipity" when you talk about neologism above? The word is old, far older than the microprocessor! So hardly an issue. On the other hand no source has currently be provided to support such an analysis of the invention --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 16:43, 26 April 2011 (UTC)
::::May be it has turned into a VERY LONG discussion regarding very small amount of text. Almost 36kB of talk incurred on changes about 1kB :S
::::Feeling much better that discussion has became moderate once again. I hope no one is feeling harassed or ridiculed anymore. In most cases my (tiny little brain's) insight coincide with ErrantX's. Although, I'm observing the talk, my responses are limited. Hope to see a consensus in general soon. --[[User:Nafsadh|<span style="color:#0077CC">নাফী ম. সাধ</span>&nbsp;]]'''<span style="color:#17afee">nafSadh</span>'''<sup>[[User talk:Nafsadh|talk]] | [[special:contributions/Nafsadh|contribs]]</sup> 17:02, 26 April 2011 (UTC)
Please stick to the current discussion, which is described by its title.  It's OK to discuss my rhetorical question about the use of neologisms but it has to happen in a different discussion.  <br>This is very simple:
*Serendipity is when you find something that you were not expecting to find
*Intel invented the first microprocessor while Busicom, a Japanese company, was paying them to develop a set of programmable calculator chips
That's it!<br>
If you have a different question, it should be in a different discussion. Please!<br>--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 17:50, 26 April 2011 (UTC)
:The word is intended to mean a fortunate discovery or a piece of luck, not just an accidental one. So, yes, it does need to be sourced as a ''fortunate'' discovery. {{small|You started the blooming off-topic discussion!}}--'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 18:02, 26 April 2011 (UTC)
::What's '''''fortunate''''' is that Busicom's engineers, management and marketing teams did not understand what was in their electronic calculator engine, otherwise (they OWN the design), it could have been: '''Busicom inside''' ....<br>
::OK, the story: Ted Hoff, '''employee number 12''' , is hired in 1968, a year before the project (ref: Stine, p.163).  It is a very, very small company and now here is what happened (ref:Stine, p163):
::"After a thorough analysis of the Busicom requirements, Hoff concluded that the proposed Busicom calculator design was far too expensive to be cost-effective in the '''desk calculator''' marketplace of the time.  He felt that what Busicom wanted '''wasn't possible''' with the state of the art and the price structure that Busicom would have to set up to stay in business, even with manufacturing costs in Japan being far lower than in the United States.  But Ted Hoff was also experimenting with one of the first IC computers, the PDP-8 minicomputer made by DEC..."
::Please, take the time to read this reference, talk to friends about it, challenge its authenticity, but it is time to end this charade!
::--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 05:37, 27 April 2011 (UTC)
:::Which is an interesting source about how the microprocessor was invented and came to be Intel's beast. However nothing in there describes it as fortunate or serendipitous; as you pointed out above all material must be sourced to a verifiable reference. You appear to be sourcing the story of 
the discovery of the microprocessor - and then saying "wasn't that lucky". It may well be, but your view is irrelevant :) --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 08:10, 27 April 2011 (UTC)

== Is the Electronic calculator a direct descendant of the Mechanical calculator ? ==

The first Electronic calculator was the Anita Mk7 developed by Sumlock Comptometer, the British maker of the Comptometer mechanical adding machine.  The calculator had the same basic user interface found on mechanical and electromechanical machines, with sets of columns of 9 keys.  The story of its invention can be found in the second paragraph of [[Sumlock ANITA calculator#History of ANITA calculators]]
--[[User:Ezrdr|Ezrdr]] ([[User talk:Ezrdr|talk]]) 14:04, 25 April 2011 (UTC)
:This is unrelated to the issues I raised above, and certainly not something I would dispute. --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 14:12, 25 April 2011 (UTC)
::You know what, I fucking give up, it's impossible to have a reasoned discussion over here, so have it your way. --'''[[user:ErrantX|Errant]]''' <sup>([[User_talk:ErrantX|chat!]])</sup> 14:19, 25 April 2011 (UTC)
:::My strong stand is that, disputed edits by [[User:Ezrdr|Ezrdr]], those we wanted and tried to resolve, should be eliminated. Any other editors' act on this regard is NECESSARY. 
:::It looks like that a resolution is not possible here. --[[User:Nafsadh|<span style="color:#0077CC">নাফী ম. সাধ</span>&nbsp;]]'''<span style="color:#17afee">nafSadh</span>'''<sup>[[User talk:Nafsadh|talk]] | [[special:contributions/Nafsadh|contribs]]</sup> 15:25, 25 April 2011 (UTC)

== Edit request from Andres.felipe.ordonez, 30 May 2011 ==

{{edit semi-protected|answered=yes}}
<!-- Begin request -->

Grammar error: 1st paragraph in the "Programs" section

change:
"...modern computers based on the von Neumann architecture ARE OFTEN HAVE machine code in the form of..."

to:
"...modern computers based on the von Neumann architecture OFTEN HAVE machine code in the form of..."
<!-- End request -->
[[User:Andres.felipe.ordonez|Andres.felipe.ordonez]] ([[User talk:Andres.felipe.ordonez|talk]]) 19:31, 30 May 2011 (UTC)
:{{done}}[[User:Jasper Deng|Jasper Deng]] [[User talk:Jasper Deng|(talk)]] 19:48, 30 May 2011 (UTC)

== Missing google chrome operating system from Operating system section ==

google chrome os  has been in beta for quite some time and is and is launching on june 15th.  <span style="font-size: smaller;" class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/173.16.191.111|173.16.191.111]] ([[User talk:173.16.191.111|talk]]) 06:42, 1 June 2011 (UTC)</span><!-- Template:UnsignedIP --> <!--Autosigned by SineBot-->

: Rather more importantly, we seem to be missing an "operating system" section entirely [[User:William M. Connolley|William M. Connolley]] ([[User talk:William M. Connolley|talk]]) 07:24, 1 June 2011 (UTC)
::IP user might be talking about the ''Operating system'' row-group of ''Computer software'' table. Using the word section (specially by a new user) is not a sin!
::Whatever, Google Chrome OS is not a new OS, it is already listed, coz it is just another flavor of Linux. » ''[[User:Nafsadh|<span style="color:#004F99">nafSadh</span>]] [[special:contributions/Nafsadh|did]] [[User talk:Nafsadh|say]]'' 14:55, 1 June 2011 (UTC)
:::Agreed.  Chrome is more properly a "Window Manager" - like KDE or Gnome.  If you take one of those fancy Cr-48 Chrome-powered laptops that Google gave away last year and flip the little switch that's hidden behind a black sticker at the back of the battery compartment, you'll see that it boots up in to an absolutely standard Linux "shell" and you can do all of the usual command-line stuff with no sign of any Chrome.  Ditto Android, Kindle and a whole bunch of other things that falsely lay claim to being "operating systems". [[User:SteveBaker|SteveBaker]] ([[User talk:SteveBaker|talk]]) 15:36, 1 June 2011 (UTC)

EXPLAIN THE GENRATIONS OF COMPUTER IN EASY WORLDING AS U CAN
AN ALSO HEADINGS OF CHRACTRISTICS OF COMPUTER & USES OF COMPUTER  <span style="font-size: smaller;" class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/61.5.137.131|61.5.137.131]] ([[User talk:61.5.137.131|talk]]) 09:44, 10 June 2011 (UTC)</span><!-- Template:UnsignedIP --> <!--Autosigned by SineBot-->
:For easy wording, you can go to [[simple:Computer|Simple English Wikipedia]] » ''[[User:Nafsadh|<span style="color:#004F99">nafSadh</span>]] [[special:contributions/Nafsadh|did]] [[User talk:Nafsadh|say]]'' 10:53, 10 June 2011 (UTC)

== Ugly gallery ==

The ugly gallery in the lead should go. Either use one image representing "the computer" (maybe the ENIAC or some other old device which doesn't hit on anyone's pet computer as much as a more recent image would do or a decent [[collage]] like in the [[1980s]] article. '''S'''<sub>peak</sub>'''F'''<sub>ree</sub> [[User talk:SpeakFree|(talk)]]  01:15, 30 June 2011 (UTC)
:The [[Colossus computer]] was not involved in breaking the cipher messages from the [[Enigma machine]], rather that from from the [[Lorenz cipher|Lorenz cipher SZ40/42]].--[[User:TedColes|TedColes]] ([[User talk:TedColes|talk]]) 05:36, 30 June 2011 (UTC)
:I agree that the ugly conglomerate of images needed to go.  The Colossus pic was reverted because it wasn't stored-program.  I found a usable shot of EDSAC and put that in. -[[User:R. S. Shaw|R. S. Shaw]] ([[User talk:R. S. Shaw|talk]]) 07:10, 2 July 2011 (UTC)
::The gallery now looks nice. If you change it, please don't mess it up by introducing unnecessary whitespaces. [[User:SpeakFree|'''S'''<sub>peak</sub>'''F'''<sub>ree</sub>]] <sup>[[User talk:SpeakFree|(talk)]]</sup><sub>([[Special:Contributions/SpeakFree|contribs]])</sub> 22:59, 28 March 2012 (UTC)

== Edit request from 82.128.87.32, 6 August 2011 ==

{{edit semi-protected|answered=yes}}
<!-- Begin request -->


<!-- End request -->
[[Special:Contributions/82.128.87.32|82.128.87.32]] ([[User talk:82.128.87.32|talk]]) 08:50, 6 August 2011 (UTC)
'''A computer is called a system ==''' 
''' It is programmable machine designed to sequentially and automatically carry out a sequence of arithmetic or logical operations. The particular sequence of operations can be changed readily, allowing the computer to solve more than one kind of problem.

Conventionally a computer consists of some form of memory for data storage, at least one element that carries out arithmetic and logic operations, and a sequencing and control element that can change the order of operations based on the information that is stored. Peripheral devices allow information to be entered from an external source, and allow the results of operations to be sent out.

A computer's processing unit executes series of instructions that make it read, manipulate and then store data. Conditional instructions change the sequence of instructions as a function of the current state of the machine or its environment.

The first electronic computers were developed in the mid-20th century (1940–1945). Originally, they were the size of a large room, consuming as much power as several hundred modern personal computers (PCs).[1]

Modern computers based on integrated circuits are millions to billions of times more capable than the early machines, and occupy a fraction of the space.[2] Simple computers are small enough to fit into mobile devices, and mobile computers can be powered by small batteries. Personal computers in their various forms are icons of the Information Age and are what most people think of as "computers". However, the embedded computers found in many devices from mp3 players to fighter aircraft and from toys to industrial robots are the most numerous.

:It would help if you could identify what you want changed. --[[User:Wtshymanski|Wtshymanski]] ([[User talk:Wtshymanski|talk]]) 20:06, 6 August 2011 (UTC)

== Magnetic storage ==

[[Edam]] said that "Zuse also later built the first computer based on magnetic storage in 1955". The [[Manchester Mark 1]] used magnetic drum storage in 1949. --[[User:TedColes|TedColes]] ([[User talk:TedColes|talk]]) 06:49, 30 October 2011 (UTC)

== Limited function computers ==

Hello, I want to object to the last sentence of the ''Limited function computers'' section, which, as of today, claims that "Living organisms (the body, ''not the brain'') are also limited-function computers ''designed'' to ''make copies'' of themselves; they cannot be ''reprogrammed'' without genetic engineering."

In my opinion, this sentence is redundant here, because it is only tangential to the topic of the article. Also, it is unnecessarily both disputable and controversial. Controversial in the sense it is suggesting that the bodies of living organisms are "designed". And disputable because it is not stated what exactly is meant by reprogramming. If the organism is viewed as a chemical computer, for instance, then "reprogramming" could be performed for example by changing concentration of some chemical in the environment. Most importantly, this example is superficial because there is little sense in considering a body without a brain, even more so in an article about computers. Also, what about animals that do not have a clearly defined "brain"?. Another thing is, that to the best of my knowledge, nowadays it is not feasible to perform genetic engineering manipulation in a whole living multicellular organism as such manipulation are restricted to single cells only.

My last and least important objection is, that while the brain is a part of the body, the sentence suggests otherwise.

I suggest
1) removing the sentence
2) or quoting the source of this claim and expanding the sentence to comprehensively reflect the source's claim. As the expert opinions on whether live organisms are or are not limited-function computers does probably differ, both sides of this argument should be then represented here --[[Special:Contributions/195.39.74.163|195.39.74.163]] ([[User talk:195.39.74.163|talk]]) 23:33, 18 December 2011 (UTC)
:It is indeed a ridiculous claim. In fact the whole section is ridiculous, so I've removed it. As a matter of interest though studying bodies without brains is far from senseless. I studied psychology in a more robust age, when pithing a living frog's brain was considered acceptable. It's quite remarkable to see how little a frog relies on its brain for even apparently quite complicated tasks such as swimming. [[User:Malleus Fatuorum|Malleus]] [[User_talk:Malleus_Fatuorum|Fatuorum]] 06:15, 19 December 2011 (UTC)
== File:Acer Aspire 8920 Gemstone by Georgy.JPG Nominated for Deletion ==

{|
|-
| [[File:Image-x-generic.svg|100px]] 
| An image used in this article, [[commons:File:Acer Aspire 8920 Gemstone by Georgy.JPG|File:Acer Aspire 8920 Gemstone by Georgy.JPG]], has been nominated for deletion at [[Wikimedia Commons]] in the following category: ''Deletion requests December 2011'' 
;What should I do?
''Don't panic''; a discussion will now take place over on Commons about whether to remove the file. This gives you an opportunity to contest the deletion, although please review Commons guidelines before doing so.
* If the image is [[WP:NFCC|non-free]] then you may need to upload it to Wikipedia (Commons does not allow fair use)
* If the image isn't freely licensed and there is no [[WP:FUR|fair use rationale]] then it cannot be uploaded or used.

''This notification is provided by a Bot'' --[[User:CommonsNotificationBot|CommonsNotificationBot]] ([[User talk:CommonsNotificationBot|talk]]) 16:15, 30 December 2011 (UTC)
|}

== Definitions of [[computer]] vs. [[computer (disambiguation)]], and [[general-purpose computer]] vs. [[special-purpose computer]] ==

*[[Computer (disambiguation)]]: A computer is a program machine that receives input, stores and manipulates data, and provides output in a useful format.
*[[Computer]]: A computer is a programmable machine designed to sequentially and automatically carry out a sequence of arithmetic or logical operations.
These two definitions should be synchronized so that Wikipedia is consistent with itself. [[User:Wbm1058|Wbm1058]] ([[User talk:Wbm1058|talk]]) 01:52, 21 January 2012 (UTC)

*[[general-purpose computer]] and [[general purpose computer]] each redirect to [[computer]]
*[[special-purpose computer]] redirects to [[embedded system]]; [[special purpose computer]] redirects to [[microcontroller]]
I believe all four of the above should be redirected to [[computer]], where general-purpose and special-purpose should be defined and a clear distinction drawn between each.  Neither [[microcontroller]] or [[embedded system]] defines ''special-purpose computer''. Seems like embedded system is better, at least the last paragraph of the lede discusses the issue:
:In general, "embedded system" is not a strictly definable term, as most systems have some element of extensibility or programmability. For example, handheld computers share some elements with embedded systems such as the operating systems and microprocessors that power them, but they allow different applications to be loaded and peripherals to be connected. Moreover, even systems that do not expose programmability as a primary feature generally need to support software updates. On a continuum from "general purpose" to "embedded", large application systems will have subcomponents at most points even if the system as a whole is "designed to perform one or a few dedicated functions", and is thus appropriate to call "embedded".

Should we create a new section in [[computer]], titled '''General- and special-purpose computers''', where each is defined?  From those definitions, might fall out a better definition for [[computer]] itself.  Will be nice to find some good sources for this.  [[User:Wbm1058|Wbm1058]] ([[User talk:Wbm1058|talk]]) 02:44, 21 January 2012 (UTC)

Refer to earlier discussions [[#Definition_of_computer|above]] for issues with the definition of [[computer]]. [[User:Wbm1058|Wbm1058]] ([[User talk:Wbm1058|talk]]) 14:31, 21 January 2012 (UTC)

Is there a difference between a [[stored-program computer]] and a [[general-purpose computer]]?  If so, what's the difference? [[User:Wbm1058|Wbm1058]] ([[User talk:Wbm1058|talk]]) 14:39, 21 January 2012 (UTC)

''[[Computer hardware]] is the collection of physical elements that comprise a [[computer]] system.''  So, from that definition, it follows that a clear definition ''[[computer]]'' is required to determine what exactly constitutes ''[[computer hardware]]'', as opposed to ordinary [[electronic hardware]]. [[User:Wbm1058|Wbm1058]] ([[User talk:Wbm1058|talk]]) 15:18, 21 January 2012 (UTC)

:Re [[general-purpose computer]] vs. [[special-purpose computer]]: From books.google.com [http://books.google.com/books?id=2FbxAAAAMAAJ&q=stored-program+general-purpose+computer&dq=stored-program+general-purpose+computer%20FORTRAN%20IV:%20a%20programmed%20instruction%20approach FORTRAN IV: a programmed instruction approach]:
::''A special-purpose computer utilizes a fixed or stored program to solve the problem it was designed for. The same problem is solved repeatedly, using different sets of data.''"
:I could not have explained it in a better way. Somewhere in the article this should be pointed out. As I mentioned on my talk page, early computers such as ABC were clearly special-purpose. Nowadays, a [[GPU]] is a good example for a special-purpose computer, while a [[GPGPU]] is generally programmable.

:While a [[microcontroller]], to which special-purpose computer currently redirects, is used for special-purpose applications, as a computer it is generally programmable and thus a general-purpose machine. Neither is a redirect to [[embedded system]] correct. These links should be fixed. [[User:Nageh|Nageh]] ([[User talk:Nageh|talk]]) 15:38, 21 January 2012 (UTC)

:Re [[stored-program computer]]: All stored-program computers are general-purpose. However, there are general-purpose machines that take the program in the form of interchangeable hardware components, such as plug boards. [[User:Nageh|Nageh]] ([[User talk:Nageh|talk]]) 15:38, 21 January 2012 (UTC)

=== [[Computer hardware]] vs. [[electronic hardware]] ===
The discussion in this section developed from my edits to change some of the many [[hardware]] links on Wikipedia to [[computer hardware]] links.  I've since become aware of an [[electronic hardware]] article, which I think should sometimes be linked to rather than [[computer hardware]], though which to link to may be somewhat a judgement call.  My thought is that when talking about "hardware" at the device level, link to [[computer hardware]], or [[personal computer hardware]], [[network hardware]], [[graphics hardware]] or [[video game hardware]] and when talking about the internals of a particular device, [[electronic hardware]].  That makes more sense to me than making the decision based on whether the "hardware" is programmable or not (i.e., general- or special-purpose).  Electronic hardware is stuff like [[Schottky transistor]]s and [[silicon gate]]s.  Examples:
* [[Graphics processing unit]], a [[graphics hardware]] device: ''...In 1987, the IBM 8514 graphics system was released as one of the first video cards for IBM PC compatibles to implement fixed-function 2D primitives in [[electronic hardware]].'''
* [[Intel 8086]], a [[personal computer hardware]] device: ''...A 64 KB (one segment) stack growing towards lower addresses is supported by [[electronic hardware]].''
[[User:Wbm1058|Wbm1058]] ([[User talk:Wbm1058|talk]]) 21:39, 23 January 2012 (UTC)

==== special-purpose computers ====
Another article to link to when discussing [[special-purpose computer]]s: [[Application-specific integrated circuit]].  I suppose these are often found in [[embedded system]]s.  [[User:Wbm1058|Wbm1058]] ([[User talk:Wbm1058|talk]]) 17:07, 18 February 2012 (UTC)
== Edit request on 12 February 2012 ==

{{edit semi-protected|answered=yes}}
Computers are fun.[[Special:Contributions/98.165.76.134|98.165.76.134]] ([[User talk:98.165.76.134|talk]]) 21:39, 12 February 2012 (UTC)
:Indeed--[[User:Jac16888|<font color="Blue">Jac</font><font color="Green">16888</font>]] [[User talk:Jac16888|<sup><font color="red">Talk</font></sup>]] 21:40, 12 February 2012 (UTC)

== Edit request on 17 April 2012 ==

{{edit semi-protected|answered=yes}}
<!-- Begin request -->
Edit Request: "and it will carry process them." changed to "and it will process them."

This edit request is in reference to the second sentence under Programs (http://en.wikipedia.org/wiki/Computer#Programs). The sentence ends with, "and it will carry process them." I believe "carry" should be removed, so it reads, "and it will process them." Perhaps at one time the sentence read "and it will carry them out", but the "carry" was not removed when it was changed to "process them".

This is my first edit request, so I hope this is the correct way to do it. Thanks! -Mark

<!-- End request -->
[[User:Markrummel83|Markrummel83]] ([[User talk:Markrummel83|talk]]) 15:56, 17 April 2012 (UTC)

:Thanks! I see [[User:TedColes|TedColes]] has already carried out the request. [[User:Nageh|Nageh]] ([[User talk:Nageh|talk]]) 17:34, 17 April 2012 (UTC)

== Edit request ==

I confess near total ignorance about how computers work (hence my interest in learning from this page), but I am medieval historian (Western, 450-1450) with a PhD and I must say that the second paragraph under Limited-function early computers, beginning "Around the end of the 10th century..." is in multiple respects inaccurate, and likely a farce.  I suggest it be removed.  I think it is beyond being modified.  I will leave it to those who know Wikipedia better to take such action as they see fit.  [[Special:Contributions/216.117.19.94|216.117.19.94]] ([[User talk:216.117.19.94|talk]]) 02:54, 30 May 2012 (UTC)  <span style="font-size: smaller;" class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/216.117.19.94|216.117.19.94]] ([[User talk:216.117.19.94|talk]]) 02:51, 30 May 2012 (UTC)</span><!-- Template:Unsigned IP --> <!--Autosigned by SineBot-->
:Can you be more specific. What about the two sources cited?--[[User:TedColes|TedColes]] ([[User talk:TedColes|talk]]) 07:08, 30 May 2012 (UTC)

Hi, Ted.  In response to your request, I have followed the links to the two sources cited in that paragraph.  Unfortunately, they provide no evidence for the claims made.  The first - Felt, Dorr E. (1916). Mechanical arithmetic, or The history of the counting machine. Chicago: Washington Institute. p. 8 - actually makes no reference whatsoever to a "a machine invented by the Moors that answered either Yes or No to the questions it was asked" as stated in the article.  What is more, the source itself gives no reference to any contemporary (i.e. 10th cen)  historical documents that might validate its claims.  The second - the parlour review january 1838 - suffers from the same deficiency.  It simply asserts - "it is said..." -  that Albertus Magnus devised a taking earthenware head which Thomas Aquinas later destroyed.  Again, no reference to the historical record.  Have we a letter from the 13th century confirming this claim? a chronicle? a history? or some other supporting document?  No.  Just an assertion.  Until we have a source that can provide a credible historical reference to these claims - preferably to a contemporary, 13th century, Latin language text, I see no reason to foist them on the interested and sincere reader. Hope this helps. [[Special:Contributions/216.117.19.94|216.117.19.94]] ([[User talk:216.117.19.94|talk]]) 01:15, 2 June 2012 (UTC)
== Advantages of Computer ==

Computer processing is very fast and accurate in comparison of human being.It is many times faster than a human being, It never tired and work consistently.  <span style="font-size: smaller;" class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/122.160.78.104|122.160.78.104]] ([[User talk:122.160.78.104|talk]]) 07:34, 28 June 2012 (UTC)</span><!-- Template:Unsigned IP --> <!--Autosigned by SineBot-->

== Archive bot ==

I've added auto archiving to this talk page. It needs it. It should archive everything over 60 days old (yes, even the stuff from 2009) except for the last 5 threads. There will always be a minimum of 5 threads on here, regardless of their age. The first archiving should happen at some point in the next 24 hours. - [[User:X201|X201]] ([[User talk:X201|talk]]) 15:29, 17 October 2012 (UTC)
== First bug - found by Grace Hooper ==

As I understand it , what was actually said was "I've found the first computer bug", she was being ironic .
The concept of bugs was already well understood well before this date.
http://cs-www.cs.yale.edu/homes/tap/Files/hopper-wit.html  <span style="font-size: smaller;" class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/93.97.31.112|93.97.31.112]] ([[User talk:93.97.31.112|talk]]) 21:06, 28 October 2012 (UTC)</span><!-- Template:Unsigned IP --> <!--Autosigned by SineBot-->

== Computer ==

Cool article----  <span style="font-size: smaller;" class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/174.102.84.111|174.102.84.111]] ([[User talk:174.102.84.111|talk]]) 16:06, 9 November 2012 (UTC)</span><!-- Template:Unsigned IP --> <!--Autosigned by SineBot-->
== Edit request on 17 November 2012 ==

{{edit semi-protected|answered=yes}}
<!-- Begin request -->
Please change 

Computers using [[vacuum tube]]s as their electronic elements were in use throughout the 1950s, but by the 1960s had been largely replaced by

to
 
Computers using [[vacuum tube]]s as their electronic elements were in use throughout the 1950s, but by the 1960s they had been largely replaced by


Please change

While some computers may have strange concepts "instructions" and "output" (see quantum computing), modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language.

to

While some computers may have strange concepts for "instructions" and "output" (see quantum computing), modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming 

language.
<!-- End request -->
[[User:Voi8|Voi8]] ([[User talk:Voi8|talk]]) 19:53, 17 November 2012 (UTC)

:[[File:Yes check.svg|20px|link=|alt=]] '''Done'''<!-- Template:ESp --> [[User:Rivertorch|Rivertorch]] ([[User talk:Rivertorch|talk]]) 06:18, 18 November 2012 (UTC)

== Suggestion for Misconceptions section. ==

I think the author of the "Misconceptions" section showed insight to include such a section. However, I believe he or she has a misconception that a computer must be a device.  Specifically, unless you consider a human being to be a biological device, I believe the first modern computers were actually women employed by the U.S. Army during WWI to create tables that soldiers in the field who operated cannons could use to determine the amount of powder to place in a cannon and the firing angle to deliver their cannonballs and other destructive loads specific distances with particular wind conditions.  I'm sorry I have no reference for this information other than my HS Math instructor, but it might be researched.  I believe the job title of such persons included the word "Computer."  <span style="font-size: smaller;" class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/174.62.95.48|174.62.95.48]] ([[User talk:174.62.95.48|talk]]) 17:33, 1 December 2012 (UTC)</span><!-- Template:Unsigned IP --> <!--Autosigned by SineBot-->

: I've [https://en.wikipedia.org/w/index.php?title=Computer&diff=525949928&oldid=525855885 updated] the article to note that (according to [[SOED]]) the word - referring to a person - dates back to the mid 17th century. http://thefreedictionary.com/computer also includes the "person" definition, as does https://en.wiktionary.org/wiki/computer#English. Realistically though, the definition as a person is obsolete; in ''modern'' usage the term invariably refers to a device - or can someone produce a reference to the contrary?
: I think an example of current usage of the word "computer" for anything other than an electronic device would be helpful. There are examples of such as [[slide rule]] and [[billiard-ball computer]] but I can't think of a currently-used, practical device that anyone calls a computer that is ''not'' electronic. [[User:Mitch Ames|Mitch Ames]] ([[User talk:Mitch Ames|talk]]) 01:33, 2 December 2012 (UTC)
== Digital computers were also developed in Germany, like the world's first functional program-controlled Turing-complete computer, the Z3, which became operational in May 1941 ==

That's just missing in the article. You're writing the first digital computers were constructed in the U. S. and in the U. K., but you don't mention Germany although the "the world's first functional program-controlled Turing-complete computer, the Z3, which became operational in May 1941" was constructed in Germany by Konrad Zuse. http://en.wikipedia.org/wiki/Konrad_Zuse <small><span class="autosigned">—&nbsp;Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:77.117.246.168 |77.117.246.168 ]] ([[User talk:77.117.246.168 |talk]] • [[Special:Contributions/77.117.246.168 |contribs]]) 12:08, 2 November 2012‎</span></small><!-- Template:Unsigned -->


seems like someone wants to rewrite history... article is not editable!
[[Special:Contributions/79.239.50.153|79.239.50.153]] ([[User talk:79.239.50.153|talk]]) 22:38, 15 December 2012 (UTC)

:The article has been [[WP:Page protection|semi-protected]] for more than five years due to high levels of [[WP:VAND|vandalism]]. If you'd like to suggest a change, please follow the procedures outlined [[Wikipedia:Edit requests|here]]. [[User:Rivertorch|Rivertorch]] ([[User talk:Rivertorch|talk]]) 22:50, 15 December 2012 (UTC)

== Edit request on 5 December 2012 ==

{{editrequest|answered=yes}}
Mr. Rahul Sharma Lucknow  

What Computer and Information Systems Managers Do
Computer and information systems managers, often called information technology managers (IT managers or IT project managers), plan, coordinate, and direct computer-related activities in an organization. They help determine the information technology goals of an organization and are responsible for implementing the appropriate computer systems to meet those goals.

Work Environment
Most large companies have computer and information systems managers. The largest concentration of IT managers works for computer systems design and related services firms. Most IT managers work full time.

How to Become a Computer and Information Systems Manager
A bachelor’s degree in computer or information science plus related work experience is typically required. Many computer and information systems managers also have a graduate degree.

Pay
The median annual wage of computer and information systems managers was $115,780 in May 2010.

Job Outlook
Employment for computer and information systems managers is projected to grow 18 percent from 2010 to 2020, about as fast as the average for all occupations. Growth will be driven by organizations upgrading their IT systems and switching to newer, faster, and more mobile networks.

Similar Occupations
Compare the job duties, education, job growth, and pay of computer and information systems managers with similar occupations.

O*NET
O*NET provides comprehensive information on key characteristics of workers and occupations.

Contacts for More Information
Learn more about computer and information systems managers by contacting these additional resources.
:'''Not done'''. This is not a suitable sub-section for this article. --[[User:Wtshymanski|Wtshymanski]] ([[User talk:Wtshymanski|talk]]) 14:43, 5 December 2012 (UTC)
== Edit Request - "Professions and organizations - Computer Engineering" - on 24 December 2012: ==

{{edit semi-protected|answered=yes}}
Computer engineering is the study of both "hardware & software" sides of computer systems.
http://en.wikipedia.org/wiki/Computer#Professions_and_organizations
I see it is included with "Hardware-related" professions only. i suggest adding it to "Software-related" too, or you can insert a new row: "Hardware & Software Related", then include the fields that combine H/W & S/W in their study (like Computer Engineering).
I also suggest to add these references about computer engineering:
* http://www.acm.org/education/education/curric_vols/CE-Final-Report.pdf "Page 4: Computer engineering is defined as the discipline that embodies the science and technology of design, construction, implementation, and maintenance of '''software and hardware''' components of modern computing systems and computer-controlled equipment. Computer engineering has traditionally been viewed as a '''combination''' of both computer science (CS) and electrical engineering (EE)."
* http://www.wisegeek.com/what-is-a-computer-engineer.htm "and integrating software options with the hardware that will drive the applications.", "Some of the common tasks associated with the computer engineer include software design that is customized for a particular industry type. Operating systems that are peculiar to the culture of a given company often require the input of a computer engineer, ....."

Thanks in advance.
--[[Special:Contributions/41.46.105.96|41.46.105.96]] ([[User talk:41.46.105.96|talk]]) 04:45, 24 December 2012 (UTC)
<br />
'''EDIT:'''<br />
Here is my request in the form of (Change X to Y):<br />
Please change:<br />
Software-related: Computer Science, ..., Video game industry, Web design<br />
to:<br />
Software-related: Computer Science, Computer Engineering, ..., Video game industry, Web design<br />
Found here: http://en.wikipedia.org/wiki/Computer#Professions_and_organizations
[[Special:Contributions/41.46.102.52|41.46.102.52]] ([[User talk:41.46.102.52|talk]]) 16:11, 25 December 2012 (UTC)
:'''Not done''' - this article has a link to [[Computer engineering]] already which would be a better place to discuss that topic. --[[User:Wtshymanski|Wtshymanski]] ([[User talk:Wtshymanski|talk]]) 23:12, 24 December 2012 (UTC)
:: Thanks for your reply, but you got me wrong. I don't mean to argue on the definition of CE. This page is about "Computer", i just say: *This* page, which is about Computers, mentioned that "CE is the hardware study of computers", which is '''FALSE''' and not true. My aim is to correct this '''FALSE''' info mentioned on *this* page. and i proved by references & citations above that CE classification on *this* page is '''NOT CORRECT''' and needs to be revised. Thanks again :D [[Special:Contributions/41.46.102.52|41.46.102.52]] ([[User talk:41.46.102.52|talk]]) 16:03, 25 December 2012 (UTC)
*[[File:Red information icon with gradient background.svg|20px|link=|alt=]] '''Not done:''' please establish a [[Wikipedia:Consensus|consensus]] for this alteration before using the {{tlx|edit semi-protected}} template.<!-- Template:ESp -->.--[[User:Canoe1967|Canoe1967]] ([[User talk:Canoe1967|talk]]) 23:00, 29 December 2012 (UTC)

== Update needed in "Software" Section ==

Under the Software section of this article, where it talks about various Operating Systems, It seems as though windows 8 is not present, I do believe this is an error and it should be updated. [[User:FranktheTank|FranktheTank]] ([[User talk:FranktheTank|talk]]) 14:31, 7 January 2013 (UTC)

{{done|done [[User:Mitch Ames|Mitch Ames]] ([[User talk:Mitch Ames|talk]]) 13:42, 8 January 2013 (UTC)}}

== what is a computer and what does it do? ==

{{hat|Lengthy text hatted—apparently unrelated to improving article}}
A computer is a programmable electronic device that accepts input; performs processing operations; outputs the results; and provides storage for data, programs or output when needed. most computers today also have communications capabilities. This progression of input, processing, output, and storage is sometimes called the information processing cycle. 

Data is the raw, unorganized facts that are input into the computer to be processed. Data that the computer has processed into a useful form is called information. Data can exist in many forms, representing text, graphics, audio and video. one of the first calculating devices was abacus. Early computing devices that predate today's computer include the slide rule, the mechanical calculator and Dr Herman Hollerith's Punch Card Tabulating Machine and Sorter. First-generation computers, such as Eniac and Univac, were powered by vacuum tubes; second - generation computers used transistors; and third-generation computers where possible because of the invention of the integrated circuits (IC). Today's fourth- generation computers use microprocessors and are frequently connected to the internet and other networks. some people bilieve that fifth-
generation computers will likely be based on artificial intelligence.

A computer is made of hardware ( the actual physical equipment that makes up the computer system ) and software ( the computer's program ).
common hardware components include the keyboard and mouse ( input devices ), the CPU ( a processing device ), monitors /display  screens and printers ( out put devices ),
and storage devices and storage media ( such as CDs, DVD drives, hard drives, USB flash drives, and flash memory cards).
Most computers today also include a modem, network adapter, or other type of communications device to allow users to connect to the internet ot other network.

All computers need system software, namely an operating system ( usually windows, mac OS, or linux), to function. The operating system assists with the boot process, and then controls the operation of the computer, such as to allow users to run other types of software and to manage their files. most software programs today use a variety of graphical objects that are selected to tell the computer what to do. The basic work space for windows users in the windows desktop.

Application software consists of programs designed to allow people to preform specific tasks or applications. such as word processing, web browsing, photo touch-up, and so on. software programs are written using a programming language. programs are written by programmers; computer users are the people who use computers to preform tasks or obtain information.  <span style="font-size: smaller;" class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/41.46.4.216|41.46.4.216]] ([[User talk:41.46.4.216|talk]]) 18:06, 7 March 2013 (UTC)</span><!-- Template:Unsigned IP --> <!--Autosigned by SineBot-->

:If this has any bearing on the article, please feel free to explain how, and uncollapse this discussion. [[User:Rivertorch|Rivertorch]] ([[User talk:Rivertorch|talk]]) 23:14, 7 March 2013 (UTC)
{{hab}}
== Edit request on 11 April 2013 ==

{{edit semi-protected|answered=yes}}
<!-- Begin request -->
HARDWARE: SOME IMPORTANT COMPONANT OF HARDWARE WHICH ARE UNDER BELOW
1.'''INPUT'''
2.MEMORY
3.PROCESSOR
ETC<!-- End request --> -  [[User:118.107.133.149|118.107.133.149]] ([[User talk:118.107.133.149|118.107.133.149]]) 07:47, 11 April 2013‎

:Like the ones in the Components section? - [[User:X201|X201]] ([[User talk:X201|talk]]) 08:06, 11 April 2013 (UTC)

== Hoaxes ==

:''Around the end of the 10th century, the French monk Gerbert d'Aurillac brought back from Spain the drawings of a machine invented by the Moors that answered either Yes or No to the questions it was asked. Again in the 13th century, the monks Albertus Magnus and Roger Bacon built talking androids without any further development (Albertus Magnus complained that he had wasted forty years of his life when Thomas Aquinas, terrified by his machine, destroyed it).''

This was in the "history of computing" section. First, I think the Moorish machine (a [[brazen head]]) was either a hoax or a glorified [[Magic 8-Ball]] sort of thing &mdash; either way, there's not much computation going on. (If the "machine" spoke aloud, it was certainly a hoax.) Second, I'm a bit baffled that it didn't mention that Magnus and Bacon's machine was a hoax. Sure, it may seem too obvious to mention, but leaving it out makes it looks like the sort of random nonsense that people sometimes put in Wikipedia. Finally, this really has very little to do with computing. People of the time had very little notion of what we would call a computer, and would likely not be able to readily equate the concept of talking machines with computation. I suspect these people weren't imagining computing devices, but rather talking inanimate objects. Does the notion of a talking mirror bring computation to mind? Surely not. I'm removing this from the article. - [[User:Furrykef|furrykef]] ([[User_talk:Furrykef|Talk at me]]) 12:41, 29 May 2013 (UTC)
== thoughts ==

Hmm, I'd bet many pounds this article was heavily edited by a Brit. I guess that's obvious to anyone reading it.

Only a Brit would take a manual describing the construction of EDVAC - "First Draft of a Report on the EDVAC", run off and make a copy of it, and then try to claim they were first. The EDVAC was a working Von Neumann computing machine, and presented to the public in 1947. It just wasn't able to be delivered to the customer until 49 because of patent disputes. So claiming that British copies of it demonstrated in 1948, based on the EDVAC manual, were first, is just plain wrong in so many ways.

Looking down the article, like the integrated circuit and other areas, I'm proud to note that in most places, no American felt the need to put in that every US first was in fact American. This does seem to be a European obsession.  <small><span class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:Dkelly1966|Dkelly1966]] ([[User talk:Dkelly1966|talk]] • [[Special:Contributions/Dkelly1966|contribs]]) 17:25, 29 May 2013 (UTC)</span></small><!-- Template:Unsigned --> <!--Autosigned by SineBot-->

:Right. Chauvinism being unheard of among Americans. [[User:Rivertorch|Rivertorch]] ([[User talk:Rivertorch|talk]]) 18:27, 29 May 2013 (UTC)

== ABC vs Z in lede ==

I see we now have a debate on who was first with the computer in the introduction of this article.  I don't think that's anything but troll-bait in general, and doesn't help explain what a computer is anyway.  We have both the AB and Z3 computers in the history section, and I think that's where they belong, only.  I'm thinking of deleting both from the lede.  --[[User:A D Monroe III|A D Monroe III]] ([[User talk:A D Monroe III|talk]]) 22:20, 26 June 2013 (UTC)

:After no objections for a week, I have removed it.  [[User:A D Monroe III|A D Monroe III]] ([[User talk:A D Monroe III|talk]]) 21:54, 2 July 2013 (UTC)

::[[User talk:IIIraute|IIIraute]] has reverted my removal of an expanding argument about who was first to make the computer from the introduction of the article, with the comment "restore sourced content - no consensus for removal".  

::As to "sourced", yes, the content is sourced, but the sources listed duplicate the sources in the history section, where they belong.

::As to "consensus", I did state the reasons for my doing this just above.  And, yes, while no one else specifically agreed with my proposal, no one disagreed, for a week.  No objections or comments counts as consensus for obvious and trivial concerns, as I consider this to be.  Myself, I would respond here before I'd do such a revert.

::The fragments of computer history that Illraute restored are incomplete, and have been and will be a target for people to insert their POV.  That's bad enough, but more importantly, a debate on who's first has nothing to do with the basic explanation of a computer, so doesn't belong in the lede at all.  The article's history section duplicates this information already, in a more complete and coherent fashion.

::If Illraute or others don't specifically state some counter-arguments, I will restore my edit in the next day or so.  [[User:A D Monroe III|A D Monroe III]] ([[User talk:A D Monroe III|talk]]) 16:27, 3 July 2013 (UTC)

:::I don't see why this information should be removed - especially since you didn't care to correct or remove the "The first electronic [[digital]] computers were developed between 1940 and 1945 in the United Kingdom and United States." claim.--[[User:IIIraute|IIIraute]] ([[User talk:IIIraute|talk]]) 16:41, 3 July 2013 (UTC)

::::Can't we get away from nationalistic promotion of "firsts"? Which computer is counted as first depends heavily on a whole range of adjectives before the word "computer". Such adjectives include "electronic" "digital" "programmable" "automatic" "binary" and "stored-program". Also, it is contentious to date the time that is appropriate in this context. Should it be the time that the idea first arose, the time that a reasonably complete design was written down, the time that some component first worked, the time that the reasonably complete machine was first demonstrated in a laboratory or workshop, or the time that it first started to serve users addressing real problems?--[[User:TedColes|TedColes]] ([[User talk:TedColes|talk]]) 17:08, 3 July 2013 (UTC)

:::::To Illraute's point, I ''did'' remove the "UK and US" part, as that implies some claim to being first; you're putting it back.  (BTW, it left out Germany, promoting future "fixes" by "pro-German" editors – real or imagined.)  I'm not saying the intro shouldn't mention history, but it should ''only'' mention it.  The whole point of the introduction is to summarize, not give details, especially details that promote reactions of POV.

:::::I agree with TedColes.  Any debate on "first" will come to arguments on definition of "computer", which usually ends up based on bias, or just being loud and insensitive.

:::::But, really, please just read the intro, but skip this "ABC vs Z3" middle paragraph.  It just reads better without it.  The paragraph contradicts itself, is disjointed, and even stoops to weasel words.  It's the result of conflicting editors with conflicting views.  If we leave it there, it will get worse.

:::::And, again, detailing claims to "first" doesn't help explain the topic of computers.  It doesn't belong in the into.  [[User:A D Monroe III|A D Monroe III]] ([[User talk:A D Monroe III|talk]]) 21:00, 3 July 2013 (UTC)

::::::You are right, you did remove the "UK and US" part - my mistake - I am really sorry! It reads better without it. I have reverted my revert of your edit. --[[User:IIIraute|IIIraute]] ([[User talk:IIIraute|talk]]) 22:33, 3 July 2013 (UTC)

== A lot of information from the [[women in computing]] article deserves to be in this article as well. ==

I put up the tag that mentions how there is information missing when it comes to women in computing in this article. [[Ada_Lovelace|Lovelace]] isn't mentioned and the use of ''computer'' to refer to women isn't mentioned at all in this article ([[human computer]] talks about humans as computers and even has a pictures of woman being computers). My changes to this article are to include the usage of ''computer'' being used to refer to humans, especially female humans.--[[User:JasonMacker|JasonMacker]] ([[User talk:JasonMacker|talk]]) 00:16, 27 April 2013 (UTC)

I added a paragraph in the history of computing section on Lovelace and added a picture of her as well. The information was pulled from the lead of [[Ada Lovelace]], as well as Lovelace's mention in [[Women in computing#Timeline_of_women_in_computing]].--[[User:JasonMacker|JasonMacker]] ([[User talk:JasonMacker|talk]]) 00:32, 27 April 2013 (UTC)

* Speaking about Ada Lovelace, it occurs to me that there is also information missing about drug addicts in computing, and other groups, such as homosexuals. [[User:Kokot.kokotisko|Kokot.kokotisko]] ([[User talk:Kokot.kokotisko|talk]]) 17:05, 5 May 2013 (UTC)

I replaced the artificially darkened picture of the ENIAC with a much clearer one of the main control panel.--[[User:JasonMacker|JasonMacker]] ([[User talk:JasonMacker|talk]]) 01:05, 27 April 2013 (UTC)

Has this issue now been resolved with the addition of information about Lovelace? I'm assuming it has (given the lack of discussion in this section for quite a long time, and the fact that I can't see a problem with this article any more), so I'll remove the tag, let me know if there still are issues, so that the tag can be put back. [[User:Cliff12345|Cliff12345]] ([[User talk:Cliff12345|talk]]) 02:13, 2 July 2013 (UTC)

:As much as I understand the appeal of, and even delight in the idea that the world's first computer programmer was a picturesque Victorian-era noblewoman with a flowery name and title (who moreover happened to be the daughter of a celebrated and influential Romantic poet), I sense a subtle sexist double standard here: Babbage was at least equally important to the history of the computer – Babbage could be described as the inventor of computing hardware and Lovelace of software –, but only Lovelace is depicted here, while a portrait of Babbage is not included (although available). Keep in mind that this helps perpetuate the insidious stereotype that women's primary function is decorative, not intellectual. I'm not trying to insinuate any of this to be conscious, let alone intentional, lest I offend the principle of assuming good faith, but it's worth considering.
:It might seem like a trivial detail if you've never really tried to understand feminism and what it is really all about (I would never have spotted subtle details like this either not long ago, and even considered it ridiculous to find fault in something like this), but once you develop a sensitivity to the issue, it does stand out. Especially since mention of Ada's role was added to the article specifically to ''further the cause'' of feminism. I'm sure you guys are well-intentioned, in fact! <small>(I'm a guy myself, by the way – my first name appears feminine to many Anglophones, I've found.)</small> --[[User:Florian Blaschke|Florian Blaschke]] ([[User talk:Florian Blaschke|talk]]) 13:16, 29 July 2013 (UTC)

== Edit request on 22 October 2013 ==

{{edit semi-protected|<!-- Page to be edited -->|ans=yes}}
<!-- Begin request -->
I have a video that I would like to embed in the computer components section of a wikipedia article. I had to create it for school, at Alverno College. Can you give me permission to embed this video. It is one minute and 54 seconds long.
[[File:Computer Components.webm|thumbnail|This is a digital representation of the components of a "slimline" desktop computer.]]

<!-- End request -->
[[User:Tlenyard|Tlenyard]] ([[User talk:Tlenyard|talk]]) 00:36, 22 October 2013 (UTC)

* Please tell me exactly where you want this video and how you want it formatted and I'll consider adding it for you.  Your alternative is to make ten useful edits in the next four days which will make you autoconfirmed and allow you to edit it yourself. [[User:Technical 13|Technical 13]] ([[User talk:Technical 13|talk]]) 02:32, 22 October 2013 (UTC)

== Edit request on 31 October 2013 ==

{{edit semi-protected|<!-- Page to be edited -->|answered=yes}}
<!-- Begin request -->
replace <nowiki>{{p.|61-62}}</nowiki> with pp. 61–62
<!-- End request -->
there is no [[template:p.]] [[Special:Contributions/174.56.57.138|174.56.57.138]] ([[User talk:174.56.57.138|talk]]) 19:55, 31 October 2013 (UTC)
:[[File:Yes check.svg|20px|link=|alt=]] '''Done'''<!-- Template:EP -->. Thanks. -- &nbsp;&nbsp;  [[User:LogX|<font color="#00CED1" style="font-family:Polo" size="3">'''L''' o g</font>]]  &nbsp;[[User talk:LogX|<font color="#FF0000" style="font-family:Polo;" size="3.5">'''X'''</font>]]&nbsp;&nbsp; 20:01, 31 October 2013 (UTC)

== Computer ==

computer
What is a Computer?
A computer is a programmable machine. The two principal characteristics of a computer are: it responds to a specific set of instructions in a well-defined manner and it can execute a prerecorded list of instructions (a program).
Modern Computers Defined
Modern computers are electronic and digital. The actual machinery -- wires, transistors, and circuits -- is called hardware; the instructions and data are called software.
All general-purpose computers require the following hardware components:
memory: enables a computer to store, at least temporarily, data and programs.
mass storage device: allows a computer to permanently retain large amounts of data. Common mass storage devices include disk drives and tape drives.
input device: usually a keyboard and mouse, the input device is the conduit through which data and instructions enter a computer.
output device: a display screen, printer, or other device that lets you see what the computer has accomplished.
central processing unit (CPU): the heart of the computer, this is the component that actually executes instructions.
In addition to these components, many others make it possible for the basic components to work together efficiently. For example, every computer requires a bus that transmits data from one part of the computer to another.Ώ  <span style="font-size: smaller;" class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/182.182.124.217|182.182.124.217]] ([[User talk:182.182.124.217|talk]]) 10:50, 10 December 2013 (UTC)</span><!-- Template:Unsigned IP --> <!--Autosigned by SineBot-->

== Usefulness to mere mortals required. ==

Hi Wikipedia people

Er - I don't understand lots and lots of things in this article. In fact it just bamboozles me half the time. I'm a college lecturer and have been using computers to make music with since 1985. I somehow think your aim should be for someone like me to understand your article, but it's way way too tech savvy to actually be of worth to people who wish to learn something from your work.

You all evidently have fantastic knowledge, but you really need to work out how to share that knowledge with other s who seek it, rather than just passing it around yourselves. Coming up with an Acronym and saying that that is a "language" or a "Compiling language" is not much use to people who don't know what a computer language is...

Please TRY to address those who have come to you for knowledge in a way that does not put them off returning to these wonderful resources.

My Name Is Andy. I do Not write for WP, but I feel that I may be allowed to comment.
Thankyou  <span style="font-size: smaller;" class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/109.154.8.72|109.154.8.72]] ([[User talk:109.154.8.72|talk]]) 21:11, 9 December 2013 (UTC)</span><!-- Template:Unsigned IP --> <!--Autosigned by SineBot-->

:You're not alone. Click [[WP:JARGON|this link]] to read a Wikipedia guideline that implores editors to make articles readable (this article isn't much worse than other articles). You might prefer to click [[Simple:Computer]] for a simpler version, but many Simple English Wikipedia articles are no simpler than English Wikipedia; editors' vanity is almost as big a problem over there.

:Do you know how to click links? I think you mean the table at [[Computer#Languages]] (click to see what it is). If you read that table and you don't know what an assembly language is, for instance, click the blue link where it says [[assembly language]]. [[User:Art LaPella|Art LaPella]] ([[User talk:Art LaPella|talk]]) 00:57, 10 December 2013 (UTC)

Thankyou very much Art LaPella - for taking the time to respond. Yes, I do realise that I can get definitions of  various terms by clicking links - but of course this is a very disruptive process for the learner and I would at least like to feel that a subject as important as The Computer could at least open with enough information to satisfy the enquiring mind without the over use of complex terminology that requires navigation away from the subject in hand. However, you seem to have a similar view. So I will not preach to the converted. Andy  <span style="font-size: smaller;" class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/109.154.8.72|109.154.8.72]] ([[User talk:109.154.8.72|talk]]) 00:29, 11 December 2013 (UTC)</span><!-- Template:Unsigned IP --> <!--Autosigned by SineBot-->

== New sections: ‎Advantages/Disadvantages of computers ==

I'm not a fan of these recently-added new sections.  They are a mix of obvious statements with a few questionable ones thrown in, and it's in list form, which we general don't go for in Wikipedia.  Plus, it's not sourced.  Is there anything can be done to fix this, or should we just remove them?  --[[User:A D Monroe III|A D Monroe III]] ([[User talk:A D Monroe III|talk]]) 18:59, 17 December 2013 (UTC)
:I agree with this. Given the importance of the topic and the large number of good, well referenced articles relating to it, I think that this article has a very long way to go to get up to a similar standard.--[[User:TedColes|TedColes]] ([[User talk:TedColes|talk]]) 22:42, 17 December 2013 (UTC)
:Remove it. The inanity of it reminds me of a grade-school textbook. [[User:Thanatosimii|Thanatosimii]] ([[User talk:Thanatosimii|talk]]) 19:57, 4 January 2014 (UTC)

These sections are inappropriate. They genernally detract from the article and violate [[WP:USEPROSE]], [[WP:CITE]], [[WP:VER]], and perhaps [[WP:POV]] and/or [[WP:OR]].  Needs to go. --[[User:R. S. Shaw|R. S. Shaw]] ([[User talk:R. S. Shaw|talk]]) 23:06, 15 January 2014 (UTC)

== Category:Computers in fiction ==

Am I to assume that when a technology becomes wide spread enough in the real world listing it's instances in fiction becomes pointless? Someone a thousand years from now may not particularly give notice when they replicate their food or walk through a teleporter; they would probably see those things as being as mundane as we see things as like using a car of a refrigerator; however someone from the past without that technology would certainly notice it. [[User:CensoredScribe|CensoredScribe]] ([[User talk:CensoredScribe|talk]]) 19:25, 18 March 2014 (UTC)
:Not necessarily. The question is not whether or not the element is common or not, but rather whether or not it is a defining element of the subject. For example, ''[[Speed Racer]]'' has fictional categories for racing drivers and motorsports. Both are reasonably common in real life, but both are ''defining characteristics'' of ''Speed Racer'' (also, [[Mach Five]] is in "Fictional racing cars", a defining characteristic of the car).
:For comparison, ''[[Tron]]'' is (appropriately, IMO) in "Artificial intelligence in fiction". It would be impossible to discuss ''Tron'' without discussing the fictional computer. ''[[Star Trek]]'', OTOH, is not in "Artificial intelligence in fiction" or any similar category, despite the ship's computer or Data being significant elements of the franchise. The computers are not defining elements of the franchise. - [[User:SummerPhD|<span style="color:#D70270;background-color:white;">Sum</span><span style="color:#734F96;background-color:white;">mer</span><span style="color:#0038A8;background-color:white;">PhD</span>]] ([[User talk:SummerPhD|talk]]) 03:09, 19 March 2014 (UTC)

== Semi-protected edit request on 24 April 2014 ==

{{edit semi-protected|Computer|answered=y}}
<!-- Begin request -->
computer is an electronic device that takes input from the user analyze it , process it and give the desired output and also provide the capability for storing data for future use. 
<!-- End request -->
[[User:Cncreate|Cncreate]] ([[User talk:Cncreate|talk]]) 05:32, 24 April 2014 (UTC)

:[[File:Red information icon with gradient background.svg|20px|link=]] '''Not done:'''<!-- Template:ESp --> as you have not requested a change.<br />If you want to suggest a change, please request this in the form "Please replace XXX with YYY" or "Please add ZZZ between PPP and QQQ".<br />Please also cite [[WP:RS|reliable sources]] to back up your request, without which no information should be added to any article. - [[User:Arjayay|Arjayay]] ([[User talk:Arjayay|talk]]) 07:20, 24 April 2014 (UTC)

== Computer System Information ==

''''''To view the detailed computer system information:''''''
You have a personal computer and you want to see the all details of your PC then you have to open the "Run" dialog box. After opening of the Run you have to type the command "sysinfo32.exe" and simply press the "enter" button. After that you have seen the popup window in that you have all the computer details.  <small><span class="autosigned">—&nbsp;Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:Rhlraypure|Rhlraypure]] ([[User talk:Rhlraypure|talk]] • [[Special:Contributions/Rhlraypure|contribs]]) 10:27, 25 September 2014 (UTC)</span></small><!-- Template:Unsigned --> <!--Autosigned by SineBot-->

== Section degradation is odd ==

I am not about make an edit, but the section on "Degradation" and the links to ants is a bad joke and should be removed.  <small class="autosigned">—&nbsp;Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/131.180.145.184|131.180.145.184]] ([[User talk:131.180.145.184|talk]]) 13:33, 17 December 2014 (UTC)</small><!-- Template:Unsigned IP --> <!--Autosigned by SineBot-->

== Bell labs, Silicon Valley ==
The entire wiki page on the history of computers is seriously lacking. There is an overwhelmingly British bias in the history section. The article hardly mentions the role of transistors and microprocessors, random access memory, along with any mention of modern programming languages, all invented in the US. It's the equivalent of cutting the history section off at the ancient Greek computers. I am saying this as a computer scientist myself. The vast majority of the history in Computers was written in the last 30 years, not the 1940's.   <small class="autosigned">—&nbsp;Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/68.198.27.88|68.198.27.88]] ([[User talk:68.198.27.88|talk]]) 01:29, 11 February 2015 (UTC)</small><!-- Template:Unsigned IP --> <!--Autosigned by SineBot-->
:You are free to add more sourced facts. But be aware there are some other articles about the topic:
:*[[History of computing]]
:*[[history of computing hardware]]
:--[[User:Kgfleischmann|Kgfleischmann]] ([[User talk:Kgfleischmann|talk]]) 05:21, 11 February 2015 (UTC)

== Semi-protected edit request on 9 February 2015 ==

{{edit semi-protected|Computer|answered=yes}}
<!-- Begin request -->
The word "medieval" in the introduction needs a "the" in front of it, because otherwise it's bad English. Or perhaps reword it to "the Middle Ages" (if that is indeed the same thing).
<!-- End request -->
[[Special:Contributions/121.74.155.190|121.74.155.190]] ([[User talk:121.74.155.190|talk]]) 21:25, 9 February 2015 (UTC)

:[[File:Yes check.svg|20px|link=]] '''Done'''<!-- Template:ESp -->, thanks. —'''[[User:Nizolan|<span style="color: #880000">Nizolan</span>]]''' <sup>[[user talk:Nizolan|<span style="color: black;">(talk)</span>]]</sup> 00:02, 10 February 2015 (UTC)

hjj



uu
↔  <small class="autosigned">—&nbsp;Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/112.198.134.17|112.198.134.17]] ([[User talk:112.198.134.17|talk]]) 04:09, 17 February 2015 (UTC)</small><!-- Template:Unsigned IP --> <!--Autosigned by SineBot-->

== How to Add Subtitles ==

Lots of media players will allow you to select multiple subtitle files to play with your movie, but sometimes you just can’t load the subtitles, no matter how hard you try. In these cases, you may want to hardcode the subtitles into the video file itself. This means that the subtitles will always appear, regardless of what media player you are using. To do this, you will need to re-encode the video file, which will add the subtitles directly to the frames. www.subtitl.xyz,Read on after the jump to find out how.  <small><span class="autosigned">—&nbsp;Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:Lolopopococo|Lolopopococo]] ([[User talk:Lolopopococo|talk]] • [[Special:Contributions/Lolopopococo|contribs]]) 10:04, 31 March 2015 (UTC)</span></small><!-- Template:Unsigned --> <!--Autosigned by SineBot-->

== Semi-protected edit request on 18 May 2015 ==

{{edit semi-protected|Computer|answered=yes}}
<!-- Begin request -->
Yossi reiche ris awesome
<!-- End request -->
[[Special:Contributions/101.2.171.162|101.2.171.162]] ([[User talk:101.2.171.162|talk]]) 04:55, 18 May 2015 (UTC)
:[[File:Red question icon with gradient background.svg|20px|link=]] '''Not done:''' it's not clear what changes you want to be made. Please mention the specific changes in a "change X to Y" format.<!-- Template:ESp -->  [[User:ekips39|<b style="color: #25f">ekips39</b>]] [[User talk:Ekips39|<span style="color: #52e">(talk)</span>]] 05:18, 18 May 2015 (UTC)

== Semi-protected edit request on 9 June 2015 ==
{{edit semi-protected|Computer|ans=yes}}
<!-- Begin request -->
Please link 'Geoff Tootill' in the section on the 'Manchester Small-Scale Experimental Machine to the relevant wikipedia page at https://en.wikipedia.org/wiki/Geoff_Tootill.

This is my father and I intend to flesh out the biography. 
I don't think I am able to make the change myself, even though I am logged in as a registered user.
Thanks, Peter Tootill
<!-- End request -->
:{{Done}}. You will be able to edit "semi-protected" articles such as this one once you have made ten edits with the account. Your request here was your second edit. -- [[User:John of Reading|John of Reading]] ([[User talk:John of Reading|talk]]) 11:01, 9 June 2015 (UTC)

== Clarification Sought on First Turing Complete Computer ==

It's not clear whether the first Turing Complete (TC) computer actually built and running was the one by C. Babbage's son, or the German Z3. A distinction and description should be made between the first TC design, and the first TC machine actually built and running. It perhaps should also be noted that the Z3 probably didn't take advantage of TC features while in use. Thus, there may be a third category: first computer that actually took advantage of TC features, although an unambiguous definition may be tricky to craft. [[Special:Contributions/146.233.0.201|146.233.0.201]] ([[User talk:146.233.0.201|talk]]) 21:57, 29 July 2015 (UTC)

== Semi-protected edit request on 28 August 2015 ==

{{edit semi-protected|Computer|answered=yes}}
<!-- Begin request -->

<!-- End request -->
[[Special:Contributions/59.88.42.73|59.88.42.73]] ([[User talk:59.88.42.73|talk]]) 15:46, 28 August 2015 (UTC)

:You need to tell us what edit you want made to the article. Add it below and set the answered field in the template to "no". - [[User:X201|X201]] ([[User talk:X201|talk]]) 15:50, 28 August 2015 (UTC)

== [[The Internet and cats]] ==

Please swing by and help improve this new article! :D--[[User:Coin945|Coin945]] ([[User talk:Coin945|talk]]) 03:30, 2 October 2015 (UTC)

== [[Westinghouse "computer"?]] ==

Some mention should be made of this 1913 device in the section on early computers, particularly if more details about it, how it works, when it was built, etc. are available.
http://gothamist.com/2015/10/15/grand_central_computer_video.php
[[Special:Contributions/173.160.221.10|173.160.221.10]] ([[User talk:173.160.221.10|talk]]) 17:35, 5 November 2015 (UTC)

== 'General purpose' part of the definition???? ==

Why is a computer defined as a general purpose device? There are many many special purpose computers!!
[[Special:Contributions/82.72.139.164|82.72.139.164]] ([[User talk:82.72.139.164|talk]]) 15:45, 4 December 2015 (UTC)
: Because that is a definition, not a description of one specific example of a computer. [[User:Andy Dingley|Andy Dingley]] ([[User talk:Andy Dingley|talk]]) 16:01, 4 December 2015 (UTC)
:: I'm not sure what you mean by that. [[Special:Contributions/82.72.139.164|82.72.139.164]] ([[User talk:82.72.139.164|talk]]) 19:08, 9 December 2015 (UTC)

== External links modified ==

Hello fellow Wikipedians,

I have just added archive links to {{plural:3|one external link|3 external links}} on [[Computer]]. Please take a moment to review [https://en.wikipedia.org/w/index.php?diff=prev&oldid=700396557 my edit]. If necessary, add {{tlx|cbignore}} after the link to keep me from modifying it. Alternatively, you can add {{tlx|nobots|deny{{=}}InternetArchiveBot}} to keep me off the page altogether. I made the following changes:
*Added archive https://web.archive.org/20130511181443/http://www.epn-online.com:80/page/22909/the-hapless-tale-of-geoffrey-dummer-this-is-the-sad-.html to http://www.epn-online.com/page/22909/the-hapless-tale-of-geoffrey-dummer-this-is-the-sad-.html
*Added archive https://web.archive.org/20080513221700/http://www.intel.com/museum/archives/4004.htm to http://www.intel.com/museum/archives/4004.htm
*Added archive https://web.archive.org/20140626022208/http://www.idc.com/getdoc.jsp?containerId=prUS24239313 to http://www.idc.com/getdoc.jsp?containerId=prUS24239313

When you have finished reviewing my changes, please set the ''checked'' parameter below to '''true''' to let others know.

{{sourcecheck|checked=false}}

Cheers.—[[User:Cyberbot II|<sup style="color:green;font-family:Courier">cyberbot II]]<small><sub style="margin-left:-14.9ex;color:green;font-family:Comic Sans MS">[[User talk:Cyberbot II|<span style="color:green">Talk to my owner]]:Online</sub></small> 07:24, 18 January 2016 (UTC)

== Semi-protected edit request on 30 January 2016 ==

{{edit semi-protected|Computer|answered=yes}}
<!-- Be sure to state UNAMBIGUOUSLY your suggested changes; editors who can edit the protected page need to know what to add or remove. Blank edit requests WILL be declined. --ADD ALL STORAGES>
<!-- Begin request -->

<!-- End request -->
[[Special:Contributions/2602:301:779F:DCA0:BCB9:44CA:70AD:384C|2602:301:779F:DCA0:BCB9:44CA:70AD:384C]] ([[User talk:2602:301:779F:DCA0:BCB9:44CA:70AD:384C|talk]]) 18:02, 30 January 2016 (UTC)
:[[File:Red question icon with gradient background.svg|20px|link=]] '''Not done:''' it's not clear what changes you want to be made. Please mention the specific changes in a "change X to Y" format.<!-- Template:ESp --> [[User:Datbubblegumdoe|Datbubblegumdoe]]<sup>[''[[User talk:Datbubblegumdoe|talk]] – [[Special:Contributions/Datbubblegumdoe|contribs]]'']</sup> 18:18, 30 January 2016 (UTC)

== External links modified ==

Hello fellow Wikipedians,

I have just added archive links to {{plural:1|one external link|1 external links}} on [[Computer]]. Please take a moment to review [https://en.wikipedia.org/w/index.php?diff=prev&oldid=705784804 my edit]. If necessary, add {{tlx|cbignore}} after the link to keep me from modifying it. Alternatively, you can add {{tlx|nobots|deny{{=}}InternetArchiveBot}} to keep me off the page altogether. I made the following changes:
*Added archive https://web.archive.org/20120130084757/http://www.laits.utexas.edu/ghazal/Chap1/dsb/chapter1.html to http://www.laits.utexas.edu/ghazal/Chap1/dsb/chapter1.html

When you have finished reviewing my changes, please set the ''checked'' parameter below to '''true''' to let others know.

{{sourcecheck|checked=false}}

Cheers.—[[User:Cyberbot II|<sup style="color:green;font-family:Courier">cyberbot II]]<small><sub style="margin-left:-14.9ex;color:green;font-family:Comic Sans MS">[[User talk:Cyberbot II|<span style="color:green">Talk to my owner]]:Online</sub></small> 15:28, 19 February 2016 (UTC)



== Notes were easy to understand ==

Computer notes were easy to understand as a form 6 and computer student. [[User:Singhtanisha|Singhtanisha]] ([[User talk:Singhtanisha|talk]]) 18:40, 22 February 2016 (UTC)
:Appreciate your feedback, but please be aware that this page is for discussions about improving the article. -- [[User:ChamithN|<span style="font-family:Segoe print; color:#CC4E5C; text-shadow:gray 0.2em 0.2em 0.4em;">ChamithN</span>]] [[User talk:ChamithN|<span style="color:#228B22">''(talk)''</span>]] 18:45, 22 February 2016 (UTC)

== Semi-protected edit request on 26 February 2016 ==

{{edit semi-protected|Computer|answered=yes}}
[[User:Waahwooh|Waahwooh]] ([[User talk:Waahwooh|talk]]) 18:51, 26 February 2016 (UTC)
<!-- Be sure to state UNAMBIGUOUSLY your suggested changes; editors who can edit the protected page need to know what to add or remove. Blank edit requests WILL be declined. -->
<!-- Begin request -->

<!-- End request -->
[[User:Waahwooh|Waahwooh]] ([[User talk:Waahwooh|talk]]) 18:51, 26 February 2016 (UTC)
:[[File:Red question icon with gradient background.svg|20px|link=]] '''Not done:''' it's not clear what changes you want to be made. Please mention the specific changes in a "change X to Y" format.<!-- Template:ESp --> [[User:EvergreenFir|'''<span style="color:#8b00ff;">Eve</span><span style="color:#6528c2;">rgr</span><span style="color:#3f5184;">een</span><span style="color:#197947;">Fir</span>''']] [[User talk:EvergreenFir|(talk)]] <small>Please &#123;&#123;[[Template:re|re]]&#125;&#125;</small> 19:26, 26 February 2016 (UTC)

== External links modified ==

Hello fellow Wikipedians,

I have just added archive links to {{plural:1|one external link|1 external links}} on [[Computer]]. Please take a moment to review [https://en.wikipedia.org/w/index.php?diff=prev&oldid=707696347 my edit]. If necessary, add {{tlx|cbignore}} after the link to keep me from modifying it. Alternatively, you can add {{tlx|nobots|deny{{=}}InternetArchiveBot}} to keep me off the page altogether. I made the following changes:
*Added archive http://web.archive.org/web/20130702173746/http://dx.doi.org/10.1109%2FMAHC.2003.1253887 to http://dx.doi.org/10.1109/MAHC.2003.1253887

When you have finished reviewing my changes, please set the ''checked'' parameter below to '''true''' or '''failed''' to let others know (documentation at {{tlx|Sourcecheck}}).

{{sourcecheck|checked=false}}

Cheers.—[[User:Cyberbot II|<sup style="color:green;font-family:Courier">cyberbot II]]<small><sub style="margin-left:-14.9ex;color:green;font-family:Comic Sans MS">[[User talk:Cyberbot II|<span style="color:green">Talk to my owner]]:Online</sub></small> 07:42, 1 March 2016 (UTC)

{{Clear}}
== "The Atanasoff–Berry Computer (ABC) was the world's first electronic digital computer, albeit not programmable."[2 ==

Following my sources, the A-B computer was first successfully used in summer 1941, while the Zuse Z3 was already successfully used in May 1941. http://de.wikipedia.org/wiki/Computer <small><span class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:178.115.250.216|178.115.250.216]] ([[User talk:178.115.250.216|talk]] • [[Special:Contributions/178.115.250.216|contribs]]) 12:45, 2 November 2012‎</span></small><!-- Template:Unsigned -->

== Edit request: fix anachronism in Roman vs Babylonian abacus ==

Currently the article states "The Roman abacus was used in Babylonia as early as 2400 BC." This makes no chronological sense - the Babylonian culture preceded the Roman culture. As a simple emergency fix, until someone finds a better solution, please replace with "The precursor of the Roman abacus was used in Babylonia as early as 2400 BC."  <small class="autosigned">—&nbsp;Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/86.158.154.88|86.158.154.88]] ([[User talk:86.158.154.88|talk]]) 10:26, 12 May 2016 (UTC)</small><!-- Template:Unsigned IP --> <!--Autosigned by SineBot-->

:{{Done}} --[[User:A D Monroe III|A D Monroe III]] ([[User talk:A D Monroe III|talk]]) 15:24, 8 July 2016 (UTC)

== Edit request: "programmable" contradiction ==

The article begins "A computer is a general purpose device that can be programmed  ..."

In the section "Analog computers" is "... many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable ..."

One or the other or both need repair.  Thanks, [[Special:Contributions/73.71.159.231|73.71.159.231]] ([[User talk:73.71.159.231|talk]]) 17:52, 14 June 2016 (UTC)

:Analog computers are specialist, not general purpose computers, and are not programmable. However, they are normally considered on topic here due to the close historical links. For example the [[Bombe]] was an analog computer.[[User:GliderMaven|GliderMaven]] ([[User talk:GliderMaven|talk]]) 19:17, 14 June 2016 (UTC

::Being "programmable" is an attribute, not a definition. A digital electronic computer is not necessarily a programmable computer, see [[Digital electronic computer]].  This article needs repair. [[Special:Contributions/73.71.159.231|73.71.159.231]] ([[User talk:73.71.159.231|talk]]) 20:15, 14 June 2016 (UTC)

:::Actually, it's [[Digital electronic computer]] that needs work; it's a stub (or worse, redundant with this article).  Current computers are programmable by all useful definitions, even if original analog computers were not.  Similarly, most of the rest of the lede did not apply to all analog or mechanical precursors to modern computers; that's not important.  --[[User:A D Monroe III|A D Monroe III]] ([[User talk:A D Monroe III|talk]]) 21:52, 14 June 2016 (UTC)

::Ah - you've trapped me. This article begins with "A computer is a general purpose device that can be programmed".  Very clever - restrict "computer" to "general purpose" and it would seem they have to be programmable.  Of course!!!  Just a little detail - is it true that there are no special purpose computers? [[Special:Contributions/73.71.159.231|73.71.159.231]] ([[User talk:73.71.159.231|talk]]) 01:59, 15 June 2016 (UTC)

:::We're trying to understand your concern; is it "programmable" or "general purpose"?  If it's some combination, that's hard to follow.  Can you make a specific statement of what it should be changed to?  --[[User:A D Monroe III|A D Monroe III]] ([[User talk:A D Monroe III|talk]]) 17:00, 20 June 2016 (UTC)

:::One stupidly nerdy point about Wikipedia is that the opening sentences don't define the term so much as the topic; the topic here is general purpose computers; that doesn't imply there are no special purpose computers; if you want to investigate other uses of the term 'computer' you shoudl check out the disambiguation page that is linked from the top.[[User:GliderMaven|GliderMaven]] ([[User talk:GliderMaven|talk]]) 19:45, 20 June 2016 (UTC)

: It would be an error to require all computers, by definition, to be "general purpose". Significant numbers of them are single specialised purpose. As general purpose computers (which means most digital computers) become cheaper to provide as overall systems, then the number of special-purpose computers will reduce. It is not though a defining requirement to be general purpose. [[User:Andy Dingley|Andy Dingley]] ([[User talk:Andy Dingley|talk]]) 17:13, 20 June 2016 (UTC)

::Is the topic "general purpose" then?  That's different from the header just above, but okay.
::As I see it, the idea behind "general purpose" is that computers are popular because they are complex machines that can be easily modified -- software.  While special purpose computerized products exist (in fact, it could be argued that no truly general purpose ones exist), the computer technology they utilize is (nowadays) based on the ability to execute any given instructions.  Gone are the days when designing a new product meant designing a new programming language, operating system, and cpu architecture; the computer inside isn't designed for the product, but vice-versa, made easy by the very general purpose nature of modern computers.  The computer hardware I'm using to write this could be found in a jet plane, or a factory, or a slaughterhouse.  
::Now, is that stated clearly in the lede?  No.  Is it sourced?  No.  Are ''these'' the issues?  --[[User:A D Monroe III|A D Monroe III]] ([[User talk:A D Monroe III|talk]]) 23:22, 20 June 2016 (UTC)

:::The lede continues to be modified for various views on this, yet also seems to get worse for opposing views.  I've expanded the lede in [https://en.wikipedia.org/w/index.php?title=Computer&diff=728918536&oldid=728514145 this edit] to attempt to combine all the concerns stated above, as I understand them.  The added wording skirts around the non-programmable or non-general purpose aspects of some early computers, even though those are no longer significant.  It also adds a paragraph on of history to highlight these changes to computers.  A history summary is needed anyway, per [[WP:LEAD]], since that's a large part of the article.  Comments?  
:::(A minor point; this adds several links, many of which also appear later in the body.  Some editors are quick to eliminate any and all duplicate links; I'm not.  Let's leave this until we see changes to the lede have settled down.)
:::--[[User:A D Monroe III|A D Monroe III]] ([[User talk:A D Monroe III|talk]]) 15:44, 8 July 2016 (UTC)

== Digital link ==

{{edit semi-protected|Computer|answered=yes}}
please change ((digital)) to ((Digital data|digital))
:[[File:Yes check.svg|20px|link=]] '''Done'''<!-- Template:ESp --> — [[User:Andy M. Wang|'''''Andy W.''''']] <span style="font-size:88%">('''[[User talk:Andy M. Wang|<span style="color:#164">talk</span>]] ·''' [[Special:Contribs/Andy M. Wang|ctb]])</span> 21:03, 9 July 2016 (UTC)

== Semi-protected edit request on 5 October 2016 ==

{{edit semi-protected|Computer|answered=yes}}

There is a simple gramatical erroe that i wish to change so people will not get acustomed to the use of wrong grammar.

[[User:Flame Rider|Flame Rider]] ([[User talk:Flame Rider|talk]]) 15:40, 5 October 2016 (UTC)
:[[File:Red question icon with gradient background.svg|20px|link=|alt=]] '''Not done:''' it's not clear what changes you want to be made. Please mention the specific changes in a "change X to Y" format.<!-- Template:ESp --> -- [[User:John of Reading|John of Reading]] ([[User talk:John of Reading|talk]]) 16:14, 5 October 2016 (UTC)

== External links modified ==

Hello fellow Wikipedians,

I have just modified {{plural:3|one external link|3 external links}} on [[Computer]]. Please take a moment to review [https://en.wikipedia.org/w/index.php?diff=prev&oldid=752059028 my edit]. If you have any questions, or need the bot to ignore the links, or the page altogether, please visit [[User:Cyberpower678/FaQs#InternetArchiveBot|this simple FaQ]] for additional information. I made the following changes:
*Added archive https://web.archive.org/web/20120130084757/http://www.laits.utexas.edu/ghazal/Chap1/dsb/chapter1.html to http://www.laits.utexas.edu/ghazal/Chap1/dsb/chapter1.html
*Added archive https://web.archive.org/web/20121103094710/https://scientific-computing.com:80/scwmayjun03computingmachines.html to http://www.scientific-computing.com/scwmayjun03computingmachines.html
*Added archive https://web.archive.org/web/20130511181443/http://www.epn-online.com/page/22909/the-hapless-tale-of-geoffrey-dummer-this-is-the-sad-.html to http://www.epn-online.com/page/22909/the-hapless-tale-of-geoffrey-dummer-this-is-the-sad-.html

When you have finished reviewing my changes, please set the ''checked'' parameter below to '''true''' or '''failed''' to let others know (documentation at {{tlx|Sourcecheck}}).

{{sourcecheck|checked=false}}

Cheers.—[[User:InternetArchiveBot|'''<span style="color:darkgrey;font-family:monospace">InternetArchiveBot</span>''']] <span style="color:green;font-family:Rockwell">([[User talk:InternetArchiveBot|Report bug]])</span> 07:43, 29 November 2016 (UTC)

== External links modified ==

Hello fellow Wikipedians,

I have just modified 2 external links on [[Computer]]. Please take a moment to review [https://en.wikipedia.org/w/index.php?diff=prev&oldid=757635888 my edit]. If you have any questions, or need the bot to ignore the links, or the page altogether, please visit [[User:Cyberpower678/FaQs#InternetArchiveBot|this simple FaQ]] for additional information. I made the following changes:
*Added archive https://web.archive.org/web/20120130084757/http://www.laits.utexas.edu/ghazal/Chap1/dsb/chapter1.html to http://www.laits.utexas.edu/ghazal/Chap1/dsb/chapter1.html
*Added archive https://web.archive.org/web/20130511181443/http://www.epn-online.com/page/22909/the-hapless-tale-of-geoffrey-dummer-this-is-the-sad-.html to http://www.epn-online.com/page/22909/the-hapless-tale-of-geoffrey-dummer-this-is-the-sad-.html

When you have finished reviewing my changes, you may follow the instructions on the template below to fix any issues with the URLs.

{{sourcecheck|checked=false|needhelp=}}

Cheers.—[[User:InternetArchiveBot|'''<span style="color:darkgrey;font-family:monospace">InternetArchiveBot</span>''']] <span style="color:green;font-family:Rockwell">([[User talk:InternetArchiveBot|Report bug]])</span> 20:34, 31 December 2016 (UTC)

== Semi-protected edit request on 1 January 2017 ==

{| class="wikitable sortable"
|-
! Header text !! Header text !! Header text
|-
| Example || Example || Example
|-
| Example || Example || Example
|-
| Example || Example || Example
|}
ê±ℳ₰[[Special:Contributions/202.70.191.30|202.70.191.30]] ([[User talk:202.70.191.30|talk]]) 02:52, 1 January 2017 (UTC)→≥

{{edit semi-protected|Computer|answered=yes}}
 [[Special:Contributions/202.70.191.30|202.70.191.30]] ([[User talk:202.70.191.30|talk]]) 02:52, 1 January 2017 (UTC)
:[[File:Red question icon with gradient background.svg|20px|link=|alt=]] '''Not done:''' it's not clear what changes you want to be made. Please mention the specific changes in a "change X to Y" format.<!-- Template:ESp --> [[User:DRAGON BOOSTER|<span style="color:blue;size=2">DRAGON BOOSTER</span>]][[User talk:DRAGON BOOSTER|<span style="color:#33dd44;size=2">  ★</span>]] 06:08, 1 January 2017 (UTC)

== Edit request: fix notes:22 (Dead Link) ==

The link under notes that is number 22  "Crash! The Story of IT: Zuse" currently listed is a dead link. There is an updated article at [https://goremotesupport.com/blog/crash-the-story-of-it-zuse/] where all that needs to be changed is the url it links too. [[User:Msearce|Msearce]] ([[User talk:Msearce|talk]]) 01:34, 14 July 2016 (UTC)

:{{Done}}. Also changed "replacement of" decimal to "rather than using" decimal since Zuse didn't know of the Babbage  decimal machine, but designed his binary one completely independently.  --[[User:A D Monroe III|A D Monroe III]] ([[User talk:A D Monroe III|talk]]) 19:32, 14 July 2016 (UTC)

Very nice [[User:Alshamiri1|Alshamiri1]] ([[User talk:Alshamiri1|talk]]) 14:56, 6 March 2017 (UTC)

== External links modified ==

Hello fellow Wikipedians,

I have just modified one external link on [[Computer]]. Please take a moment to review [https://en.wikipedia.org/w/index.php?diff=prev&oldid=785334799 my edit]. If you have any questions, or need the bot to ignore the links, or the page altogether, please visit [[User:Cyberpower678/FaQs#InternetArchiveBot|this simple FaQ]] for additional information. I made the following changes:
*Added archive https://web.archive.org/web/20130921055055/http://www.cs.ncl.ac.uk/publications/articles/papers/398.pdf to http://www.cs.ncl.ac.uk/publications/articles/papers/398.pdf

When you have finished reviewing my changes, you may follow the instructions on the template below to fix any issues with the URLs.

{{sourcecheck|checked=false|needhelp=}}

Cheers.—[[User:InternetArchiveBot|'''<span style="color:darkgrey;font-family:monospace">InternetArchiveBot</span>''']] <span style="color:green;font-family:Rockwell">([[User talk:InternetArchiveBot|Report bug]])</span> 23:18, 12 June 2017 (UTC)

== Resetability ==

This article contains a major mistake in how the functionality of a computer is defined.
The mistake is, that the definition does not mention, that a computer is always resetable in order to carry out successive operations and is therefore in fact different from a system comparable to a child being instructed to eat, defined by gender, his or her peas and doing so.
This is a 2 part complain, containing:
i.) resetability of operation
and ii.) inevitability of operation  <!-- Template:Unsigned IP --><small class="autosigned">—&nbsp;Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/88.65.146.27|88.65.146.27]] ([[User talk:88.65.146.27#top|talk]]) 16:20, 1 August 2017 (UTC)</small> <!--Autosigned by SineBot-->
[[Special:Contributions/88.65.146.27|88.65.146.27]] ([[User talk:88.65.146.27|talk]])  <!--Template:Undated--><small class="autosigned">—Preceding [[Wikipedia:Signatures|undated]] comment added 16:15, 1 August 2017 (UTC)</small> <!--Autosigned by SineBot-->

== additional errors in definition of computer ==

paragraph 2: </br>
  sentence 1: my Personal Computer did not control a part of Wikipedia, because I was reading the wikipedia-article titled "Computer". </br>
    Ergo: not all computers are control systems. </br>
  
  sentence 2, subsentence 3: it needs to be "general purpose devices" instead of "in general purpose devices", </br>
    due to the inexistance of the latter. </br>
paragraph 3: </br>
  sentence 1, subsentence 2: there were people not in possession of an abacus, </br>
    even during a version of the badly defined period of "ancient times", </br>
    therefore they could not have been aided by simple manual devices like the abacus. </br>
    Instead only some people were enabled to receive aid due using the abacus. </br>

    I do aknowledge the use of "ancient times" as usefull for the sake of using stylistic devices, </br>
    which is said to increase the likelyhood for some people to understand the meaning of statements. </br>
paragraph 4: </br>
  sentence 4: a modern computer needs to have at least one input (e.g. a set of buttons) and one output (e.g. a display). </br>
    If a modern computer does not contain one out of the two, then a modern computer is useless, </br>
    because it would either be a black box with or without input, </br>
    or it would not be able to execute operations due to the lack of input. </br>
    Therefore peripheral devices are not optional in order for a modern computer and any other computer to be of a use to the user. </br>
[[Special:Contributions/88.65.146.27|88.65.146.27]] ([[User talk:88.65.146.27|talk]])  <!--Template:Undated--><small class="autosigned">—Preceding [[Wikipedia:Signatures|undated]] comment added 16:58, 1 August 2017 (UTC)</small> <!--Autosigned by SineBot-->

== External links modified ==

Hello fellow Wikipedians,

I have just modified 3 external links on [[Computer]]. Please take a moment to review [https://en.wikipedia.org/w/index.php?diff=prev&oldid=793560509 my edit]. If you have any questions, or need the bot to ignore the links, or the page altogether, please visit [[User:Cyberpower678/FaQs#InternetArchiveBot|this simple FaQ]] for additional information. I made the following changes:
*Added archive https://web.archive.org/web/20160918203643/https://goremotesupport.com/blog/crash-the-story-of-it-zuse/ to https://goremotesupport.com/blog/crash-the-story-of-it-zuse
*Added archive https://web.archive.org/web/20090105031620/http://www.computer50.org/mark1/contemporary.html to http://www.computer50.org/mark1/contemporary.html
*Added archive https://web.archive.org/web/20081026080604/http://www.computer50.org/mark1/mark1intro.html to http://www.computer50.org/mark1/mark1intro.html

When you have finished reviewing my changes, you may follow the instructions on the template below to fix any issues with the URLs.

{{sourcecheck|checked=false|needhelp=}}

Cheers.—[[User:InternetArchiveBot|'''<span style="color:darkgrey;font-family:monospace">InternetArchiveBot</span>''']] <span style="color:green;font-family:Rockwell">([[User talk:InternetArchiveBot|Report bug]])</span> 14:34, 2 August 2017 (UTC)

== Introduction description is semantic/other whimsy? ==

"device that can be instructed to carry out arbitrary sequences" 7viii2k17rdg

Question: Computer use is non arbitrary - rather inter-posed procession of tool use & simulacra systems to r/k selection symbiotic type mutation/exploration?

Mechagnosis fractal of a always destined machined reality could have hard research to use other than 'frame drag'/'latent potential mutation relevance' synarchies? First level description correctness?

[[User:text_mdnp]] ~, 7 August 2017 (UTC)
: "Arbitrary" is correct. It means that any sequence of instructions may be defined (i.e. "arbitrary" choices, not choices restricted by the processor), and the processor will then carry them out. It's useful for this sequence to form a valid and correct program, but the processor will run incorrect programs as well as correct ones.
: I cannot understand any of your posting on this page. [[User:Andy Dingley|Andy Dingley]] ([[User talk:Andy Dingley|talk]]) 09:40, 7 August 2017 (UTC)
:: The Internet has become self-aware and is editing Wikipedia! --[[User:Wtshymanski|Wtshymanski]] ([[User talk:Wtshymanski|talk]]) 20:44, 7 August 2017 (UTC)
::: Self aware?  That's more than I'm seeing. [[User:Andy Dingley|Andy Dingley]] ([[User talk:Andy Dingley|talk]]) 22:26, 7 August 2017 (UTC)

== Semi-protected edit request on 13 April 2018 ==

{{edit semi-protected|Computer|answered=yes}}
The full form of the word 'computer' should be written there with the working of the machine written in that article. I just wanted to make edit on that by adding two more lines(at least 20 more words) making it easier because 80% of the audience to this topic are teens and youngster's. [[User:Yadav sharn117|Yadav sharn117]] ([[User talk:Yadav sharn117|talk]]) 17:32, 13 April 2018 (UTC)
:'''Not done'''.  It's not clear what you want done. Please state specifically what you would like to see in the form "change ''x'' to ''y''". --[[User:Wtshymanski|Wtshymanski]] ([[User talk:Wtshymanski|talk]]) 19:57, 13 April 2018 (UTC)

== Semi-protected edit request on 26 April 2018 ==

{{edit semi-protected|Computer|answered=yes}}
In the Software section of the article, can you please remove the '''Operating systems''' header because it lists software other than operating systems? [[Special:Contributions/192.107.120.90|192.107.120.90]] ([[User talk:192.107.120.90|talk]]) 13:37, 26 April 2018 (UTC)
:[[File:Yes check.svg|20px|link=|alt=]] '''Done'''<!-- Template:ESp --> Removed. Was done by [[User:Ryanking16|Ryanking16]] with this [https://en.wikipedia.org/w/index.php?diff=763958058&oldid=763321391&title=Computer edit]. Looks like they forgot to "reaarange more [and] add sections in few days." [[User:ChamithN|<span style="font-family:Segoe print; color:#CC4E5C; text-shadow:gray 0.2em 0.2em 0.4em;">ChamithN</span>]] [[User talk:ChamithN|<span style="color:#228B22">''(talk)''</span>]] 17:16, 26 April 2018 (UTC)

== More Display for PC at home... like page of book ==

Is there a normal PC (not "mobile") with more display (for Windows, Apple, Kindle, ecc) "to move to right and back left" like normal pages of a big book?  <!-- Template:Unsigned IP --><small class="autosigned">—&nbsp;Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/93.38.65.148|93.38.65.148]] ([[User talk:93.38.65.148#top|talk]]) 06:40, 21 June 2018 (UTC)</small> <!--Autosigned by SineBot-->

== Lead images purports to show computers from different eras, but none are from earlier than the late 80's at most ==

And there's earlier images within the article already to use.--[[User:Occono|<font color="orange">occono</font>]] ([[User talk:Occono|talk]]) 18:53, 23 June 2018 (UTC)

== Semi-protected edit request on 22 July 2018 ==

{{edit semi-protected|Computer|answered=yes}}
 [[Special:Contributions/2405:205:A161:5315:9259:920C:B18F:518A|2405:205:A161:5315:9259:920C:B18F:518A]] ([[User talk:2405:205:A161:5315:9259:920C:B18F:518A|talk]]) 13:54, 22 July 2018 (UTC)
:[[File:Red question icon with gradient background.svg|20px|link=|alt=]] '''Not done:''' it's not clear what changes you want to be made. Please mention the specific changes in a "change X to Y" format and provide a [[Wikipedia:Identifying reliable sources|reliable source]] if appropriate.<!-- Template:ESp --> <span style="text-shadow:#396 0.2em 0.2em 0.5em; class=texhtml">[[User:L293D|<b style="color:#060">L293D</b>]]&nbsp;([[User talk:L293D#top|<b style="color:#000">☎</b>]]&nbsp;•&nbsp;[[Special:Contributions/L293D|<b style="color:#000">✎</b>]])</span> 14:11, 22 July 2018 (UTC)
