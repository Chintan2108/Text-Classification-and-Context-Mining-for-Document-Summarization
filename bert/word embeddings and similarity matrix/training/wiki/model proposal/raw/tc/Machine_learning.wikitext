{{Vital article|topic=Technology|level=5|class=C}}
{{WikiProjectBannerShell|1=
{{WikiProject Robotics |class=B |importance=top}}
{{WikiProject Statistics |class=B |importance=mid}}
{{WikiProject Systems |class=B |importance=high |field=Cybernetics}}
{{WikiProject Computing |class=B |importance=high}}
{{WikiProject Computer science |importance=high |class=B}}
}}
{{summary in|learning}}
{{IEP assignment|course=Wikipedia:India Education Program/Courses/Fall 2011/Artificial Intelligence|university=College Of Engineering Pune|term=2011 Q3}}
{{User:MiszaBot/config
| algo=old(365d)
| archive=Talk:Machine learning/Archive %(counter)d
| counter=1
| maxarchivesize=75K
| archiveheader={{Automatic archive navigator}}
| minthreadsleft=5
| minthreadstoarchive=1
}}
{{Archive box|auto=yes}}

== Reinforcement Learning Placement ==

Shouldn't reinforcement learning be a subset of unsupervised learning?

:I don't think so. Reinforcement learning is not completely unsupervised: the algorithm has access to a supervision signal (the reward). It's just that it is difficult to determine which action(s) led to the reward, and there's an exploitation vs. exploration tradeoff. So, it isn't strictly supervised learning, either. It's somewhere in-between. -- [[User:Hike395|hike395]] July 1, 2005 07:08 (UTC)

:: I agree, that it is somewhat a hybrid, but given that supervised learning is described in the same section as 'Supervised learning: The computer is presented with example inputs and their desired outputs, given by a "teacher"[...]' is it right to have reinforcement learning as a subitem of that? Reinforcement learning is explicitly not learning with a teacher, but rather with a critic, isn't it? As I have encountered it over the years, it has been regarded as a third paradigm besides supervised and unsupervised learning, also because of its different applications. But I could also err...  <!-- Template:Unsigned IP --><small class="autosigned">—&nbsp;Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/195.166.125.3|195.166.125.3]] ([[User talk:195.166.125.3#top|talk]]) 13:32, 25 July 2018 (UTC)</small> <!--Autosigned by SineBot-->

== Proposed merge with [[Machine learning lowers risks of credit]] ==

The content of this article probably has some value but I suggest to merge it into main article as a small sub-section. [[User:Bbarmadillo|Bbarmadillo]] ([[User talk:Bbarmadillo|talk]]) 21:08, 17 March 2018 (UTC)

== New and developing methods ==

I have recently added a section to include current research on a new machine learning method known as Linear Poisson Modelling.<ref>{{Cite journal|date=2015-07-01|title=Automated quantitative measurements and associated error covariances for planetary image analysis|url=https://www.sciencedirect.com/science/article/pii/S0273117715002549|journal=Advances in Space Research|language=en|volume=56|issue=1|pages=92–105|doi=10.1016/j.asr.2015.03.043|issn=0273-1177}}</ref> <ref>{{Cite journal|last=Deepaisarn|first=S|last2=Tar|first2=P D|last3=Thacker|first3=N A|last4=Seepujak|first4=A|last5=McMahon|first5=A W|date=2017-10-28|title=Quantifying biological samples using Linear Poisson Independent Component Analysis for MALDI-ToF mass spectra|url=https://academic.oup.com/bioinformatics/article/34/6/1001/4575137|journal=Bioinformatics|language=en|volume=34|issue=6|pages=1001–1008|doi=10.1093/bioinformatics/btx630|issn=1367-4803}}</ref> <ref>{{Cite journal|last=Tar|first=P. D.|last2=Thacker|first2=N. A.|last3=Babur|first3=M.|last4=Watson|first4=Y.|last5=Cheung|first5=S.|last6=Little|first6=R. A.|last7=Gieling|first7=R. G.|last8=Williams|first8=K. J.|last9=O’Connor|first9=J. P. B.|title=A new method for the high-precision assessment of tumor changes in response to treatment|url=https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty115/4934935|journal=Bioinformatics|language=en|doi=10.1093/bioinformatics/bty115}}</ref> <ref>https://link.springer.com/article/10.1007/s11038-016-9499-9</ref> <ref>https://www.sciencedirect.com/science/article/pii/S1387380617304906</ref> <ref>http://www.bmva.org/annals/2014/2014-0001.pdf</ref>As this method has not been widely communicated, I can understand why some would rather not include such work on the main Machine Learning page at present. However, the method is now associated with more than a dozen co-authors in application-specific areas, so I believe it is worth noting. I have tentatively place this in a new section regarding new and developing methods. Perhaps other new and developing methods could be placed there too? What criteria should be considered before inclusion?<!-- Template:Unsigned IP --><small class="autosigned">—&nbsp;Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/82.23.74.236|82.23.74.236]] ([[User talk:82.23.74.236#top|talk]]) 17:26, 9 June 2018‎</small>
{{Reflist-talk}}

: {{reply to|82.23.74.236}} All of these references are around one single author. If they have any citations at all, these are ''all self-cites''. They are not ''independent'' reliable sources, and this raises the question of a [[Wikipedia:Conflict of interest]] - in particular, as this IP is in the same region as that shared author!
: If we would cover every single obscure topic, the article would be millions of lines. It is an overview article, even key topics such as "Deep learning" used by thousands of authors only get a single paragraph. LPM clearly is not on the same level, including it here would likely give [[Wikipedia:Neutral_point_of_view#Undue_weight|WP:undue weight]] to a single author's, not independently verified, work. If it were independently used, it may eventually be worth including it in, e.g., [[Poisson regression]], which will be reachable via [[Regression analysis]]. But even then, that may take a few years and ''should be added by someone independent''. Not everything needs to be front page! [[User:HelpUsStopSpam|HelpUsStopSpam]] ([[User talk:HelpUsStopSpam|talk]]) 18:37, 9 June 2018 (UTC)

: {{reply to|130.88.234.208}} the same thing still applies, even if you use a different IP and different article: {{diff|The_Three_Rs|845383381|845146230|The Three Rs}}. Do not cite yourself, you have a [[Wikipedia:Conflict of interest]]. Leave it for others to - later - decide what was a noteworthy research contribution. [[User:HelpUsStopSpam|HelpUsStopSpam]] ([[User talk:HelpUsStopSpam|talk]]) 19:43, 11 June 2018 (UTC)


== Relation to statistics ==

The first paragraph of this section is very good, IMHO, but the last two are problematic. The second seems random and a little unfinished. The third raises an important point, but saying that statistical learning arose because "[s]ome statisticians have adopted methods from machine learning" is arguably confusing the chicken with the egg. It should also be mentioned here that ''statistical machine learning'' is a relatively well-established term (cf. e.g. [https://www.elsevier.com/books/introduction-to-statistical-machine-learning/sugiyama/978-0-12-802121-7# this book]) which has a meaning somewhere in between ''machine learning'' and ''statistical learning''. [[User:Thomas Tvileren|Thomas Tvileren]] ([[User talk:Thomas Tvileren|talk]]) 13:07, 1 November 2018 (UTC)

== Reorganizing the Approaches section ==

I reorganized the [[Machine learning#Approaches|Approaches]] section to more accurately represent the parent-child relationships of machine learning articles, as described in [[WP:SUMMARY]] style guidelines, and added text where I could by borrowing it from the lead sections of the child articles. I deleted the reference to [[List of machine learning algorithms]] as the primary main article (right under the section name) because it is not a more detailed version of the Approaches section as a whole. It is the opposite, a condensed list with no details. In a couple of other places, there were links to "main articles" that were not in fact child articles, as the label was intended for. It makes more sense to me to consider broader topics like the types of learning algorithms, the processes/techniques, and the models/frameworks used in ML to be the direct "children" of the Approaches section, so I created those headings and then sorted the text between them. I hope this makes the text easier to understand, and grasp at a higher level of understanding. [[User:Romhilde|Romhilde]] ([[User talk:Romhilde|talk]]) 02:44, 25 November 2018 (UTC)
{{Automatic archive navigator}}

== [[Structured Data Mining]] is missing ==

The category [[Structured Data Mining]] is missing. See
[http://hms.liacs.nl/ summarization]
Especially the sub-categories are also missing:
*[[Molecule mining]]
*[[Graph mining]]
*[[Tree mining]]
*[[Sequence mining]]
*[[Database mining]]

Two important books are:
* Kernel Methods in Computational Biology, Bernhard Scholkopf, Koji Tsuda, Jean-Philippe Vert
* Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology

[[User:Joerg Kurt Wegner|JKW]] 11:50, 8 April 2006 (UTC)

== deductive learning? ==

"''At a general level, there are two types of learning: inductive, and deductive.''"

What's deductive learning? Isn't learning inductive? --[[User:Took|Took]] 01:48, 10 April 2006 (UTC)

From a purely writing view, the rest of the paragraph (after the above quote) goes on to explain what inductive machine learning is, but deductive machine learning isn't covered at all.  --[[User:Ferris37|Ferris37]] 03:49, 9 July 2006 (UTC)

:I don't think the statement that the two basic learning approaches are inductive and deductive makes any sense. In supervised learning there is inductive and transductive learning, but I am not sure about the "deductive" one. At least I wouldn't know what it is. 
: The biggest learning categories are usually identified as: Supervised-, semi-supervised-, unsupervised- and reinforcement learning. Although reinforcement learning can be viewed as a special case of supervised learning. 
: There are also more subtle categories, such as e.g. active-learning, online-learning, batch-learning.

== Radial basis function ==

Should this article link to the "radial basis function" article, instead of linking to the two articles "radial" and "basic function"?
: Absolutely {{done}} --[[User:Adoniscik|Adoniscik]] ([[User talk:Adoniscik|talk]]) 20:54, 9 March 2008 (UTC)

== Non-homogeneous reference format ==

It's a minor, but I see in this article the format of the references is inconsistent. Bishop is cited once as Christopher M. Bishop and another one as Bishop, C.M. Is there a standard format for wikipedia references? Jose
: I use [[WP:CITET]] inside [[WP:FOOT]] --[[User:Adoniscik|Adoniscik]] ([[User talk:Adoniscik|talk]]) 20:58, 9 March 2008 (UTC)

== Blogs ==
Some people, mainly researchers of this field (ML) are blogging about this subject. Some blogs are really interesting. Is there a space in an encyclopedia for links to those blogs ?
I can see 3 pbs with this:
* advertising for people/blogs?
* how to select relevant blogs
* necessity to check if those blogs are enougth often updated.
What do you think of adding a blog links section ? [[User:Dangauthier|Dangauthier]] 14:11, 13 March 2006 (UTC)

: Can be interesting, the question is of course which ones to include.  I posted recently a list of machine learning blogs on my blog: http://www.inma.ucl.ac.be/~francois/blog/entries/entry_155.php [[User:Damienfrancois|Damienfrancois]] 09:09, 7 June 2006 (UTC)

I deleted the link to a supposed ML blog [http://kinfkong.spaces.live.com/] which wasn't relevant, and was not in english.

: I oppose the inclusion of blogs. Most of the article right now consists of links. See [[WP:Linkspam]] --[[User:Adoniscik|Adoniscik]] ([[User talk:Adoniscik|talk]]) 21:01, 9 March 2008 (UTC)

== Archive bin required?  ==

suggestion = archive bin required [[User:Sanjiv swarup|Sanjiv swarup]] ([[User talk:Sanjiv swarup|talk]]) 07:44, 17 September 2008 (UTC)
: If you mean that the talk page should be archived I disagree. It is pretty managable at the moment. Typically months pass in between comments! --[[User:Adoniscik|Adoniscik]]<sup><small>([[User_talk:Adoniscik|t]], [[Special:Contributions/Adoniscik|c]])</small></sup> 08:04, 17 September 2008 (UTC)

== Column formatting ==

Is there any reason that the See Also section is formatted in columns? Or was that just the result of some vestigial code... [[User:WDavis1911|WDavis1911]] ([[User talk:WDavis1911|talk]]) 20:38, 27 July 2009 (UTC)

== "Labeled examples" ==

On this page, and the main [[unsupervised learning]] page, the phrase "labeled examples" is not explained or defined before being used. Can somebody come up with a concise definition? --[[User:Bcjordan|Bcjordan]] ([[User talk:Bcjordan|talk]]) 16:31, 15 September 2009 (UTC)

== Help needed with "learn" ==

Hi,

In the following context ''As a broad subfield of artificial intelligence, machine learning is concerned with the design and development of algorithms and techniques that allow computers to "learn"'' ,no definition of the last word in the sentense - "learn" - is given. However, it appears very essential, because it's central to this main definition.

A definition like "machine learning is an algorithm that allows machines to learn" sounds to me like a perfectly tautologous definition.

It's my understading that this article is about either computer science, or mathematics, or statistics, or some other "exact" discipline. All of these disciplines have quite exact definitions of everything, exept for those very few undefined terms that are declared upfront  as axioms or undefined concepts. Examples: point, set, "Axiom of choice".

In this article, the purpose of Machine Learning and the tools it uses are clear to me as a reader. But the very method is obscure - what exactly it means for a machine to 'learn'. Would somebody please define "learn" in precise terms without resortiong to other obscure and not exactly defined in the technical world words like 'understand' or 'intelligence'?

There must exist a formal definition of 'learn', but if not, then, in my opinion, in order to avoid confusion, it should be clearly stated upfront that the very subject of machine learning is not clearly defined.

Compare this, for example, to how 'mathematics' is defined, or how the functions of ASIMO robot are clearly defined in Wikipedia.

Thanks in advance,
[[User:Raokramer|Raokramer]] 13:28, 8 October 2007 (UTC)

There are formal definitions of what "learn" means. Basically it is about generalizing from a finite set of training examples, to allow the learning agent to do something (e.g. make a prediction, a classification, predict a probability, find a good representation) well (according to some mathematically defined criterion, such as prediction error) on new examples (that have something in common with the training examples, e.g., typically they are assumed to come from the same underlying distribution).

[[User:Yoshua.Bengio|Yoshua Bengio]] March 26th, 2011.  <span style="font-size: smaller;" class="autosigned">—Preceding [[Wikipedia:Signatures|undated]] comment added 01:18, 26 March 2011 (UTC).</span><!--Template:Undated--> <!--Autosigned by SineBot-->

== Promoting the article's growth ==

Does anyone think snipping the FR section (and moving it here) would encourage people to actually write something? --[[User:Adoniscik|Adoniscik]]<sup><small>([[User_talk:Adoniscik|t]], [[Special:Contributions/Adoniscik|c]])</small></sup> 02:40, 13 October 2008 (UTC)
: FR? [[User:Pgr94|pgr94]] ([[User talk:Pgr94|talk]]) 12:04, 1 May 2011 (UTC)

== Ref mess ==

In [http://en.wikipedia.org/w/index.php?title=Machine_learning&diff=next&oldid=212067492 this diff], the "Bibliography" section was converted to "Further reading".  Looking at the history, it's clearly an aggregation of actual sources with other things just added for the heck of it.  It is sometimes possible to see what an editor was adding when he added a source there, so there are good clues for how we could go away citing sources for the contents of the article.  It's too bad it developed so far so early, before there was much of an ethic of actually citing sources, because now it will be a real pain to fix.  Anyone up for working on it?  [[User:Dicklyon|Dicklyon]] ([[User talk:Dicklyon|talk]]) 18:56, 10 April 2011 (UTC)
: [[WP:BOLD|Be bold]]! [[User:Pgr94|pgr94]] ([[User talk:Pgr94|talk]]) 12:05, 1 May 2011 (UTC)

== Connection to pattern recognition ==

This article should definitely link to pattern recognition.
And I feel there should be some discussion on what belongs on pattern recognition and what on machine learning.
[[User:T3kcit|T3kcit]] ([[User talk:T3kcit|talk]]) 06:21, 23 August 2011 (UTC)

== Are there any learning algorithms that don't work by search? ==

Do all learning algorithms perform search?  All rule/decision-tree algorithms certainly do search.   Are there any exceptions?  
*[[Naive bayes classifier]]s may be one exception as it is computed by a closed-form formula.
Are there any other exceptions?
[[User:Pgr94|Pgr94]] ([[User talk:Pgr94|talk]]) 12:31, 16 April 2008 (UTC)

: Most learning algorithms don't do search. Search is more an AI thing, not so much learning. Many algorithms are based on convex optimization: Support Vector Machines, Conditional Random Fields, logistic regression, etc.
::Optimization is a kind of search: http://en.wikipedia.org/wiki/Optimization_%28mathematics%29#Optimization_problems [[User:Pgr94|pgr94]] ([[User talk:Pgr94|talk]]) 12:02, 1 May 2011 (UTC)
:::If you define search as "finding the solution to a mathematical formular" as wikipedia says, then optimization is search. And learning has to be search, too. Then naive Bayes is search to, because it solves a mathematical formula. Imho saying solving a formular is search is a little misleading. I think the term is mostly used for discrete problems, not continuous ones. But I would agree that most learning algorithms use some kind of optimization. 
:::Also, one might ask the question "What is the search used for?"
:::Saying learning algorithms work by search sounds like they produce their answer by doing a lookup, which is certainly not the case for most algorithms. Most learning algorithms build some kind of model. Usually by some formula. If solving a formula is search, well then what other choices are there? Btw, this is really the wrong place for this kind of discussion so I'd be glad if you remove it. If you have questions about machine learning, I'd suggest metaoptimize.com. [[User:T3kcit|T3kcit]] ([[User talk:T3kcit|talk]]) 06:16, 23 August 2011 (UTC)
:Thank you for your reply T3k.  The article currently does not mention the relationship between learning and search.  According to Mitchell's seminal article generalization is a search problem.
<blockquote>
One capability central to many kinds of learning is the ability to ''generalize'' [...] The purpose of this paper is to compare various approaches to generalization in terms of a single framework.  Toward this end, generalization is cast as a search problem, and alternative methods for generalization are characterized in terms of search strategies that they employ. [...]  Conclusion: The problem of generalization may be viewed as a search problem involving a large hypothesis space of generalizations.  [...]  
Generalization as search, Tom Mitchell, Artificial Intelligence (1982)  {{doi|10.1016/0004-3702(82)90040-6}}
</blockquote>
:I am enquiring here if there are any more recent publications that qualify this very general principle. [[User:Pgr94|pgr94]] ([[User talk:Pgr94|talk]]) 20:10, 23 August 2011 (UTC)

::Saying that different approaches can be ''cast'' as search doesn't mean that they ''are'' search, nor that they ''use'' search.  20:18, 23 August 2011 (UTC)
:I am not quite sure this is what you are looking for but there is the study of [[empirical risk minimization]]. This is a standard formulation of the learning problem. You could say that it defines learning as a search problem, also I guess most people would rather call it an optimization problem. [[User:T3kcit|T3kcit]] ([[User talk:T3kcit|talk]]) 10:24, 24 August 2011 (UTC)

== Representation learning notabiity ==

Is "representation learning" sufficiently notable to warrant a subsection?  The machine learning journal and journal of machine learning research have no articles with "representation learning" in the title.  Does anyone have any machine learning textbooks with a chapter on the topic (none of mine do)?  There is no wikipedia article on the subject.  Any objections to deleting?   [[User:Pgr94|pgr94]] ([[User talk:Pgr94|talk]]) 22:38, 15 August 2011 (UTC)

:I would agree that it might not yet pass [[WP:Notability]].  And that's why it doesn't have its own article.  But a paragraph seems OK.  Other sources that discuss the topic include [http://www.tandfonline.com/doi/abs/10.1080/09540099108946592 this 1991 paper] and [http://citeseer.ist.psu.edu/viewdoc/download;jsessionid=FB3A02D170023A6206E832544B80C074?doi=10.1.1.145.8948&rep=rep1&type=pdf this 1997 paper], and [http://portal.acm.org/citation.cfm?id=1953039 this 2010 paper]; others may use different words for the same ideas. [[User:Dicklyon|Dicklyon]] ([[User talk:Dicklyon|talk]]) 00:09, 16 August 2011 (UTC)
::Machine learning is a large field spanning 50 odd years.  Three or four articles is therefore hardly notable.  [[WP:UNDUE]] states that "represents all significant viewpoints [..] in proportion to the prominence of each viewpoint".  Unless there is more evidence for the significance of representation learning this section needs to be removed. [[User:Pgr94|pgr94]] ([[User talk:Pgr94|talk]]) 19:37, 26 August 2011 (UTC)
:::Is the term "representation learning" what you think is too uncommon?  The small paragraph in question is just a very quick survey of some techniques that are in common use these days.  There are [http://scholar.google.com/scholar?hl=en&q=representation+manifold+learning+dimensionality+reduction+sparse+coding+unsupervised+clustering&btnG=Search&as_sdt=0%2C5&as_ylo=&as_vis=1 tons of sources] covering the topics of that paragraph.  [[User:Dicklyon|Dicklyon]] ([[User talk:Dicklyon|talk]]) 22:08, 26 August 2011 (UTC)
::::I have issue with the term "representation learning" which is uncommon.  The section should be renamed [[dimension reduction]].  This is the more common term.  Do you have any objection? [[User:Pgr94|pgr94]] ([[User talk:Pgr94|talk]]) 09:51, 2 October 2011 (UTC)
:::::That leaves out the other end of the spectrum, sparse coding, which is usually a dimension increase.  [[User:Dicklyon|Dicklyon]] ([[User talk:Dicklyon|talk]]) 16:22, 2 October 2011 (UTC)
::::::I think you're pushing a point of view that is not supported by the literature.  As editors, we should reflect the literature, and not seek to adapt it.  As I have already said above, there are few references for "representation learning".  I really don't see why you're insisting...  [[User:Pgr94|pgr94]] ([[User talk:Pgr94|talk]]) 16:41, 2 October 2011 (UTC)

== Adversarial Machine Learning ==

Recently I've heard the term '''''Adversarial Machine Learning''''' a few times but I can't find anything about it on Wikipedia. Is this a real field which should be covered in this article, or even get its own article? &mdash; [[User:Hippietrail|Hippietrail]] ([[User talk:Hippietrail|talk]]) 07:47, 29 July 2012 (UTC)

== Lead section badly-written and confusing ==

The lead section of the article is badly-written and very confusing.
{{Cquote|Machine learning, a branch of artificial intelligence, is a scientific discipline concerned with the design and development of algorithms that take as input empirical data, such as that from sensors or databases, and yield patterns or predictions thought to be features of the underlying mechanism that generated the data. A learner can take advantage of examples (data) to capture characteristics of interest of their unknown underlying probability distribution. Data can be seen as instances of the possible relations between observed variables. A major focus of machine learning research is the design of algorithms that recognize complex patterns and make intelligent decisions based on input data. One fundamental difficulty is that the set of all possible behaviors given all possible inputs is too large to be included in the set of observed examples (training data). Hence the learner must generalize from the given examples in order to produce a useful output in new cases.}}
For example, the word "learner" is introduced without any context. For another example, the beginning sentence is very long and meandering. Finally, the end sentence is very poorly explained and seems to be a detail which does not belong in a lead section. A lot of words are tagged on. This lead certainly does not summarize the article. Thus I am tagging this article. [[User:JoshuSasori|JoshuSasori]] ([[User talk:JoshuSasori|talk]]) 06:09, 28 September 2012 (UTC)
: Tried to improve the Lead based on your feedback. Any further feedback that you may provide would be helpful. Thanks. [[User:IjonTichyIjonTichy|IjonTichyIjonTichy]] ([[User talk:IjonTichyIjonTichy|talk]]) 15:05, 5 November 2012 (UTC)

== A section for preprocessing for learning? ==

I recently read an article about distance metric learning (jmlr.csail.mit.edu/papers/volume13/ying12a/ying12a.pdf) and it appears that there should be a section dedicated to preprocessing techniques. Distance metric learning has to do with learning a Mahalanobis distance which describes whether samples are similar or not. One could proceed to transform the data into a space where irrelevant variation is minimized and the variation that is correlated to the learning task is preserved (relevant component analysis). I think feature selection/extraction should also be mentioned.

I believe a brief section discussing preprocessing and linking to the relevant sections would be beneficial. However, such a change should have the support of the community. Please comment and provide your opinions.  <span style="font-size: smaller;" class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/150.135.222.151|150.135.222.151]] ([[User talk:150.135.222.151|talk]]) 22:36, 28 September 2012 (UTC)</span><!-- Template:Unsigned IP --> <!--Autosigned by SineBot-->
: This is a good idea; the Mahalanobis distance is used in practice in industry, and should be mentioned here. But probably only briefly, as the article seems to be quite technical already and not so easy to read for non-experts. [[User:IjonTichyIjonTichy|IjonTichyIjonTichy]] ([[User talk:IjonTichyIjonTichy|talk]]) 15:10, 5 November 2012 (UTC)

== Algorithm Types ==
"Algorithm Types" should probably not link to [[Taxonomy]]. It is simpler and more precise to say "machine learning algorithms can be categorized by different qualities." 
[[User:StatueOfMike|StatueOfMike]] ([[User talk:StatueOfMike|talk]]) 18:18, 26 February 2013 (UTC)

=== Problem Types ===
I find the "Algorithm Types" section very help for providing context for the rest of the article. I propose adding a section/subsection "Problem Types" to provide a more complete context. For example. many portions of the rest of the article will say something like "is [[supervised learning]] method used for [[classification]] and [[regression]]". "Supervised Learning" is explained somewhat under the "Algorithm Types" section, but the problem types are not. [[Structured learning]] already has a good breakdown of problem types in machine learning. We could incorporate that here, and hopefully expand on it. [[User:StatueOfMike|StatueOfMike]] ([[User talk:StatueOfMike|talk]]) 23:12, 8 February 2013 (UTC)

==General discussion==

I find the machine learning page pretty good. However, the distinction between machine learning and data mining presented in this article is misleading and probably not right. The terms 'data mining' and 'machine learning' are used interchangeably by the masters of the field along with plenty of us regular practitioners. The distinction presented in this article--that one deals with knowns and the other with unknowns--just isn't right. I'm not sure how to be positive about it.  Data mining and machine learning engage in dealing with both knowns and unknowns because they're both really the same thing.

My primary source for there being no difference between the terms is the author of the definitive and most highly cited machine learning/data mining text, "Machine Learning" ([http://www.cs.cmu.edu/~tom/mlbook.html Mitchell, Tom M. Burr Ridge, IL: McGraw Hill, 1997]), Carnegie Mellon Machine Learning Department chief, Tom Mitchell. Mitchell actually tackles head-on the lack of real distinction between the terms in a paper he published for Communications of the ACM, published in 1999 (http://dl.acm.org/citation.cfm?id=319388).  I've also been in the field for a number of years and support Mitchell's unwillingness to distinguish the two.

Now, I can *imagine* that when we use the term 'data mining' we are also including 'web mining' under the umbrella of 'data mining.' We mining is a task that may involve data extraction performed without learning algorithms. 'Machine learning' places emphasis on the algorithmic learning aspect of mining. The [http://books.google.com/books?id=6lVEKlrTq8EC&printsec=frontcover&dq=Data+Mining:+Practical+Machine+Learning+Tools+and+Techniques+witten+frank&hl=en&sa=X&ei=N7uOUdLJJojA9QTxhoHQCw&ved=0CD8Q6AEwAQ#v=onepage&q=Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques%20witten%20frank&f=false widely used Weka text written by Witten and Frank] does differentiate the two terms in this way. But more than a few of us in the community felt that when that text came out, as useful as it is for using Weka and teaching neophytes, the distinction was without precedent. It struck us as something the authors invented while writing the book's first edition. Their distinction is more along the learning versus extraction distinction, but that's a false distinction as learning is often used for extraction for structuring data, and learning patterns in a data set is always a sort of "extraction," "discovery," etc.  But even Witten and Frank aren't suggesting that one is more for unknowns and the other for knowns, or one is more for prediction and the other for description. Data mining/machine learning is used in a statistical framework, where statistics is quite clearly a field dedicated to handling uncertainty, which is to say it's hard to predict, forecast, or understand the patterns within data.

I feel that 'data mining' should redirect to 'machine learning,' or 'machine learning' redirect to 'data mining,' the section distinguishing the two should be removed, and the contents of the two pages merged.   [[User:Textminer|Textminer]] ([[User talk:Textminer|talk]]) 21:44, 11 May 2013 (UTC)

----
There is no discussion of validation, over-fit and the bias/variance tradeoff. To me this is the whole point and the reason why wide data problems are so elusive.
[[User:Izmirlig|Izmirlig]] ([[User talk:Izmirlig|talk]])
----  <small><span class="autosigned">— Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:Izmirlig|Izmirlig]] ([[User talk:Izmirlig|talk]] • [[Special:Contributions/Izmirlig|contribs]]) 18:42, 12 September 2013 (UTC)</span></small><!-- Template:Unsigned --> <!--Autosigned by SineBot-->

I modified the strong claim that Machine Learning systems try to create programs without an engineer's intutition. When a machine learning task is specified, a human decides how the data are to be represented (e.g. which attributes will be used or how the data need to be preprocessed). This is the "observation language". The designer also decides the "hypothesis language", i.e. how the learned concept will be represented. Decision trees, neural nets, SVMs all have subtlety different ways of describing the learned concept. The designer also decides on the kind of search that will be used, which biases the end result.

----

The way the page is written now, there is no distinguishing between machine learning and [[pattern recognition]]. machine learning is much more than simple classification. Robots that learn how to act in groups is machine learning but not pattern recognition. I am not an expert at ML, but am an expert in pattern recognition. So I hope that someone will edit this page and put in more information about machine learning that is not also pattern recognition.

:I don't agree with this: I believe that pattern recognition is generally restricted to classification, while this page explicitly says that ML covers classification, supervised learning (which includes regression), unsupervised learning (such as clustering), and reinforcement learning. 
:: Careful not to pigeonhole into the "unsupervised learning is clustering and vice versa". The data mining folks think this way and they're completely wrong, as my ML prof once said. [[User:65.50.71.194]]
::: Notice that I said "such as clustering". The article does clearly state that unsupervised learning is modeling. -- [[User:Hike395|hike395]] 16:02, 2 Mar 2005 (UTC)
:Further, I don't think of pattern recognition as a specific method, but rather a collection of methods, generally described in the 1st edition of Duda and Hart. So, I deleted pattern recognition from "common methods". Also, a genetic algorithm is a generic optimization algorithm, not a machine learning algorithm. So, I removed it, too. -- [[User:Hike395|hike395]] 01:13, 20 Dec 2004 (UTC)
:: There are those who would disagree on the subject of Genetic Algorithms and their relation to ML. Machine learning takes it's basic principles from those found in naturally occurring systems, so do GA's. You could call evolution a kind of "intelligence", I suppose. Anyway the call's been made, but there should be some mention in the "related".
:::I disagree with this statement --- machine learning has completely divorced itself from any natural "intelligent" system: it is a branch of statistics. I think you are thinking of the term "computational intelligence" (which is the new name for an IEEE society). I'm happy to have ''See also'' links to AI and CI. -- [[User:Hike395|hike395]] 16:02, 2 Mar 2005 (UTC)

>''You could call evolution a kind of "intelligence"''

No. Evolution is not goal-directed.

[[User:BlaiseFEgan|Blaise]] 17:32, 30 Apr 2005 (UTC)

Unlike many in the ML community, who want to find computationally lightweight algorithms that scale to very large data sets, many statisticians are currently interested in computationally ''intensive'' algorithms. (We're interested in getting models that are as faithful as possible to the situation, and we generally work with smaller data sets, so the scaling isn't such a big issue.) The point I'm making is that the statement that "ML is synonymous with computational statistics" is just plain wrong. 

[[User:BlaiseFEgan|Blaise]] 17:29, 30 Apr 2005 (UTC)

:I had misgivings about that statement, too, so I just deleted it. Notice that I also deleted your edit that statistics deals with data uncertainty only, but ML deals with certain and uncertain data. I'd be willing to bet that you are a frequentist (right?). At the 50 kilometer level, frequentist statisticians deal with data uncertainty, but Bayesian statisticians deal with model uncertainty (keeping the observed data as an absolute, and integrating over different model parameters). I don't think you can make the distinction that statisticians are only frequentist (deal with data uncertainty), since Bayesian statisticians would violently disagree.

:Now, if you say that ML people care more about accurate predictions, while statisticians care more about accurate models, that may be true, although I don't believe you can make an absolute statement. --- [[User:Hike395|hike395]] 23:02, 30 Apr 2005 (UTC)

== Generalization in lede ==

From the lede:

:The core of machine learning deals with representation and generalization.  Representation of data instances and functions evaluated on these instances are part of all machine learning systems.  Generalization is the property that the system will perform well on unseen data instances;

This doesn't cover [[transductive learning]], where the data are finite and available upfront, but the pattern is unknown. Much unsupervised learning (clustering, topic modeling) follows this pattern as well. [[User:Qwertyus|Q<small>VVERTYVS</small>]] <small>([[User talk:Qwertyus|hm?]])</small> 17:32, 23 July 2014 (UTC)

:I got rid of the offending paragraph and wrote a completely new lede. [[User:Qwertyus|Q<small>VVERTYVS</small>]] <small>([[User talk:Qwertyus|hm?]])</small> 18:07, 23 July 2014 (UTC)

== New section on genetic algorithms ==

I just hedged the new GA section by stating, and proving with references, that "genetic algorithms found some uses in the 1980s and 1990s". But actually, I'd much rather remove the passage, because AFAIC ''very'' little serious work on GAs is done in the machine learning community as opposed to serious stuff like graphical models, convex optimization, and other topics that are much less sexy than "pseudobiology" (as Skiena put it). I think devoting a section, however short, to GAs and not to, say, gradient descent optimization, is an utter misrepresentation of the field. [[User:Qwertyus|Q<small>VVERTYVS</small>]] <small>([[User talk:Qwertyus|hm?]])</small> 17:07, 21 October 2014 (UTC)

Here are some figures to make my point more clearly. The only recent, reasonably well-cited paper on GAs in ML that I could find is

* [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1250182&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D1250182 Feature selection for support vector machines by means of genetic algorithm] (240 citations according to GScholar).

By comparison:

* [http://ntucsv.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf LIBSVM: a library for support vector machines A practical guide to support vector classification] has 19064.
* [http://dl.acm.org/citation.cfm?id=1442794 LIBLINEAR: A library for large linear classification] has 2306.
* [http://www.coli.uni-saarland.de/groups/HU/HUwm4/Slides/malouf.pdf A comparison of algorithms for maximum entropy parameter estimation] has 550 citations.

I picked these papers because they all discuss optimization. They represent the algorithms that are actually in use, i.e., [[Sequential minimal optimization|SMO]], [[L-BFGS]], [[coordinate descent]]. Not GAs. [[User:Qwertyus|Q<small>VVERTYVS</small>]] <small>([[User talk:Qwertyus|hm?]])</small> 17:57, 21 October 2014 (UTC)

Furthermore, GAs do not appear ''at all'' in Bishop's ''Pattern Recognition and Machine Learning'', one of the foremost textbooks in the field. [[User:Qwertyus|Q<small>VVERTYVS</small>]] <small>([[User talk:Qwertyus|hm?]])</small> 18:09, 21 October 2014 (UTC)

:You're free to go ahead and write a section about gradient descent optimization if you want, but I wanted to write about [[genetic algorithm]]s. I know that [[Ben Goertzel]] for example are in favor of GAs, and just because they are not mentioned in Bishop's book doesn't mean they are not machine learning. They fit all descriptions of machine learning I know off. —[[User:Kri|Kri]] ([[User talk:Kri|talk]]) 20:00, 21 October 2014 (UTC)

::Fitting your definition doesn't mean GAs important enough to mention. GAs are not commonly used in the ML field that I know, they don't appear regularly in research, they don't fuel the major applications. I'd also never heard of this Goertzel guy before and I can't find any publications of his at NIPS, ICML or ECML or in JMLR (but maybe I didn't look hard enough). [[User:Qwertyus|Q<small>VVERTYVS</small>]] <small>([[User talk:Qwertyus|hm?]])</small> 07:54, 22 October 2014 (UTC)

:::My impression is that Goertzel plays a fairly big role within the [[artificial general intelligence]] community since he is chairman of the Artificial General Intelligence Society.

:::Since the section says "Approaches", I guessed it was an attempt to cover all approaches that has been taken to machine learning. If we should remove the unimportant approaches, I think the section should be called "Common approaches" or something like that to reflect the fact that not all approaches are listed.

:::And who decides whether an approach is important enough? Sure, GAs may not be the most commonly taken approach to machine learning, but it is one of the first approaches you will bump in to when you start to read about AI, and there are at least some serious research done about it, for example this [http://metacog.org/main.pdf PhD thesis] (includes [http://wiki.opencog.org/w/MOSES MOSES], which is an important component in the Goertzel-co-founded [[OpenCog]]) which seems to be fairly close to a GA. —[[User:Kri|Kri]] ([[User talk:Kri|talk]]) 10:47, 22 October 2014 (UTC)

::::Reliable sources do, that's why I cited Bishop. I'm sure Goertzel is a big player in AGI, but that's quite a different field, or at least a different academic/engineering community (see [http://spectrum.ieee.org/robotics/artificial-intelligence/machinelearning-maestro-michael-jordan-on-the-delusions-of-big-data-and-other-huge-engineering-efforts#qaTopicFive this interview] with [[Michael I. Jordan]] to see what I mean).
::::For completeness, we can split approaches into historical and currently common; then we can also cover older stuff like inductive logic programming (currently in approaches) and symbolic learning, both of which have pretty much completely fallen out of grace. ''[[Artificial Intelligence: A Modern Approach|AIMA]]'' provides an overview of this stuff. [[User:Qwertyus|Q<small>VVERTYVS</small>]] <small>([[User talk:Qwertyus|hm?]])</small> 11:23, 22 October 2014 (UTC)

:: I'm not sure if GA qualifies as a machine learning approach. It is a *meta optimization approach* that can be applied to machine learning models, like [[gradient descent]]. But one may easily argue that it isn't part of machine learning itself; but instead of pure mathematical optimization that just happens to be applicable in machine learning. Some of the most prominent GA demos - such as that Nasa Antenna - clearly are engineering optimization and not ML. --[[User:Chire|Chire]] ([[User talk:Chire|talk]]) 09:48, 23 October 2014 (UTC)

::: Okay, I see what you mean, genetic algorithms is just in the same family as e.g. [[backtracking]] or any other optimization method. Besides, it is not machine learning in itself, but machine learning is rather something you obtain when you have a whole system that is capable of learning and improving from experience. So GA would have to be accompanied by some data structure it can work on in order to actually achieve anything. But I would still say that GA (or perhaps [[genetic programming]] or [[evolutionary programming]]) is a way in which people have approached machine learning. Do you agree with that? —[[User:Kri|Kri]] ([[User talk:Kri|talk]]) 10:56, 23 October 2014 (UTC)

:::: It has been ''used'' in ML, but people have also used random generators for the same purpose - and we certainly shouldn't discuss monte carlo approaches in this article either. I do think it would fit an article "optimization techniques in machine learning", but it isn't all about the actual optimization method. Often the optimization used is quite interchangeable. For example, it should be possible to build SVMs using EA; it's only that other optimization strategies were more effective. --[[User:Chire|Chire]] ([[User talk:Chire|talk]]) 11:18, 23 October 2014 (UTC)

::::: Why should we certainly not discuss monte carlo approaches? I feel that you have some understanding of what the approaches-section should be about that I don't have. If it shouldn't include GAs or monte carlo, I think it perhaps has an improper name, since those two are obviously also approaches to machine learning. Perhaps we should consider calling it something else? —[[User:Kri|Kri]] ([[User talk:Kri|talk]]) 12:58, 23 October 2014 (UTC)

:::::: This discussion is getting a bit abstract. "it should be possible to build SVMs using EA" — yes, or by brute-force search, for that matter, but that's a purely academic exercise. SVMs have earned their place in ML as a go-to method because they have practical training algorithms like SMO.
:::::: My proposal to get rid of GAs or move them to a history section is not because they're not, in theory, applicable to machine learning problems, but because they don't represent the state of the art. MCMC, a similarly abstract "meta-algorithm", ''is'' commonly used (see [http://cis.temple.edu/~latecki/Courses/RobotFall07/PapersFall07/andrieu03introduction.pdf Andrieu ''et al.''] — >1000 citations, or Bishop, or just browse GScholar). I think that would deserve mention, and given the prevalence of optimization in ML, I don't see why we should not discuss it in an encyclopedic overview. [[User:Qwertyus|Q<small>VVERTYVS</small>]] <small>([[User talk:Qwertyus|hm?]])</small> 20:00, 23 October 2014 (UTC)
:::::::I believe evolutionary approaches have been quite successful when it comes to [[symbolic regression]].  e.g. 
:::::::* Schmidt M., Lipson, H. (2009) “Distilling Free-Form Natural Laws from Experimental Data,” Science, Vol. 324, no. 5923, pp. 81–85.
:::::::* Bongard J., Lipson H. (2007), “Automated reverse engineering of nonlinear dynamical systems", Proceedings of the National Academy of Science, vol. 104,  no. 24, pp. 9943–9948
:::::::These are high-impact factor journals, but I don't know if that is sufficient to warrant coverage. [[User:Pgr94|pgr94]] ([[User talk:Pgr94|talk]]) 20:19, 23 October 2014 (UTC)

::::::::They're not machine learning journals. Given that modern ML is largely defined by its methods rather than its goals, I say this is not machine learning proper. (Following the goal-directed definition given by Mitchell it would be, but that's so broad that it's not usable as a guideline on WP.) [[User:Qwertyus|Q<small>VVERTYVS</small>]] <small>([[User talk:Qwertyus|hm?]])</small> 11:03, 26 October 2014 (UTC)

:::::: The point is that GA should go into appropriate sub-article, not top-level. E.g. in [[Mathematical optimization]] (aka: [[Optimization algorithm]]). Because that is what it is: an optimization algorithm, right? --[[User:Chire|Chire]] ([[User talk:Chire|talk]]) 08:58, 24 October 2014 (UTC)

:::::::You're right; it is an optimization method, but it kind of feels like you're missing my point. Did you read my last comment? Just because it is an optimization method doesn't mean that it isn't also an approach to machine learning. —[[User:Kri|Kri]] ([[User talk:Kri|talk]]) 10:07, 24 October 2014 (UTC)


I would like to point out that mathematical optimization and machine learning are two completely different things. Genetic Algorithms and gradient descend are optimization algorithms (global and local respectively), which can be applied in contexts that have no connection to machine learning whatsoever (I can cite countless examples). When we talk about machine learning we talk about "training algorithms", not optimization algorithms. Many training algorithms are derived from and can be expressed as mathematical optimization problems (most typical examples being the Perceptron and SVM) and they apply some soft of optimization algorithm (gradient descend, GA, simulated annealing, etc) to solve those problems. You can solve the Linear Perceptron using the default Delta rule (which derives from gradient descend optimization) or you can solve it in a completely different manner using a Genetic Algorithm. The fact that machine learning uses optimization doesn't mean that an optimization algorithm is a machine learning (training) algorithm. [[User:Delafé|Delafé]] ([[User talk:Delafé|talk]]) 08:46, 11 February 2015 (UTC)


Machine learning was used in 2010 for breakthrough SSL by NSA.  <small class="autosigned">—&nbsp;Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/92.153.180.35|92.153.180.35]] ([[User talk:92.153.180.35|talk]]) 21:58, 9 March 2015 (UTC)</small><!-- Template:Unsigned IP --> <!--Autosigned by SineBot-->

==Need citation/Clarification==
Can somebody please provide the reference or context for arriving with following statement? 
"When employed in industrial contexts, machine learning methods may be referred to as predictive analytics or predictive modelling."
I am of opinion that this statement implying that in Industry "Predictive analytics" or "Predictive modelling" is considered as machine learning methods and so Machine learning is basis for predictive analytics. That doesn't seem to be true and I believe Predictive analytics form the base for machine learning and for it's applications. And how can we club "Predictive Anlytics" and "Modelling" together as these methods are applied in different stages of Data processing/Utilization and are vary different from one another.   

Thanks [[User:nvattikuti|Naren]] ([[User talk:nvattikuti|talk]]) 12:05, 12 June 2015 (UTC)

== Stanford lecture on Machine learning ==

Found this http://openclassroom.stanford.edu/MainFolder/VideoPage.php?course=MachineLearning&video=01.2-Introduction-WhatIsMachineLearning&speed=100   
I think it's useful for the article--[[User:Arado|Arado]] ([[User talk:Arado|talk]]) 14:50, 2 July 2015 (UTC)

== removing mention of finance-related fields ==

hi guys,

given that the great Sir [[Andrew Wiles| Wiles]] has rejected the application of mathematics to finance, and machine learning itself is a manifestation of sophisticated mathematics, can we start the discussion about removing mentions of fields such as "[[computational finance]]" and/or "[[mathematical finance]]".

both fields, to me, have always felt dishonest and uncomfortable given their lack of rigorousness http://mathbabe.org/2013/10/06/sir-andrew-wiles-smacks-down-unethical-use-of-mathematics-for-profit/ it is long overdue for those who love to learn, to take a stand against the abuse of our beloved maths.
[[Special:Contributions/174.3.155.181|174.3.155.181]] ([[User talk:174.3.155.181|talk]]) 19:46, 2 April 2016 (UTC)

: I have reverted your edit to remove the mention that machine learning has been applied in finance.  Wikipedia should be from a [[Wikipedia:Neutral point of view|neutral point of view]].  Sir Andrew Wiles in the article you have shared does not claim that the mathematics is not rigorous, he is concerned with the ethical implications of the application.  To have concerns about the ethical use of mathematics is important, but does not warrant the removal of mentions of fields that exist from wikipedia as we should be neutral as far as possible. [[User:Zfeinst|Zfeinst]] ([[User talk:Zfeinst|talk]]) 13:25, 3 April 2016 (UTC)

== trimming or removing commercial software section ==

any thoughts by the *community* about hte relevance of some of the commercial software entries? i am thinking this list can be long if we start adding arbitrary software. i was wondering if people would be open to trimming the list or removing it all together.
my thinking is that any prospective students should understand that this field is intense on mathematics, and while there is commercial appeal, much of the real work is done in the trenches.

things like google API and stuff can stay, obviously, but with the recent addition of a useless piece of software, i thought it'd be fruitful to have this discussion to prevent the list from growing.

there must be a healthy compromise that can be reached.  <small class="autosigned">—&nbsp;Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/174.3.155.181|174.3.155.181]] ([[User talk:174.3.155.181|talk]]) 18:25, 19 April 2016 (UTC)</small><!-- Template:Unsigned IP --> <!--Autosigned by SineBot-->

== Numerical Optimization in Statistics might be a big mistake ==

This is because an optimizer constructed with sample data is a random variable, and the extreme value of the optimizer (minimum or maximun) cannot be more significant than other values of the optimizer. We should take the expectation of the optimizer to do statistical decision, e.g. model selection. [[User:Yuanfangdelang|Yuanfangdelang]] ([[User talk:Yuanfangdelang|talk]]) 19:59, 30 August 2016 (UTC)

: Wikipedia is not a statistics journal. To discuss what statisticians should do, or should not do, is outside the scope of Wikipedia. Publish your opinion in relevant statistics journals instead and "fix" it there ''first''. Wikipedia is an encyclopedia, which summarizes and references ''important'' prior work only and does not do [[WP:OR|original research]]. We literally do not care of what "might be a big mistake" (as long as it is a mistake common e.g. in literature): Wikipedia has an article on [[Flat Earth]] despite this being a "mistake" because it used to be a dominant concept.  [[User:HelpUsStopSpam|HelpUsStopSpam]] ([[User talk:HelpUsStopSpam|talk]]) 09:48, 31 August 2016 (UTC)

==Self-learning chip==
There seem to be few chips on the market that are self-learning.
There's at least one being manufactured today, see [https://gopressmobility.be/2017/05/09/belgian-imec-creates-first-self-learning-chip/ here]
[[User:KVDP|KVDP]] ([[User talk:KVDP|talk]]) 13:21, 9 May 2017 (UTC)

== Definition by Samuel ==
The definition by Arthur Samuel,(1959) seems to be non-existent. Some papers/books cite his key-paper on ML in Checkers-games (see: http://aitopics.org/sites/default/files/classic/Feigenbaum_Feldman/Computers_And_Thought-Part_1_Checkers.pdf) but that doesn't contain a definition whatsoever (better yet, it states "While this is not the place to dwell on the importance of machine-learning procedures, or to discourse on the philosophical aspects" p.71). So I wonder whether we should keep that definition in the wiki-page... Otherwise I'm happy to receive the source+page where that definition is stated :)

Agree with above - this is a clear problem, as the WP leading quote can be found in many, many places around the Internet (as of 2017) with no actual citation. I've marked that reference as "disputed", since it doesn't cite any actual paper.  <!-- Template:Unsigned IP --><small class="autosigned">—&nbsp;Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[Special:Contributions/54.240.196.185|54.240.196.185]] ([[User talk:54.240.196.185#top|talk]]) 16:17, 14 August 2017 (UTC)</small> <!--Autosigned by SineBot-->

The second source added by [[User:HelpUsStopSpam]] is behind a paywall and so isn't clear on the content. Can you excerpt the exact phrase and context used in that paper? [[Special:Contributions/54.240.196.171|54.240.196.171]] ([[User talk:54.240.196.171|talk]]) 18:53, 17 August 2017 (UTC)

:Yes, this is a problem that should be solved. Why hasn't it been? The first sentence absolutely does not need to contain the definition from the first time the term occurred. The first sentence shall confer to the reader an understanding what it is all about. Naturally, the concept of ML has changed and deepened enormously since 1959. I suggest a paraphrase of this: ''the difficulties face by systems relying on hard-coded knowledge suggest that AI systems need the ability to acquire their own knowledge, by extracting patterns from raw data. Thi capability is known as machine learning.'' Goodfellow, Bengio, Courville; Deep Learning; MIT Press; 2016; page 2. --[[User:Ettrig|Ettrig]] ([[User talk:Ettrig|talk]]) 10:43, 13 November 2017 (UTC)

:: The "definition" paraphrased from Samuel seems to be the the most common one. The second source (Koza et al. 1996) says "Paraphrasing Arthur Samuel": "How can '''computers learn''' to solve problems '''without being explicitly programmed'''?". So this ''is'' in the source, and so is the paraphrased-from attribution to Arthur Samuel. A) Arthur Samuel is frequently cited/paraphrased throughout literature; this ("without being explicitly programmed") is a widely accepted definition. B) it is an early source. Samuel said something like this in 1959. Much of the other sources we've seen here just re-iterate what they read in other works that repeated what they read in other works and so on. Goodfellow and Bengio is certainly not a bad source, but he did not ''coin'' that term; they are also very much focused on the subset of machine learning that is neural networks. I'd rather stick with [[Arthur Samuel]]. [[User:Chire|Chire]] ([[User talk:Chire|talk]]) 12:24, 13 November 2017 (UTC)
:: So the main question to me is, if Koza et al. 1996 were the first to use this "paraphrase" of Samuel, and everybody else copied it from them, or if they again read this somewhere else. (There is also a 1995 paper from Koza). And yes, it says "How can computer learn", not "machine learning is defined as", so what? Arthur Samuel is commonly credited for pioneering this field. [[User:Chire|Chire]] ([[User talk:Chire|talk]]) 13:05, 13 November 2017 (UTC)

== External links modified ==

Hello fellow Wikipedians,

I have just modified one external link on [[Machine learning]]. Please take a moment to review [[special:diff/819820658|my edit]]. If you have any questions, or need the bot to ignore the links, or the page altogether, please visit [[User:Cyberpower678/FaQs#InternetArchiveBot|this simple FaQ]] for additional information. I made the following changes:
*Added archive https://web.archive.org/web/20091110212529/http://www-stat.stanford.edu/~tibs/ElemStatLearn/ to http://www-stat.stanford.edu/~tibs/ElemStatLearn/

When you have finished reviewing my changes, you may follow the instructions on the template below to fix any issues with the URLs.

{{sourcecheck|checked=false|needhelp=}}

Cheers.—[[User:InternetArchiveBot|'''<span style="color:darkgrey;font-family:monospace">InternetArchiveBot</span>''']] <span style="color:green;font-family:Rockwell">([[User talk:InternetArchiveBot|Report bug]])</span> 12:51, 11 January 2018 (UTC)
