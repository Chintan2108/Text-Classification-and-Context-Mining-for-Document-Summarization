,A,B,label
0,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
1,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",0
2,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",0
3,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",0
4,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",0
5,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
6,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",0
7,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
8,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",The choice of P in the robust objective (1) affectsboth the richness of the uncertainty set we wish to consider as well as the tractability of the result-ing optimization problem,0
9,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
10,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,0
11,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",0
12,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","For example,Ben-Tal et al",0
13,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",0
14,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
15,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
16,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
17,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",2.2) gives the result.,0
18,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",The major beneﬁt of our approach is its simplicityand wide applicability across many models and machine-learning scenarios.There remain many avenues for future investigation,0
19,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Veness, M",0
20,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
21,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",Experimental results on synthetic data,0
22,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
23,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
24,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
25,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
26,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
27,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
28,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
29,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
30,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Jha, M",0
31,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
32,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",Theorem 7,0
33,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
34,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
35,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features",0
36,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
37,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","In classiﬁcation settings, we have Y ∈{1, ",0
38,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
39,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
40,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,0
41,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
42,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
43,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
44,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",B,0
45,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
46,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
47,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",0
48,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
49,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
50,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",0
51,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
52,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",(a) and (b) show test misclassiﬁcation error vs,0
53,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
54,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
55,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
56,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
57,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
58,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
59,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
60,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
61,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
62,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)",0
63,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Namely, we draw the nominal",0
64,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
65,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
66,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
67,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
68,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
69,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",We use reward r(β) := e−|β| for the angle β of the pole from the vertical,0
70,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
71,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
72,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
73,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
74,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞",0
75,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
76,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
77,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
78,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",0
79,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
80,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Then using thepreceding display, we have",0
81,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,0
82,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
83,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
84,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
85,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))",0
86,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,0
87,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
88,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
89,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",0
90,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
91,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
92,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
93,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
94,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
95,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
96,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",0
97,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
98,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)",0
99,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",1
100,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
101,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization","For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",0
102,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,Distillation as a defense to adversarialperturbations against deep neural networks,0
103,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
104,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
105,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization","AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",0
106,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
107,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
108,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
109,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",is closed-valued and measurable,0
110,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
111,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",0
112,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
113,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",the adversarial perturbation level adv,0
114,"By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
115,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
116,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
117,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
118,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss","Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)",0
119,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
120,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.","Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.",0
121,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
122,"By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data","57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞",0
123,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
124,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
125,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
126,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
127,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",0
128,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
129,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,0
130,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
131,"By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",0
132,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
133,"By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",0
134,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization","IEEE, 2016c.",0
135,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
136,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization","9–10)), we have",0
137,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
138,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.",The action space is binary: push the cartleft or right with a ﬁxed force,0
139,"By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
140,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss","When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",0
141,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
142,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
143,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,0
144,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,0
145,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations","We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",0
146,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
147,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
148,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
149,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
150,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",0
151,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
152,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",0
153,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",0
154,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
155,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
156,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,0
157,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
158,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.",Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,0
159,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
160,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,0
161,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.",with the convention that 0 · ∞ = 0,0
162,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
163,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
164,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,0
165,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
166,"By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
167,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations","Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞",0
168,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
169,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
170,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.",K,0
171,"By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data","Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",0
172,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
173,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.","Then, the bounds (11) and (12) hold with",0
174,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
175,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss","However, WRM withReLU’s still suffers from sensitivities (e.g",0
176,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
177,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
178,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.","Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",0
179,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
180,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",(f) Test error vs,0
181,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
182,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
183,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
184,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
185,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
186,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",372–387,0
187,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
188,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
189,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.","By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",0
190,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,0
191,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
192,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
193,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations","Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",0
194,"For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,0
195,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.",JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,0
196,"By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data","To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",0
197,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",0
198,"For imperceptible perturbations, our method matches or outperformsheuristic approaches.",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,1
199,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations","In this regime(small γ, large ), performance between WRM and other methods diverge",0
200,"Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
201,We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
202,"Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
203,"In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
204,"Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
205,"In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
206,We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
207,"Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks","Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques",0
208,"Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)","We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms",0
209,"Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
210,"In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
211,"As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)","Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",0
212,"In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
213,"In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance","Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.",0
214,"Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks","Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",0
215,"In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)","In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks",0
216,"These works provide an initial foundation for adversarial training, but itis challenging to rigorously identify the classes of attacks against which they can defend (or if theyexist)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",0
217,We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
218,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
219,"These works provide an initial foundation for adversarial training, but itis challenging to rigorously identify the classes of attacks against which they can defend (or if theyexist)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
220,"As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",0
221,"Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",0
222,"As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
223,"Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
224,"In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
225,"As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
226,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),0
227,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",0
228,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
229,"In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
230,We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
231,"In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
232,"In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
233,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
234,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",Explicit distributional robustness of the form (5) is intractable except in limited cases,0
235,"As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)","On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
236,"As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
237,"Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
238,"Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
239,"These works provide an initial foundation for adversarial training, but itis challenging to rigorously identify the classes of attacks against which they can defend (or if theyexist)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
240,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function","Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",0
241,"In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
242,"Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
243,We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
244,"As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
245,We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
246,"In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance",We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,0
247,"Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
248,"Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
249,"Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)","Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",0
250,"Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)","Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)",0
251,"Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks",Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,0
252,We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,"This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al",0
253,"These works provide an initial foundation for adversarial training, but itis challenging to rigorously identify the classes of attacks against which they can defend (or if theyexist)",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,0
254,We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",0
255,"These works provide an initial foundation for adversarial training, but itis challenging to rigorously identify the classes of attacks against which they can defend (or if theyexist)","Of course, the certiﬁcate (15) stillholds regardless.More broadly, this work focuses on small-perturbation attacks, and our theoretical guarantees showthat it is possible to efﬁciently build models that provably guard against such attacks",0
256,"In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance","Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness",0
257,"As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
258,"Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
259,We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",0
260,"Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks","In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",0
261,"In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",0
262,"Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
263,"In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
264,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
265,"Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks","Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",0
266,"As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)","χ2- or Kullback-Leibler divergences) which are effectivewhen the support of the distribution P0 is ﬁxed, a Wasserstein ball around P0 includes distributionsQ with different support and allows (in a sense) robustness to unseen data.Many authors have studied tractable classes of uncertainty sets P and losses (cid:96)",0
267,"In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
268,"Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
269,"Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
270,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
271,"These works provide an initial foundation for adversarial training, but itis challenging to rigorously identify the classes of attacks against which they can defend (or if theyexist)","These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:c((x, y), (x0, y0)) = (cid:107)x − x0(cid:107)2∞ + ∞ · 1{y (cid:54)= y0} .",0
272,We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",0
273,We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",0
274,"Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
275,We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
276,"In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,0
277,"In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
278,"As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
279,"In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)",We use reward r(β) := e−|β| for the angle β of the pole from the vertical,0
280,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function","Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt",0
281,"Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",0
282,"Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,0
283,"In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
284,"Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
285,We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",0
286,We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
287,"Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)",2.2) gives the result.,0
288,"Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)","Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ",0
289,"In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
290,"Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks",Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,0
291,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
292,"These works provide an initial foundation for adversarial training, but itis challenging to rigorously identify the classes of attacks against which they can defend (or if theyexist)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
293,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
294,"In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
295,"Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
296,"In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance","In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.",0
297,"Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
298,"In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",1
299,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function","Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",0
300,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
301,"For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",All models are trained in the ∞-norm,0
302,"Weprovide an adversarial training procedure that, for smooth (cid:96), enjoys convergence guarantees simi-lar to non-robust approaches while certifying performance even for the worst-case population losssupP∈P EP [(cid:96)(θ; Z)]","Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)",0
303,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach","However, WRM withReLU’s still suffers from sensitivities (e.g",0
304,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
305,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)","We compare standard WRM with ∞-norm PGM, FGM,IFGM",0
306,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
307,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,0
308,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
309,"For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
310,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
311,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",372–387,0
312,"Weprovide an adversarial training procedure that, for smooth (cid:96), enjoys convergence guarantees simi-lar to non-robust approaches while certifying performance even for the worst-case population losssupP∈P EP [(cid:96)(θ; Z)]",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
313,"Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
314,"Weprovide an adversarial training procedure that, for smooth (cid:96), enjoys convergence guarantees simi-lar to non-robust approaches while certifying performance even for the worst-case population losssupP∈P EP [(cid:96)(θ; Z)]",We use reward r(β) := e−|β| for the angle β of the pole from the vertical,0
315,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
316,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)","Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",0
317,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,0
318,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
319,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
320,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",0
321,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
322,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
323,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
324,"For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",0
325,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,0
326,"For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ","For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering",0
327,"Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
328,"Weprovide an adversarial training procedure that, for smooth (cid:96), enjoys convergence guarantees simi-lar to non-robust approaches while certifying performance even for the worst-case population losssupP∈P EP [(cid:96)(θ; Z)]","In classiﬁcation settings, we have Y ∈{1, ",0
329,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
330,"Weprovide an adversarial training procedure that, for smooth (cid:96), enjoys convergence guarantees simi-lar to non-robust approaches while certifying performance even for the worst-case population losssupP∈P EP [(cid:96)(θ; Z)]",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
331,"For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
332,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
333,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach","The smoothness of (cid:96) is essential so that a penalized version(cid:96)(θ, z) − γc(z, z0) is concave in z (which can be approximately veriﬁed by computing Hessians∇2zz(cid:96)(θ, z) for each training datapoint), allowing computation and our coming certiﬁcates of opti-mality",0
334,"For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
335,"Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
336,"For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
337,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
338,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
339,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)","Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",0
340,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
341,"Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation","Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",0
342,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
343,"Weprovide an adversarial training procedure that, for smooth (cid:96), enjoys convergence guarantees simi-lar to non-robust approaches while certifying performance even for the worst-case population losssupP∈P EP [(cid:96)(θ; Z)]",All of our resultssuitably generalize to this setting with minor modiﬁcations to the robust surrogate (2b) and theabove assumptions (see Section D),0
344,"Weprovide an adversarial training procedure that, for smooth (cid:96), enjoys convergence guarantees simi-lar to non-robust approaches while certifying performance even for the worst-case population losssupP∈P EP [(cid:96)(θ; Z)]","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",0
345,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,0
346,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
347,"Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
348,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
349,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
350,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)","Forour approach we use γ = 2, and to make fair comparisons with FGM we use",0
351,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
352,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",the adversarial perturbation level adv,0
353,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
354,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
355,"Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
356,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",Roy,0
357,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
358,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
359,"Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
360,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",See Section C.2 for the proof,0
361,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)","In this regime(small γ, large ), performance between WRM and other methods diverge",0
362,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
363,"For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",0
364,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
365,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)","Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
366,"For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
367,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
368,"For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
369,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
370,"Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation","These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
371,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
372,"Weprovide an adversarial training procedure that, for smooth (cid:96), enjoys convergence guarantees simi-lar to non-robust approaches while certifying performance even for the worst-case population losssupP∈P EP [(cid:96)(θ; Z)]",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
373,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
374,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,V,0
375,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)","As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)",0
376,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",", Tadv, where Π denotes projection onto B,p(xi) := {x : (cid:107)x − xi(cid:107)p ≤ }.The adversarial training literature (e.g",0
377,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
378,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,(f) Test error vs,0
379,"Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation","WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ",0
380,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
381,"Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
382,"For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
383,"Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,0
384,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
385,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
386,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",N,0
387,"For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",0
388,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)","Due to the nonstationary, policy-dependent radius for the Wassersteinball, an analogous  for the fast-gradient method (or other variants) is not well-deﬁned",0
389,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
390,"For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",PGM attacks on the MNIST dataset,0
391,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
392,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
393,"Weprovide an adversarial training procedure that, for smooth (cid:96), enjoys convergence guarantees simi-lar to non-robust approaches while certifying performance even for the worst-case population losssupP∈P EP [(cid:96)(θ; Z)]","In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets",0
394,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
395,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
396,"For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ","All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training",0
397,"Weprovide an adversarial training procedure that, for smooth (cid:96), enjoys convergence guarantees simi-lar to non-robust approaches while certifying performance even for the worst-case population losssupP∈P EP [(cid:96)(θ; Z)]",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,1
398,"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)","Papernot, P",0
399,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,adv for (cid:107)·(cid:107)∞-IFGM attackFigure 6,0
400,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
401,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
402,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
403,We typically solve the penalty problem (2) with P0 replaced,(16)) in θ,0
404,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
405,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",We illustrate test misclassiﬁcation error vs,0
406,We typically solve the penalty problem (2) with P0 replaced,"Thus, weonly compare with an agent trained on the nominal MDP",0
407,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",0
408,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
409,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
410,We typically solve the penalty problem (2) with P0 replaced,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",0
411,We typically solve the penalty problem (2) with P0 replaced,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
412,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
413,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
414,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
415,We typically solve the penalty problem (2) with P0 replaced,K,0
416,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
417,We typically solve the penalty problem (2) with P0 replaced,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",0
418,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
419,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
420,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
421,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
422,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",0
423,We typically solve the penalty problem (2) with P0 replaced,"Fawzi, and P",0
424,We typically solve the penalty problem (2) with P0 replaced,For large adversaries(i.e,0
425,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
426,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",0
427,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
428,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",We omit the certiﬁcate’s error,0
429,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
430,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
431,We typically solve the penalty problem (2) with P0 replaced,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",0
432,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
433,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives",0
434,We typically solve the penalty problem (2) with P0 replaced,"The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
435,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
436,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
437,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",0
438,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
439,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
440,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","Wu, S",0
441,We typically solve the penalty problem (2) with P0 replaced,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",0
442,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
443,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
444,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","Using only assumptions about the smoothness of the loss function(cid:96), we prove that our method enjoys strong statistical guarantees and fast optimization rates for alarge class of problems",0
445,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",The function cx : X × X → R+ is continuous,0
446,We typically solve the penalty problem (2) with P0 replaced,"McDaniel, S",0
447,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
448,We typically solve the penalty problem (2) with P0 replaced,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
449,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
450,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z",0
451,We typically solve the penalty problem (2) with P0 replaced,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
452,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
453,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
454,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
455,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
456,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,0
457,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",Frossard,0
458,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
459,We typically solve the penalty problem (2) with P0 replaced,"Finally, we consider a reinforcement learning problemin Section 4.3, where the Markov decision process used for training differs from that for testing.WRM enjoys the theoretical guarantees of Sections 2 and 3 for large γ, but for small γ (large adver-sarial budgets), WRM becomes a heuristic like other methods",0
460,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",It isthus important to distinguish the methods’ abilities to combat attacks,0
461,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",A,0
462,We typically solve the penalty problem (2) with P0 replaced,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",0
463,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",0
464,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
465,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
466,We typically solve the penalty problem (2) with P0 replaced,"Ried-miller, A",0
467,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
468,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
469,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
470,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","Now, let x(z) beany measurable function that is -close to attaining the supremum above",0
471,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","These results suggest advantages of networks with smooth activations
rather than ReLU’s",0
472,We typically solve the penalty problem (2) with P0 replaced,by solving (7),0
473,We typically solve the penalty problem (2) with P0 replaced,"Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",0
474,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
475,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",0
476,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,0
477,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
478,We typically solve the penalty problem (2) with P0 replaced,"(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D",0
479,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
480,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
481,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
482,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",Roy,0
483,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
484,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
485,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
486,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
487,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
488,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","In the former case, since the update (22)simply modiﬁes the state-action transitions (independent of Q), standard results on convergence fortabular Q-learning (e.g",0
489,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",Explicit distributional robustness of the form (5) is intractable except in limited cases,0
490,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0",0
491,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
492,We typically solve the penalty problem (2) with P0 replaced,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",0
493,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
494,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
495,We typically solve the penalty problem (2) with P0 replaced,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
496,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",1
497,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,0
498,We typically solve the penalty problem (2) with P0 replaced,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM",0
499,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ","In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8",0
500,"These results suggest advantages of networks with smooth activations
rather than ReLU’s","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
501,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)",Distillation as a defense to adversarialperturbations against deep neural networks,0
502,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]",Robust MDP’s consider an ambiguity set Psa for state-action transitions,0
503,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)","As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes",0
504,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
505,"Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z","In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",0
506,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
507,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
508,"Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective",with the convention that 0 · ∞ = 0,0
509,"Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
510,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)","14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping",0
511,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,0
512,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
513,"We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
514,"We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.","Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",0
515,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
516,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]",(a) and (b) show test misclassiﬁcation error vs,0
517,"We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",0
518,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
519,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
520,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
521,"Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective","Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes",0
522,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
523,"Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
524,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
525,"Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
526,"Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z","The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
527,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
528,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
529,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
530,"Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective","Bellemare, A",0
531,"Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
532,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
533,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
534,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)","For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",0
535,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
536,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
537,"We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
538,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
539,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
540,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]",Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,0
541,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
542,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
543,"Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
544,"These results suggest advantages of networks with smooth activations
rather than ReLU’s","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
545,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
546,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
547,"These results suggest advantages of networks with smooth activations
rather than ReLU’s","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
548,"Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective","The smoothness of (cid:96) is essential so that a penalized version(cid:96)(θ, z) − γc(z, z0) is concave in z (which can be approximately veriﬁed by computing Hessians∇2zz(cid:96)(θ, z) for each training datapoint), allowing computation and our coming certiﬁcates of opti-mality",0
549,"We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.","(2017), Dziugaite & Roy (2017), andNeyshabur et al",0
550,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
551,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
552,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)","χ2- or Kullback-Leibler divergences) which are effectivewhen the support of the distribution P0 is ﬁxed, a Wasserstein ball around P0 includes distributionsQ with different support and allows (in a sense) robustness to unseen data.Many authors have studied tractable classes of uncertainty sets P and losses (cid:96)",0
553,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
554,"Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
555,"These results suggest advantages of networks with smooth activations
rather than ReLU’s","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
556,"Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
557,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
558,"We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.","For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",0
559,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)","The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds",0
560,"We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
561,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",We observe that similar trends as inSection A.5.1 hold again.,0
562,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)","Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",0
563,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
564,"These results suggest advantages of networks with smooth activations
rather than ReLU’s","The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)",0
565,"These results suggest advantages of networks with smooth activations
rather than ReLU’s","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
566,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
567,"We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
568,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
569,"Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
570,"These results suggest advantages of networks with smooth activations
rather than ReLU’s","Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",0
571,"These results suggest advantages of networks with smooth activations
rather than ReLU’s","Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",0
572,"We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
573,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
574,"We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",0
575,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
576,"Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
577,"Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
578,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","for t = 1, ",0
579,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
580,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,0
581,"Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
582,"These results suggest advantages of networks with smooth activations
rather than ReLU’s","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
583,"We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
584,"We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.","Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",0
585,"Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
586,"These results suggest advantages of networks with smooth activations
rather than ReLU’s","Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",0
587,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)","Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",0
588,"We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
589,"Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective","Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =",0
590,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
591,"Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
592,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
593,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
594,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
595,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]","Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",0
596,"Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
597,"Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z","Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
598,"We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
599,"These results suggest advantages of networks with smooth activations
rather than ReLU’s","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",1
600,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","With that said, the strong dualityresult, Proposition 1, still applies to any distribution",0
601,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)",0
602,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
603,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)",0
604,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
605,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
606,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
607,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","However, WRM withReLU’s still suffers from sensitivities (e.g",0
608,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",Roy,0
609,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
610,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","One natural appli-cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-pointproblem (1) and give specialized bounds for domain adaptation problems",0
611,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.",0
612,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",0
613,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
614,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
615,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
616,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1",0
617,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
618,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
619,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
620,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
621,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
622,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",0
623,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",0
624,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
625,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",0
626,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
627,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","Consider feedforward neural networks with ReLU’s and let U := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U (cid:96)(θ; z + u) is NPO",0
628,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
629,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
630,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
631,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",This procedureprovides robustness to uncertainties in state-action transitions,0
632,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Szepesv´ari & Littman (1999)) apply under these adversarial dynamics.We test our adversarial training procedure in the cart-pole environment, where the goal is to balancea pole on a cart by moving the cart left or right",0
633,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z",0
634,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
635,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",large desired robustness values) our approach becomes a heuristic just like the other approaches.,0
636,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5",0
637,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
638,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
639,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,0
640,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",Explicit distributional robustness of the form (5) is intractable except in limited cases,0
641,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
642,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
643,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)",0
644,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
645,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5",0
646,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
647,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
648,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
649,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
650,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,0
651,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
652,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
653,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5",0
654,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
655,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
656,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
657,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",0
658,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
659,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
660,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
661,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
662,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
663,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
664,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
665,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
666,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
667,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
668,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",All models are trained in the ∞-norm,0
669,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
670,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","In classiﬁcation settings, we have Y ∈{1, ",0
671,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",Swami,0
672,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
673,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,0
674,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞",0
675,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
676,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
677,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",582–597,0
678,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",0
679,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",0
680,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",0
681,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
682,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
683,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,0
684,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,0
685,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",Other models do not exhibit this behavior with the same consistency(if at all),0
686,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,0
687,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",The,0
688,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",Experimental results on synthetic data,0
689,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
690,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)",0
691,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
692,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
693,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
694,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Fidjeland, G",0
695,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
696,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",1
697,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",the adversarial perturbation level adv,0
698,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",372–387,0
699,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",0
700,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
701,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)","Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",0
702,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd","Then using thepreceding display, we have",0
703,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd","Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
704,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.",Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,0
705,Madry et al,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
706,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.",Attacks on the MNIST dataset,0
707,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)","We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",0
708,Madry et al,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",0
709,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.",0
710,Madry et al,We illustrate testmisclassiﬁcation error vs,0
711,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","IEEE, 2016c.",0
712,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time",0
713,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd",AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
714,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.","Recalling Rockafellar & Wets (1998, Def",0
715,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,0
716,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
717,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.",K,0
718,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
719,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",0
720,Madry et al,"In this regime(small γ, large ), performance between WRM and other methods diverge",0
721,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
722,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)","These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
723,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",0
724,Madry et al,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
725,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
726,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
727,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.",We typically solve the penalty problem (2) with P0 replaced,0
728,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
729,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
730,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",0
731,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)","In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets",0
732,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
733,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
734,Madry et al,"Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective",0
735,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.","For worst-case regions P formed by Wasserstein balls, Esfahani & Kuhn (2015),Shaﬁeezadeh-Abadeh et al",0
736,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
737,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
738,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
739,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
740,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
741,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",0
742,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
743,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
744,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
745,Madry et al,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
746,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
747,Madry et al,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
748,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)","Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B",0
749,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",0
750,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)",Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,0
751,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
752,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
753,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms",0
754,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)","We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms",0
755,Madry et al,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
756,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
757,Madry et al,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",0
758,Madry et al,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
759,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
760,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd",Our optimization result (Theorem 2) appliesonly for small values of robustness ρ and to a limited class of Wasserstein costs,0
761,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
762,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
763,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.","Fawzi, and P",0
764,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
765,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
766,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
767,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
768,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","9–10)), we have",0
769,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
770,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd","Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",0
771,Madry et al,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,0
772,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
773,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)","The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",0
774,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)","WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",0
775,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd","This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd",0
776,Madry et al,the adversarial perturbationlevel adv,0
777,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
778,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
779,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
780,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",0
781,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
782,Madry et al,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
783,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",0
784,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)","Due to the nonstationary, policy-dependent radius for the Wassersteinball, an analogous  for the fast-gradient method (or other variants) is not well-deﬁned",0
785,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty",0
786,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
787,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
788,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
789,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","In classiﬁcation settings, we have Y ∈{1, ",0
790,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
791,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
792,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
793,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)","Recalling Rockafellar & Wets (1998, Def",0
794,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",0
795,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
796,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",1
797,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)",V,0
798,Madry et al,"We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",0
799,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)","In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",0
800,"In this work, we treat a much larger class of losses and costs and provide directsolution methods for a Lagrangian relaxation of the saddle-point problem (1)","For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",0
801,In constrast to f-divergences (e.g,"Of course, the certiﬁcate (15) stillholds regardless.More broadly, this work focuses on small-perturbation attacks, and our theoretical guarantees showthat it is possible to efﬁciently build models that provably guard against such attacks",0
802,(2013) and Namkoong & Duchi (2017) use convex optimization approaches for f-divergence balls,All of our resultssuitably generalize to this setting with minor modiﬁcations to the robust surrogate (2b) and theabove assumptions (see Section D),0
803,"(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c","For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0",0
804,"Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
805,"χ2- or Kullback-Leibler divergences) which are effectivewhen the support of the distribution P0 is ﬁxed, a Wasserstein ball around P0 includes distributionsQ with different support and allows (in a sense) robustness to unseen data.Many authors have studied tractable classes of uncertainty sets P and losses (cid:96)",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
806,"For example,Ben-Tal et al","Forour approach we use γ = 2, and to make fair comparisons with FGM we use",0
807,"(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
808,"(2015), and Blanchet et al","The smoothness of (cid:96) is essential so that a penalized version(cid:96)(θ, z) − γc(z, z0) is concave in z (which can be approximately veriﬁed by computing Hessians∇2zz(cid:96)(θ, z) for each training datapoint), allowing computation and our coming certiﬁcates of opti-mality",0
809,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",0
810,(2013) and Namkoong & Duchi (2017) use convex optimization approaches for f-divergence balls,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
811,"(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
812,(2013) and Namkoong & Duchi (2017) use convex optimization approaches for f-divergence balls,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
813,(2013) and Namkoong & Duchi (2017) use convex optimization approaches for f-divergence balls,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",0
814,"For example,Ben-Tal et al","Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
815,"(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c","In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",0
816,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",(e) Test error vs,0
817,"χ2- or Kullback-Leibler divergences) which are effectivewhen the support of the distribution P0 is ﬁxed, a Wasserstein ball around P0 includes distributionsQ with different support and allows (in a sense) robustness to unseen data.Many authors have studied tractable classes of uncertainty sets P and losses (cid:96)","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
818,"For example,Ben-Tal et al","For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance",0
819,In constrast to f-divergences (e.g,In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,0
820,"In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
821,"χ2- or Kullback-Leibler divergences) which are effectivewhen the support of the distribution P0 is ﬁxed, a Wasserstein ball around P0 includes distributionsQ with different support and allows (in a sense) robustness to unseen data.Many authors have studied tractable classes of uncertainty sets P and losses (cid:96)","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
822,(2013) and Namkoong & Duchi (2017) use convex optimization approaches for f-divergence balls,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
823,"χ2- or Kullback-Leibler divergences) which are effectivewhen the support of the distribution P0 is ﬁxed, a Wasserstein ball around P0 includes distributionsQ with different support and allows (in a sense) robustness to unseen data.Many authors have studied tractable classes of uncertainty sets P and losses (cid:96)","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
824,"In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
825,"In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.","We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A",0
826,"(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c","These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
827,"(2015), and Blanchet et al",adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,0
828,"For worst-case regions P formed by Wasserstein balls, Esfahani & Kuhn (2015),Shaﬁeezadeh-Abadeh et al","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
829,"(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
830,In constrast to f-divergences (e.g,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
831,"Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)",(f) Test error vs,0
832,"In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.","Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",0
833,"For example,Ben-Tal et al","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
834,"One natural appli-cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-pointproblem (1) and give specialized bounds for domain adaptation problems","AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",0
835,"χ2- or Kullback-Leibler divergences) which are effectivewhen the support of the distribution P0 is ﬁxed, a Wasserstein ball around P0 includes distributionsQ with different support and allows (in a sense) robustness to unseen data.Many authors have studied tractable classes of uncertainty sets P and losses (cid:96)","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
836,(2013) and Namkoong & Duchi (2017) use convex optimization approaches for f-divergence balls,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
837,(2013) and Namkoong & Duchi (2017) use convex optimization approaches for f-divergence balls,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have",0
838,"In this work, we treat a much larger class of losses and costs and provide directsolution methods for a Lagrangian relaxation of the saddle-point problem (1)","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
839,"In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
840,"One natural appli-cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-pointproblem (1) and give specialized bounds for domain adaptation problems","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
841,The choice of P in the robust objective (1) affectsboth the richness of the uncertainty set we wish to consider as well as the tractability of the result-ing optimization problem,Attacks on the MNIST dataset,0
842,"(2015), and Blanchet et al","Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",0
843,"For worst-case regions P formed by Wasserstein balls, Esfahani & Kuhn (2015),Shaﬁeezadeh-Abadeh et al","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
844,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",It isthus important to distinguish the methods’ abilities to combat attacks,0
845,"Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)","Kavukcuoglu, D",0
846,"(2015), and Blanchet et al","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
847,"(2015), and Blanchet et al","Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",0
848,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
849,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
850,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
851,"(2015), and Blanchet et al","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
852,In constrast to f-divergences (e.g,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",0
853,"Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)",2.2) gives the result.,0
854,"(2015), and Blanchet et al","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
855,"In this work, we treat a much larger class of losses and costs and provide directsolution methods for a Lagrangian relaxation of the saddle-point problem (1)",Esfahani and D,0
856,"For worst-case regions P formed by Wasserstein balls, Esfahani & Kuhn (2015),Shaﬁeezadeh-Abadeh et al","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
857,"In this work, we treat a much larger class of losses and costs and provide directsolution methods for a Lagrangian relaxation of the saddle-point problem (1)","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",0
858,In constrast to f-divergences (e.g,"Nature, 518(7540):529–533, 2015.",0
859,"In this work, we treat a much larger class of losses and costs and provide directsolution methods for a Lagrangian relaxation of the saddle-point problem (1)","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
860,"One natural appli-cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-pointproblem (1) and give specialized bounds for domain adaptation problems","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
861,The choice of P in the robust objective (1) affectsboth the richness of the uncertainty set we wish to consider as well as the tractability of the result-ing optimization problem,"In the former case, since the update (22)simply modiﬁes the state-action transitions (independent of Q), standard results on convergence fortabular Q-learning (e.g",0
862,"In this work, we treat a much larger class of losses and costs and provide directsolution methods for a Lagrangian relaxation of the saddle-point problem (1)","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
863,In constrast to f-divergences (e.g,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
864,"χ2- or Kullback-Leibler divergences) which are effectivewhen the support of the distribution P0 is ﬁxed, a Wasserstein ball around P0 includes distributionsQ with different support and allows (in a sense) robustness to unseen data.Many authors have studied tractable classes of uncertainty sets P and losses (cid:96)","Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",0
865,"χ2- or Kullback-Leibler divergences) which are effectivewhen the support of the distribution P0 is ﬁxed, a Wasserstein ball around P0 includes distributionsQ with different support and allows (in a sense) robustness to unseen data.Many authors have studied tractable classes of uncertainty sets P and losses (cid:96)","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
866,"One natural appli-cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-pointproblem (1) and give specialized bounds for domain adaptation problems","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
867,The choice of P in the robust objective (1) affectsboth the richness of the uncertainty set we wish to consider as well as the tractability of the result-ing optimization problem,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
868,"For worst-case regions P formed by Wasserstein balls, Esfahani & Kuhn (2015),Shaﬁeezadeh-Abadeh et al","Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",0
869,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
870,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",the adversarial perturbation level adv,0
871,"One natural appli-cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-pointproblem (1) and give specialized bounds for domain adaptation problems","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
872,"(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c","In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8",0
873,"In this work, we treat a much larger class of losses and costs and provide directsolution methods for a Lagrangian relaxation of the saddle-point problem (1)","In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets",0
874,"(2015), and Blanchet et al",Deﬁne the scale parameter βt > 0 by,0
875,"For worst-case regions P formed by Wasserstein balls, Esfahani & Kuhn (2015),Shaﬁeezadeh-Abadeh et al","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
876,"In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
877,"In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",0
878,"Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
879,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning","Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",0
880,"For worst-case regions P formed by Wasserstein balls, Esfahani & Kuhn (2015),Shaﬁeezadeh-Abadeh et al","We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",0
881,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
882,"For example,Ben-Tal et al","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
883,"One natural appli-cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-pointproblem (1) and give specialized bounds for domain adaptation problems","One natural appli-cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-pointproblem (1) and give specialized bounds for domain adaptation problems",0
884,"Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)",Frossard,0
885,"One natural appli-cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-pointproblem (1) and give specialized bounds for domain adaptation problems","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
886,"For example,Ben-Tal et al",adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,0
887,(2013) and Namkoong & Duchi (2017) use convex optimization approaches for f-divergence balls,"Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))",0
888,"For worst-case regions P formed by Wasserstein balls, Esfahani & Kuhn (2015),Shaﬁeezadeh-Abadeh et al","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
889,In constrast to f-divergences (e.g,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
890,"χ2- or Kullback-Leibler divergences) which are effectivewhen the support of the distribution P0 is ﬁxed, a Wasserstein ball around P0 includes distributionsQ with different support and allows (in a sense) robustness to unseen data.Many authors have studied tractable classes of uncertainty sets P and losses (cid:96)","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",0
891,"For example,Ben-Tal et al","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
892,"(2015), and Blanchet et al","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
893,"One natural appli-cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-pointproblem (1) and give specialized bounds for domain adaptation problems",The distinctions in performance between various methodsare less apparent now,0
894,"χ2- or Kullback-Leibler divergences) which are effectivewhen the support of the distribution P0 is ﬁxed, a Wasserstein ball around P0 includes distributionsQ with different support and allows (in a sense) robustness to unseen data.Many authors have studied tractable classes of uncertainty sets P and losses (cid:96)","The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.",0
895,"In this work, we treat a much larger class of losses and costs and provide directsolution methods for a Lagrangian relaxation of the saddle-point problem (1)","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
896,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
897,"Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
898,"One natural appli-cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-pointproblem (1) and give specialized bounds for domain adaptation problems","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",1
899,"In this work, we treat a much larger class of losses and costs and provide directsolution methods for a Lagrangian relaxation of the saddle-point problem (1)",This procedureprovides robustness to uncertainties in state-action transitions,0
900,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)",Goodfellow et al,0
901,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",0
902,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
903,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,Figure 2,0
904,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
905,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",Madry et al,0
906,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","From Lemma 5, our previous results (Theorems 2 and 4) follow",0
907,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
908,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
909,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
910,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)",We have the following theorem.Theorem 5,0
911,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)",0
912,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
913,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
914,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
915,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","In this work, we treat a much larger class of losses and costs and provide directsolution methods for a Lagrangian relaxation of the saddle-point problem (1)",0
916,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"Throughout, we assume Θ ⊆ Rd.Lemma 1",0
917,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","IEEE, 2016c.",0
918,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
919,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
920,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",Let ∆F ≥ F (θ0) − inf θ F (θ),0
921,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
922,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering",0
923,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
924,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,0
925,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","Papernot, P",0
926,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","From Lemma 5, our previous results (Theorems 2 and 4) follow",0
927,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
928,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",0
929,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",0
930,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",Robust MDP’s consider an ambiguity set Psa for state-action transitions,0
931,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
932,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",Madry et al,0
933,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,with the convention that 0 · ∞ = 0,0
934,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",0
935,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",We illustrate testmisclassiﬁcation error vs,0
936,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,We illustrate test misclassiﬁcation error vs,0
937,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)",0
938,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM",0
939,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",0
940,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
941,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","Namely, we draw the nominal",0
942,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4",0
943,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
944,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
945,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞",0
946,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models",0
947,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
948,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
949,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)",Attacks on the MNIST dataset,0
950,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance",0
951,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives",0
952,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","In most scenarios, however,it is reasonable to defend against weaker adversaries that instead perturb inﬂuential features more.We consider this setting and train against (cid:107)·(cid:107)2-norm attacks",0
953,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"Left column: Euclidean-normattacks, right column: ∞-norm attacks",0
954,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,(16)) in θ,0
955,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
956,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
957,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
958,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
959,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)",adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,0
960,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",0
961,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
962,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
963,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
964,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",0
965,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
966,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
967,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"Finally, we consider a reinforcement learning problemin Section 4.3, where the Markov decision process used for training differs from that for testing.WRM enjoys the theoretical guarantees of Sections 2 and 3 for large γ, but for small γ (large adver-sarial budgets), WRM becomes a heuristic like other methods",0
968,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
969,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",0
970,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.",0
971,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
972,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
973,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data",0
974,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,0
975,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
976,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
977,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
978,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
979,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
980,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
981,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
982,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
983,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
984,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations",0
985,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
986,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
987,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
988,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",Goodfellow et al,0
989,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,0
990,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
991,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,0
992,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",0
993,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
994,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"For any distribution Q and any ρ > 0,(5)",0
995,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
996,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","McDaniel, S",0
997,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
998,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
999,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",1
1000,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1001,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",Deepfool: a simple and accurate method to fooldeep neural networks,0
1002,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space","Papernot, P",0
1003,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0","Jha, and A",0
1004,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1005,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",S.-M,0
1006,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1007,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",the adversarial perturbationlevel adv,0
1008,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,0
1009,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1010,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1011,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
1012,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1013,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions","Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",0
1014,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space","because f − c is upper semi-continuous, and the latter function is measurable",0
1015,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1016,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",0
1017,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions","2574–2582, 2016.",0
1018,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",", K}",0
1019,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1020,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1021,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1022,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1023,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1024,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0","Mnih, K",0
1025,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",We omit the certiﬁcate’s error,0
1026,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1027,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1028,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1029,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1030,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1031,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space","Wu, S",0
1032,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1033,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1034,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,0
1035,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1036,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and","Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",0
1037,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0","Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))",0
1038,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1039,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions","In most scenarios, however,it is reasonable to defend against weaker adversaries that instead perturb inﬂuential features more.We consider this setting and train against (cid:107)·(cid:107)2-norm attacks",0
1040,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",0
1041,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,"9–10)), we have",0
1042,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",Theorem 7,0
1043,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1044,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1045,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1046,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1047,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and","We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms",0
1048,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",0
1049,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",In scenarios where the underlying environment has a continuous state-space and we representQ with a differentiable function (e.g,0
1050,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1051,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1052,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1053,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1054,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1055,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1056,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1057,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1058,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0","Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",0
1059,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1060,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions","Throughout, we assume Θ ⊆ Rd.Lemma 1",0
1061,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions","Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)",0
1062,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1063,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",0
1064,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1065,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space","Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B",0
1066,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and","Namely, we draw the nominal",0
1067,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1068,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1069,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",0
1070,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and","Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument",0
1071,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and","14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",0
1072,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",We illustrate test misclassiﬁcation error vs,0
1073,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,0
1074,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",0
1075,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1076,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0","WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ",0
1077,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",with the convention that 0 · ∞ = 0,0
1078,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1079,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1080,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and","By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",0
1081,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1082,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1083,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1084,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1085,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1086,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0","From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have",0
1087,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",0
1088,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",A,0
1089,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
1090,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space","In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",0
1091,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1092,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1093,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1094,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,0
1095,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",0
1096,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1097,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1098,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,1
1099,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",All models are trained in the ∞-norm,0
1100,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0","Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label",0
1101,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,B,0
1102,"Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1103,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1104,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave",0
1105,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1106,"Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1107,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1108,"For any distribution Q and any ρ > 0,(5)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1109,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts",0
1110,"For any distribution Q and any ρ > 0,(5)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1111,"For any distribution Q and any ρ > 0,(5)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1112,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1113,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.",0
1114,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1",Frossard,0
1115,"Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)","arXiv:1703.11008 [cs.LG], 2017.P",0
1116,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1117,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1118,"For any distribution Q and any ρ > 0,(5)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1119,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1120,"For any distribution Q and any ρ > 0,(5)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1121,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"Moosavi-Dezfooli, A",0
1122,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0","Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
1123,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)","When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",0
1124,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
1125,"Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)","Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples",0
1126,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",0
1127,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",0
1128,"For any distribution Q and any ρ > 0,(5)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1129,"For any distribution Q and any ρ > 0,(5)","As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)",0
1130,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,Let ∆F ≥ F (θ0) − inf θ F (θ),0
1131,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1132,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)",Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,0
1133,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1134,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1135,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)","Moosavi-Dezfooli, A",0
1136,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1137,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1138,"For any distribution Q and any ρ > 0,(5)",See Section E.1 for the proof of the proposition,0
1139,"For any distribution Q and any ρ > 0,(5)","By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data",0
1140,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1141,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","Papernot, P",0
1142,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1143,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)",Figure 1,0
1144,"Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)","We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",0
1145,"Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1146,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1147,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1148,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"Then using thepreceding display, we have",0
1149,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1150,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms",0
1151,"For any distribution Q and any ρ > 0,(5)","This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd",0
1152,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","The smoothness of (cid:96) is essential so that a penalized version(cid:96)(θ, z) − γc(z, z0) is concave in z (which can be approximately veriﬁed by computing Hessians∇2zz(cid:96)(θ, z) for each training datapoint), allowing computation and our coming certiﬁcates of opti-mality",0
1153,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0","From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have",0
1154,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1155,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0","Celik, and A",0
1156,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1157,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1158,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1159,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1160,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)",Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,0
1161,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1162,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1163,"For any distribution Q and any ρ > 0,(5)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1164,"Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1165,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1166,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1",Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,0
1167,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1168,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1169,"Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1170,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1171,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1172,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0","In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",0
1173,"For any distribution Q and any ρ > 0,(5)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1174,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)",Deepfool: a simple and accurate method to fooldeep neural networks,0
1175,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1176,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1177,"For any distribution Q and any ρ > 0,(5)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1178,"Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
1179,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1180,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1181,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1182,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1183,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1184,"Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)",The below general duality result gives Proposition 1 as an immediate specialcase,0
1185,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1186,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1187,"For any distribution Q and any ρ > 0,(5)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1188,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument",0
1189,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","Veness, M",0
1190,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1191,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)",0
1192,"Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)",N,0
1193,"Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",0
1194,"Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1195,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1196,"For any distribution Q and any ρ > 0,(5)","Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ",0
1197,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",1
1198,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)","Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z",0
1199,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)",is closed-valued and measurable,0
1200,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1201,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Mnih, K",0
1202,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1203,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1204,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1205,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance",0
1206,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","because f − c is upper semi-continuous, and the latter function is measurable",0
1207,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1208,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Papernot, P",0
1209,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training",0
1210,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.",0
1211,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1212,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1213,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1214,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1215,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",The choice of P in the robust objective (1) affectsboth the richness of the uncertainty set we wish to consider as well as the tractability of the result-ing optimization problem,0
1216,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1217,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",0
1218,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)",0
1219,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1220,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models",0
1221,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s",0
1222,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
1223,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",0
1224,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1225,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",All of our resultssuitably generalize to this setting with minor modiﬁcations to the robust surrogate (2b) and theabove assumptions (see Section D),0
1226,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1227,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",0
1228,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","IEEE, 2016c.",0
1229,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1230,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1231,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1232,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Throughout, we assume Θ ⊆ Rd.Lemma 1",0
1233,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models",0
1234,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1235,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1236,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)",0
1237,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",0
1238,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","These works provide an initial foundation for adversarial training, but itis challenging to rigorously identify the classes of attacks against which they can defend (or if theyexist)",0
1239,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1240,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,0
1241,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",We illustrate test misclassiﬁcation error vs.theadversarial perturbation level adv,0
1242,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1243,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,0
1244,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3",0
1245,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1246,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",0
1247,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",0
1248,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",0
1249,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1250,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
1251,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1252,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",0
1253,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.",0
1254,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1255,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,0
1256,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1257,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Left column:Euclidean-norm attacks, right column: ∞-norm attacks",0
1258,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1259,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1260,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1261,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1262,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1263,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1264,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",See Section C.2 for the proof,0
1265,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",8.7.1) to obtain,0
1266,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",B,0
1267,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1268,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",0
1269,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1270,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1271,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1272,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1273,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1274,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",Theorem 7,0
1275,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1276,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",HN was partially supported by aSamsung Fellowship,0
1277,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Top row: PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",0
1278,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1279,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",0
1280,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1281,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",Other models do not exhibit this behavior with the same consistency(if at all),0
1282,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,0
1283,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1284,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",0
1285,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1286,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1287,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",0
1288,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1289,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1290,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1291,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","2574–2582, 2016.",0
1292,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",0
1293,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",0
1294,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,0
1295,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",0
1296,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0",0
1297,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1",0
1298,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",1
1299,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart","The vertical bar in (a), (c), and (e) indicates the perturbation level that was",0
1300,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1301,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1302,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).","Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM",0
1303,The function c : Z×Z → R+ is continuous,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1304,The function c : Z×Z → R+ is continuous,We illustrate testmisclassiﬁcation error vs,0
1305,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",0
1306,The function c : Z×Z → R+ is continuous,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1307,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","Wu, S",0
1308,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1309,The function c : Z×Z → R+ is continuous,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
1310,The function c : Z×Z → R+ is continuous,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1311,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","Consider feedforward neural networks with ReLU’s and let U := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U (cid:96)(θ; z + u) is NPO",0
1312,The function c : Z×Z → R+ is continuous,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or",0
1313,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",Training data are shown in blue and red,0
1314,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1315,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1316,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1317,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",0
1318,The function c : Z×Z → R+ is continuous,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1319,The function c : Z×Z → R+ is continuous,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",0
1320,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1321,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1322,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",0
1323,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).","For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ",0
1324,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1325,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1326,The function c : Z×Z → R+ is continuous,"Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques",0
1327,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1328,The function c : Z×Z → R+ is continuous,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1329,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1330,The function c : Z×Z → R+ is continuous,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",0
1331,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).",Swami,0
1332,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",0
1333,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1334,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).","Top row: PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",0
1335,The function c : Z×Z → R+ is continuous,Madry et al,0
1336,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1337,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1338,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","Mnih, K",0
1339,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",Further attacks on the MNIST dataset,0
1340,The function c : Z×Z → R+ is continuous,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1341,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",For large adversaries(i.e,0
1342,The function c : Z×Z → R+ is continuous,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",0
1343,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",0
1344,The function c : Z×Z → R+ is continuous,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1345,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1346,The function c : Z×Z → R+ is continuous,"In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",0
1347,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1348,The function c : Z×Z → R+ is continuous,"These works provide an initial foundation for adversarial training, but itis challenging to rigorously identify the classes of attacks against which they can defend (or if theyexist)",0
1349,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",is closed-valued and measurable,0
1350,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",0
1351,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",0
1352,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",It isthus important to distinguish the methods’ abilities to combat attacks,0
1353,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1354,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","Finally, we consider a reinforcement learning problemin Section 4.3, where the Markov decision process used for training differs from that for testing.WRM enjoys the theoretical guarantees of Sections 2 and 3 for large γ, but for small γ (large adver-sarial budgets), WRM becomes a heuristic like other methods",0
1355,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","The NP-hardness of certifying robustness for ReLU networks, coupled withour empirical success and theoretical certiﬁcates for smooth networks in deep learning, suggest thatusing smooth networks may be preferable if we wish to guarantee robustness",0
1356,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",0
1357,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","Veness, M",0
1358,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",0
1359,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1360,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1361,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",the adversarial perturbationlevel adv,0
1362,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1363,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1364,The function c : Z×Z → R+ is continuous,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1365,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1366,The function c : Z×Z → R+ is continuous,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1367,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1368,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).","In the former case, since the update (22)simply modiﬁes the state-action transitions (independent of Q), standard results on convergence fortabular Q-learning (e.g",0
1369,The function c : Z×Z → R+ is continuous,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1370,The function c : Z×Z → R+ is continuous,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1371,The function c : Z×Z → R+ is continuous,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1372,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1373,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).",the adversarial perturbation level adv,0
1374,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",", K}",0
1375,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",0
1376,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","These results suggest advantages of networks with smooth activations
rather than ReLU’s",0
1377,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1378,The function c : Z×Z → R+ is continuous,"If f is inf-compact, then ¯f is directionally differentiable with",0
1379,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1380,The function c : Z×Z → R+ is continuous,N,0
1381,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
1382,The function c : Z×Z → R+ is continuous,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",0
1383,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1384,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",0
1385,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","Rusu, J",0
1386,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),0
1387,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",0
1388,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1389,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1390,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1391,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1392,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z",0
1393,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",M,0
1394,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)","WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",0
1395,The function c : Z×Z → R+ is continuous,"WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.",0
1396,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","Szepesv´ari & Littman (1999)) apply under these adversarial dynamics.We test our adversarial training procedure in the cart-pole environment, where the goal is to balancea pole on a cart by moving the cart left or right",0
1397,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques",0
1398,The function c : Z×Z → R+ is continuous,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1399,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",1
1400,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1401,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",The function c : Z×Z → R+ is continuous,0
1402,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes",0
1403,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1404,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",0
1405,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1406,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1407,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,0
1408,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1409,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","With that said, the strong dualityresult, Proposition 1, still applies to any distribution",0
1410,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1411,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1412,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1413,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
1414,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1415,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time",0
1416,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",0
1417,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",K,0
1418,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1419,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1420,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1421,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1422,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1423,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1424,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1425,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",In scenarios where the underlying environment has a continuous state-space and we representQ with a differentiable function (e.g,0
1426,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)",0
1427,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1428,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"Thus, we see that our adversarial-training method defends against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint untilthe model misclassiﬁes it",0
1429,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"Using only assumptions about the smoothness of the loss function(cid:96), we prove that our method enjoys strong statistical guarantees and fast optimization rates for alarge class of problems",0
1430,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1431,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,", K}",0
1432,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,0
1433,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1434,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1435,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,0
1436,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1437,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",0
1438,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1439,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques",0
1440,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",0
1441,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","Ried-miller, A",0
1442,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,with the convention that 0 · ∞ = 0,0
1443,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1444,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1445,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",We omit the certiﬁcate’s error,0
1446,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1447,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1448,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",0
1449,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1450,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
1451,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
1452,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",Computing nonvacuous generalization bounds for deep (stochastic)neural networks with many more parameters than training data,0
1453,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","In this regime(small γ, large ), performance between WRM and other methods diverge",0
1454,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1455,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1456,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1457,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1458,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",0
1459,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",We omit the certiﬁcate’s error,0
1460,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1461,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","Mnih, K",0
1462,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1463,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1464,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",0
1465,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,Figure 9,0
1466,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1",0
1467,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",The vertical bar indicates the achieved level,0
1468,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1469,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",0
1470,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1471,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",0
1472,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,0
1473,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label",0
1474,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1475,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","Szepesv´ari & Littman (1999)) apply under these adversarial dynamics.We test our adversarial training procedure in the cart-pole environment, where the goal is to balancea pole on a cart by moving the cart left or right",0
1476,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",0
1477,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1478,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
1479,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1480,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",(2017) attempt to mitigate this shortcoming,0
1481,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,0
1482,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,All mod-els are trained in the ∞-norm,0
1483,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",The action space is binary: push the cartleft or right with a ﬁxed force,0
1484,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1485,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions","One natural appli-cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-pointproblem (1) and give specialized bounds for domain adaptation problems",0
1486,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1487,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,0
1488,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,0
1489,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",0
1490,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","for all γ, ρ ≥ 0",0
1491,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1492,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
1493,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,0
1494,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1495,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",0
1496,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",Figure 2,0
1497,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1498,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",1
1499,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",0
1500,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","Thus, theoptimization problem is NP-hard.",0
1501,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",0
1502,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",B,0
1503,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",0
1504,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1505,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
1506,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:c((x, y), (x0, y0)) = (cid:107)x − x0(cid:107)2∞ + ∞ · 1{y (cid:54)= y0} .",0
1507,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ",0
1508,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",with the convention that 0 · ∞ = 0,0
1509,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1510,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",0
1511,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",0
1512,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",We illustrate test misclassiﬁcation error vs,0
1513,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",the adversarial perturbation level adv,0
1514,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",0
1515,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1516,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)",0
1517,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",0
1518,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1519,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",Swami,0
1520,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1521,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1522,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1523,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,0
1524,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",0
1525,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1526,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1527,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt",0
1528,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",0
1529,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1530,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3",0
1531,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1532,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1533,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1534,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1535,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",372–387,0
1536,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)",0
1537,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1538,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1539,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1540,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",Other models do not exhibit this behavior with the same consistency(if at all),0
1541,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",(16)) in θ,0
1542,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1543,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D",0
1544,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",0
1545,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1546,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1547,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1548,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","Thus, weonly compare with an agent trained on the nominal MDP",0
1549,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1550,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",0
1551,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",the adversarial perturbationlevel adv,0
1552,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping",0
1553,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",HN was partially supported by aSamsung Fellowship,0
1554,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1555,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,0
1556,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1557,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞",0
1558,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1559,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",We assume without loss of2 − z0 (cid:54)= 0,0
1560,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping",0
1561,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)",0
1562,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8",0
1563,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1564,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
1565,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",0
1566,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1567,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1568,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1569,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",0
1570,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",with the convention that 0 · ∞ = 0,0
1571,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)",0
1572,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0",0
1573,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1574,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1575,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1576,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1577,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",0
1578,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1579,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1580,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","In classiﬁcation settings, we have Y ∈{1, ",0
1581,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1582,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1583,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1584,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",0
1585,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1586,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1587,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",Human-level control through deep reinforcementlearning,0
1588,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,0
1589,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1590,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1591,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).",0
1592,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt",0
1593,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","Using only assumptions about the smoothness of the loss function(cid:96), we prove that our method enjoys strong statistical guarantees and fast optimization rates for alarge class of problems",0
1594,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1595,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,0
1596,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1597,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1598,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1599,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",1
1600,"Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1601,"Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,0
1602,"Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1603,"Throughout, we assume Θ ⊆ Rd.Lemma 1",We thank Jacob Steinhardt for valuable feedback,0
1604,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1605,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1606,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,Experimental results on synthetic data,0
1607,"Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",We use reward r(β) := e−|β| for the angle β of the pole from the vertical,0
1608,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,372–387,0
1609,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1610,"Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",0
1611,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1612,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)","For imperceptible perturbations, our method matches or outperformsheuristic approaches.",0
1613,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1614,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,0
1615,"Throughout, we assume Θ ⊆ Rd.Lemma 1",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1616,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1617,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1618,"Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",0
1619,"Throughout, we assume Θ ⊆ Rd.Lemma 1",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1620,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1621,"Throughout, we assume Θ ⊆ Rd.Lemma 1","Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
1622,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1623,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",This result is essentially due to Katz et al.(2017a),0
1624,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",0
1625,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,"Top row: FGM attacks, bottom row: IFGM attacks",0
1626,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1627,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1628,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)","For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",0
1629,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)","The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",0
1630,"Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1631,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1632,"Throughout, we assume Θ ⊆ Rd.Lemma 1","For any distribution Q and any ρ > 0,(5)",0
1633,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1634,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1635,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,0
1636,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",Theorem 7,0
1637,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)","However, WRM withReLU’s still suffers from sensitivities (e.g",0
1638,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1639,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)","2574–2582, 2016.",0
1640,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1641,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1642,"Throughout, we assume Θ ⊆ Rd.Lemma 1",The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,0
1643,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,0
1644,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",The action space is binary: push the cartleft or right with a ﬁxed force,0
1645,"Throughout, we assume Θ ⊆ Rd.Lemma 1",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1646,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1647,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1648,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",0
1649,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1650,"Throughout, we assume Θ ⊆ Rd.Lemma 1",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1651,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1652,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,Training data are shown in blue and red,0
1653,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,"In the former case, since the update (22)simply modiﬁes the state-action transitions (independent of Q), standard results on convergence fortabular Q-learning (e.g",0
1654,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1655,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1656,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1657,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1658,"Throughout, we assume Θ ⊆ Rd.Lemma 1",(a) and (b) show test misclassiﬁcation error vs,0
1659,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,"Fredrikson, Z",0
1660,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",(2017) attempt to mitigate this shortcoming,0
1661,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,0
1662,"Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ",0
1663,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1664,"Throughout, we assume Θ ⊆ Rd.Lemma 1","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",0
1665,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)","For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",0
1666,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1667,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1668,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1669,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",with the convention that 0 · ∞ = 0,0
1670,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)","Thus, theoptimization problem is NP-hard.",0
1671,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",Let ∆F ≥ F (θ0) − inf θ F (θ),0
1672,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1673,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",Since 70% of the data are of the,0
1674,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1675,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1676,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)","The vertical bar in (a), (c), and (e) indicates the perturbation level that was",0
1677,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,0
1678,"Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","Jha, M",0
1679,"Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1680,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)","Then, we have",0
1681,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1682,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ","Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",0
1683,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,0
1684,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)","These results suggest advantages of networks with smooth activations
rather than ReLU’s",0
1685,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1686,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1687,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1688,"Throughout, we assume Θ ⊆ Rd.Lemma 1","In most scenarios, however,it is reasonable to defend against weaker adversaries that instead perturb inﬂuential features more.We consider this setting and train against (cid:107)·(cid:107)2-norm attacks",0
1689,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",0
1690,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",All of our resultssuitably generalize to this setting with minor modiﬁcations to the robust surrogate (2b) and theabove assumptions (see Section D),0
1691,"Throughout, we assume Θ ⊆ Rd.Lemma 1",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1692,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",0
1693,"Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1694,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1695,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth","Ried-miller, A",0
1696,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth","Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",0
1697,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)","For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering",0
1698,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1699,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,1
1700,See Section C.2 for the proof,See Section C.2 for the proof,1
1701,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1702,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",All mod-els are trained in the ∞-norm,0
1703,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+","Due to the nonstationary, policy-dependent radius for the Wassersteinball, an analogous  for the fast-gradient method (or other variants) is not well-deﬁned",0
1704,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",M,0
1705,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+","Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",0
1706,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2","ERM and FGM suffer from sensitivities to various regions of the data,as evidenced by the lack of symmetry in their classiﬁcation boundaries",0
1707,See Section C.2 for the proof,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",0
1708,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+","Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.",0
1709,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,0
1710,See Section C.2 for the proof,"(2015), and Blanchet et al",0
1711,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+","Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes",0
1712,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1713,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+","Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B",0
1714,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1715,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1716,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",See Section C.2 for the proof,1
1717,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1","Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv",0
1718,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1","Mnih, K",0
1719,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1720,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",See Section C.2 for the proof,1
1721,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",See Section C.2 for the proof,1
1722,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+","For each γadv, we consider the distance to adversarial examples in the test dataset",0
1723,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+","For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM",0
1724,See Section C.2 for the proof,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,0
1725,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",See Section C.2 for the proof,1
1726,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,0
1727,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",See Section C.2 for the proof,1
1728,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",See Section C.2 for the proof,1
1729,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",2.2) gives the result.,0
1730,See Section C.2 for the proof,See Section C.2 for the proof,1
1731,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",See Section C.2 for the proof,1
1732,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",0
1733,See Section C.2 for the proof,V,0
1734,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1","Then using thepreceding display, we have",0
1735,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,0
1736,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",See Section C.2 for the proof,1
1737,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+","6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",0
1738,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),0
1739,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1740,See Section C.2 for the proof,It isthus important to distinguish the methods’ abilities to combat attacks,0
1741,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1742,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",0
1743,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",The choice of P in the robust objective (1) affectsboth the richness of the uncertainty set we wish to consider as well as the tractability of the result-ing optimization problem,0
1744,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",See Section C.2 for the proof,1
1745,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1746,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",0
1747,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1748,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1749,See Section C.2 for the proof,HN was partially supported by aSamsung Fellowship,0
1750,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",See Section C.2 for the proof,1
1751,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1752,See Section C.2 for the proof,See Section C.2 for the proof,1
1753,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1754,See Section C.2 for the proof,See Section C.2 for the proof,1
1755,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",See Section C.2 for the proof,1
1756,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,0
1757,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1758,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",Theorem 7,0
1759,See Section C.2 for the proof,It isthus important to distinguish the methods’ abilities to combat attacks,0
1760,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",See Section C.2 for the proof,1
1761,See Section C.2 for the proof,See Section C.2 for the proof,1
1762,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+","Throughout, we assume Θ ⊆ Rd.Lemma 1",0
1763,See Section C.2 for the proof,See Section C.2 for the proof,1
1764,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",0
1765,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",See Section C.2 for the proof,1
1766,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+","14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",0
1767,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1768,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2","McDaniel, X",0
1769,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1","In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.",0
1770,See Section C.2 for the proof,See Section C.2 for the proof,1
1771,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1772,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",See Section C.2 for the proof,1
1773,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1774,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2","Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",0
1775,See Section C.2 for the proof,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0",0
1776,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+","From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",0
1777,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+","Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes",0
1778,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1779,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",See Section C.2 for the proof,1
1780,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1781,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",Roy,0
1782,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",See Section C.2 for the proof,1
1783,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",See Section C.2 for the proof,1
1784,See Section C.2 for the proof,See Section C.2 for the proof,1
1785,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+","PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",0
1786,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1","Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)",0
1787,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1788,See Section C.2 for the proof,See Section C.2 for the proof,1
1789,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1","Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ",0
1790,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",See Section C.2 for the proof,1
1791,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,0
1792,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",See Section C.2 for the proof,1
1793,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1794,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1795,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2","Celik, and A",0
1796,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",See Section C.2 for the proof,1
1797,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",adv for (cid:107) · (cid:107)2-IFGM attackFigure 12,0
1798,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",See Section C.2 for the proof,1
1799,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",See Section C.2 for the proof,1
1800,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1801,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ",0
1802,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1803,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",0
1804,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1805,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3",0
1806,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",Training data are shown in blue and red,0
1807,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","Then using thepreceding display, we have",0
1808,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1809,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","In Security and Privacy (SP), 2016 IEEE Symposiumon, pp",0
1810,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,0
1811,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
1812,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1813,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","ERM and FGM suffer from sensitivities to various regions of the data,as evidenced by the lack of symmetry in their classiﬁcation boundaries",0
1814,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",M,0
1815,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1816,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",Dziugaite and D,0
1817,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","For large γ, we can again solve problem (22) efﬁciently using gradient descent",0
1818,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1819,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",Our optimization result (Theorem 2) appliesonly for small values of robustness ρ and to a limited class of Wasserstein costs,0
1820,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",We illustrate test misclassiﬁcation error vs.theadversarial perturbation level adv,0
1821,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1822,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1823,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp",0
1824,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",0
1825,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A",0
1826,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1827,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",0
1828,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1829,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1830,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",allowing us to prevent attacks on the test set,0
1831,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1832,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","In classiﬁcation settings, we have Y ∈{1, ",0
1833,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section",0
1834,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",0
1835,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1836,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1837,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1838,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time",0
1839,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1840,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,0
1841,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g",0
1842,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1843,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","From Lemma 5, our previous results (Theorems 2 and 4) follow",0
1844,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
1845,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1846,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1847,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1848,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1849,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",14.27 and Prop,0
1850,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",0
1851,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",0
1852,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1853,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1854,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",0
1855,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1856,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",0
1857,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","With that said, the strong dualityresult, Proposition 1, still applies to any distribution",0
1858,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","The decision problem Turing-reducesto the optimization problem, as the decision problem can be solved with one call to O",0
1859,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1860,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1861,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1862,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B",0
1863,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,0
1864,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1865,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,0
1866,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",0
1867,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.",0
1868,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",0
1869,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",See Section E.1 for the proof of the proposition,0
1870,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,0
1871,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",0
1872,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1873,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1874,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1875,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1876,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1877,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",0
1878,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,0
1879,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","The vertical bar in (a), (c), and (e) indicates the perturbation level that was",0
1880,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1881,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",0
1882,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","Throughout, we assume Θ ⊆ Rd.Lemma 1",0
1883,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
1884,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1885,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",Dziugaite and D,0
1886,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1887,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1888,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","Moosavi-Dezfooli, A",0
1889,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1890,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1891,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",0
1892,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","Thus, we see that our adversarial-training method defends against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint untilthe model misclassiﬁes it",0
1893,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",the adversarial perturbation level adv,0
1894,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
1895,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1896,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",1
1897,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,0
1898,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
1899,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",", K}",0
1900,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes",We have the following theorem.Theorem 5,0
1901,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1902,Recallthat F (θ) = EP0 [φγ(θ; Z)] is the robust surrogate objective for the Lagrangian relaxation (2).Theorem 2 (Convergence of Nonconvex SGD),from which it is easy to see that f is inf-compact,0
1903,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",The vertical bar indicates the achieved level,0
1904,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),"In classiﬁcation settings, we have Y ∈{1, ",0
1905,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),"Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z",0
1906,Let ∆F ≥ F (θ0) − inf θ F (θ),"(2017), Dziugaite & Roy (2017), andNeyshabur et al",0
1907,"When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ","For example,Ben-Tal et al",0
1908,Recallthat F (θ) = EP0 [φγ(θ; Z)] is the robust surrogate objective for the Lagrangian relaxation (2).Theorem 2 (Convergence of Nonconvex SGD),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1909,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1910,Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,Theorem 7,0
1911,Let ∆F ≥ F (θ0) − inf θ F (θ),Since 70% of the data are of the,0
1912,Let ∆F ≥ F (θ0) − inf θ F (θ),"Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ",0
1913,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz","In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.",0
1914,Recallthat F (θ) = EP0 [φγ(θ; Z)] is the robust surrogate objective for the Lagrangian relaxation (2).Theorem 2 (Convergence of Nonconvex SGD),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1915,"When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ",large desired robustness values) our approach becomes a heuristic just like the other approaches.,0
1916,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1917,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",(f) Test error vs,0
1918,Let ∆F ≥ F (θ0) − inf θ F (θ),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1919,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
1920,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes","Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",0
1921,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1922,Let ∆F ≥ F (θ0) − inf θ F (θ),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1923,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1924,Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",0
1925,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1926,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1927,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1928,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1929,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes","Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",0
1930,Let ∆F ≥ F (θ0) − inf θ F (θ),"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
1931,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1932,Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1933,Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1934,Let ∆F ≥ F (θ0) − inf θ F (θ),"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
1935,"When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1936,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),"Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ",0
1937,Recallthat F (θ) = EP0 [φγ(θ; Z)] is the robust surrogate objective for the Lagrangian relaxation (2).Theorem 2 (Convergence of Nonconvex SGD),Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,0
1938,Recallthat F (θ) = EP0 [φγ(θ; Z)] is the robust surrogate objective for the Lagrangian relaxation (2).Theorem 2 (Convergence of Nonconvex SGD),We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,0
1939,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1940,Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1941,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1942,"When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1943,Recallthat F (θ) = EP0 [φγ(θ; Z)] is the robust surrogate objective for the Lagrangian relaxation (2).Theorem 2 (Convergence of Nonconvex SGD),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1944,Let ∆F ≥ F (θ0) − inf θ F (θ),"These results suggest advantages of networks with smooth activations
rather than ReLU’s",0
1945,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1946,Let ∆F ≥ F (θ0) − inf θ F (θ),"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",0
1947,"When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1948,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1949,Let ∆F ≥ F (θ0) − inf θ F (θ),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1950,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1951,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1952,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1953,Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1954,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1955,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",0
1956,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz","Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)",0
1957,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes","By appropriatescaling of θ, v, and w, Katz et al",0
1958,Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,"WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ",0
1959,Let ∆F ≥ F (θ0) − inf θ F (θ),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1960,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1961,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1962,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",0
1963,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
1964,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),We illustrate test misclassiﬁcation error vs.theadversarial perturbation level adv,0
1965,"When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ",We typically solve the penalty problem (2) with P0 replaced,0
1966,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1967,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",The distinctions in performance between various methodsare less apparent now,0
1968,Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1969,Let ∆F ≥ F (θ0) − inf θ F (θ),"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",0
1970,Let ∆F ≥ F (θ0) − inf θ F (θ),The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,0
1971,Let ∆F ≥ F (θ0) − inf θ F (θ),Goodfellow et al,0
1972,Recallthat F (θ) = EP0 [φγ(θ; Z)] is the robust surrogate objective for the Lagrangian relaxation (2).Theorem 2 (Convergence of Nonconvex SGD),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1973,"When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1974,"When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ","because f − c is upper semi-continuous, and the latter function is measurable",0
1975,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz","Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",0
1976,Recallthat F (θ) = EP0 [φγ(θ; Z)] is the robust surrogate objective for the Lagrangian relaxation (2).Theorem 2 (Convergence of Nonconvex SGD),"9–10)), we have",0
1977,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1978,Let ∆F ≥ F (θ0) − inf θ F (θ),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1979,"When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ",Deepfool: a simple and accurate method to fooldeep neural networks,0
1980,Let ∆F ≥ F (θ0) − inf θ F (θ),"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",0
1981,Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",0
1982,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1983,Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",0
1984,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,0
1985,Recallthat F (θ) = EP0 [φγ(θ; Z)] is the robust surrogate objective for the Lagrangian relaxation (2).Theorem 2 (Convergence of Nonconvex SGD),"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",0
1986,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1987,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1988,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1989,Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1990,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",Further attacks on the MNIST dataset,0
1991,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1992,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes","Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",0
1993,Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1994,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1995,Recallthat F (θ) = EP0 [φγ(θ; Z)] is the robust surrogate objective for the Lagrangian relaxation (2).Theorem 2 (Convergence of Nonconvex SGD),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1996,Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1997,Recallthat F (θ) = EP0 [φγ(θ; Z)] is the robust surrogate objective for the Lagrangian relaxation (2).Theorem 2 (Convergence of Nonconvex SGD),(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),1
1998,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz","The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time",0
1999,"When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ","Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",0
2000,"Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X",M,0
2001,"The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2002,All of our resultssuitably generalize to this setting with minor modiﬁcations to the robust surrogate (2b) and theabove assumptions (see Section D),"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2003,"The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2004,"Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2005,"Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",See Section C.2 for the proof,0
2006,"By suitably modifying the cost function c(z, z(cid:48))to take value ∞ outside this small region, our general formulation covers such variants.","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2007,"Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)",Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),0
2008,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2009,"The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)",the adversarial perturbationlevel adv,0
2010,"The smoothness of (cid:96) is essential so that a penalized version(cid:96)(θ, z) − γc(z, z0) is concave in z (which can be approximately veriﬁed by computing Hessians∇2zz(cid:96)(θ, z) for each training datapoint), allowing computation and our coming certiﬁcates of opti-mality","AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",0
2011,"Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)",14.27 and Prop,0
2012,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z","Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",0
2013,"The smoothness of (cid:96) is essential so that a penalized version(cid:96)(θ, z) − γc(z, z0) is concave in z (which can be approximately veriﬁed by computing Hessians∇2zz(cid:96)(θ, z) for each training datapoint), allowing computation and our coming certiﬁcates of opti-mality","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2014,"The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)","Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations",0
2015,"The smoothness of (cid:96) is essential so that a penalized version(cid:96)(θ, z) − γc(z, z0) is concave in z (which can be approximately veriﬁed by computing Hessians∇2zz(cid:96)(θ, z) for each training datapoint), allowing computation and our coming certiﬁcates of opti-mality","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2016,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2017,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2018,"Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2019,"The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)",the adversarial perturbation level adv,0
2020,"Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2021,"Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)","Left column:Euclidean-norm attacks, right column: ∞-norm attacks",0
2022,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z","Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes",0
2023,"Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2024,"The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2025,"By suitably modifying the cost function c(z, z(cid:48))to take value ∞ outside this small region, our general formulation covers such variants.","On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
2026,"The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2027,"Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",adv for (cid:107)·(cid:107)∞-IFGM attackFigure 6,0
2028,"Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2029,"The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)","For each γadv, we consider the distance to adversarial examples in the test dataset",0
2030,"Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2031,"By suitably modifying the cost function c(z, z(cid:48))to take value ∞ outside this small region, our general formulation covers such variants.","All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training",0
2032,"Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",0
2033,"Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)","We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",0
2034,"Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)",We illustrate test misclassiﬁcationerror vs,0
2035,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2036,"The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2037,"Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)","For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance",0
2038,"Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
2039,"Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2040,"Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X",Madry et al,0
2041,"By suitably modifying the cost function c(z, z(cid:48))to take value ∞ outside this small region, our general formulation covers such variants.","Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",0
2042,"Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2043,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2044,"Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)",All models are trained in the ∞-norm,0
2045,"Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2046,"Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2047,"Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X","These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
2048,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z","Papernot, P",0
2049,All of our resultssuitably generalize to this setting with minor modiﬁcations to the robust surrogate (2b) and theabove assumptions (see Section D),JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,0
2050,"By suitably modifying the cost function c(z, z(cid:48))to take value ∞ outside this small region, our general formulation covers such variants.","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2051,"Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2052,"Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X","To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost",0
2053,"The smoothness of (cid:96) is essential so that a penalized version(cid:96)(θ, z) − γc(z, z0) is concave in z (which can be approximately veriﬁed by computing Hessians∇2zz(cid:96)(θ, z) for each training datapoint), allowing computation and our coming certiﬁcates of opti-mality",Deﬁne the scale parameter βt > 0 by,0
2054,"Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",0
2055,"Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2056,"Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2057,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z","In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets",0
2058,"Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",0
2059,"The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)","In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.",0
2060,"Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X","Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.",0
2061,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
2062,"Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2063,"Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2064,"Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2065,"Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2066,"Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)",Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,0
2067,"Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2068,"The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2069,"Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2070,"By suitably modifying the cost function c(z, z(cid:48))to take value ∞ outside this small region, our general formulation covers such variants.","For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",0
2071,"By suitably modifying the cost function c(z, z(cid:48))to take value ∞ outside this small region, our general formulation covers such variants.","Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)",0
2072,"The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)",", Tadv, where Π denotes projection onto B,p(xi) := {x : (cid:107)x − xi(cid:107)p ≤ }.The adversarial training literature (e.g",0
2073,"Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)","For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering",0
2074,"Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2075,"Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2076,"Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z","The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and",0
2077,All of our resultssuitably generalize to this setting with minor modiﬁcations to the robust surrogate (2b) and theabove assumptions (see Section D),"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2078,"Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)","Ostrovski, et al",0
2079,"Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2080,"Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2081,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2082,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2083,"The smoothness of (cid:96) is essential so that a penalized version(cid:96)(θ, z) − γc(z, z0) is concave in z (which can be approximately veriﬁed by computing Hessians∇2zz(cid:96)(θ, z) for each training datapoint), allowing computation and our coming certiﬁcates of opti-mality",JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,0
2084,"Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)",Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,0
2085,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z","We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R",0
2086,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2087,"The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2088,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2089,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,0
2090,"By suitably modifying the cost function c(z, z(cid:48))to take value ∞ outside this small region, our general formulation covers such variants.","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2091,"Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),0
2092,"Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2093,"The smoothness of (cid:96) is essential so that a penalized version(cid:96)(θ, z) − γc(z, z0) is concave in z (which can be approximately veriﬁed by computing Hessians∇2zz(cid:96)(θ, z) for each training datapoint), allowing computation and our coming certiﬁcates of opti-mality","Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =",0
2094,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z","Jha, and A",0
2095,"Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2096,"the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2097,"Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",1
2098,"The smoothness of (cid:96) is essential so that a penalized version(cid:96)(θ, z) − γc(z, z0) is concave in z (which can be approximately veriﬁed by computing Hessians∇2zz(cid:96)(θ, z) for each training datapoint), allowing computation and our coming certiﬁcates of opti-mality","This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al",0
2099,"Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,0
2100,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2101,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",the adversarial perturbationlevel adv,0
2102,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2103,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2104,allowing us to prevent attacks on the test set,"Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
2105,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2106,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1","By suitably modifying the cost function c(z, z(cid:48))to take value ∞ outside this small region, our general formulation covers such variants.",0
2107,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2108,allowing us to prevent attacks on the test set,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
2109,allowing us to prevent attacks on the test set,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",0
2110,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2111,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2112,allowing us to prevent attacks on the test set,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",0
2113,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2114,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2115,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,0
2116,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2117,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",HN was partially supported by aSamsung Fellowship,0
2118,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2119,allowing us to prevent attacks on the test set,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,0
2120,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2121,allowing us to prevent attacks on the test set,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,0
2122,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2123,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",We illustrate test misclassiﬁcation error vs.theadversarial perturbation level adv,0
2124,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2125,allowing us to prevent attacks on the test set,"Recalling Rockafellar & Wets (1998, Def",0
2126,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2127,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ","From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",0
2128,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2129,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2130,allowing us to prevent attacks on the test set,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",0
2131,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2132,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2133,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2134,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2135,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2136,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ","AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",0
2137,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2138,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1","Left column:Euclidean-norm attacks, right column: ∞-norm attacks",0
2139,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2140,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2141,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2142,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2143,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",582–597,0
2144,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2145,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2146,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2147,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2148,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",We typically solve the penalty problem (2) with P0 replaced,0
2149,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",is closed-valued and measurable,0
2150,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ","Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",0
2151,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
2152,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2153,allowing us to prevent attacks on the test set,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label",0
2154,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2155,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2156,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2157,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2158,allowing us to prevent attacks on the test set,Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,0
2159,allowing us to prevent attacks on the test set,We illustrate test misclassiﬁcation error vs.theadversarial perturbation level adv,0
2160,allowing us to prevent attacks on the test set,(e) Test error vs,0
2161,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2162,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2163,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2164,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2165,allowing us to prevent attacks on the test set,"In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp",0
2166,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
2167,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2168,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",We illustrate test misclassiﬁcation error vs.theadversarial perturbation level adv,0
2169,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2170,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1","As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)",0
2171,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ","Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",0
2172,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2173,allowing us to prevent attacks on the test set,"Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z",0
2174,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",the adversarial perturbation level adv,0
2175,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2176,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2177,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",(16)) in θ,0
2178,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ","Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",0
2179,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1","Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",0
2180,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1","9–10)), we have",0
2181,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2182,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2183,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",Deﬁne the scale parameter βt > 0 by,0
2184,allowing us to prevent attacks on the test set,"Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))",0
2185,allowing us to prevent attacks on the test set,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label",0
2186,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2187,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,0
2188,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2189,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2190,allowing us to prevent attacks on the test set,"Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",0
2191,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ","Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",0
2192,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",allowing us to prevent attacks on the test set,1
2193,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1","WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",0
2194,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2195,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2196,allowing us to prevent attacks on the test set,allowing us to prevent attacks on the test set,1
2197,allowing us to prevent attacks on the test set,The vertical bar indicates the achieved level,0
2198,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",allowing us to prevent attacks on the test set,1
2199,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",0
2200,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
2201,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",0
2202,by solving (7),"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",0
2203,by solving (7),"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",0
2204,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).","The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and",0
2205,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2206,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).","Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",0
2207,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2208,by solving (7),"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",0
2209,by solving (7),00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
2210,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2211,by solving (7),by solving (7),1
2212,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2213,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2214,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2215,by solving (7),the adversarial perturbation level adv,0
2216,by solving (7),"Moosavi-Dezfooli, A",0
2217,by solving (7),"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",0
2218,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,"We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",0
2219,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2220,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2221,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2222,by solving (7),by solving (7),1
2223,by solving (7),"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",0
2224,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",The major beneﬁt of our approach is its simplicityand wide applicability across many models and machine-learning scenarios.There remain many avenues for future investigation,0
2225,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).","Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)",0
2226,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,"Due to the nonstationary, policy-dependent radius for the Wassersteinball, an analogous  for the fast-gradient method (or other variants) is not well-deﬁned",0
2227,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).","If f is inf-compact, then ¯f is directionally differentiable with",0
2228,by solving (7),by solving (7),1
2229,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2230,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,"Graves, M",0
2231,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,"In this section, we give an adapation ofthe results in Sections 2 and 3 (Theorems 2 and 4) to such scenarios",0
2232,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",0
2233,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives",0
2234,by solving (7),00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
2235,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2236,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,0
2237,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2238,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,"Then, we have",0
2239,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2240,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",The distinctions in performance between various methodsare less apparent now,0
2241,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2242,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2243,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2244,by solving (7),We providea principled method for efﬁciently guaranteeing distributional robustness with a simple form ofadversarial data perturbation,0
2245,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,"Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",0
2246,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",All mod-els are trained in the ∞-norm,0
2247,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
2248,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2249,by solving (7),by solving (7),1
2250,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2251,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2252,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2253,by solving (7),by solving (7),1
2254,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2255,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2256,by solving (7),by solving (7),1
2257,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2258,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).","Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",0
2259,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",adv for (cid:107) · (cid:107)2-IFGM attackFigure 12,0
2260,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2261,by solving (7),by solving (7),1
2262,by solving (7),"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",0
2263,by solving (7),PGM attacks on the MNIST dataset,0
2264,by solving (7),by solving (7),1
2265,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",0
2266,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
2267,by solving (7),by solving (7),1
2268,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,See Section E.1 for the proof of the proposition,0
2269,by solving (7),by solving (7),1
2270,by solving (7),by solving (7),1
2271,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",Explicit distributional robustness of the form (5) is intractable except in limited cases,0
2272,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,We assume without loss of2 − z0 (cid:54)= 0,0
2273,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,"In this proof, we drop the subscript on the iteration t to ease notation",0
2274,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2275,by solving (7),by solving (7),1
2276,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,"McDaniel, X",0
2277,by solving (7),by solving (7),1
2278,by solving (7),"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",0
2279,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2280,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).","As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",0
2281,by solving (7),"We compare standard WRM with ∞-norm PGM, FGM,IFGM",0
2282,by solving (7),Human-level control through deep reinforcementlearning,0
2283,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2284,by solving (7),by solving (7),1
2285,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2286,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2287,by solving (7),by solving (7),1
2288,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",0
2289,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2290,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,"In Security and Privacy (SP), 2016 IEEE Symposiumon, pp",0
2291,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",HN was partially supported by aSamsung Fellowship,0
2292,by solving (7),"We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",0
2293,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,"In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s",0
2294,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2295,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,by solving (7),1
2296,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2297,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,0
2298,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,0
2299,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",by solving (7),1
2300,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2301,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2302,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2303,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,All of our resultssuitably generalize to this setting with minor modiﬁcations to the robust surrogate (2b) and theabove assumptions (see Section D),0
2304,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]",0
2305,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2306,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",Mnih et al,0
2307,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)","Graves, M",0
2308,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2309,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)",0
2310,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)",(e) Test error vs,0
2311,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability","In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",0
2312,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",0
2313,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
2314,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",We observe that similar trends as inSection A.5.1 hold again.,0
2315,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2316,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2317,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2318,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2319,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",0
2320,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2321,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)","(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness",0
2322,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2323,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2324,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",0
2325,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2326,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)","Szepesv´ari & Littman (1999)) apply under these adversarial dynamics.We test our adversarial training procedure in the cart-pole environment, where the goal is to balancea pole on a cart by moving the cart left or right",0
2327,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)","In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp",0
2328,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2329,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2330,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3",We have the following theorem.Theorem 5,0
2331,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",0
2332,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",0
2333,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2334,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).",0
2335,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain",0
2336,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2337,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2338,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2339,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2340,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2341,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2342,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","Papernot, P",0
2343,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3",M,0
2344,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2345,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2346,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,"9–10)), we have",0
2347,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2348,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2349,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2350,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)",(16)) in θ,0
2351,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2352,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,The function c : Z×Z → R+ is continuous,0
2353,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",0
2354,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,"Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",0
2355,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",0
2356,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2357,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2358,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability","We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",0
2359,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2360,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2361,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",0
2362,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al",0
2363,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",0
2364,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2365,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2366,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2367,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2368,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2369,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",0
2370,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",0
2371,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2372,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2373,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2374,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2375,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2376,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2377,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3",The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,0
2378,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2379,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",0
2380,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","For each γadv, we consider the distance to adversarial examples in the test dataset",0
2381,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",Madry et al,0
2382,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2383,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability","In this proof, we drop the subscript on the iteration t to ease notation",0
2384,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",0
2385,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2386,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2387,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability","Wu, S",0
2388,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",0
2389,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes",0
2390,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2391,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section",Madry et al,0
2392,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2393,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2394,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2395,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",1
2396,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,0
2397,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section",", Tadv, where Π denotes projection onto B,p(xi) := {x : (cid:107)x − xi(cid:107)p ≤ }.The adversarial training literature (e.g",0
2398,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3",B,0
2399,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)","Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",0
2400,See Section C.4 for its proof,See Section C.4 for its proof,1
2401,"Then, the bounds (11) and (12) hold with","Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",0
2402,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1","However, WRM withReLU’s still suffers from sensitivities (e.g",0
2403,"Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞",See Section C.4 for its proof,1
2404,"Then, the bounds (11) and (12) hold with",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),0
2405,Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,See Section C.4 for its proof,1
2406,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty",See Section C.4 for its proof,1
2407,"Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞",We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,0
2408,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1","The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and",0
2409,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",0
2410,"Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞",(16)) in θ,0
2411,See Section C.4 for its proof,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",0
2412,"Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞","Graves, M",0
2413,"Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",0
2414,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",We assume without loss of2 − z0 (cid:54)= 0,0
2415,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1","Silver, A",0
2416,See Section C.4 for its proof,All of our resultssuitably generalize to this setting with minor modiﬁcations to the robust surrogate (2b) and theabove assumptions (see Section D),0
2417,See Section C.4 for its proof,See Section C.4 for its proof,1
2418,"Then, the bounds (11) and (12) hold with","Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
2419,"Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞",See Section C.4 for its proof,1
2420,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",Let ∆F ≥ F (θ0) − inf θ F (θ),0
2421,Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,"In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)",0
2422,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",See Section C.4 for its proof,1
2423,See Section C.4 for its proof,"In the former case, since the update (22)simply modiﬁes the state-action transitions (independent of Q), standard results on convergence fortabular Q-learning (e.g",0
2424,See Section C.4 for its proof,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",0
2425,"Then, the bounds (11) and (12) hold with",See Section C.4 for its proof,1
2426,See Section C.4 for its proof,"Papernot, P",0
2427,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",See Section C.4 for its proof,1
2428,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
2429,Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,"For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",0
2430,See Section C.4 for its proof,See Section C.4 for its proof,1
2431,"Then, the bounds (11) and (12) hold with",See Section C.4 for its proof,1
2432,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty","Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",0
2433,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",See Section C.4 for its proof,1
2434,See Section C.4 for its proof,See Section C.4 for its proof,1
2435,See Section C.4 for its proof,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,0
2436,See Section C.4 for its proof,"All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training",0
2437,"Then, the bounds (11) and (12) hold with",See Section C.4 for its proof,1
2438,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1","Then there exists θ such that
this optimization problem is also NP-hard.",0
2439,Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,See Section C.4 for its proof,1
2440,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",See Section C.4 for its proof,1
2441,Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,See Section C.4 for its proof,1
2442,See Section C.4 for its proof,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",0
2443,"Then, the bounds (11) and (12) hold with",See Section C.4 for its proof,1
2444,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty","NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",0
2445,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty",The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,0
2446,"Then, the bounds (11) and (12) hold with",See Section C.4 for its proof,1
2447,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",See Section C.4 for its proof,1
2448,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1","All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training",0
2449,"Then, the bounds (11) and (12) hold with",See Section C.4 for its proof,1
2450,"Then, the bounds (11) and (12) hold with","Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",0
2451,"Then, the bounds (11) and (12) hold with","Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",0
2452,See Section C.4 for its proof,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",0
2453,"Then, the bounds (11) and (12) hold with","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",0
2454,See Section C.4 for its proof,See Section C.4 for its proof,1
2455,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
2456,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",See Section C.4 for its proof,1
2457,"Then, the bounds (11) and (12) hold with","From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",0
2458,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",See Section C.4 for its proof,1
2459,See Section C.4 for its proof,In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,0
2460,"Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞",See Section C.4 for its proof,1
2461,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty","Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes",0
2462,"Then, the bounds (11) and (12) hold with","For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",0
2463,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty",Explicit distributional robustness of the form (5) is intractable except in limited cases,0
2464,Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",0
2465,Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,See Section C.4 for its proof,1
2466,Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,", K}",0
2467,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty",See Section C.4 for its proof,1
2468,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",0
2469,"Then, the bounds (11) and (12) hold with",See Section C.4 for its proof,1
2470,See Section C.4 for its proof,"In most scenarios, however,it is reasonable to defend against weaker adversaries that instead perturb inﬂuential features more.We consider this setting and train against (cid:107)·(cid:107)2-norm attacks",0
2471,"Then, the bounds (11) and (12) hold with",the adversarial perturbation level adv,0
2472,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
2473,"Then, the bounds (11) and (12) hold with","In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp",0
2474,"Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞","From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have",0
2475,"Then, the bounds (11) and (12) hold with","Ried-miller, A",0
2476,"Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞",See Section C.4 for its proof,1
2477,See Section C.4 for its proof,See Section C.4 for its proof,1
2478,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty",See Section C.4 for its proof,1
2479,Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument",0
2480,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",See Section C.4 for its proof,1
2481,"Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞","For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",0
2482,"Then, the bounds (11) and (12) hold with",Swami,0
2483,"Then, the bounds (11) and (12) hold with",(f) Test error vs,0
2484,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty",JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,0
2485,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty","We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
2486,"Then, the bounds (11) and (12) hold with","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",0
2487,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty",adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,0
2488,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",See Section C.4 for its proof,1
2489,See Section C.4 for its proof,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R",0
2490,"Then, the bounds (11) and (12) hold with",Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,0
2491,Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞",0
2492,"Then, the bounds (11) and (12) hold with",See Section C.4 for its proof,1
2493,See Section C.4 for its proof,See Section C.4 for its proof,1
2494,"Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞",See Section C.4 for its proof,1
2495,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty",adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
2496,See Section C.4 for its proof,"Then, the bounds (11) and (12) hold with",0
2497,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty","We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",0
2498,"Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1","If f is inf-compact, then ¯f is directionally differentiable with",0
2499,"Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞","The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)",0
2500,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2501,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",We observe that similar trends as inSection A.5.1 hold again.,0
2502,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2503,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",(a) and (b) show test misclassiﬁcation error vs,0
2504,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2505,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2506,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2507,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ",0
2508,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",0
2509,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",K,0
2510,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2511,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2512,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ",0
2513,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2514,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2515,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2516,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",0
2517,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
2518,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2519,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2520,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.",0
2521,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,0
2522,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping",0
2523,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",0
2524,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2525,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","In this section, we give an adapation ofthe results in Sections 2 and 3 (Theorems 2 and 4) to such scenarios",0
2526,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2527,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","Indeed, inthe large-perturbation regime, efﬁciently training certiﬁably secure systems remains an importantopen question",0
2528,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","Indeed, inthe large-perturbation regime, efﬁciently training certiﬁably secure systems remains an importantopen question",0
2529,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",Swami,0
2530,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2531,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2532,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",0
2533,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",Training data are shown in blue and red,0
2534,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2535,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2536,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2537,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2538,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","For large γ, we can again solve problem (22) efﬁciently using gradient descent",0
2539,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2540,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2541,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2542,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2543,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",Since 70% of the data are of the,0
2544,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",Robust MDP’s consider an ambiguity set Psa for state-action transitions,0
2545,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2546,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2547,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","Fredrikson, Z",0
2548,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),0
2549,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2550,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2551,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2552,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2553,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2554,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2555,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2556,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","The NP-hardness of certifying robustness for ReLU networks, coupled withour empirical success and theoretical certiﬁcates for smooth networks in deep learning, suggest thatusing smooth networks may be preferable if we wish to guarantee robustness",0
2557,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2558,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",0
2559,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave",0
2560,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",0
2561,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",0
2562,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2563,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
2564,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2565,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2566,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2567,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",Robust MDP’s consider an ambiguity set Psa for state-action transitions,0
2568,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),0
2569,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2570,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",Frossard,0
2571,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","Left column:Euclidean-norm attacks, right column: ∞-norm attacks",0
2572,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",(16)) in θ,0
2573,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2574,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,0
2575,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section",0
2576,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2577,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2578,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2579,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
2580,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",0
2581,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",0
2582,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2583,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2584,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)",0
2585,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2586,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2587,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2588,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data",0
2589,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2590,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2591,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2592,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2593,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2594,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2595,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,0
2596,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","Ried-miller, A",0
2597,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",0
2598,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",1
2599,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,0
2600,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2601,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test","Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)",0
2602,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2603,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,0
2604,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2605,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,"Jha, and A",0
2606,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2607,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2608,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution","The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)",0
2609,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2610,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2611,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2612,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,0
2613,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",0
2614,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution","Indeed, inthe large-perturbation regime, efﬁciently training certiﬁably secure systems remains an importantopen question",0
2615,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2616,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
2617,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,"WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ",0
2618,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",0
2619,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2620,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2621,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,the adversarial perturbation level adv,0
2622,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2623,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,(16)) in θ,0
2624,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2625,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2626,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",0
2627,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2628,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2629,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2630,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2631,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2632,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2633,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The below general duality result gives Proposition 1 as an immediate specialcase,0
2634,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2635,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2636,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",See Section C.4 for its proof,0
2637,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution","McDaniel, S",0
2638,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test","As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",0
2639,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2640,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2641,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution","The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds",0
2642,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2643,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2644,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2645,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test","Namely, we draw the nominal",0
2646,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test","Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",0
2647,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2648,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2649,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2650,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
2651,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test","Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",0
2652,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,0
2653,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",S.-M,0
2654,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2655,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2656,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2657,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,"When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ",0
2658,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2659,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2660,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2661,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2662,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2663,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2664,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test","Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",0
2665,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2666,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2667,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2668,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2669,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,"Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",0
2670,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2671,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test","Fidjeland, G",0
2672,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2673,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2674,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2675,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2676,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2677,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2678,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2679,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2680,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2681,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2682,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution","Due to the nonstationary, policy-dependent radius for the Wassersteinball, an analogous  for the fast-gradient method (or other variants) is not well-deﬁned",0
2683,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",0
2684,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2685,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),0
2686,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
2687,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution","For imperceptible perturbations, our method matches or outperformsheuristic approaches.",0
2688,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,"Then, with probability at leastner, 1996, Ch",0
2689,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]",0
2690,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution","For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM",0
2691,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",0
2692,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",Attacks on the MNIST dataset,0
2693,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2694,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2695,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2696,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",0
2697,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2698,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,1
2699,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution","We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",0
2700,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2701,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2702,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2703,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2704,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2705,"for all γ, ρ ≥ 0",We illustrate test misclassiﬁcation error vs,0
2706,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2707,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2708,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2709,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",Other models do not exhibit this behavior with the same consistency(if at all),0
2710,"for all γ, ρ ≥ 0","These works provide an initial foundation for adversarial training, but itis challenging to rigorously identify the classes of attacks against which they can defend (or if theyexist)",0
2711,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2712,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2713,"for all γ, ρ ≥ 0",adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,0
2714,"for all γ, ρ ≥ 0",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,0
2715,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",0
2716,"for all γ, ρ ≥ 0",The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),0
2717,"for all γ, ρ ≥ 0",Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,0
2718,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2719,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",Theorem 7,0
2720,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2721,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2722,"for all γ, ρ ≥ 0","PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",0
2723,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2724,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2725,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2726,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2727,"for all γ, ρ ≥ 0","Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",0
2728,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)",0
2729,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2730,"for all γ, ρ ≥ 0",Training data are shown in blue and red,0
2731,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),0
2732,"for all γ, ρ ≥ 0","Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks",0
2733,"for all γ, ρ ≥ 0","Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)",0
2734,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2735,"for all γ, ρ ≥ 0","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",0
2736,"for all γ, ρ ≥ 0","Thus, weonly compare with an agent trained on the nominal MDP",0
2737,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)",0
2738,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",Let ∆F ≥ F (θ0) − inf θ F (θ),0
2739,"for all γ, ρ ≥ 0","Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",0
2740,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2741,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2742,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2743,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2744,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2745,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","Thus, weonly compare with an agent trained on the nominal MDP",0
2746,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","Namely, we draw the nominal",0
2747,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2748,"for all γ, ρ ≥ 0",the adversarial perturbation level adv,0
2749,"for all γ, ρ ≥ 0","14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping",0
2750,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2751,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2752,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",0
2753,"for all γ, ρ ≥ 0",Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,0
2754,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2755,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training",0
2756,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2757,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2758,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2759,"for all γ, ρ ≥ 0",adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,0
2760,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2761,"for all γ, ρ ≥ 0","Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)",0
2762,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2763,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2764,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","With that said, the strong dualityresult, Proposition 1, still applies to any distribution",0
2765,"for all γ, ρ ≥ 0",M,0
2766,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",0
2767,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2768,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2769,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2770,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2771,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2772,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2773,"for all γ, ρ ≥ 0","For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",0
2774,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2775,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2776,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2777,"for all γ, ρ ≥ 0",See Section C.2 for the proof,0
2778,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2779,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
2780,"for all γ, ρ ≥ 0","Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",0
2781,"for all γ, ρ ≥ 0",(c) Test error vs,0
2782,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,0
2783,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",0
2784,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2785,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2786,"for all γ, ρ ≥ 0",We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,0
2787,"for all γ, ρ ≥ 0",Theboundaries are shown with the training data as well as separately with the true class boundaries.,0
2788,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",K,0
2789,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",The limitations ofdeep learning in adversarial settings,0
2790,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
2791,"for all γ, ρ ≥ 0","(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features",0
2792,"for all γ, ρ ≥ 0","Then, with probability at leastner, 1996, Ch",0
2793,"for all γ, ρ ≥ 0","For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",0
2794,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",See Section C.2 for the proof,0
2795,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2796,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2797,"for all γ, ρ ≥ 0","for all γ, ρ ≥ 0",1
2798,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.","for all γ, ρ ≥ 0",1
2799,"for all γ, ρ ≥ 0","In this regime(small γ, large ), performance between WRM and other methods diverge",0
2800,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2801,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,0
2802,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,0
2803,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2804,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2805,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2806,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4",Further attacks on the MNIST dataset,0
2807,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost",Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,0
2808,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2809,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2810,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2811,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples","Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",0
2812,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples","Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",0
2813,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2814,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2815,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples","Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞",0
2816,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2817,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2818,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2819,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4",adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,0
2820,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",0
2821,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples",All of our resultssuitably generalize to this setting with minor modiﬁcations to the robust surrogate (2b) and theabove assumptions (see Section D),0
2822,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",0
2823,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2824,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2825,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts",0
2826,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2827,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2828,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",0
2829,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2830,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",0
2831,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2832,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples","Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",0
2833,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2834,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2835,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain",0
2836,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples",Let ∆F ≥ F (θ0) − inf θ F (θ),0
2837,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2838,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2839,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples","Then, we have",0
2840,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples",PGM attacks on the MNIST dataset,0
2841,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2842,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",2.2) gives the result.,0
2843,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",0
2844,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2845,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks",0
2846,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2847,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4",We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,0
2848,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4",Dziugaite and D,0
2849,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","By appropriatescaling of θ, v, and w, Katz et al",0
2850,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples",is closed-valued and measurable,0
2851,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","Fawzi, and P",0
2852,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",0
2853,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",Experimental results on synthetic data,0
2854,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2855,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples","Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",0
2856,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2857,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2858,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2859,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost",We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,0
2860,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2861,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",0
2862,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2863,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4",Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,0
2864,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","Fredrikson, Z",0
2865,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
2866,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2867,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost",JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,0
2868,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2869,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ",0
2870,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",0
2871,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","We note thattudes of ∇z(cid:96)(θ; z) in a neighborhood of the nominal input, which ensures stability of the model.",0
2872,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2873,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2874,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","Top row: PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",0
2875,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples","Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",0
2876,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2877,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms",0
2878,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples","By appropriatescaling of θ, v, and w, Katz et al",0
2879,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",0
2880,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.",0
2881,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)",0
2882,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2883,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2884,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2885,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","Indeed, inthe large-perturbation regime, efﬁciently training certiﬁably secure systems remains an importantopen question",0
2886,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","arXiv:1505.05116 [math.OC],2015.",0
2887,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2888,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2889,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","In this regime(small γ, large ), performance between WRM and other methods diverge",0
2890,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2891,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",0
2892,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",0
2893,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2894,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2895,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",0
2896,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2897,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples","From Lemma 5, our previous results (Theorems 2 and 4) follow",0
2898,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2899,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",1
2900,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2901,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1","The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",0
2902,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",0
2903,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
2904,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2905,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,"2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",0
2906,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,0
2907,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,0
2908,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)","Silver, A",0
2909,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2910,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2911,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0",0
2912,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2913,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty",0
2914,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
2915,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,"Graves, M",0
2916,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,"Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)",0
2917,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,"The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds",0
2918,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2919,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2920,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2921,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2922,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2923,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2924,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2925,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1","Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
2926,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,"Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)",0
2927,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2928,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",0
2929,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2930,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1","Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ",0
2931,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,372–387,0
2932,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)","for all γ, ρ ≥ 0",0
2933,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2934,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",All of our resultssuitably generalize to this setting with minor modiﬁcations to the robust surrogate (2b) and theabove assumptions (see Section D),0
2935,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2936,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2937,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,"Nature, 518(7540):529–533, 2015.",0
2938,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,It isthus important to distinguish the methods’ abilities to combat attacks,0
2939,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2940,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1","Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",0
2941,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,Further attacks on the MNIST dataset,0
2942,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2943,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2944,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2945,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,the adversarial perturbation level adv,0
2946,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,Attacks on the MNIST dataset,0
2947,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2948,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1","Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
2949,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)","Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",0
2950,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,We observe that similar trends as inSection A.5.1 hold again.,0
2951,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2952,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",0
2953,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",0
2954,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1","Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",0
2955,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
2956,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2957,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1","Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness",0
2958,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2959,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)","In Security and Privacy (SP), 2016 IEEE Symposiumon, pp",0
2960,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),0
2961,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,"(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c",0
2962,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",G,0
2963,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2964,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2965,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2966,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
2967,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2968,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)","Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
2969,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2970,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1","The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
2971,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2972,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
2973,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2974,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,"Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",0
2975,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2976,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1","Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)",0
2977,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)","On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
2978,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2979,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,We illustrate test misclassiﬁcationerror vs,0
2980,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,"Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B",0
2981,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2982,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",0
2983,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2984,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2985,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2986,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",large desired robustness values) our approach becomes a heuristic just like the other approaches.,0
2987,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2988,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)","The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",0
2989,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",0
2990,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1","Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",0
2991,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2992,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Thestatistical error term n(t) is omitted from the certiﬁcate,0
2993,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)","Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",0
2994,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2995,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2996,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2997,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
2998,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,0
2999,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,1
3000,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features","for t = 1, ",1
3001,"Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness",14.27 and Prop,0
3002,In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",0
3003,", Tadv, where Π denotes projection onto B,p(xi) := {x : (cid:107)x − xi(cid:107)p ≤ }.The adversarial training literature (e.g","Celik, and A",0
3004,"WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.","for t = 1, ",1
3005,"WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.","for t = 1, ",1
3006,"for t = 1, ","for t = 1, ",1
3007,In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,"for t = 1, ",1
3008,"Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞","for t = 1, ",1
3009,"In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets","for t = 1, ",1
3010,Goodfellow et al,The vertical bar indicates the achieved level,0
3011,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features","for t = 1, ",1
3012,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features",AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
3013,"for t = 1, ","6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",0
3014,"In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets","By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",0
3015,Goodfellow et al,"Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)",0
3016,In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,"for t = 1, ",1
3017,In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",0
3018,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features","for t = 1, ",1
3019,"for t = 1, ","for t = 1, ",1
3020,", Tadv, where Π denotes projection onto B,p(xi) := {x : (cid:107)x − xi(cid:107)p ≤ }.The adversarial training literature (e.g","Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B",0
3021,"Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness","Replacing our current covering num-ber arguments with more intricate notions such as margin-based bounds (Bartlett et al., 2017) wouldextend the scope and usefulness of our theoretical guarantees",0
3022,"Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",0
3023,"In most scenarios, however,it is reasonable to defend against weaker adversaries that instead perturb inﬂuential features more.We consider this setting and train against (cid:107)·(cid:107)2-norm attacks",Figure 3,0
3024,"Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness","This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd",0
3025,", Tadv, where Π denotes projection onto B,p(xi) := {x : (cid:107)x − xi(cid:107)p ≤ }.The adversarial training literature (e.g",Deﬁne the scale parameter βt > 0 by,0
3026,"Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness",Since 70% of the data are of the,0
3027,Goodfellow et al,"for t = 1, ",1
3028,"In most scenarios, however,it is reasonable to defend against weaker adversaries that instead perturb inﬂuential features more.We consider this setting and train against (cid:107)·(cid:107)2-norm attacks","Thus, we see that our adversarial-training method defends against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint untilthe model misclassiﬁes it",0
3029,"In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,0
3030,"In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets","for t = 1, ",1
3031,"In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries","Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective",0
3032,"In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets","(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c",0
3033,"Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness","Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",0
3034,"Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness","for t = 1, ",1
3035,"In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",K,0
3036,"WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.","for t = 1, ",1
3037,"Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞",We providea principled method for efﬁciently guaranteeing distributional robustness with a simple form ofadversarial data perturbation,0
3038,"Finally, we consider a reinforcement learning problemin Section 4.3, where the Markov decision process used for training differs from that for testing.WRM enjoys the theoretical guarantees of Sections 2 and 3 for large γ, but for small γ (large adver-sarial budgets), WRM becomes a heuristic like other methods","for t = 1, ",1
3039,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features",Dziugaite and D,0
3040,"In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries","for t = 1, ",1
3041,"for t = 1, ","for t = 1, ",1
3042,"In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries","for t = 1, ",1
3043,"In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets","Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)",0
3044,"In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries","Papernot, P",0
3045,", Tadv, where Π denotes projection onto B,p(xi) := {x : (cid:107)x − xi(cid:107)p ≤ }.The adversarial training literature (e.g","Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)",0
3046,"In most scenarios, however,it is reasonable to defend against weaker adversaries that instead perturb inﬂuential features more.We consider this setting and train against (cid:107)·(cid:107)2-norm attacks","For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",0
3047,"In most scenarios, however,it is reasonable to defend against weaker adversaries that instead perturb inﬂuential features more.We consider this setting and train against (cid:107)·(cid:107)2-norm attacks","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
3048,"Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞","for t = 1, ",1
3049,"Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞","for t = 1, ",1
3050,"Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness","for t = 1, ",1
3051,"In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets","for t = 1, ",1
3052,In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,"for t = 1, ",1
3053,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features","On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
3054,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,0
3055,"Finally, we consider a reinforcement learning problemin Section 4.3, where the Markov decision process used for training differs from that for testing.WRM enjoys the theoretical guarantees of Sections 2 and 3 for large γ, but for small γ (large adver-sarial budgets), WRM becomes a heuristic like other methods","The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3",0
3056,"In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries","for t = 1, ",1
3057,In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,"for t = 1, ",1
3058,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features","Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks",0
3059,"Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,0
3060,In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,"for t = 1, ",1
3061,", Tadv, where Π denotes projection onto B,p(xi) := {x : (cid:107)x − xi(cid:107)p ≤ }.The adversarial training literature (e.g","Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",0
3062,"In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries","for t = 1, ",1
3063,"WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.","for t = 1, ",1
3064,"Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness",by solving (7),0
3065,Goodfellow et al,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,0
3066,In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
3067,", Tadv, where Π denotes projection onto B,p(xi) := {x : (cid:107)x − xi(cid:107)p ≤ }.The adversarial training literature (e.g","for t = 1, ",1
3068,Goodfellow et al,"(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D",0
3069,"In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets","Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞",0
3070,Goodfellow et al,"for t = 1, ",1
3071,"In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries","for t = 1, ",1
3072,"Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞","for t = 1, ",1
3073,"for t = 1, ","(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)",0
3074,Goodfellow et al,"for t = 1, ",1
3075,"In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets",Attacks on the MNIST dataset,0
3076,"Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness","for t = 1, ",1
3077,"Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞","for t = 1, ",1
3078,"In most scenarios, however,it is reasonable to defend against weaker adversaries that instead perturb inﬂuential features more.We consider this setting and train against (cid:107)·(cid:107)2-norm attacks","We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1",0
3079,"In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries","for t = 1, ",1
3080,"In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets","for t = 1, ",1
3081,"In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets","for t = 1, ",1
3082,"In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries","for t = 1, ",1
3083,"WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.","for t = 1, ",1
3084,Goodfellow et al,"for t = 1, ",1
3085,", Tadv, where Π denotes projection onto B,p(xi) := {x : (cid:107)x − xi(cid:107)p ≤ }.The adversarial training literature (e.g","for t = 1, ",1
3086,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features","for t = 1, ",1
3087,"Finally, we consider a reinforcement learning problemin Section 4.3, where the Markov decision process used for training differs from that for testing.WRM enjoys the theoretical guarantees of Sections 2 and 3 for large γ, but for small γ (large adver-sarial budgets), WRM becomes a heuristic like other methods",Distillation as a defense to adversarialperturbations against deep neural networks,0
3088,"WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.","The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time",0
3089,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",0
3090,"WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",0
3091,"WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.","Then, we have",0
3092,"Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness","(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D",0
3093,"for t = 1, ","for t = 1, ",1
3094,"WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.",adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,0
3095,", Tadv, where Π denotes projection onto B,p(xi) := {x : (cid:107)x − xi(cid:107)p ≤ }.The adversarial training literature (e.g","WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",0
3096,"Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞","Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv",0
3097,", Tadv, where Π denotes projection onto B,p(xi) := {x : (cid:107)x − xi(cid:107)p ≤ }.The adversarial training literature (e.g","for t = 1, ",1
3098,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features","2574–2582, 2016.",0
3099,"Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness",Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,0
3100,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3101,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","We compare standard WRM with ∞-norm PGM, FGM,IFGM",0
3102,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3103,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use",Further attacks on the MNIST dataset,0
3104,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",0
3105,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use",We typically solve the penalty problem (2) with P0 replaced,0
3106,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3107,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3108,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",0
3109,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",0
3110,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","Left column:Euclidean-norm attacks, right column: ∞-norm attacks",0
3111,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",0
3112,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3113,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3114,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM",JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,0
3115,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3116,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM",Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,0
3117,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)",14.27 and Prop,0
3118,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3119,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3120,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,0
3121,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3122,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3123,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)",0
3124,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",0
3125,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3126,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)",We illustrate test misclassiﬁcationerror vs,0
3127,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3128,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",the adversarial perturbation level adv,0
3129,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3130,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",0
3131,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3132,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","Left column:Euclidean-norm attacks, right column: ∞-norm attacks",0
3133,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",Deﬁne the scale parameter βt > 0 by,0
3134,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3135,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
3136,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)",The major beneﬁt of our approach is its simplicityand wide applicability across many models and machine-learning scenarios.There remain many avenues for future investigation,0
3137,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","For worst-case regions P formed by Wasserstein balls, Esfahani & Kuhn (2015),Shaﬁeezadeh-Abadeh et al",0
3138,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
3139,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",Weillustrate test misclassiﬁcation error vs,0
3140,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3141,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",0
3142,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3143,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM","Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",0
3144,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3145,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM",Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,0
3146,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3147,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3148,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)",The major beneﬁt of our approach is its simplicityand wide applicability across many models and machine-learning scenarios.There remain many avenues for future investigation,0
3149,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3150,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM","Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",0
3151,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3152,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3153,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM",We illustrate test misclassiﬁcationerror vs,0
3154,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use",Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,0
3155,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3156,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3157,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM","Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM",0
3158,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","The NP-hardness of certifying robustness for ReLU networks, coupled withour empirical success and theoretical certiﬁcates for smooth networks in deep learning, suggest thatusing smooth networks may be preferable if we wish to guarantee robustness",0
3159,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3160,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",0
3161,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use",Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,0
3162,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3163,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3164,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv",0
3165,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3166,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
3167,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use",The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,0
3168,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",0
3169,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","Graves, M",0
3170,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",0
3171,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",0
3172,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","Ried-miller, A",0
3173,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks",0
3174,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3175,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",0
3176,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3177,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","Celik, and A",0
3178,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3179,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",G,0
3180,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3181,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3182,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3183,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)",The below general duality result gives Proposition 1 as an immediate specialcase,0
3184,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM","The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",0
3185,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3186,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3187,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",Weillustrate test misclassiﬁcation error vs,0
3188,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3189,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
3190,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
3191,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3192,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM",from which it is easy to see that f is inf-compact,0
3193,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3194,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use",Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,0
3195,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)",the adversarial perturbation level adv,0
3196,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3197,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)","The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
3198,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3199,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",1
3200,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",0
3201,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3202,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv",0
3203,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3204,Since 70% of the data are of the,"For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",0
3205,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",0
3206,Since 70% of the data are of the,See Section C.2 for the proof,0
3207,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3208,Since 70% of the data are of the,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",0
3209,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
3210,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3211,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)",0
3212,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3213,Since 70% of the data are of the,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",0
3214,Since 70% of the data are of the,"The vertical bar in (a), (c), and (e) indicates the perturbation level that was",0
3215,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","Thus, weonly compare with an agent trained on the nominal MDP",0
3216,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3217,Since 70% of the data are of the,"In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp",0
3218,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3219,Since 70% of the data are of the,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",0
3220,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3221,Since 70% of the data are of the,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
3222,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3223,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,0
3224,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3225,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",Explicit distributional robustness of the form (5) is intractable except in limited cases,0
3226,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3227,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
3228,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",0
3229,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3230,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3231,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3232,Since 70% of the data are of the,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",0
3233,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3234,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3235,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","Ostrovski, et al",0
3236,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3237,Since 70% of the data are of the,Goodfellow et al,0
3238,Since 70% of the data are of the,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features",0
3239,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3240,Since 70% of the data are of the,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",0
3241,Since 70% of the data are of the,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",0
3242,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3243,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3244,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3245,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3246,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3247,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3248,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3249,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3250,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3251,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3252,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",This result is essentially due to Katz et al.(2017a),0
3253,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3254,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness",0
3255,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3256,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3257,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3258,Since 70% of the data are of the,"For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",0
3259,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3260,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3261,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3262,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3263,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",0
3264,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3265,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3266,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3267,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3268,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3269,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",We assume without loss of2 − z0 (cid:54)= 0,0
3270,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3271,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",0
3272,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3273,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3274,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3275,Since 70% of the data are of the,by solving (7),0
3276,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3277,Since 70% of the data are of the,2.2) gives the result.,0
3278,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
3279,Since 70% of the data are of the,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features",0
3280,Since 70% of the data are of the,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",0
3281,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3282,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",the adversarial perturbation level adv,0
3283,Since 70% of the data are of the,"Mnih, K",0
3284,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3285,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3286,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3287,Since 70% of the data are of the,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",0
3288,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3289,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",is closed-valued and measurable,0
3290,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",0
3291,Since 70% of the data are of the,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
3292,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3293,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets",0
3294,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3295,Since 70% of the data are of the,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)",0
3296,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3297,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3298,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3299,Since 70% of the data are of the,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",1
3300,Training data are shown in blue and red,Figure 1,1
3301,Figure 1,Figure 1,1
3302,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",large desired robustness values) our approach becomes a heuristic just like the other approaches.,0
3303,Training data are shown in blue and red,"Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",0
3304,Experimental results on synthetic data,Figure 1,1
3305,Theboundaries are shown with the training data as well as separately with the true class boundaries.,Figure 1,1
3306,Training data are shown in blue and red,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",0
3307,Theboundaries are shown with the training data as well as separately with the true class boundaries.,Figure 1,1
3308,Theboundaries are shown with the training data as well as separately with the true class boundaries.,Figure 1,1
3309,Figure 1,Figure 1,1
3310,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",Figure 1,1
3311,Training data are shown in blue and red,Figure 1,1
3312,Training data are shown in blue and red,Figure 1,1
3313,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",Figure 1,1
3314,Experimental results on synthetic data,"Recalling Rockafellar & Wets (1998, Def",0
3315,Experimental results on synthetic data,Figure 1,1
3316,Figure 1,Figure 1,1
3317,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",Figure 1,1
3318,Figure 1,Figure 1,1
3319,Experimental results on synthetic data,Figure 1,1
3320,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively","All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training",0
3321,Experimental results on synthetic data,Figure 1,1
3322,Experimental results on synthetic data,Figure 1,1
3323,Theboundaries are shown with the training data as well as separately with the true class boundaries.,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",0
3324,Theboundaries are shown with the training data as well as separately with the true class boundaries.,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",0
3325,Figure 1,Figure 1,1
3326,Training data are shown in blue and red,"If f is inf-compact, then ¯f is directionally differentiable with",0
3327,Training data are shown in blue and red,Figure 1,1
3328,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",Figure 1,1
3329,Theboundaries are shown with the training data as well as separately with the true class boundaries.,Figure 1,1
3330,Experimental results on synthetic data,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
3331,Experimental results on synthetic data,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
3332,Experimental results on synthetic data,Figure 1,1
3333,Training data are shown in blue and red,"(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D",0
3334,Figure 1,G,0
3335,Experimental results on synthetic data,Figure 1,1
3336,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively","In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks",0
3337,Experimental results on synthetic data,"Ostrovski, et al",0
3338,Theboundaries are shown with the training data as well as separately with the true class boundaries.,"Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv",0
3339,Experimental results on synthetic data,Figure 1,1
3340,Training data are shown in blue and red,Figure 1,1
3341,Figure 1,The action space is binary: push the cartleft or right with a ﬁxed force,0
3342,Figure 1,Figure 1,1
3343,Figure 1,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",0
3344,Figure 1,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,0
3345,Training data are shown in blue and red,Figure 1,1
3346,Figure 1,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,0
3347,Experimental results on synthetic data,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",0
3348,Figure 1,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),0
3349,Experimental results on synthetic data,Figure 1,1
3350,Figure 1,Figure 1,1
3351,Theboundaries are shown with the training data as well as separately with the true class boundaries.,"We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",0
3352,Theboundaries are shown with the training data as well as separately with the true class boundaries.,"In this regime(small γ, large ), performance between WRM and other methods diverge",0
3353,Theboundaries are shown with the training data as well as separately with the true class boundaries.,Figure 1,1
3354,Experimental results on synthetic data,Figure 1,1
3355,Figure 1,"These works provide an initial foundation for adversarial training, but itis challenging to rigorously identify the classes of attacks against which they can defend (or if theyexist)",0
3356,Training data are shown in blue and red,Figure 1,1
3357,Experimental results on synthetic data,"Thus, theoptimization problem is NP-hard.",0
3358,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",Figure 1,1
3359,Theboundaries are shown with the training data as well as separately with the true class boundaries.,Figure 1,1
3360,Experimental results on synthetic data,Figure 1,1
3361,Theboundaries are shown with the training data as well as separately with the true class boundaries.,Figure 1,1
3362,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",Figure 1,1
3363,Training data are shown in blue and red,Figure 1,1
3364,Figure 1,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",0
3365,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively","On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
3366,Theboundaries are shown with the training data as well as separately with the true class boundaries.,Figure 1,1
3367,Theboundaries are shown with the training data as well as separately with the true class boundaries.,582–597,0
3368,Training data are shown in blue and red,Figure 1,1
3369,Figure 1,Figure 1,1
3370,Theboundaries are shown with the training data as well as separately with the true class boundaries.,Figure 1,1
3371,Figure 1,"IEEE, 2016c.",0
3372,Figure 1,"Veness, M",0
3373,Theboundaries are shown with the training data as well as separately with the true class boundaries.,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",0
3374,Training data are shown in blue and red,Figure 1,1
3375,Theboundaries are shown with the training data as well as separately with the true class boundaries.,"Celik, and A",0
3376,Training data are shown in blue and red,Figure 1,1
3377,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",0
3378,Figure 1,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",0
3379,Theboundaries are shown with the training data as well as separately with the true class boundaries.,Figure 1,1
3380,Training data are shown in blue and red,Figure 1,1
3381,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively","Rusu, J",0
3382,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively","In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts",0
3383,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",Figure 1,1
3384,Figure 1,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",0
3385,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively","We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms",0
3386,Experimental results on synthetic data,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
3387,Training data are shown in blue and red,Figure 1,1
3388,Theboundaries are shown with the training data as well as separately with the true class boundaries.,Figure 1,1
3389,Figure 1,"Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)",0
3390,Training data are shown in blue and red,"IEEE, 2016b.",0
3391,Theboundaries are shown with the training data as well as separately with the true class boundaries.,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",0
3392,Theboundaries are shown with the training data as well as separately with the true class boundaries.,Figure 1,1
3393,Experimental results on synthetic data,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",0
3394,Experimental results on synthetic data,Figure 1,1
3395,Theboundaries are shown with the training data as well as separately with the true class boundaries.,Training data are shown in blue and red,0
3396,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",Figure 1,1
3397,Experimental results on synthetic data,"Consider feedforward neural networks with ReLU’s and let U := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U (cid:96)(θ; z + u) is NPO",0
3398,Figure 1,"By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data",0
3399,Theboundaries are shown with the training data as well as separately with the true class boundaries.,In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,0
3400,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
3401,We omit the certiﬁcate’s error,Figure 2,1
3402,We omit the certiﬁcate’s error,Figure 2,1
3403,We omit the certiﬁcate’s error,Figure 2,1
3404,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Our optimization result (Theorem 2) appliesonly for small values of robustness ρ and to a limited class of Wasserstein costs,0
3405,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,"We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5",0
3406,Figure 2,Figure 2,1
3407,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3408,Figure 2,In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,0
3409,We omit the certiﬁcate’s error,Figure 2,1
3410,Figure 2,Dziugaite and D,0
3411,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,G,0
3412,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3413,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,The major beneﬁt of our approach is its simplicityand wide applicability across many models and machine-learning scenarios.There remain many avenues for future investigation,0
3414,Figure 2,Figure 2,1
3415,We omit the certiﬁcate’s error,"Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)",0
3416,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3417,We omit the certiﬁcate’s error,Figure 2,1
3418,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Esfahani and D,0
3419,We omit the certiﬁcate’s error,"Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)",0
3420,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3421,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3422,Figure 2,The action space is binary: push the cartleft or right with a ﬁxed force,0
3423,Figure 2,"In this work, we treat a much larger class of losses and costs and provide directsolution methods for a Lagrangian relaxation of the saddle-point problem (1)",0
3424,We omit the certiﬁcate’s error,Figure 2,1
3425,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3426,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3427,Figure 2,Figure 2,1
3428,Figure 2,Figure 2,1
3429,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3430,Figure 2,Figure 2,1
3431,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3432,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,"Left column: Euclidean-normattacks, right column: ∞-norm attacks",0
3433,We omit the certiﬁcate’s error,Figure 2,1
3434,We omit the certiﬁcate’s error,Attacks on the MNIST dataset,0
3435,Figure 2,Figure 2,1
3436,Figure 2,(e) Test error vs,0
3437,Figure 2,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",0
3438,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,"In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance",0
3439,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3440,We omit the certiﬁcate’s error,Figure 2,1
3441,We omit the certiﬁcate’s error,Figure 2,1
3442,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3443,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3444,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,N,0
3445,We omit the certiﬁcate’s error,Figure 2,1
3446,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,We omit the certiﬁcate’s error,0
3447,We omit the certiﬁcate’s error,Figure 2,1
3448,Figure 2,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
3449,Figure 2,Figure 2,1
3450,We omit the certiﬁcate’s error,Figure 2,1
3451,We omit the certiﬁcate’s error,Figure 2,1
3452,Figure 2,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",0
3453,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3454,We omit the certiﬁcate’s error,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",0
3455,Figure 2,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",0
3456,We omit the certiﬁcate’s error,Figure 2,1
3457,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3458,Figure 2,Figure 2,1
3459,We omit the certiﬁcate’s error,Figure 2,1
3460,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3461,We omit the certiﬁcate’s error,Figure 2,1
3462,We omit the certiﬁcate’s error,We illustrate test misclassiﬁcation error vs.theadversarial perturbation level adv,0
3463,Figure 2,We illustrate test misclassiﬁcationerror vs,0
3464,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,0
3465,Figure 2,Mnih et al,0
3466,We omit the certiﬁcate’s error,Figure 2,1
3467,Figure 2,Figure 2,1
3468,Figure 2,Figure 2,1
3469,We omit the certiﬁcate’s error,Figure 2,1
3470,We omit the certiﬁcate’s error,A,0
3471,We omit the certiﬁcate’s error,Figure 2,1
3472,Figure 2,Figure 2,1
3473,Figure 2,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
3474,Figure 2,Figure 2,1
3475,We omit the certiﬁcate’s error,Figure 2,1
3476,We omit the certiﬁcate’s error,Figure 2,1
3477,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
3478,We omit the certiﬁcate’s error,"The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.",0
3479,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt",0
3480,Figure 2,Our optimization result (Theorem 2) appliesonly for small values of robustness ρ and to a limited class of Wasserstein costs,0
3481,We omit the certiﬁcate’s error,Figure 2,1
3482,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost",0
3483,We omit the certiﬁcate’s error,", K}",0
3484,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3485,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,(f) Test error vs,0
3486,Figure 2,Figure 2,1
3487,Figure 2,Figure 2,1
3488,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,"We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",0
3489,We omit the certiﬁcate’s error,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",0
3490,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,Figure 2,1
3491,Figure 2,Figure 2,1
3492,Figure 2,Figure 2,1
3493,We omit the certiﬁcate’s error,Figure 2,1
3494,Figure 2,Figure 2,1
3495,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",0
3496,We omit the certiﬁcate’s error,"We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms",0
3497,Figure 2,"2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",0
3498,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",0
3499,Figure 2,Figure 2,1
3500,"WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3501,"2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3502,"WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3503,radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),0
3504,"However, WRM withReLU’s still suffers from sensitivities (e.g","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3505,"In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3506,"2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary","WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.",0
3507,"In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s","for all γ, ρ ≥ 0",0
3508,"Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3509,"For each γadv, we consider the distance to adversarial examples in the test dataset","The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3",0
3510,radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,"Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)",0
3511,radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,"2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3512,"WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3513,"For each γadv, we consider the distance to adversarial examples in the test dataset","(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features",0
3514,"However, WRM withReLU’s still suffers from sensitivities (e.g","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",0
3515,radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),0
3516,"2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary","The NP-hardness of certifying robustness for ReLU networks, coupled withour empirical success and theoretical certiﬁcates for smooth networks in deep learning, suggest thatusing smooth networks may be preferable if we wish to guarantee robustness",0
3517,"ERM and FGM suffer from sensitivities to various regions of the data,as evidenced by the lack of symmetry in their classiﬁcation boundaries","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3518,"Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3519,"ERM and FGM suffer from sensitivities to various regions of the data,as evidenced by the lack of symmetry in their classiﬁcation boundaries","The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
3520,"WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ",Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,0
3521,"Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3522,"WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ","We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",0
3523,"In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3524,"Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3525,"However, WRM withReLU’s still suffers from sensitivities (e.g","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3526,"However, WRM withReLU’s still suffers from sensitivities (e.g",S.-M,0
3527,"Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv",Other models do not exhibit this behavior with the same consistency(if at all),0
3528,"However, WRM withReLU’s still suffers from sensitivities (e.g","Szepesv´ari & Littman (1999)) apply under these adversarial dynamics.We test our adversarial training procedure in the cart-pole environment, where the goal is to balancea pole on a cart by moving the cart left or right",0
3529,"For each γadv, we consider the distance to adversarial examples in the test dataset",Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,0
3530,"WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3531,"ERM and FGM suffer from sensitivities to various regions of the data,as evidenced by the lack of symmetry in their classiﬁcation boundaries","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3532,radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",0
3533,"For each γadv, we consider the distance to adversarial examples in the test dataset","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3534,"WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3535,radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,"Then there exists θ such that
this optimization problem is also NP-hard.",0
3536,radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",0
3537,"However, WRM withReLU’s still suffers from sensitivities (e.g",by solving (7),0
3538,"For each γadv, we consider the distance to adversarial examples in the test dataset","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3539,"However, WRM withReLU’s still suffers from sensitivities (e.g",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,0
3540,"WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3541,"In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3542,radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,"2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3543,"For each γadv, we consider the distance to adversarial examples in the test dataset","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3544,"However, WRM withReLU’s still suffers from sensitivities (e.g","If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",0
3545,"For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3546,"In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3547,"For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM","Ostrovski, et al",0
3548,"ERM and FGM suffer from sensitivities to various regions of the data,as evidenced by the lack of symmetry in their classiﬁcation boundaries","Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z",0
3549,"However, WRM withReLU’s still suffers from sensitivities (e.g","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3550,"For each γadv, we consider the distance to adversarial examples in the test dataset","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3551,"For each γadv, we consider the distance to adversarial examples in the test dataset","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3552,"WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3553,"However, WRM withReLU’s still suffers from sensitivities (e.g","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3554,"For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3555,"For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM",M,0
3556,"2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or",0
3557,"However, WRM withReLU’s still suffers from sensitivities (e.g","Bellemare, A",0
3558,"For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM","Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations",0
3559,"In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s",Deepfool: a simple and accurate method to fooldeep neural networks,0
3560,"For each γadv, we consider the distance to adversarial examples in the test dataset","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3561,"However, WRM withReLU’s still suffers from sensitivities (e.g","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3562,"For each γadv, we consider the distance to adversarial examples in the test dataset","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3563,"However, WRM withReLU’s still suffers from sensitivities (e.g",We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,0
3564,"However, WRM withReLU’s still suffers from sensitivities (e.g",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
3565,"Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3566,"For each γadv, we consider the distance to adversarial examples in the test dataset","This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al",0
3567,"ERM and FGM suffer from sensitivities to various regions of the data,as evidenced by the lack of symmetry in their classiﬁcation boundaries","Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)",0
3568,"For each γadv, we consider the distance to adversarial examples in the test dataset","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3569,"For each γadv, we consider the distance to adversarial examples in the test dataset",Frossard,0
3570,radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,"2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3571,"For each γadv, we consider the distance to adversarial examples in the test dataset",the adversarial perturbationlevel adv,0
3572,"However, WRM withReLU’s still suffers from sensitivities (e.g","Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv",0
3573,"In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s","Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
3574,"2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary","Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",0
3575,"ERM and FGM suffer from sensitivities to various regions of the data,as evidenced by the lack of symmetry in their classiﬁcation boundaries","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3576,"Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv","To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section",0
3577,"In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s","Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z",0
3578,"In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s","For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering",0
3579,"WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ","Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",0
3580,"2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3581,"In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3582,"For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM","9–10)), we have",0
3583,radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),0
3584,"WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",0
3585,radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,"2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3586,"For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3587,"2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary","Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",0
3588,"2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3589,"ERM and FGM suffer from sensitivities to various regions of the data,as evidenced by the lack of symmetry in their classiﬁcation boundaries","Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
3590,"For each γadv, we consider the distance to adversarial examples in the test dataset","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",0
3591,"For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3592,"For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3593,"For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3594,"In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",1
3595,"Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
3596,"ERM and FGM suffer from sensitivities to various regions of the data,as evidenced by the lack of symmetry in their classiﬁcation boundaries",adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,0
3597,"Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",0
3598,"For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",0
3599,"2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,0
3600,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
3601,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.",0
3602,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",(e) Test error vs,0
3603,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3604,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",with the convention that 0 · ∞ = 0,0
3605,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3606,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3607,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",0
3608,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",0
3609,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3610,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3611,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3612,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3613,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3614,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",0
3615,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3616,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3617,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3618,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3619,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3620,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",Theorem 7,0
3621,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞",0
3622,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",0
3623,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),0
3624,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3625,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3626,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3627,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3628,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3629,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping",0
3630,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",0
3631,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)",0
3632,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3633,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3634,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
3635,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,0
3636,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3637,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3638,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",0
3639,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)",0
3640,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,0
3641,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3642,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3643,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data",0
3644,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",The,0
3645,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",Theorem 7,0
3646,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3647,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations",0
3648,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3649,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3650,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",0
3651,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",The action space is binary: push the cartleft or right with a ﬁxed force,0
3652,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3653,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3654,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3655,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3656,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",(e) Test error vs,0
3657,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3658,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3659,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3660,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",Weillustrate test misclassiﬁcation error vs,0
3661,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",Weillustrate test misclassiﬁcation error vs,0
3662,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,0
3663,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3664,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",0
3665,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3666,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",(a) and (b) show test misclassiﬁcation error vs,0
3667,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","Of course, the certiﬁcate (15) stillholds regardless.More broadly, this work focuses on small-perturbation attacks, and our theoretical guarantees showthat it is possible to efﬁciently build models that provably guard against such attacks",0
3668,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",This result is essentially due to Katz et al.(2017a),0
3669,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",0
3670,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","Papernot, P",0
3671,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",N,0
3672,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,0
3673,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",582–597,0
3674,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3675,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3676,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3677,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3678,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",Explicit distributional robustness of the form (5) is intractable except in limited cases,0
3679,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","From Lemma 5, our previous results (Theorems 2 and 4) follow",0
3680,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",0
3681,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",0
3682,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3683,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","Forour approach we use γ = 2, and to make fair comparisons with FGM we use",0
3684,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3685,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3686,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3687,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3688,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3689,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",0
3690,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3691,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3692,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",0
3693,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3694,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","Jha, and A",0
3695,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","arXiv:1505.05116 [math.OC],2015.",0
3696,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",B,0
3697,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","arXiv:1505.05116 [math.OC],2015.",0
3698,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",(a) and (b) show test misclassiﬁcation error vs,0
3699,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",1
3700,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,Figure 3,1
3701,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,Figure 3,1
3702,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and","In this proof, we drop the subscript on the iteration t to ease notation",0
3703,PGM attacks on the MNIST dataset,"Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)",0
3704,PGM attacks on the MNIST dataset,"(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave",0
3705,Figure 3,"Papernot, P",0
3706,(a) and (b) show test misclassiﬁcation error vs,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,0
3707,Figure 3,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)",0
3708,PGM attacks on the MNIST dataset,Figure 3,1
3709,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and",the adversarial perturbation level adv,0
3710,PGM attacks on the MNIST dataset,Figure 3,1
3711,(a) and (b) show test misclassiﬁcation error vs,Figure 3,1
3712,(a) and (b) show test misclassiﬁcation error vs,Figure 3,1
3713,(a) and (b) show test misclassiﬁcation error vs,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",0
3714,Figure 3,Figure 3,1
3715,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,Figure 3,1
3716,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,Figure 3,1
3717,(a) and (b) show test misclassiﬁcation error vs,Figure 3,1
3718,Figure 3,"These results suggest advantages of networks with smooth activations
rather than ReLU’s",0
3719,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,Figure 3,1
3720,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
3721,(a) and (b) show test misclassiﬁcation error vs,Frossard,0
3722,(a) and (b) show test misclassiﬁcation error vs,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",0
3723,Figure 3,Figure 3,1
3724,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and","Silver, A",0
3725,PGM attacks on the MNIST dataset,"In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",0
3726,Figure 3,Figure 3,1
3727,Figure 3,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",0
3728,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,A,0
3729,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,Figure 3,1
3730,PGM attacks on the MNIST dataset,Figure 3,1
3731,Figure 3,"Mnih, K",0
3732,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and","Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)",0
3733,Figure 3,"Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",0
3734,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and",Figure 3,1
3735,(a) and (b) show test misclassiﬁcation error vs,"In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp",0
3736,(a) and (b) show test misclassiﬁcation error vs,Figure 3,1
3737,Figure 3,"The NP-hardness of certifying robustness for ReLU networks, coupled withour empirical success and theoretical certiﬁcates for smooth networks in deep learning, suggest thatusing smooth networks may be preferable if we wish to guarantee robustness",0
3738,PGM attacks on the MNIST dataset,Figure 3,1
3739,PGM attacks on the MNIST dataset,Figure 3,1
3740,Figure 3,Figure 3,1
3741,Figure 3,Figure 3,1
3742,PGM attacks on the MNIST dataset,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label",0
3743,PGM attacks on the MNIST dataset,Figure 3,1
3744,(a) and (b) show test misclassiﬁcation error vs,Figure 3,1
3745,(a) and (b) show test misclassiﬁcation error vs,We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,0
3746,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and",Figure 3,1
3747,Figure 3,Figure 3,1
3748,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and","Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
3749,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and","Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",0
3750,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and",Figure 3,1
3751,Figure 3,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",0
3752,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,"Jha, and A",0
3753,Figure 3,Figure 3,1
3754,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,Figure 3,1
3755,Figure 3,Figure 3,1
3756,Figure 3,large desired robustness values) our approach becomes a heuristic just like the other approaches.,0
3757,PGM attacks on the MNIST dataset,Figure 3,1
3758,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and","Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes",0
3759,PGM attacks on the MNIST dataset,"Left column: Euclidean-normattacks, right column: ∞-norm attacks",0
3760,(a) and (b) show test misclassiﬁcation error vs,K,0
3761,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and",Figure 3,1
3762,PGM attacks on the MNIST dataset,Figure 3,1
3763,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and","By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",0
3764,(a) and (b) show test misclassiﬁcation error vs,Figure 3,1
3765,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,"Ostrovski, et al",0
3766,PGM attacks on the MNIST dataset,Figure 3,1
3767,Figure 3,"Ried-miller, A",0
3768,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,Figure 3,1
3769,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,Figure 3,1
3770,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and",Figure 3,1
3771,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",0
3772,Figure 3,Figure 3,1
3773,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z",0
3774,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and","By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data",0
3775,Figure 3,Figure 3,1
3776,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",0
3777,Figure 3,Figure 3,1
3778,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,Figure 3,1
3779,(a) and (b) show test misclassiﬁcation error vs,Figure 3,1
3780,PGM attacks on the MNIST dataset,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",0
3781,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,Figure 3,1
3782,PGM attacks on the MNIST dataset,Figure 3,1
3783,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
3784,(a) and (b) show test misclassiﬁcation error vs,Figure 3,1
3785,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and",We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,0
3786,(a) and (b) show test misclassiﬁcation error vs,"We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.",0
3787,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,Figure 3,1
3788,Figure 3,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
3789,Figure 3,Figure 3,1
3790,PGM attacks on the MNIST dataset,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
3791,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,Figure 3,1
3792,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,Figure 3,1
3793,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,Figure 3,1
3794,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and","IEEE, 2016b.",0
3795,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,"We note thattudes of ∇z(cid:96)(θ; z) in a neighborhood of the nominal input, which ensures stability of the model.",0
3796,"The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and","To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section",0
3797,Figure 3,Figure 3,1
3798,PGM attacks on the MNIST dataset,Figure 3,1
3799,Figure 3,"Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",0
3800,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks",0
3801,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","Graves, M",0
3802,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3803,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3804,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3805,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3806,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",0
3807,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3808,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
3809,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,0
3810,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3811,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,0
3812,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3813,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",We omit the certiﬁcate’s error,0
3814,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","For worst-case regions P formed by Wasserstein balls, Esfahani & Kuhn (2015),Shaﬁeezadeh-Abadeh et al",0
3815,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3816,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",by solving (7),0
3817,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,0
3818,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",(2017) attempt to mitigate this shortcoming,0
3819,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","because f − c is upper semi-continuous, and the latter function is measurable",0
3820,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.",0
3821,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",0
3822,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3823,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3824,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",", Tadv, where Π denotes projection onto B,p(xi) := {x : (cid:107)x − xi(cid:107)p ≤ }.The adversarial training literature (e.g",0
3825,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,0
3826,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,0
3827,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","Left column: Euclidean-normattacks, right column: ∞-norm attacks",0
3828,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al",0
3829,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3830,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
3831,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3832,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3833,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3834,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","Top row: FGM attacks, bottom row: IFGM attacks",0
3835,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
3836,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","Ostrovski, et al",0
3837,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,0
3838,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",0
3839,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3840,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",0
3841,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3842,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","If f is inf-compact, then ¯f is directionally differentiable with",0
3843,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",0
3844,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3845,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
3846,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3847,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)",0
3848,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3849,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3850,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",HN was partially supported by aSamsung Fellowship,0
3851,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3852,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3853,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3854,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3855,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
3856,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",0
3857,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3858,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3859,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3860,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
3861,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3862,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","Rusu, J",0
3863,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al",0
3864,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,0
3865,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3866,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",0
3867,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",0
3868,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","Silver, A",0
3869,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3870,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3871,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3872,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3873,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
3874,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3875,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3876,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3877,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3878,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3879,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3880,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3881,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3882,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3883,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3884,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3885,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3886,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3887,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,0
3888,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",0
3889,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
3890,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","because f − c is upper semi-continuous, and the latter function is measurable",0
3891,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations",0
3892,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",(f) Test error vs,0
3893,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3894,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes",0
3895,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3896,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",0
3897,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3898,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",1
3899,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",In constrast to f-divergences (e.g,0
3900,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3901,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
3902,"We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms","Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",0
3903,"In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks","Replacing our current covering num-ber arguments with more intricate notions such as margin-based bounds (Bartlett et al., 2017) wouldextend the scope and usefulness of our theoretical guarantees",0
3904,We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,"By appropriatescaling of θ, v, and w, Katz et al",0
3905,We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,0
3906,"All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3907,We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,the adversarial perturbation level adv,0
3908,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3909,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3910,It isthus important to distinguish the methods’ abilities to combat attacks,"Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)",0
3911,"In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3912,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ","Namely, we draw the nominal",0
3913,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3914,"In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",0
3915,It isthus important to distinguish the methods’ abilities to combat attacks,"2574–2582, 2016.",0
3916,We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3917,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",from which it is easy to see that f is inf-compact,0
3918,We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3919,"All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training",372–387,0
3920,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,"When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ",0
3921,"We note thattudes of ∇z(cid:96)(θ; z) in a neighborhood of the nominal input, which ensures stability of the model.","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3922,We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,"Veness, M",0
3923,"All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3924,It isthus important to distinguish the methods’ abilities to combat attacks,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",0
3925,We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3926,We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1",0
3927,It isthus important to distinguish the methods’ abilities to combat attacks,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3928,"All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training","Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",0
3929,"In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks","From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have",0
3930,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3931,It isthus important to distinguish the methods’ abilities to combat attacks,Frossard,0
3932,"We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms","The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)",0
3933,It isthus important to distinguish the methods’ abilities to combat attacks,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
3934,We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3935,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",0
3936,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3937,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3938,It isthus important to distinguish the methods’ abilities to combat attacks,We illustrate testmisclassiﬁcation error vs,0
3939,"All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training","Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques",0
3940,We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
3941,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3942,We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,We illustrate testmisclassiﬁcation error vs,0
3943,We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3944,We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,Distillation as a defense to adversarialperturbations against deep neural networks,0
3945,"In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks","For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",0
3946,"In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3947,We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3948,"We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms","Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",0
3949,"In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3950,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3951,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3952,We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3953,We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3954,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,"Weprovide an adversarial training procedure that, for smooth (cid:96), enjoys convergence guarantees simi-lar to non-robust approaches while certifying performance even for the worst-case population losssupP∈P EP [(cid:96)(θ; Z)]",0
3955,It isthus important to distinguish the methods’ abilities to combat attacks,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3956,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ","This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al",0
3957,We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,Further attacks on the MNIST dataset,0
3958,"We note thattudes of ∇z(cid:96)(θ; z) in a neighborhood of the nominal input, which ensures stability of the model.","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3959,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3960,"In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3961,We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3962,"We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3963,We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4",0
3964,"In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks","Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",0
3965,We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3966,"All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training","Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",0
3967,"All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3968,"We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3969,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3970,"We note thattudes of ∇z(cid:96)(θ; z) in a neighborhood of the nominal input, which ensures stability of the model.","AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",0
3971,"We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3972,"All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3973,"All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training","By suitably modifying the cost function c(z, z(cid:48))to take value ∞ outside this small region, our general formulation covers such variants.",0
3974,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
3975,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",0
3976,"We note thattudes of ∇z(cid:96)(θ; z) in a neighborhood of the nominal input, which ensures stability of the model.","As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes",0
3977,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ","The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",0
3978,"All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3979,We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,"Recalling Rockafellar & Wets (1998, Def",0
3980,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ","For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM",0
3981,"We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms","We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R",0
3982,"We note thattudes of ∇z(cid:96)(θ; z) in a neighborhood of the nominal input, which ensures stability of the model.","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3983,It isthus important to distinguish the methods’ abilities to combat attacks,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3984,"All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3985,It isthus important to distinguish the methods’ abilities to combat attacks,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3986,We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3987,"We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3988,"All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3989,We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3990,"All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3991,It isthus important to distinguish the methods’ abilities to combat attacks,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3992,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3993,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3994,"In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3995,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3996,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ","See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",0
3997,We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,"Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",0
3998,"We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",1
3999,It isthus important to distinguish the methods’ abilities to combat attacks,The,0
4000,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4001,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4002,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","Mnih, K",0
4003,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",Figure 1,0
4004,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","In this regime(small γ, large ), performance between WRM and other methods diverge",0
4005,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),0
4006,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4007,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4008,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4009,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4010,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4011,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",0
4012,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,0
4013,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples",0
4014,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
4015,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4016,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4017,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4018,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",For large adversaries(i.e,0
4019,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
4020,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4021,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4022,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,0
4023,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",from which it is easy to see that f is inf-compact,0
4024,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,0
4025,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","In this regime(small γ, large ), performance between WRM and other methods diverge",0
4026,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4027,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4028,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4029,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4030,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",0
4031,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4032,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4033,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4034,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4035,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4036,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4037,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","Nature, 518(7540):529–533, 2015.",0
4038,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",0
4039,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",Robust MDP’s consider an ambiguity set Psa for state-action transitions,0
4040,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","Left column:Euclidean-norm attacks, right column: ∞-norm attacks",0
4041,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4042,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",0
4043,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4044,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4045,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4046,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4047,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
4048,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","Graves, M",0
4049,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4050,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",M,0
4051,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c",0
4052,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4053,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",It isthus important to distinguish the methods’ abilities to combat attacks,0
4054,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,0
4055,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples",0
4056,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",0
4057,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4058,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4059,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4060,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","With that said, the strong dualityresult, Proposition 1, still applies to any distribution",0
4061,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4062,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",0
4063,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4064,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",0
4065,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4066,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",0
4067,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",All models are trained in the ∞-norm,0
4068,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4069,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4070,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
4071,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4072,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4073,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4074,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","Szepesv´ari & Littman (1999)) apply under these adversarial dynamics.We test our adversarial training procedure in the cart-pole environment, where the goal is to balancea pole on a cart by moving the cart left or right",0
4075,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4076,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",14.27 and Prop,0
4077,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","From Lemma 5, our previous results (Theorems 2 and 4) follow",0
4078,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4079,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
4080,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4081,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4082,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",0
4083,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4084,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4085,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4086,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",0
4087,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4088,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4089,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",0
4090,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","Consider feedforward neural networks with ReLU’s and let U := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U (cid:96)(θ; z + u) is NPO",0
4091,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4092,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",0
4093,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4094,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","Nature, 518(7540):529–533, 2015.",0
4095,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞",0
4096,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4097,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4098,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",0
4099,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",1
4100,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4101,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.","(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D",0
4102,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4103,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable","Celik, and A",0
4104,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,"We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5",0
4105,"Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4106,"Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input","Weprovide an adversarial training procedure that, for smooth (cid:96), enjoys convergence guarantees simi-lar to non-robust approaches while certifying performance even for the worst-case population losssupP∈P EP [(cid:96)(θ; Z)]",0
4107,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4108,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4109,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.","McDaniel, S",0
4110,"Thus, we see that our adversarial-training method defends against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint untilthe model misclassiﬁes it","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4111,"Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4112,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.","If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",0
4113,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4114,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.",Deepfool: a simple and accurate method to fooldeep neural networks,0
4115,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
4116,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
4117,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4118,"Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4119,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable","As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",0
4120,"Thus, we see that our adversarial-training method defends against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint untilthe model misclassiﬁes it","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4121,"Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input","57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞",0
4122,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4123,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,0
4124,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4125,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.","With that said, the strong dualityresult, Proposition 1, still applies to any distribution",0
4126,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.",14.27 and Prop,0
4127,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.",The vertical bar indicates the achieved level,0
4128,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4129,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4130,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4131,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4132,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",0
4133,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,0
4134,"Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input","On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",0
4135,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4136,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4137,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4138,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4139,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4140,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4141,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4142,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4143,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4144,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.",allowing us to prevent attacks on the test set,0
4145,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4146,"Thus, we see that our adversarial-training method defends against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint untilthe model misclassiﬁes it","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4147,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.","Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",0
4148,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4149,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable","Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",0
4150,"Thus, we see that our adversarial-training method defends against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint untilthe model misclassiﬁes it","Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",0
4151,"Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",0
4152,"Thus, we see that our adversarial-training method defends against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint untilthe model misclassiﬁes it","Veness, M",0
4153,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable","arXiv:1703.11008 [cs.LG], 2017.P",0
4154,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4155,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4156,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",0
4157,"Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4158,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",G,0
4159,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4160,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.",Thestatistical error term n(t) is omitted from the certiﬁcate,0
4161,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4162,"Thus, we see that our adversarial-training method defends against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint untilthe model misclassiﬁes it","These results suggest advantages of networks with smooth activations
rather than ReLU’s",0
4163,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3",We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,0
4164,"Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input","By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",0
4165,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4166,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3",the adversarial perturbation level adv,0
4167,"Thus, we see that our adversarial-training method defends against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint untilthe model misclassiﬁes it","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4168,"Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4169,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,"In this proof, we drop the subscript on the iteration t to ease notation",0
4170,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
4171,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4172,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",0
4173,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
4174,"Thus, we see that our adversarial-training method defends against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint untilthe model misclassiﬁes it","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4175,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4176,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4177,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,0
4178,"Thus, we see that our adversarial-training method defends against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint untilthe model misclassiﬁes it",Roy,0
4179,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4180,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4181,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4182,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,"because f − c is upper semi-continuous, and the latter function is measurable",0
4183,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain",0
4184,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable","Thus, theoptimization problem is NP-hard.",0
4185,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.",Swami,0
4186,"Thus, we see that our adversarial-training method defends against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint untilthe model misclassiﬁes it","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4187,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.","Namely, we draw the nominal",0
4188,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4189,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.","Forour approach we use γ = 2, and to make fair comparisons with FGM we use",0
4190,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4191,"Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4192,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable","(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)",0
4193,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4194,"Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4195,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",We have the following theorem.Theorem 5,0
4196,"Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input","Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
4197,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4198,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",1
4199,"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",Distillation as a defense to adversarialperturbations against deep neural networks,0
4200,Robust MDP’s consider an ambiguity set Psa for state-action transitions,Figure 2,0
4201,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4202,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4203,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4204,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4205,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,0
4206,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c",0
4207,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4208,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4209,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4210,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4211,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,0
4212,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4213,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4214,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4215,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4216,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations","Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)",0
4217,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4218,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4219,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"Nature, 518(7540):529–533, 2015.",0
4220,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4221,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4222,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4223,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance",0
4224,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",0
4225,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations",The major beneﬁt of our approach is its simplicityand wide applicability across many models and machine-learning scenarios.There remain many avenues for future investigation,0
4226,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",(16)) in θ,0
4227,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations","The vertical bar in (a), (c), and (e) indicates the perturbation level that was",0
4228,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4229,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"In Security and Privacy (SP), 2016 IEEE Symposiumon, pp",0
4230,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4231,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",Let ∆F ≥ F (θ0) − inf θ F (θ),0
4232,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost",0
4233,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label",0
4234,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,0
4235,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4236,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,Attacks on the MNIST dataset,0
4237,Robust MDP’s consider an ambiguity set Psa for state-action transitions,Thestatistical error term n(t) is omitted from the certiﬁcate,0
4238,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R",Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,0
4239,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4240,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4241,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4242,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
4243,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"By suitably modifying the cost function c(z, z(cid:48))to take value ∞ outside this small region, our general formulation covers such variants.",0
4244,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4245,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",All mod-els are trained in the ∞-norm,0
4246,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4247,Robust MDP’s consider an ambiguity set Psa for state-action transitions,The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,0
4248,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4249,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks",0
4250,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4251,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4252,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",0
4253,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4254,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4255,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4256,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4257,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4258,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations","With that said, the strong dualityresult, Proposition 1, still applies to any distribution",0
4259,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques",0
4260,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4261,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4262,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4263,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4264,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4265,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",0
4266,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4267,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4268,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4269,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique","Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
4270,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4271,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4272,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4273,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4274,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4275,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4276,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4277,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations",V,0
4278,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4279,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,Explicit distributional robustness of the form (5) is intractable except in limited cases,0
4280,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",0
4281,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4282,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples",0
4283,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z",0
4284,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,0
4285,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique","Veness, M",0
4286,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4287,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4288,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4289,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4290,The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4291,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",0
4292,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",G,0
4293,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique","Jha, M",0
4294,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4295,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations",See Section C.2 for the proof,0
4296,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations",adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,0
4297,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",0
4298,Robust MDP’s consider an ambiguity set Psa for state-action transitions,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4299,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",1
4300,Mnih et al,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4301,Mnih et al,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",0
4302,In scenarios where the underlying environment has a continuous state-space and we representQ with a differentiable function (e.g,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",0
4303,"Namely, we draw the nominal","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4304,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4305,"Namely, we draw the nominal","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4306,"Namely, we draw the nominal","For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",0
4307,Mnih et al,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4308,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4309,Mnih et al,"Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",0
4310,Mnih et al,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4311,"Namely, we draw the nominal",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
4312,Mnih et al,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4313,"Namely, we draw the nominal","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4314,In scenarios where the underlying environment has a continuous state-space and we representQ with a differentiable function (e.g,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4315,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4316,In scenarios where the underlying environment has a continuous state-space and we representQ with a differentiable function (e.g,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4317,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4318,Mnih et al,8.7.1) to obtain,0
4319,Mnih et al,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4320,"Namely, we draw the nominal","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",0
4321,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4322,Mnih et al,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4323,Mnih et al,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4324,Mnih et al,"Consider feedforward neural networks with ReLU’s and let U := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U (cid:96)(θ; z + u) is NPO",0
4325,In scenarios where the underlying environment has a continuous state-space and we representQ with a differentiable function (e.g,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
4326,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","Fawzi, and P",0
4327,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4328,In scenarios where the underlying environment has a continuous state-space and we representQ with a differentiable function (e.g,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4329,"Namely, we draw the nominal",Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,0
4330,"Namely, we draw the nominal",Theorem 7,0
4331,Mnih et al,See Section C.2 for the proof,0
4332,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4333,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward","(2017), Dziugaite & Roy (2017), andNeyshabur et al",0
4334,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4335,Mnih et al,We typically solve the penalty problem (2) with P0 replaced,0
4336,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4337,In scenarios where the underlying environment has a continuous state-space and we representQ with a differentiable function (e.g,14.27 and Prop,0
4338,"Namely, we draw the nominal","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4339,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward","Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",0
4340,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4341,Mnih et al,"(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
4342,"Namely, we draw the nominal","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4343,Mnih et al,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
4344,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward","57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞",0
4345,In scenarios where the underlying environment has a continuous state-space and we representQ with a differentiable function (e.g,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",0
4346,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",0
4347,In scenarios where the underlying environment has a continuous state-space and we representQ with a differentiable function (e.g,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4348,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",the adversarial perturbationlevel adv,0
4349,"Namely, we draw the nominal",(e) Test error vs,0
4350,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4351,"Namely, we draw the nominal","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",0
4352,"Namely, we draw the nominal","Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",0
4353,Mnih et al,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4354,Mnih et al,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4355,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",The function cx : X × X → R+ is continuous,0
4356,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4357,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4358,Mnih et al,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4359,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4360,Mnih et al,We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,0
4361,"Namely, we draw the nominal","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4362,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4363,In scenarios where the underlying environment has a continuous state-space and we representQ with a differentiable function (e.g,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4364,"Namely, we draw the nominal","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4365,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4366,Mnih et al,2.2) gives the result.,0
4367,In scenarios where the underlying environment has a continuous state-space and we representQ with a differentiable function (e.g,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4368,In scenarios where the underlying environment has a continuous state-space and we representQ with a differentiable function (e.g,"arXiv:1505.05116 [math.OC],2015.",0
4369,"Namely, we draw the nominal","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4370,Mnih et al,Roy,0
4371,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",We illustrate test misclassiﬁcation error vs,0
4372,In scenarios where the underlying environment has a continuous state-space and we representQ with a differentiable function (e.g,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4373,"Namely, we draw the nominal",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,0
4374,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4375,"Namely, we draw the nominal",Dziugaite and D,0
4376,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness",The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,0
4377,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4378,"Namely, we draw the nominal","Top row: PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",0
4379,Mnih et al,"Silver, A",0
4380,"Namely, we draw the nominal","Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)",0
4381,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4382,"Namely, we draw the nominal","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4383,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4384,Mnih et al,"We compare standard WRM with ∞-norm PGM, FGM,IFGM",0
4385,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",The vertical bar indicates the achieved level,0
4386,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4387,"Namely, we draw the nominal",Explicit distributional robustness of the form (5) is intractable except in limited cases,0
4388,Mnih et al,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4389,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward","On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
4390,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4391,"Namely, we draw the nominal","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4392,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3",0
4393,Mnih et al,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4394,"(2015)), we can modify the update (21) with anadversarial state perturbation to incorporate distributional robustness","Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label",0
4395,"Namely, we draw the nominal","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4396,"Namely, we draw the nominal","Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",0
4397,Mnih et al,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",1
4398,Mnih et al,"Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)",0
4399,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",0
4400,"In the former case, since the update (22)simply modiﬁes the state-action transitions (independent of Q), standard results on convergence fortabular Q-learning (e.g","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4401,"For large γ, we can again solve problem (22) efﬁciently using gradient descent","We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty",0
4402,"Szepesv´ari & Littman (1999)) apply under these adversarial dynamics.We test our adversarial training procedure in the cart-pole environment, where the goal is to balancea pole on a cart by moving the cart left or right","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4403,We use reward r(β) := e−|β| for the angle β of the pole from the vertical,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4404,The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4405,"We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4406,"For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4407,"In the former case, since the update (22)simply modiﬁes the state-action transitions (independent of Q), standard results on convergence fortabular Q-learning (e.g","Wu, S",0
4408,"The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4409,"The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4410,"For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering",AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
4411,"The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.","AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",0
4412,Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),"θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",0
4413,The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,"Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks",0
4414,"Thus, weonly compare with an agent trained on the nominal MDP","Papernot, P",0
4415,"For large γ, we can again solve problem (22) efﬁciently using gradient descent","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4416,The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),0
4417,"Thus, weonly compare with an agent trained on the nominal MDP","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4418,We use reward r(β) := e−|β| for the angle β of the pole from the vertical,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4419,This procedureprovides robustness to uncertainties in state-action transitions,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4420,This procedureprovides robustness to uncertainties in state-action transitions,"Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞",0
4421,"For large γ, we can again solve problem (22) efﬁciently using gradient descent","In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",0
4422,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,0
4423,"For large γ, we can again solve problem (22) efﬁciently using gradient descent","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4424,Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4425,"Thus, weonly compare with an agent trained on the nominal MDP","We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",0
4426,"For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4427,Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),582–597,0
4428,"The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4429,The action space is binary: push the cartleft or right with a ﬁxed force,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4430,The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4431,"For large γ, we can again solve problem (22) efﬁciently using gradient descent","Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes",0
4432,The action space is binary: push the cartleft or right with a ﬁxed force,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4433,We use reward r(β) := e−|β| for the angle β of the pole from the vertical,(c) Test error vs,0
4434,Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A",0
4435,"In the former case, since the update (22)simply modiﬁes the state-action transitions (independent of Q), standard results on convergence fortabular Q-learning (e.g","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4436,"Thus, weonly compare with an agent trained on the nominal MDP","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4437,This procedureprovides robustness to uncertainties in state-action transitions,"Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness",0
4438,"We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4439,"For large γ, we can again solve problem (22) efﬁciently using gradient descent","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4440,The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4441,The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4442,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,0
4443,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",Further attacks on the MNIST dataset,0
4444,"For large γ, we can again solve problem (22) efﬁciently using gradient descent","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4445,"Thus, weonly compare with an agent trained on the nominal MDP",Robust MDP’s consider an ambiguity set Psa for state-action transitions,0
4446,This procedureprovides robustness to uncertainties in state-action transitions,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4447,We use reward r(β) := e−|β| for the angle β of the pole from the vertical,"Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞",0
4448,The action space is binary: push the cartleft or right with a ﬁxed force,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4449,This procedureprovides robustness to uncertainties in state-action transitions,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4450,Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4451,Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4452,"Due to the nonstationary, policy-dependent radius for the Wassersteinball, an analogous  for the fast-gradient method (or other variants) is not well-deﬁned","In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp",0
4453,"For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering",372–387,0
4454,The action space is binary: push the cartleft or right with a ﬁxed force,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4455,"Thus, weonly compare with an agent trained on the nominal MDP","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4456,"Thus, weonly compare with an agent trained on the nominal MDP","Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",0
4457,"Thus, weonly compare with an agent trained on the nominal MDP","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4458,Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4459,"Thus, weonly compare with an agent trained on the nominal MDP","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4460,"Thus, weonly compare with an agent trained on the nominal MDP","For example,Ben-Tal et al",0
4461,This procedureprovides robustness to uncertainties in state-action transitions,In constrast to f-divergences (e.g,0
4462,"The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4463,"For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4464,Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,0
4465,"For large γ, we can again solve problem (22) efﬁciently using gradient descent","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4466,"We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5","For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",0
4467,The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4468,We use reward r(β) := e−|β| for the angle β of the pole from the vertical,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",0
4469,We use reward r(β) := e−|β| for the angle β of the pole from the vertical,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4470,Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4471,"The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.","See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",0
4472,"In the former case, since the update (22)simply modiﬁes the state-action transitions (independent of Q), standard results on convergence fortabular Q-learning (e.g","Finally, we consider a reinforcement learning problemin Section 4.3, where the Markov decision process used for training differs from that for testing.WRM enjoys the theoretical guarantees of Sections 2 and 3 for large γ, but for small γ (large adver-sarial budgets), WRM becomes a heuristic like other methods",0
4473,This procedureprovides robustness to uncertainties in state-action transitions,"In this proof, we drop the subscript on the iteration t to ease notation",0
4474,The action space is binary: push the cartleft or right with a ﬁxed force,"The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",0
4475,"The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4476,"For large γ, we can again solve problem (22) efﬁciently using gradient descent","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4477,"The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.","The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",0
4478,"Due to the nonstationary, policy-dependent radius for the Wassersteinball, an analogous  for the fast-gradient method (or other variants) is not well-deﬁned","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",0
4479,"Thus, weonly compare with an agent trained on the nominal MDP","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4480,"We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4481,"Due to the nonstationary, policy-dependent radius for the Wassersteinball, an analogous  for the fast-gradient method (or other variants) is not well-deﬁned",by solving (7),0
4482,Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),We assume without loss of2 − z0 (cid:54)= 0,0
4483,"Due to the nonstationary, policy-dependent radius for the Wassersteinball, an analogous  for the fast-gradient method (or other variants) is not well-deﬁned","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4484,We use reward r(β) := e−|β| for the angle β of the pole from the vertical,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4485,"For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering",Frossard,0
4486,"We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5","The vertical bar in (a), (c), and (e) indicates the estimated",0
4487,"Thus, weonly compare with an agent trained on the nominal MDP","We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",0
4488,"Szepesv´ari & Littman (1999)) apply under these adversarial dynamics.We test our adversarial training procedure in the cart-pole environment, where the goal is to balancea pole on a cart by moving the cart left or right","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4489,"For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering","In Security and Privacy (SP), 2016 IEEE Symposiumon, pp",0
4490,"The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4491,The action space is binary: push the cartleft or right with a ﬁxed force,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
4492,"Due to the nonstationary, policy-dependent radius for the Wassersteinball, an analogous  for the fast-gradient method (or other variants) is not well-deﬁned","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4493,The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,In constrast to f-divergences (e.g,0
4494,"In the former case, since the update (22)simply modiﬁes the state-action transitions (independent of Q), standard results on convergence fortabular Q-learning (e.g","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4495,The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4496,The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4497,"Due to the nonstationary, policy-dependent radius for the Wassersteinball, an analogous  for the fast-gradient method (or other variants) is not well-deﬁned","For large γ, we can again solve problem (22) efﬁciently using gradient descent",1
4498,The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,"The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)",0
4499,"For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering","Jha, and A",0
4500,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
4501,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,with the convention that 0 · ∞ = 0,0
4502,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",We thank Jacob Steinhardt for valuable feedback,0
4503,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4504,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4505,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
4506,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
4507,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4508,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4509,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4510,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4511,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP","Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",0
4512,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",0
4513,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4514,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","for t = 1, ",0
4515,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4516,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4517,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4518,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4519,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",0
4520,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4521,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4522,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4523,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",0
4524,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.",0
4525,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP","Thus, theoptimization problem is NP-hard.",0
4526,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,"Indeed, inthe large-perturbation regime, efﬁciently training certiﬁably secure systems remains an importantopen question",0
4527,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4528,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP","Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
4529,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",0
4530,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4531,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features",0
4532,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,"When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ",0
4533,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4534,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4535,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.",0
4536,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP","For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",0
4537,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4538,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4539,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4540,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",Theorem 7,0
4541,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4542,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4543,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",0
4544,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes",0
4545,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",0
4546,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4547,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4548,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP","For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",0
4549,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4550,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4551,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,0
4552,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4553,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4554,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",0
4555,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP","Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)",0
4556,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4557,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,"(2017), Dziugaite & Roy (2017), andNeyshabur et al",0
4558,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","Recalling Rockafellar & Wets (1998, Def",0
4559,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4560,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","For each γadv, we consider the distance to adversarial examples in the test dataset",0
4561,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,"Veness, M",0
4562,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP","Then, with probability at leastner, 1996, Ch",0
4563,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4564,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4565,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4566,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4567,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4568,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,Let ∆F ≥ F (θ0) − inf θ F (θ),0
4569,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,0
4570,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4571,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",0
4572,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.",0
4573,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",0
4574,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4575,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4576,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4577,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g",0
4578,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,0
4579,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,0
4580,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4581,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",0
4582,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",0
4583,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","Moosavi-Dezfooli, A",0
4584,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
4585,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP","The vertical bar in (a), (c), and (e) indicates the perturbation level that was",0
4586,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4587,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1",0
4588,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","Using only assumptions about the smoothness of the loss function(cid:96), we prove that our method enjoys strong statistical guarantees and fast optimization rates for alarge class of problems",0
4589,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ",0
4590,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4591,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4592,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4593,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4594,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,We providea principled method for efﬁciently guaranteeing distributional robustness with a simple form ofadversarial data perturbation,0
4595,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4596,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4597,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,"Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z",0
4598,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments","Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective",0
4599,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,1
4600,"In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4601,"Indeed, inthe large-perturbation regime, efﬁciently training certiﬁably secure systems remains an importantopen question",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4602,"Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks","Silver, A",0
4603,"Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4604,"The NP-hardness of certifying robustness for ReLU networks, coupled withour empirical success and theoretical certiﬁcates for smooth networks in deep learning, suggest thatusing smooth networks may be preferable if we wish to guarantee robustness","Then, we have",0
4605,We providea principled method for efﬁciently guaranteeing distributional robustness with a simple form ofadversarial data perturbation,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",0
4606,"Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,0
4607,"This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4608,"We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4609,Explicit distributional robustness of the form (5) is intractable except in limited cases,"The smoothness of (cid:96) is essential so that a penalized version(cid:96)(θ, z) − γc(z, z0) is concave in z (which can be approximately veriﬁed by computing Hessians∇2zz(cid:96)(θ, z) for each training datapoint), allowing computation and our coming certiﬁcates of opti-mality",0
4610,"Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4611,We providea principled method for efﬁciently guaranteeing distributional robustness with a simple form ofadversarial data perturbation,"Then there exists θ such that
this optimization problem is also NP-hard.",0
4612,"Replacing our current covering num-ber arguments with more intricate notions such as margin-based bounds (Bartlett et al., 2017) wouldextend the scope and usefulness of our theoretical guarantees",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4613,"The NP-hardness of certifying robustness for ReLU networks, coupled withour empirical success and theoretical certiﬁcates for smooth networks in deep learning, suggest thatusing smooth networks may be preferable if we wish to guarantee robustness",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4614,"We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4615,"(2017), Dziugaite & Roy (2017), andNeyshabur et al",It isthus important to distinguish the methods’ abilities to combat attacks,0
4616,"Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4617,"We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image","On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",0
4618,"The NP-hardness of certifying robustness for ReLU networks, coupled withour empirical success and theoretical certiﬁcates for smooth networks in deep learning, suggest thatusing smooth networks may be preferable if we wish to guarantee robustness",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4619,(2017) attempt to mitigate this shortcoming,Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4620,"Using only assumptions about the smoothness of the loss function(cid:96), we prove that our method enjoys strong statistical guarantees and fast optimization rates for alarge class of problems","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
4621,Our optimization result (Theorem 2) appliesonly for small values of robustness ρ and to a limited class of Wasserstein costs,Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4622,Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",0
4623,"Using only assumptions about the smoothness of the loss function(cid:96), we prove that our method enjoys strong statistical guarantees and fast optimization rates for alarge class of problems",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4624,Our optimization result (Theorem 2) appliesonly for small values of robustness ρ and to a limited class of Wasserstein costs,"Thus, weonly compare with an agent trained on the nominal MDP",0
4625,"Replacing our current covering num-ber arguments with more intricate notions such as margin-based bounds (Bartlett et al., 2017) wouldextend the scope and usefulness of our theoretical guarantees",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4626,"This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al","The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and",0
4627,"In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts","In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks",0
4628,"Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4629,"This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al",In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,0
4630,"Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks","See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",0
4631,"Using only assumptions about the smoothness of the loss function(cid:96), we prove that our method enjoys strong statistical guarantees and fast optimization rates for alarge class of problems",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4632,"The NP-hardness of certifying robustness for ReLU networks, coupled withour empirical success and theoretical certiﬁcates for smooth networks in deep learning, suggest thatusing smooth networks may be preferable if we wish to guarantee robustness",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4633,"We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image","Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",0
4634,"Indeed, inthe large-perturbation regime, efﬁciently training certiﬁably secure systems remains an importantopen question",We illustrate test misclassiﬁcation error vs,0
4635,"The NP-hardness of certifying robustness for ReLU networks, coupled withour empirical success and theoretical certiﬁcates for smooth networks in deep learning, suggest thatusing smooth networks may be preferable if we wish to guarantee robustness","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",0
4636,"Using only assumptions about the smoothness of the loss function(cid:96), we prove that our method enjoys strong statistical guarantees and fast optimization rates for alarge class of problems",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4637,"Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",The distinctions in performance between various methodsare less apparent now,0
4638,"In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts","Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.",0
4639,"Replacing our current covering num-ber arguments with more intricate notions such as margin-based bounds (Bartlett et al., 2017) wouldextend the scope and usefulness of our theoretical guarantees",adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
4640,"We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4641,"This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4642,"Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4643,Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
4644,Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4645,"The NP-hardness of certifying robustness for ReLU networks, coupled withour empirical success and theoretical certiﬁcates for smooth networks in deep learning, suggest thatusing smooth networks may be preferable if we wish to guarantee robustness",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4646,"We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4647,"In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4648,"Replacing our current covering num-ber arguments with more intricate notions such as margin-based bounds (Bartlett et al., 2017) wouldextend the scope and usefulness of our theoretical guarantees",Experimental results on synthetic data,0
4649,"We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4650,"Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4651,"Replacing our current covering num-ber arguments with more intricate notions such as margin-based bounds (Bartlett et al., 2017) wouldextend the scope and usefulness of our theoretical guarantees","Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
4652,Explicit distributional robustness of the form (5) is intractable except in limited cases,"Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",0
4653,"Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4654,We providea principled method for efﬁciently guaranteeing distributional robustness with a simple form ofadversarial data perturbation,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",0
4655,"Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques","such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",0
4656,"In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts","The vertical bar in (a), (c), and (e) indicates the estimated",0
4657,"The NP-hardness of certifying robustness for ReLU networks, coupled withour empirical success and theoretical certiﬁcates for smooth networks in deep learning, suggest thatusing smooth networks may be preferable if we wish to guarantee robustness","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",0
4658,The major beneﬁt of our approach is its simplicityand wide applicability across many models and machine-learning scenarios.There remain many avenues for future investigation,Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4659,"In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4660,(2017) attempt to mitigate this shortcoming,Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4661,Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,the adversarial perturbation level adv,0
4662,We providea principled method for efﬁciently guaranteeing distributional robustness with a simple form ofadversarial data perturbation,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features",0
4663,"Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4664,Explicit distributional robustness of the form (5) is intractable except in limited cases,Further attacks on the MNIST dataset,0
4665,"This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4666,Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4667,"Using only assumptions about the smoothness of the loss function(cid:96), we prove that our method enjoys strong statistical guarantees and fast optimization rates for alarge class of problems",from which it is easy to see that f is inf-compact,0
4668,"Of course, the certiﬁcate (15) stillholds regardless.More broadly, this work focuses on small-perturbation attacks, and our theoretical guarantees showthat it is possible to efﬁciently build models that provably guard against such attacks",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,0
4669,Our optimization result (Theorem 2) appliesonly for small values of robustness ρ and to a limited class of Wasserstein costs,Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4670,"Using only assumptions about the smoothness of the loss function(cid:96), we prove that our method enjoys strong statistical guarantees and fast optimization rates for alarge class of problems",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4671,"Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
4672,Explicit distributional robustness of the form (5) is intractable except in limited cases,Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4673,"Replacing our current covering num-ber arguments with more intricate notions such as margin-based bounds (Bartlett et al., 2017) wouldextend the scope and usefulness of our theoretical guarantees",This procedureprovides robustness to uncertainties in state-action transitions,0
4674,"Replacing our current covering num-ber arguments with more intricate notions such as margin-based bounds (Bartlett et al., 2017) wouldextend the scope and usefulness of our theoretical guarantees",radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,0
4675,"Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques","Ried-miller, A",0
4676,Our optimization result (Theorem 2) appliesonly for small values of robustness ρ and to a limited class of Wasserstein costs,Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4677,"Using only assumptions about the smoothness of the loss function(cid:96), we prove that our method enjoys strong statistical guarantees and fast optimization rates for alarge class of problems","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
4678,"In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts","In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks",0
4679,(2017) attempt to mitigate this shortcoming,"Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
4680,"Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4681,"(2017), Dziugaite & Roy (2017), andNeyshabur et al",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4682,Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,Figure 1,0
4683,"This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",0
4684,"Indeed, inthe large-perturbation regime, efﬁciently training certiﬁably secure systems remains an importantopen question",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4685,"(2017), Dziugaite & Roy (2017), andNeyshabur et al","For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",0
4686,The major beneﬁt of our approach is its simplicityand wide applicability across many models and machine-learning scenarios.There remain many avenues for future investigation,We thank Jacob Steinhardt for valuable feedback,0
4687,"In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts","Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",0
4688,"Of course, the certiﬁcate (15) stillholds regardless.More broadly, this work focuses on small-perturbation attacks, and our theoretical guarantees showthat it is possible to efﬁciently build models that provably guard against such attacks",We use reward r(β) := e−|β| for the angle β of the pole from the vertical,0
4689,"(2017), Dziugaite & Roy (2017), andNeyshabur et al",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4690,"Using only assumptions about the smoothness of the loss function(cid:96), we prove that our method enjoys strong statistical guarantees and fast optimization rates for alarge class of problems",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4691,"(2017), Dziugaite & Roy (2017), andNeyshabur et al","We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty",0
4692,(2017) attempt to mitigate this shortcoming,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",0
4693,"Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4694,"The NP-hardness of certifying robustness for ReLU networks, coupled withour empirical success and theoretical certiﬁcates for smooth networks in deep learning, suggest thatusing smooth networks may be preferable if we wish to guarantee robustness",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4695,"Using only assumptions about the smoothness of the loss function(cid:96), we prove that our method enjoys strong statistical guarantees and fast optimization rates for alarge class of problems",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4696,Our optimization result (Theorem 2) appliesonly for small values of robustness ρ and to a limited class of Wasserstein costs,Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4697,"We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",Explicit distributional robustness of the form (5) is intractable except in limited cases,1
4698,"Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",0
4699,(2017) attempt to mitigate this shortcoming,"ERM and FGM suffer from sensitivities to various regions of the data,as evidenced by the lack of symmetry in their classiﬁcation boundaries",0
4700,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",(16)) in θ,0
4701,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,"Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv",0
4702,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",We thank Jacob Steinhardt for valuable feedback,1
4703,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)",0
4704,HN was partially supported by aSamsung Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4705,HN was partially supported by aSamsung Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4706,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,We thank Jacob Steinhardt for valuable feedback,1
4707,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",We thank Jacob Steinhardt for valuable feedback,1
4708,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,We thank Jacob Steinhardt for valuable feedback,1
4709,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,0
4710,HN was partially supported by aSamsung Fellowship,Swami,0
4711,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,We thank Jacob Steinhardt for valuable feedback,1
4712,HN was partially supported by aSamsung Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4713,We thank Jacob Steinhardt for valuable feedback,We thank Jacob Steinhardt for valuable feedback,1
4714,HN was partially supported by aSamsung Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4715,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",We thank Jacob Steinhardt for valuable feedback,1
4716,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",We thank Jacob Steinhardt for valuable feedback,1
4717,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,We thank Jacob Steinhardt for valuable feedback,1
4718,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
4719,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,We thank Jacob Steinhardt for valuable feedback,1
4720,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,"Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",0
4721,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research","For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g",0
4722,HN was partially supported by aSamsung Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4723,HN was partially supported by aSamsung Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4724,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",We thank Jacob Steinhardt for valuable feedback,1
4725,We thank Jacob Steinhardt for valuable feedback,We thank Jacob Steinhardt for valuable feedback,1
4726,We thank Jacob Steinhardt for valuable feedback,allowing us to prevent attacks on the test set,0
4727,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,We thank Jacob Steinhardt for valuable feedback,1
4728,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",We thank Jacob Steinhardt for valuable feedback,1
4729,HN was partially supported by aSamsung Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4730,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research","a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",0
4731,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research","WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",0
4732,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4733,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",0
4734,We thank Jacob Steinhardt for valuable feedback,"Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))",0
4735,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,"For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM",0
4736,HN was partially supported by aSamsung Fellowship,"In this regime(small γ, large ), performance between WRM and other methods diverge",0
4737,HN was partially supported by aSamsung Fellowship,"Then, the bounds (11) and (12) hold with",0
4738,We thank Jacob Steinhardt for valuable feedback,We thank Jacob Steinhardt for valuable feedback,1
4739,We thank Jacob Steinhardt for valuable feedback,"Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",0
4740,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4741,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
4742,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research","Celik, and A",0
4743,HN was partially supported by aSamsung Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4744,We thank Jacob Steinhardt for valuable feedback,We thank Jacob Steinhardt for valuable feedback,1
4745,HN was partially supported by aSamsung Fellowship,(c) Test error vs,0
4746,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",We thank Jacob Steinhardt for valuable feedback,1
4747,We thank Jacob Steinhardt for valuable feedback,We thank Jacob Steinhardt for valuable feedback,1
4748,HN was partially supported by aSamsung Fellowship,"By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data",0
4749,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",0
4750,We thank Jacob Steinhardt for valuable feedback,We thank Jacob Steinhardt for valuable feedback,1
4751,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4752,We thank Jacob Steinhardt for valuable feedback,We thank Jacob Steinhardt for valuable feedback,1
4753,We thank Jacob Steinhardt for valuable feedback,"Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",0
4754,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8",0
4755,HN was partially supported by aSamsung Fellowship,"Thus, theoptimization problem is NP-hard.",0
4756,HN was partially supported by aSamsung Fellowship,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",0
4757,HN was partially supported by aSamsung Fellowship,The action space is binary: push the cartleft or right with a ﬁxed force,0
4758,We thank Jacob Steinhardt for valuable feedback,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",0
4759,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0",0
4760,HN was partially supported by aSamsung Fellowship,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
4761,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,"Kavukcuoglu, D",0
4762,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞",0
4763,HN was partially supported by aSamsung Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4764,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4765,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",We thank Jacob Steinhardt for valuable feedback,1
4766,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4767,We thank Jacob Steinhardt for valuable feedback,We thank Jacob Steinhardt for valuable feedback,1
4768,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",We thank Jacob Steinhardt for valuable feedback,1
4769,We thank Jacob Steinhardt for valuable feedback,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),0
4770,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4771,We thank Jacob Steinhardt for valuable feedback,We thank Jacob Steinhardt for valuable feedback,1
4772,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",We thank Jacob Steinhardt for valuable feedback,1
4773,HN was partially supported by aSamsung Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4774,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",We thank Jacob Steinhardt for valuable feedback,1
4775,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =",0
4776,We thank Jacob Steinhardt for valuable feedback,We thank Jacob Steinhardt for valuable feedback,1
4777,We thank Jacob Steinhardt for valuable feedback,G,0
4778,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",We thank Jacob Steinhardt for valuable feedback,1
4779,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research","Thus, theoptimization problem is NP-hard.",0
4780,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",We thank Jacob Steinhardt for valuable feedback,1
4781,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,0
4782,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4783,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,We thank Jacob Steinhardt for valuable feedback,1
4784,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4785,We thank Jacob Steinhardt for valuable feedback,We thank Jacob Steinhardt for valuable feedback,1
4786,We thank Jacob Steinhardt for valuable feedback,We thank Jacob Steinhardt for valuable feedback,1
4787,HN was partially supported by aSamsung Fellowship,We thank Jacob Steinhardt for valuable feedback,1
4788,We thank Jacob Steinhardt for valuable feedback,We thank Jacob Steinhardt for valuable feedback,1
4789,We thank Jacob Steinhardt for valuable feedback,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
4790,We thank Jacob Steinhardt for valuable feedback,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
4791,HN was partially supported by aSamsung Fellowship,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",0
4792,We thank Jacob Steinhardt for valuable feedback,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
4793,We thank Jacob Steinhardt for valuable feedback,"For worst-case regions P formed by Wasserstein balls, Esfahani & Kuhn (2015),Shaﬁeezadeh-Abadeh et al",0
4794,HN was partially supported by aSamsung Fellowship,"In Security and Privacy (SP), 2016 IEEE Symposiumon, pp",0
4795,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",0
4796,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",0
4797,We thank Jacob Steinhardt for valuable feedback,We thank Jacob Steinhardt for valuable feedback,1
4798,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,We thank Jacob Steinhardt for valuable feedback,1
4799,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
4800,Roy,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
4801,M,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",0
4802,Roy,G,1
4803,M,G,1
4804,K,14.27 and Prop,0
4805,Kuhn,G,1
4806,"arXiv:1703.11008 [cs.LG], 2017.P","9–10)), we have",0
4807,"arXiv:1505.05116 [math.OC],2015.",G,1
4808,Computing nonvacuous generalization bounds for deep (stochastic)neural networks with many more parameters than training data,Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,0
4809,M,"WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.",0
4810,Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,G,1
4811,Roy,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",0
4812,"arXiv:1505.05116 [math.OC],2015.",G,1
4813,Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,G,1
4814,M,G,1
4815,M,"In this regime(small γ, large ), performance between WRM and other methods diverge",0
4816,"arXiv:1703.11008 [cs.LG], 2017.P",the adversarial perturbation level adv,0
4817,M,G,1
4818,Computing nonvacuous generalization bounds for deep (stochastic)neural networks with many more parameters than training data,"Jha, and A",0
4819,Dziugaite and D,G,1
4820,Esfahani and D,G,1
4821,"arXiv:1505.05116 [math.OC],2015.","Then there exists θ such that
this optimization problem is also NP-hard.",0
4822,K,G,1
4823,Roy,G,1
4824,Dziugaite and D,G,1
4825,M,"In this regime(small γ, large ), performance between WRM and other methods diverge",0
4826,Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,G,1
4827,Roy,G,1
4828,"arXiv:1703.11008 [cs.LG], 2017.P","By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",0
4829,G,"The vertical bar in (a), (c), and (e) indicates the perturbation level that was",0
4830,Roy,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",0
4831,M,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",0
4832,"arXiv:1703.11008 [cs.LG], 2017.P",Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,0
4833,K,G,1
4834,Dziugaite and D,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
4835,Esfahani and D,G,1
4836,M,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",0
4837,M,All models are trained in the ∞-norm,0
4838,"arXiv:1505.05116 [math.OC],2015.",G,1
4839,K,We omit the certiﬁcate’s error,0
4840,Computing nonvacuous generalization bounds for deep (stochastic)neural networks with many more parameters than training data,We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,0
4841,M,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",0
4842,M,G,1
4843,"arXiv:1505.05116 [math.OC],2015.",G,1
4844,"arXiv:1703.11008 [cs.LG], 2017.P",G,1
4845,K,G,1
4846,M,G,1
4847,Kuhn,G,1
4848,Esfahani and D,G,1
4849,Esfahani and D,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,0
4850,Computing nonvacuous generalization bounds for deep (stochastic)neural networks with many more parameters than training data,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",0
4851,M,"ERM and FGM suffer from sensitivities to various regions of the data,as evidenced by the lack of symmetry in their classiﬁcation boundaries",0
4852,K,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",0
4853,M,G,1
4854,Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,G,1
4855,"arXiv:1505.05116 [math.OC],2015.",G,1
4856,Roy,372–387,0
4857,K,G,1
4858,Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,G,1
4859,"arXiv:1703.11008 [cs.LG], 2017.P","In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",0
4860,Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",0
4861,"arXiv:1703.11008 [cs.LG], 2017.P","We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1",0
4862,Esfahani and D,"For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering",0
4863,"arXiv:1505.05116 [math.OC],2015.","AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",0
4864,G,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",0
4865,Roy,"2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",0
4866,Kuhn,"For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM",0
4867,Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,G,1
4868,Dziugaite and D,"Kavukcuoglu, D",0
4869,"arXiv:1505.05116 [math.OC],2015.",G,1
4870,M,G,1
4871,Computing nonvacuous generalization bounds for deep (stochastic)neural networks with many more parameters than training data,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",0
4872,M,"Wetrain a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELUactivations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM",0
4873,"arXiv:1703.11008 [cs.LG], 2017.P","a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",0
4874,Kuhn,"Indeed, inthe large-perturbation regime, efﬁciently training certiﬁably secure systems remains an importantopen question",0
4875,Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,"In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",0
4876,Dziugaite and D,All models are trained in the ∞-norm,0
4877,Computing nonvacuous generalization bounds for deep (stochastic)neural networks with many more parameters than training data,G,1
4878,K,"These results suggest advantages of networks with smooth activations
rather than ReLU’s",0
4879,Computing nonvacuous generalization bounds for deep (stochastic)neural networks with many more parameters than training data,In constrast to f-divergences (e.g,0
4880,"arXiv:1703.11008 [cs.LG], 2017.P",This result is essentially due to Katz et al.(2017a),0
4881,"arXiv:1703.11008 [cs.LG], 2017.P","Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)",0
4882,M,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost",0
4883,"arXiv:1505.05116 [math.OC],2015.",G,1
4884,M,PGM attacks on the MNIST dataset,0
4885,Dziugaite and D,G,1
4886,Dziugaite and D,For large adversaries(i.e,0
4887,M,Explicit distributional robustness of the form (5) is intractable except in limited cases,0
4888,Dziugaite and D,G,1
4889,Esfahani and D,G,1
4890,Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,"With that said, the strong dualityresult, Proposition 1, still applies to any distribution",0
4891,G,G,1
4892,Esfahani and D,G,1
4893,Dziugaite and D,G,1
4894,Esfahani and D,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
4895,Computing nonvacuous generalization bounds for deep (stochastic)neural networks with many more parameters than training data,G,1
4896,Esfahani and D,G,1
4897,K,"arXiv:1703.11008 [cs.LG], 2017.P",0
4898,K,"Ried-miller, A",0
4899,"arXiv:1505.05116 [math.OC],2015.",Kuhn,0
4900,"Graves, M",V,1
4901,"Kavukcuoglu, D",V,1
4902,K,This result is essentially due to Katz et al.(2017a),0
4903,"Kavukcuoglu, D",V,1
4904,"Nature, 518(7540):529–533, 2015.","These results suggest advantages of networks with smooth activations
rather than ReLU’s",0
4905,"Ried-miller, A",V,1
4906,"Ried-miller, A","Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",0
4907,A,"In this section, we give an adapation ofthe results in Sections 2 and 3 (Theorems 2 and 4) to such scenarios",0
4908,V,V,1
4909,"Kavukcuoglu, D","Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.",0
4910,"Veness, M",V,1
4911,"Silver, A",V,1
4912,G,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",0
4913,"Graves, M",Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),0
4914,"Kavukcuoglu, D",V,1
4915,"Graves, M","Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)",0
4916,"Bellemare, A","From Lemma 5, our previous results (Theorems 2 and 4) follow",0
4917,"Bellemare, A",We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,0
4918,A,V,1
4919,"Fidjeland, G",V,1
4920,"Bellemare, A",V,1
4921,"Rusu, J",In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,0
4922,A,V,1
4923,V,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",0
4924,"Ried-miller, A","As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes",0
4925,"Veness, M",V,1
4926,V,V,1
4927,"Bellemare, A","In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",0
4928,"Fidjeland, G","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",0
4929,"Rusu, J",V,1
4930,"Kavukcuoglu, D",V,1
4931,"Rusu, J",V,1
4932,G,V,1
4933,V,from which it is easy to see that f is inf-compact,0
4934,"Graves, M","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",0
4935,"Graves, M",V,1
4936,"Mnih, K","The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",0
4937,"Nature, 518(7540):529–533, 2015.","Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness",0
4938,"Rusu, J",V,1
4939,"Mnih, K","See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",0
4940,V,Madry et al,0
4941,A,V,1
4942,"Mnih, K","Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",0
4943,G,"(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave",0
4944,V,V,1
4945,A,V,1
4946,V,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",0
4947,"Ostrovski, et al",V,1
4948,G,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",0
4949,Human-level control through deep reinforcementlearning,V,1
4950,Human-level control through deep reinforcementlearning,"Throughout, we assume Θ ⊆ Rd.Lemma 1",0
4951,A,V,1
4952,"Fidjeland, G",AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
4953,K,V,1
4954,"Graves, M","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",0
4955,"Mnih, K",V,1
4956,Human-level control through deep reinforcementlearning,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",0
4957,"Ried-miller, A",V,1
4958,"Ried-miller, A",V,1
4959,"Kavukcuoglu, D",V,1
4960,"Bellemare, A",Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,0
4961,Human-level control through deep reinforcementlearning,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes",0
4962,"Rusu, J",V,1
4963,"Fidjeland, G",V,1
4964,"Rusu, J",V,1
4965,Human-level control through deep reinforcementlearning,"Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞",0
4966,"Ostrovski, et al","Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B",0
4967,"Rusu, J","To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section",0
4968,"Nature, 518(7540):529–533, 2015.","Fidjeland, G",0
4969,"Nature, 518(7540):529–533, 2015.",V,1
4970,"Bellemare, A","Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.",0
4971,"Nature, 518(7540):529–533, 2015.","arXiv:1703.11008 [cs.LG], 2017.P",0
4972,"Graves, M",V,1
4973,"Graves, M",V,1
4974,"Bellemare, A",V,1
4975,"Graves, M",V,1
4976,"Ostrovski, et al","See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",0
4977,"Mnih, K","Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)",0
4978,"Graves, M",V,1
4979,"Bellemare, A",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
4980,"Silver, A",V,1
4981,"Kavukcuoglu, D",V,1
4982,"Silver, A",V,1
4983,G,V,1
4984,"Kavukcuoglu, D",Madry et al,0
4985,"Nature, 518(7540):529–533, 2015.",V,1
4986,"Kavukcuoglu, D","Jha, M",0
4987,"Ostrovski, et al",V,1
4988,"Fidjeland, G",We typically solve the penalty problem (2) with P0 replaced,0
4989,"Veness, M","9–10)), we have",0
4990,"Ostrovski, et al",theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,0
4991,"Graves, M",V,1
4992,"Nature, 518(7540):529–533, 2015.",The below general duality result gives Proposition 1 as an immediate specialcase,0
4993,G,V,1
4994,"Graves, M","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",0
4995,"Graves, M","Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",0
4996,"Silver, A",V,1
4997,"Mnih, K",V,1
4998,"Bellemare, A","Bellemare, A",0
4999,A,V,1
5000,"2574–2582, 2016.","We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty",0
5001,Deepfool: a simple and accurate method to fooldeep neural networks,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
5002,S.-M,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R",0
5003,Frossard,S.-M,1
5004,"Fawzi, and P",S.-M,1
5005,Frossard,We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,0
5006,Frossard,S.-M,1
5007,Frossard,S.-M,1
5008,"2574–2582, 2016.","(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c",0
5009,"Fawzi, and P",S.-M,1
5010,Deepfool: a simple and accurate method to fooldeep neural networks,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",0
5011,"In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp","for t = 1, ",0
5012,Frossard,S.-M,1
5013,S.-M,S.-M,1
5014,"In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp","In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8",0
5015,Frossard,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,0
5016,S.-M,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,0
5017,Frossard,K,0
5018,"Fawzi, and P",S.-M,1
5019,Deepfool: a simple and accurate method to fooldeep neural networks,S.-M,1
5020,"2574–2582, 2016.","Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",0
5021,S.-M,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",0
5022,S.-M,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",0
5023,"Fawzi, and P",S.-M,1
5024,Frossard,Weillustrate test misclassiﬁcation error vs,0
5025,"Moosavi-Dezfooli, A","Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",0
5026,"2574–2582, 2016.",S.-M,1
5027,Deepfool: a simple and accurate method to fooldeep neural networks,S.-M,1
5028,"Moosavi-Dezfooli, A",adv for (cid:107)·(cid:107)∞-IFGM attackFigure 6,0
5029,Frossard,S.-M,1
5030,"In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp",S.-M,1
5031,"Fawzi, and P",S.-M,1
5032,"In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp","(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)",0
5033,"2574–2582, 2016.",8.7.1) to obtain,0
5034,Frossard,"arXiv:1703.11008 [cs.LG], 2017.P",0
5035,"Fawzi, and P",S.-M,1
5036,Deepfool: a simple and accurate method to fooldeep neural networks,S.-M,1
5037,"Moosavi-Dezfooli, A",S.-M,1
5038,"Moosavi-Dezfooli, A",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
5039,"Fawzi, and P","Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞",0
5040,Deepfool: a simple and accurate method to fooldeep neural networks,Kuhn,0
5041,"Fawzi, and P",S.-M,1
5042,"Moosavi-Dezfooli, A",S.-M,1
5043,Frossard,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",0
5044,S.-M,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features",0
5045,"2574–2582, 2016.",S.-M,1
5046,"2574–2582, 2016.",S.-M,1
5047,Deepfool: a simple and accurate method to fooldeep neural networks,S.-M,1
5048,S.-M,S.-M,1
5049,"2574–2582, 2016.",S.-M,1
5050,"Fawzi, and P",Figure 3,0
5051,"2574–2582, 2016.",This procedureprovides robustness to uncertainties in state-action transitions,0
5052,S.-M,"Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",0
5053,"In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp",The limitations ofdeep learning in adversarial settings,0
5054,Frossard,S.-M,1
5055,S.-M,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
5056,"Fawzi, and P","The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.",0
5057,"Fawzi, and P",S.-M,1
5058,"2574–2582, 2016.","We compare standard WRM with ∞-norm PGM, FGM,IFGM",0
5059,S.-M,"The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",0
5060,"Fawzi, and P","(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features",0
5061,Frossard,S.-M,1
5062,"2574–2582, 2016.",S.-M,1
5063,"Fawzi, and P",S.-M,1
5064,"In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp",Our optimization result (Theorem 2) appliesonly for small values of robustness ρ and to a limited class of Wasserstein costs,0
5065,S.-M,S.-M,1
5066,Frossard,8.7.1) to obtain,0
5067,"In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp","On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
5068,S.-M,S.-M,1
5069,"Moosavi-Dezfooli, A",Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,0
5070,S.-M,S.-M,1
5071,"Moosavi-Dezfooli, A",S.-M,1
5072,"Fawzi, and P",Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,0
5073,Frossard,S.-M,1
5074,"2574–2582, 2016.",S.-M,1
5075,"Moosavi-Dezfooli, A",S.-M,1
5076,"Moosavi-Dezfooli, A",S.-M,1
5077,"Fawzi, and P",The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),0
5078,Deepfool: a simple and accurate method to fooldeep neural networks,S.-M,1
5079,Deepfool: a simple and accurate method to fooldeep neural networks,S.-M,1
5080,Frossard,We illustrate test misclassiﬁcationerror vs,0
5081,"2574–2582, 2016.",The action space is binary: push the cartleft or right with a ﬁxed force,0
5082,"Moosavi-Dezfooli, A",Esfahani and D,0
5083,"In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp",All of our resultssuitably generalize to this setting with minor modiﬁcations to the robust surrogate (2b) and theabove assumptions (see Section D),0
5084,"Fawzi, and P",S.-M,1
5085,"Moosavi-Dezfooli, A",S.-M,1
5086,Deepfool: a simple and accurate method to fooldeep neural networks,S.-M,1
5087,"Moosavi-Dezfooli, A","14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping",0
5088,S.-M,S.-M,1
5089,"2574–2582, 2016.","From Lemma 5, our previous results (Theorems 2 and 4) follow",0
5090,Frossard,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,0
5091,Frossard,S.-M,1
5092,"2574–2582, 2016.",S.-M,1
5093,"2574–2582, 2016.","Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",0
5094,Frossard,"One natural appli-cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-pointproblem (1) and give specialized bounds for domain adaptation problems",0
5095,S.-M,"Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",0
5096,"2574–2582, 2016.",S.-M,1
5097,"In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp",S.-M,1
5098,"2574–2582, 2016.",S.-M,1
5099,"In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp","Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",0
5100,"In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp",N,1
5101,372–387,"Rusu, J",0
5102,"Fredrikson, Z","In the former case, since the update (22)simply modiﬁes the state-action transitions (independent of Q), standard results on convergence fortabular Q-learning (e.g",0
5103,Swami,N,1
5104,The limitations ofdeep learning in adversarial settings,"Ried-miller, A",0
5105,"IEEE, 2016b.",with the convention that 0 · ∞ = 0,0
5106,N,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",0
5107,"Fredrikson, Z",N,1
5108,372–387,N,1
5109,"McDaniel, S",N,1
5110,"Papernot, P",N,1
5111,B,N,1
5112,"McDaniel, S",N,1
5113,"Jha, M","Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",0
5114,N,N,1
5115,372–387,N,1
5116,"Papernot, P","Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",0
5117,Swami,allowing us to prevent attacks on the test set,0
5118,The limitations ofdeep learning in adversarial settings,N,1
5119,"In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp",N,1
5120,B,N,1
5121,"In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp",The choice of P in the robust objective (1) affectsboth the richness of the uncertainty set we wish to consider as well as the tractability of the result-ing optimization problem,0
5122,"In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp",N,1
5123,"McDaniel, S","14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping",0
5124,Swami,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,0
5125,"IEEE, 2016b.",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
5126,"In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp","Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)",0
5127,"IEEE, 2016b.",See Section E.1 for the proof of the proposition,0
5128,"Jha, M","Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective",0
5129,"Papernot, P",N,1
5130,"McDaniel, S","By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",0
5131,372–387,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",0
5132,"McDaniel, S",N,1
5133,"Jha, M","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",0
5134,"Celik, and A","Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",0
5135,372–387,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",0
5136,"IEEE, 2016b.","For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ",0
5137,"Jha, M",N,1
5138,372–387,N,1
5139,"Papernot, P",N,1
5140,"Papernot, P","In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s",0
5141,Swami,We omit the certiﬁcate’s error,0
5142,"In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp",N,1
5143,The limitations ofdeep learning in adversarial settings,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,0
5144,"Celik, and A",N,1
5145,B,N,1
5146,"Jha, M","Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞",0
5147,"In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp",N,1
5148,B,Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,0
5149,Swami,A,0
5150,"Fredrikson, Z",N,1
5151,"Jha, M",N,1
5152,"Celik, and A",We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,0
5153,"IEEE, 2016b.",N,1
5154,B,N,1
5155,372–387,AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
5156,"Fredrikson, Z",N,1
5157,"IEEE, 2016b.","Then, we have",0
5158,"McDaniel, S",N,1
5159,"In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",0
5160,"Celik, and A",N,1
5161,"Celik, and A",N,1
5162,372–387,N,1
5163,"Papernot, P",Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,0
5164,"Jha, M",N,1
5165,B,N,1
5166,372–387,N,1
5167,Swami,N,1
5168,"Celik, and A",N,1
5169,B,"These works provide an initial foundation for adversarial training, but itis challenging to rigorously identify the classes of attacks against which they can defend (or if theyexist)",0
5170,"Papernot, P",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
5171,"Fredrikson, Z",We use reward r(β) := e−|β| for the angle β of the pole from the vertical,0
5172,The limitations ofdeep learning in adversarial settings,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)",0
5173,"IEEE, 2016b.","a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",0
5174,N,N,1
5175,"In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp",N,1
5176,N,N,1
5177,"Jha, M",N,1
5178,The limitations ofdeep learning in adversarial settings,N,1
5179,Swami,"Szepesv´ari & Littman (1999)) apply under these adversarial dynamics.We test our adversarial training procedure in the cart-pole environment, where the goal is to balancea pole on a cart by moving the cart left or right",0
5180,B,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
5181,B,N,1
5182,"In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp",Let ∆F ≥ F (θ0) − inf θ F (θ),0
5183,N,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label",0
5184,"In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp","Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",0
5185,"McDaniel, S","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",0
5186,Swami,N,1
5187,"Jha, M",N,1
5188,"IEEE, 2016b.",Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,0
5189,Swami,N,1
5190,"IEEE, 2016b.","Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",0
5191,"In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp","WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",0
5192,"Jha, M",N,1
5193,Swami,"The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds",0
5194,"IEEE, 2016b.",N,1
5195,"Fredrikson, Z",N,1
5196,372–387,The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,0
5197,"Jha, M","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",0
5198,"In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp",N,1
5199,"Celik, and A","Then, with probability at leastner, 1996, Ch",0
5200,"Wu, S","In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",0
5201,"Wu, S","Ostrovski, et al",0
5202,"McDaniel, X","For imperceptible perturbations, our method matches or outperformsheuristic approaches.",0
5203,582–597,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",0
5204,"In Security and Privacy (SP), 2016 IEEE Symposiumon, pp","Wu, S",0
5205,582–597,"Moosavi-Dezfooli, A",0
5206,"Wu, S",N,1
5207,582–597,"Thus, weonly compare with an agent trained on the nominal MDP",0
5208,N,N,1
5209,N,N,1
5210,N,N,1
5211,Swami,The limitations ofdeep learning in adversarial settings,0
5212,"McDaniel, X",Since 70% of the data are of the,0
5213,N,"Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",0
5214,"In Security and Privacy (SP), 2016 IEEE Symposiumon, pp",Deepfool: a simple and accurate method to fooldeep neural networks,0
5215,Swami,N,1
5216,"Wu, S","Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",0
5217,Distillation as a defense to adversarialperturbations against deep neural networks,N,1
5218,"In Security and Privacy (SP), 2016 IEEE Symposiumon, pp",N,1
5219,"Jha, and A",N,1
5220,"Papernot, P","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",0
5221,N,N,1
5222,"McDaniel, X",N,1
5223,"IEEE, 2016c.","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",0
5224,"Papernot, P",N,1
5225,"IEEE, 2016c.",N,1
5226,"Jha, and A",N,1
5227,"In Security and Privacy (SP), 2016 IEEE Symposiumon, pp",N,1
5228,"McDaniel, X",N,1
5229,"Wu, S","These results suggest advantages of networks with smooth activations
rather than ReLU’s",0
5230,"Wu, S","Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
5231,"McDaniel, X",N,1
5232,Swami,N,1
5233,N,N,1
5234,"IEEE, 2016c.",N,1
5235,582–597,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,0
5236,"Wu, S",N,1
5237,"Papernot, P","a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",0
5238,"IEEE, 2016c.",N,1
5239,582–597,B,0
5240,"Papernot, P",N,1
5241,"McDaniel, X",N,1
5242,"IEEE, 2016c.",N,1
5243,Swami,HN was partially supported by aSamsung Fellowship,0
5244,N,Let ∆F ≥ F (θ0) − inf θ F (θ),0
5245,"Papernot, P","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",0
5246,"IEEE, 2016c.","(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D",0
5247,N,"Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)",0
5248,"IEEE, 2016c.","To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",0
5249,"IEEE, 2016c.",N,1
5250,N,Frossard,0
5251,Swami,372–387,0
5252,Distillation as a defense to adversarialperturbations against deep neural networks,The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),0
5253,"Papernot, P","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",0
5254,"Wu, S",N,1
5255,"Wu, S",Figure 3,0
5256,N,N,1
5257,Distillation as a defense to adversarialperturbations against deep neural networks,N,1
5258,582–597,N,1
5259,Swami,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,0
5260,Swami,N,1
5261,N,N,1
5262,582–597,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",0
5263,"Wu, S",N,1
5264,"IEEE, 2016c.","The smoothness of (cid:96) is essential so that a penalized version(cid:96)(θ, z) − γc(z, z0) is concave in z (which can be approximately veriﬁed by computing Hessians∇2zz(cid:96)(θ, z) for each training datapoint), allowing computation and our coming certiﬁcates of opti-mality",0
5265,"Wu, S","We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5",0
5266,Swami,"In this proof, we drop the subscript on the iteration t to ease notation",0
5267,N,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",0
5268,"Wu, S",N,1
5269,Swami,N,1
5270,"Papernot, P",N,1
5271,"IEEE, 2016c.",N,1
5272,"Wu, S","In Security and Privacy (SP), 2016 IEEE Symposiumon, pp",0
5273,"In Security and Privacy (SP), 2016 IEEE Symposiumon, pp","Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)",0
5274,"McDaniel, X",N,1
5275,"Wu, S",N,1
5276,"Papernot, P",N,1
5277,"Papernot, P",Madry et al,0
5278,Distillation as a defense to adversarialperturbations against deep neural networks,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
5279,"IEEE, 2016c.","We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5",0
5280,Swami,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,0
5281,"Papernot, P","the proof.See Section C.3 forthe condition2] ≤ σ2 holds (to within a constant factor) whenever (cid:107)∇θ(cid:96)(θ, z)(cid:107)2 ≤ σE[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2for all θ, z",0
5282,582–597,N,1
5283,582–597,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",0
5284,"IEEE, 2016c.",Figure 9,0
5285,N,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",0
5286,"IEEE, 2016c.","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",0
5287,"IEEE, 2016c.","In this regime(small γ, large ), performance between WRM and other methods diverge",0
5288,"Wu, S","Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
5289,Swami,"Fredrikson, Z",0
5290,"Papernot, P","In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]",0
5291,"Jha, and A","From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",0
5292,"McDaniel, X","Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)",0
5293,"In Security and Privacy (SP), 2016 IEEE Symposiumon, pp",N,1
5294,Swami,"In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts",0
5295,"IEEE, 2016c.",N,1
5296,582–597,N,1
5297,N,"Kavukcuoglu, D",0
5298,N,HN was partially supported by aSamsung Fellowship,0
5299,"In Security and Privacy (SP), 2016 IEEE Symposiumon, pp",N,1
5300,Further attacks on the MNIST dataset,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",0
5301,(c) Test error vs,"When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",0
5302,adv for (cid:107)·(cid:107)∞-IFGM attackFigure 6,"Papernot, P",0
5303,"Top row: FGM attacks, bottom row: IFGM attacks",We illustrate test misclassiﬁcationerror vs,0
5304,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,0
5305,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",Attacks on the MNIST dataset,0
5306,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",0
5307,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,(c) Test error vs,1
5308,(c) Test error vs,(c) Test error vs,1
5309,We illustrate test misclassiﬁcation error vs.theadversarial perturbation level adv,(c) Test error vs,1
5310,"Top row: FGM attacks, bottom row: IFGM attacks","In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",0
5311,adv for (cid:107)·(cid:107)∞-IFGM attackFigure 6,"By appropriatescaling of θ, v, and w, Katz et al",0
5312,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",(c) Test error vs,1
5313,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",We observe that similar trends as inSection A.5.1 hold again.,0
5314,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",(c) Test error vs,1
5315,Further attacks on the MNIST dataset,(c) Test error vs,1
5316,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",0
5317,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,(c) Test error vs,1
5318,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,"This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al",0
5319,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",Robust MDP’s consider an ambiguity set Psa for state-action transitions,0
5320,"Top row: FGM attacks, bottom row: IFGM attacks","The vertical bar in (a), (c), and (e) indicates the perturbation level that was",0
5321,"Top row: FGM attacks, bottom row: IFGM attacks",(c) Test error vs,1
5322,(c) Test error vs,(c) Test error vs,1
5323,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
5324,"Top row: FGM attacks, bottom row: IFGM attacks",(c) Test error vs,1
5325,"Top row: FGM attacks, bottom row: IFGM attacks","We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",0
5326,adv for (cid:107)·(cid:107)∞-IFGM attackFigure 6,"The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.",0
5327,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",0
5328,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
5329,We illustrate test misclassiﬁcation error vs.theadversarial perturbation level adv,The action space is binary: push the cartleft or right with a ﬁxed force,0
5330,"Top row: FGM attacks, bottom row: IFGM attacks","Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",0
5331,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",(c) Test error vs,1
5332,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",(c) Test error vs,1
5333,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks","In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks",0
5334,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks","Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))",0
5335,(c) Test error vs,(c) Test error vs,1
5336,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,(c) Test error vs,1
5337,Further attacks on the MNIST dataset,(c) Test error vs,1
5338,Further attacks on the MNIST dataset,(c) Test error vs,1
5339,"Top row: FGM attacks, bottom row: IFGM attacks",(c) Test error vs,1
5340,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,(c) Test error vs,1
5341,(c) Test error vs,(c) Test error vs,1
5342,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",0
5343,(c) Test error vs,"For any distribution Q and any ρ > 0,(5)",0
5344,Further attacks on the MNIST dataset,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",0
5345,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",(c) Test error vs,1
5346,adv for (cid:107)·(cid:107)∞-IFGM attackFigure 6,(c) Test error vs,1
5347,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated","From Lemma 5, our previous results (Theorems 2 and 4) follow",0
5348,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,(c) Test error vs,1
5349,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated","Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
5350,Further attacks on the MNIST dataset,Explicit distributional robustness of the form (5) is intractable except in limited cases,0
5351,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,(c) Test error vs,1
5352,"Top row: FGM attacks, bottom row: IFGM attacks","By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives",0
5353,(c) Test error vs,(c) Test error vs,1
5354,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",(c) Test error vs,1
5355,(c) Test error vs,(c) Test error vs,1
5356,"Top row: FGM attacks, bottom row: IFGM attacks",(f) Test error vs,0
5357,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",(c) Test error vs,1
5358,Further attacks on the MNIST dataset,(c) Test error vs,1
5359,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",(c) Test error vs,1
5360,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,"Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)",0
5361,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,(c) Test error vs,1
5362,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,(c) Test error vs,1
5363,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",(c) Test error vs,1
5364,Further attacks on the MNIST dataset,We illustrate test misclassiﬁcation error vs,0
5365,Further attacks on the MNIST dataset,(c) Test error vs,1
5366,adv for (cid:107)·(cid:107)∞-IFGM attackFigure 6,"Veness, M",0
5367,"Top row: FGM attacks, bottom row: IFGM attacks",(c) Test error vs,1
5368,adv for (cid:107)·(cid:107)∞-IFGM attackFigure 6,"Bellemare, A",0
5369,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",(c) Test error vs,1
5370,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated","For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",0
5371,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,(c) Test error vs,1
5372,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,(c) Test error vs,1
5373,(c) Test error vs,(c) Test error vs,1
5374,Further attacks on the MNIST dataset,We use reward r(β) := e−|β| for the angle β of the pole from the vertical,0
5375,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,(c) Test error vs,1
5376,We illustrate test misclassiﬁcation error vs.theadversarial perturbation level adv,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",0
5377,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",(c) Test error vs,1
5378,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",(c) Test error vs,1
5379,adv for (cid:107)·(cid:107)∞-IFGM attackFigure 6,(c) Test error vs,1
5380,Further attacks on the MNIST dataset,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,0
5381,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",0
5382,(c) Test error vs,(c) Test error vs,1
5383,(c) Test error vs,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,0
5384,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",(c) Test error vs,1
5385,We illustrate test misclassiﬁcation error vs.theadversarial perturbation level adv,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes",0
5386,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",14.27 and Prop,0
5387,Further attacks on the MNIST dataset,Esfahani and D,0
5388,adv for (cid:107)·(cid:107)∞-IFGM attackFigure 6,(a) and (b) show test misclassiﬁcation error vs,0
5389,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,(c) Test error vs,1
5390,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,0
5391,"Top row: FGM attacks, bottom row: IFGM attacks",(c) Test error vs,1
5392,adv for (cid:107)·(cid:107)∞-IFGM attackFigure 6,(c) Test error vs,1
5393,(c) Test error vs,(c) Test error vs,1
5394,adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,"Finally, we consider a reinforcement learning problemin Section 4.3, where the Markov decision process used for training differs from that for testing.WRM enjoys the theoretical guarantees of Sections 2 and 3 for large γ, but for small γ (large adver-sarial budgets), WRM becomes a heuristic like other methods",0
5395,"Top row: FGM attacks, bottom row: IFGM attacks","Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)",0
5396,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
5397,adv for (cid:107)·(cid:107)∞-IFGM attackFigure 6,(c) Test error vs,1
5398,adv for (cid:107)·(cid:107)∞-IFGM attackFigure 6,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4",0
5399,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,0
5400,Other models do not exhibit this behavior with the same consistency(if at all),"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5401,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5402,Other models do not exhibit this behavior with the same consistency(if at all),"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5403,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5404,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞",0
5405,Other models do not exhibit this behavior with the same consistency(if at all),"In this work, we treat a much larger class of losses and costs and provide directsolution methods for a Lagrangian relaxation of the saddle-point problem (1)",0
5406,Other models do not exhibit this behavior with the same consistency(if at all),"Fidjeland, G",0
5407,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",(16)) in θ,0
5408,Other models do not exhibit this behavior with the same consistency(if at all),All of our resultssuitably generalize to this setting with minor modiﬁcations to the robust surrogate (2b) and theabove assumptions (see Section D),0
5409,Other models do not exhibit this behavior with the same consistency(if at all),"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5410,Other models do not exhibit this behavior with the same consistency(if at all),"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",0
5411,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering",0
5412,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5413,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5414,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5415,Other models do not exhibit this behavior with the same consistency(if at all),"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5416,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5417,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5418,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
5419,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1",0
5420,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","Weprovide an adversarial training procedure that, for smooth (cid:96), enjoys convergence guarantees simi-lar to non-robust approaches while certifying performance even for the worst-case population losssupP∈P EP [(cid:96)(θ; Z)]",0
5421,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",0
5422,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5423,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5424,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5425,Other models do not exhibit this behavior with the same consistency(if at all),Further attacks on the MNIST dataset,0
5426,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"In most scenarios, however,it is reasonable to defend against weaker adversaries that instead perturb inﬂuential features more.We consider this setting and train against (cid:107)·(cid:107)2-norm attacks",0
5427,Other models do not exhibit this behavior with the same consistency(if at all),"Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)",0
5428,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds",0
5429,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",0
5430,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.",0
5431,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,The below general duality result gives Proposition 1 as an immediate specialcase,0
5432,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",Robust MDP’s consider an ambiguity set Psa for state-action transitions,0
5433,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5434,Other models do not exhibit this behavior with the same consistency(if at all),"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",0
5435,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5436,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5437,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,Swami,0
5438,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5439,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5440,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","Thus, weonly compare with an agent trained on the nominal MDP",0
5441,Other models do not exhibit this behavior with the same consistency(if at all),the adversarial perturbation level adv,0
5442,Other models do not exhibit this behavior with the same consistency(if at all),We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,0
5443,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",0
5444,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5445,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5446,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5447,Other models do not exhibit this behavior with the same consistency(if at all),"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5448,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5449,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",See Section E.1 for the proof of the proposition,0
5450,Other models do not exhibit this behavior with the same consistency(if at all),"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5451,Other models do not exhibit this behavior with the same consistency(if at all),"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5452,Other models do not exhibit this behavior with the same consistency(if at all),"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5453,Other models do not exhibit this behavior with the same consistency(if at all),"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5454,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,See Section C.4 for its proof,0
5455,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",with the convention that 0 · ∞ = 0,0
5456,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
5457,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",0
5458,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),0
5459,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","For any distribution Q and any ρ > 0,(5)",0
5460,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","Ried-miller, A",0
5461,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5462,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,8.7.1) to obtain,0
5463,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5464,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5465,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave",0
5466,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",The,0
5467,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5468,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds",0
5469,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5470,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or",0
5471,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",0
5472,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",0
5473,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
5474,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5475,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5476,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,The action space is binary: push the cartleft or right with a ﬁxed force,0
5477,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",0
5478,Other models do not exhibit this behavior with the same consistency(if at all),"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5479,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"For worst-case regions P formed by Wasserstein balls, Esfahani & Kuhn (2015),Shaﬁeezadeh-Abadeh et al",0
5480,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",0
5481,Other models do not exhibit this behavior with the same consistency(if at all),"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5482,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,"We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",0
5483,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5484,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",See Section E.1 for the proof of the proposition,0
5485,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5486,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",See Section C.4 for its proof,0
5487,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
5488,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5489,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","Ried-miller, A",0
5490,Other models do not exhibit this behavior with the same consistency(if at all),"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",0
5491,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5492,Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,0
5493,Other models do not exhibit this behavior with the same consistency(if at all),"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5494,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5495,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",14.27 and Prop,0
5496,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",A,0
5497,Other models do not exhibit this behavior with the same consistency(if at all),"In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5498,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5499,"In Figure 7, we repeat the illustration in Figure 4(b) for more digits","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",1
5500,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",0
5501,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5502,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5503,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",N,0
5504,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5505,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,0
5506,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5507,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5508,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5509,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5510,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5511,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g",0
5512,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5513,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",0
5514,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5515,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5516,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",0
5517,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5518,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5519,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",Further attacks on the MNIST dataset,0
5520,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5521,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5522,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",0
5523,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",0
5524,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5525,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5526,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",0
5527,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5528,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5529,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",Further attacks on the MNIST dataset,0
5530,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","Thus, weonly compare with an agent trained on the nominal MDP",0
5531,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5532,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5533,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5534,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5535,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,0
5536,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5537,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",0
5538,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",See Section C.2 for the proof,0
5539,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",Figure 1,0
5540,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","IEEE, 2016b.",0
5541,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5542,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5543,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5544,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5545,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5546,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5547,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",0
5548,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",All of our resultssuitably generalize to this setting with minor modiﬁcations to the robust surrogate (2b) and theabove assumptions (see Section D),0
5549,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5550,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",We illustrate test misclassiﬁcationerror vs,0
5551,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,0
5552,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",All models are trained in the ∞-norm,0
5553,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5554,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5555,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",0
5556,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
5557,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5558,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5559,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",0
5560,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
5561,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5562,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5563,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5564,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",0
5565,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5566,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",We illustrate test misclassiﬁcation error vs,0
5567,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,0
5568,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","Then using thepreceding display, we have",0
5569,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5570,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5571,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,0
5572,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",0
5573,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5574,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5575,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts",0
5576,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5577,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5578,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5579,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",0
5580,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",0
5581,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ",0
5582,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",0
5583,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:c((x, y), (x0, y0)) = (cid:107)x − x0(cid:107)2∞ + ∞ · 1{y (cid:54)= y0} .",0
5584,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","Fawzi, and P",0
5585,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5586,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5587,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",0
5588,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5589,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5590,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5591,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5592,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5593,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",(c) Test error vs,0
5594,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","Silver, A",0
5595,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5596,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5597,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",1
5598,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",Dziugaite and D,0
5599,"In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ","If f is inf-compact, then ¯f is directionally differentiable with",0
5600,For large adversaries(i.e,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5601,large desired robustness values) our approach becomes a heuristic just like the other approaches.,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",0
5602,large desired robustness values) our approach becomes a heuristic just like the other approaches.,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd",0
5603,"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
5604,"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance",Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,0
5605,"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance",The function c : Z×Z → R+ is continuous,0
5606,large desired robustness values) our approach becomes a heuristic just like the other approaches.,"This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al",0
5607,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5608,"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5609,For large adversaries(i.e,"In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance",0
5610,For large adversaries(i.e,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5611,For large adversaries(i.e,"WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ",0
5612,"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance","Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
5613,The distinctions in performance between various methodsare less apparent now,Our optimization result (Theorem 2) appliesonly for small values of robustness ρ and to a limited class of Wasserstein costs,0
5614,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5615,The distinctions in performance between various methodsare less apparent now,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5616,large desired robustness values) our approach becomes a heuristic just like the other approaches.,the adversarial perturbation level adv,0
5617,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5618,For large adversaries(i.e,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",0
5619,For large adversaries(i.e,Deﬁne the scale parameter βt > 0 by,0
5620,The distinctions in performance between various methodsare less apparent now,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5621,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",0
5622,large desired robustness values) our approach becomes a heuristic just like the other approaches.,"arXiv:1703.11008 [cs.LG], 2017.P",0
5623,large desired robustness values) our approach becomes a heuristic just like the other approaches.,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5624,For large adversaries(i.e,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5625,large desired robustness values) our approach becomes a heuristic just like the other approaches.,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5626,For large adversaries(i.e,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5627,The distinctions in performance between various methodsare less apparent now,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",0
5628,large desired robustness values) our approach becomes a heuristic just like the other approaches.,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z",0
5629,For large adversaries(i.e,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",0
5630,"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5631,The distinctions in performance between various methodsare less apparent now,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5632,The distinctions in performance between various methodsare less apparent now,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
5633,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5634,The distinctions in performance between various methodsare less apparent now,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5635,"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance",G,0
5636,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5637,large desired robustness values) our approach becomes a heuristic just like the other approaches.,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,0
5638,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5639,For large adversaries(i.e,"All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training",0
5640,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5641,For large adversaries(i.e,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5642,The distinctions in performance between various methodsare less apparent now,"Jha, and A",0
5643,large desired robustness values) our approach becomes a heuristic just like the other approaches.,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5644,For large adversaries(i.e,The major beneﬁt of our approach is its simplicityand wide applicability across many models and machine-learning scenarios.There remain many avenues for future investigation,0
5645,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance",0
5646,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5647,For large adversaries(i.e,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5648,"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5649,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",0
5650,For large adversaries(i.e,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",0
5651,"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5652,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models",0
5653,The distinctions in performance between various methodsare less apparent now,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5654,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5655,The distinctions in performance between various methodsare less apparent now,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5656,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5657,For large adversaries(i.e,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5658,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5659,For large adversaries(i.e,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5660,"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance","(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave",0
5661,large desired robustness values) our approach becomes a heuristic just like the other approaches.,(f) Test error vs,0
5662,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",The function cx : X × X → R+ is continuous,0
5663,The distinctions in performance between various methodsare less apparent now,"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",0
5664,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5665,large desired robustness values) our approach becomes a heuristic just like the other approaches.,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5666,For large adversaries(i.e,(f) Test error vs,0
5667,large desired robustness values) our approach becomes a heuristic just like the other approaches.,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5668,For large adversaries(i.e,In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,0
5669,large desired robustness values) our approach becomes a heuristic just like the other approaches.,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,0
5670,large desired robustness values) our approach becomes a heuristic just like the other approaches.,Attacks on the MNIST dataset,0
5671,large desired robustness values) our approach becomes a heuristic just like the other approaches.,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",0
5672,For large adversaries(i.e,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5673,For large adversaries(i.e,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5674,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",8.7.1) to obtain,0
5675,The distinctions in performance between various methodsare less apparent now,"As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)",0
5676,large desired robustness values) our approach becomes a heuristic just like the other approaches.,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5677,For large adversaries(i.e,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5678,For large adversaries(i.e,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5679,The distinctions in performance between various methodsare less apparent now,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5680,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),0
5681,large desired robustness values) our approach becomes a heuristic just like the other approaches.,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",0
5682,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Rusu, J",0
5683,"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance",Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,0
5684,large desired robustness values) our approach becomes a heuristic just like the other approaches.,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
5685,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5686,"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5687,The distinctions in performance between various methodsare less apparent now,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5688,For large adversaries(i.e,"The vertical bar in (a), (c), and (e) indicates the perturbation level that was",0
5689,The distinctions in performance between various methodsare less apparent now,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5690,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5691,The distinctions in performance between various methodsare less apparent now,WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,0
5692,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5693,The distinctions in performance between various methodsare less apparent now,"arXiv:1505.05116 [math.OC],2015.",0
5694,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,0
5695,For large adversaries(i.e,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5696,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",0
5697,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,0
5698,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets","We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",0
5699,"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",1
5700,Figure 9,"(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c",0
5701,Figure 9,"Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",0
5702,Figure 9,Figure 9,1
5703,Thestatistical error term n(t) is omitted from the certiﬁcate,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",0
5704,Figure 9,Figure 9,1
5705,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,Figure 9,1
5706,Thestatistical error term n(t) is omitted from the certiﬁcate,Figure 9,1
5707,Thestatistical error term n(t) is omitted from the certiﬁcate,The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,0
5708,The vertical bar indicates the achieved level,Figure 9,1
5709,Thestatistical error term n(t) is omitted from the certiﬁcate,Figure 9,1
5710,The vertical bar indicates the achieved level,"Veness, M",0
5711,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,Figure 9,1
5712,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,Figure 9,1
5713,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,Figure 9,1
5714,The vertical bar indicates the achieved level,"Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞",0
5715,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",0
5716,Thestatistical error term n(t) is omitted from the certiﬁcate,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",0
5717,The vertical bar indicates the achieved level,Figure 9,1
5718,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,Figure 9,1
5719,The vertical bar indicates the achieved level,Attacks on the MNIST dataset,0
5720,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,Figure 9,1
5721,Figure 9,We assume without loss of2 − z0 (cid:54)= 0,0
5722,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,Deepfool: a simple and accurate method to fooldeep neural networks,0
5723,Figure 9,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",0
5724,Thestatistical error term n(t) is omitted from the certiﬁcate,Figure 9,1
5725,Thestatistical error term n(t) is omitted from the certiﬁcate,"Top row: FGM attacks, bottom row: IFGM attacks",0
5726,Thestatistical error term n(t) is omitted from the certiﬁcate,"We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",0
5727,The vertical bar indicates the achieved level,"Kavukcuoglu, D",0
5728,The vertical bar indicates the achieved level,"Assume thatΘ ⊂ Rd satisﬁes diam(Θ) = supθ,θ(cid:48)∈Θ (cid:107)θ − θ(cid:48)(cid:107) < ∞",0
5729,Thestatistical error term n(t) is omitted from the certiﬁcate,Figure 9,1
5730,Figure 9,The action space is binary: push the cartleft or right with a ﬁxed force,0
5731,Figure 9,Figure 9,1
5732,Figure 9,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives",0
5733,Thestatistical error term n(t) is omitted from the certiﬁcate,Figure 9,1
5734,Figure 9,Figure 9,1
5735,Figure 9,Figure 9,1
5736,Thestatistical error term n(t) is omitted from the certiﬁcate,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).",0
5737,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,"Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)",0
5738,The vertical bar indicates the achieved level,Figure 9,1
5739,The vertical bar indicates the achieved level,Figure 9,1
5740,The vertical bar indicates the achieved level,The limitations ofdeep learning in adversarial settings,0
5741,Figure 9,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations",0
5742,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,The function cx : X × X → R+ is continuous,0
5743,Figure 9,Figure 9,1
5744,Thestatistical error term n(t) is omitted from the certiﬁcate,"(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D",0
5745,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,Figure 9,1
5746,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,We assume without loss of2 − z0 (cid:54)= 0,0
5747,The vertical bar indicates the achieved level,Figure 9,1
5748,The vertical bar indicates the achieved level,Kuhn,0
5749,The vertical bar indicates the achieved level,"We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3",0
5750,Thestatistical error term n(t) is omitted from the certiﬁcate,Figure 9,1
5751,Thestatistical error term n(t) is omitted from the certiﬁcate,"Wu, S",0
5752,Thestatistical error term n(t) is omitted from the certiﬁcate,Figure 9,1
5753,The vertical bar indicates the achieved level,with the convention that 0 · ∞ = 0,0
5754,Thestatistical error term n(t) is omitted from the certiﬁcate,Figure 9,1
5755,Figure 9,All models are trained in the ∞-norm,0
5756,Figure 9,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",0
5757,Figure 9,"In this regime(small γ, large ), performance between WRM and other methods diverge",0
5758,Thestatistical error term n(t) is omitted from the certiﬁcate,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,0
5759,Figure 9,Figure 9,1
5760,Figure 9,Figure 9,1
5761,Thestatistical error term n(t) is omitted from the certiﬁcate,"However, WRM withReLU’s still suffers from sensitivities (e.g",0
5762,Thestatistical error term n(t) is omitted from the certiﬁcate,"Thus, we see that our adversarial-training method defends against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint untilthe model misclassiﬁes it",0
5763,The vertical bar indicates the achieved level,Figure 9,1
5764,The vertical bar indicates the achieved level,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",0
5765,Figure 9,"Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z",0
5766,Thestatistical error term n(t) is omitted from the certiﬁcate,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z",0
5767,Figure 9,"9–10)), we have",0
5768,The vertical bar indicates the achieved level,Figure 9,1
5769,The vertical bar indicates the achieved level,Figure 9,1
5770,The vertical bar indicates the achieved level,Figure 9,1
5771,Figure 9,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",0
5772,The vertical bar indicates the achieved level,Figure 9,1
5773,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",0
5774,The vertical bar indicates the achieved level,Figure 9,1
5775,Figure 9,"Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)",0
5776,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,Figure 9,1
5777,Thestatistical error term n(t) is omitted from the certiﬁcate,Figure 9,1
5778,The vertical bar indicates the achieved level,Figure 9,1
5779,Figure 9,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",0
5780,Figure 9,allowing us to prevent attacks on the test set,0
5781,Thestatistical error term n(t) is omitted from the certiﬁcate,"arXiv:1703.11008 [cs.LG], 2017.P",0
5782,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",0
5783,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,0
5784,Figure 9,Figure 9,1
5785,Figure 9,"For worst-case regions P formed by Wasserstein balls, Esfahani & Kuhn (2015),Shaﬁeezadeh-Abadeh et al",0
5786,Thestatistical error term n(t) is omitted from the certiﬁcate,Figure 9,1
5787,Thestatistical error term n(t) is omitted from the certiﬁcate,Figure 9,1
5788,Thestatistical error term n(t) is omitted from the certiﬁcate,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",0
5789,The vertical bar indicates the achieved level,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",0
5790,Figure 9,Figure 9,1
5791,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,"(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D",0
5792,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,Let ∆F ≥ F (θ0) − inf θ F (θ),0
5793,Thestatistical error term n(t) is omitted from the certiﬁcate,"Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)",0
5794,The vertical bar indicates the achieved level,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:c((x, y), (x0, y0)) = (cid:107)x − x0(cid:107)2∞ + ∞ · 1{y (cid:54)= y0} .",0
5795,Thestatistical error term n(t) is omitted from the certiﬁcate,Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,0
5796,The vertical bar indicates the achieved level,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",0
5797,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,"Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques",0
5798,Figure 9,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,0
5799,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,Figure 9,1
5800,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,(f) Test error vs,1
5801,the adversarial perturbation level adv,Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,0
5802,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",0
5803,(f) Test error vs,S.-M,0
5804,(f) Test error vs,(f) Test error vs,1
5805,the adversarial perturbation level adv,(f) Test error vs,1
5806,"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",Let ∆F ≥ F (θ0) − inf θ F (θ),0
5807,(f) Test error vs,"In most scenarios, however,it is reasonable to defend against weaker adversaries that instead perturb inﬂuential features more.We consider this setting and train against (cid:107)·(cid:107)2-norm attacks",0
5808,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
5809,Weillustrate test misclassiﬁcation error vs,(f) Test error vs,1
5810,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,0
5811,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks","Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",0
5812,Weillustrate test misclassiﬁcation error vs,"In this regime(small γ, large ), performance between WRM and other methods diverge",0
5813,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,(f) Test error vs,1
5814,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",(f) Test error vs,1
5815,Weillustrate test misclassiﬁcation error vs,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
5816,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",(f) Test error vs,1
5817,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",0
5818,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,(f) Test error vs,1
5819,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",0
5820,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",(f) Test error vs,1
5821,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,Mnih et al,0
5822,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,(f) Test error vs,1
5823,"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks","In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets",0
5824,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
5825,the adversarial perturbation level adv,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,0
5826,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,(f) Test error vs,1
5827,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",(f) Test error vs,1
5828,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,(f) Test error vs,1
5829,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,by solving (7),0
5830,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,"Thus, theoptimization problem is NP-hard.",0
5831,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,(f) Test error vs,1
5832,"The vertical bar in (a), (c), and (e) indicates the perturbation level that was",(f) Test error vs,1
5833,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,(f) Test error vs,1
5834,Weillustrate test misclassiﬁcation error vs,"The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",0
5835,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",(f) Test error vs,1
5836,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,(f) Test error vs,1
5837,(f) Test error vs,(2017) attempt to mitigate this shortcoming,0
5838,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,(f) Test error vs,1
5839,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"In this section, we give an adapation ofthe results in Sections 2 and 3 (Theorems 2 and 4) to such scenarios",0
5840,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,(f) Test error vs,1
5841,(f) Test error vs,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",0
5842,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",(f) Test error vs,1
5843,the adversarial perturbation level adv,(f) Test error vs,1
5844,"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks","As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",0
5845,"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",582–597,0
5846,(f) Test error vs,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
5847,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",Goodfellow et al,0
5848,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",(f) Test error vs,0
5849,the adversarial perturbation level adv,Figure 3,0
5850,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,(f) Test error vs,1
5851,Weillustrate test misclassiﬁcation error vs,(f) Test error vs,1
5852,"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",(f) Test error vs,1
5853,Weillustrate test misclassiﬁcation error vs,"In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts",0
5854,the adversarial perturbation level adv,Computing nonvacuous generalization bounds for deep (stochastic)neural networks with many more parameters than training data,0
5855,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks","Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques",0
5856,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,(f) Test error vs,1
5857,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
5858,"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks","The vertical bar in (a), (c), and (e) indicates the perturbation level that was",0
5859,"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",(f) Test error vs,1
5860,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,(f) Test error vs,1
5861,"The vertical bar in (a), (c), and (e) indicates the perturbation level that was",Figure 9,0
5862,Weillustrate test misclassiﬁcation error vs,"Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z",0
5863,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,(f) Test error vs,1
5864,"The vertical bar in (a), (c), and (e) indicates the perturbation level that was",(f) Test error vs,1
5865,the adversarial perturbation level adv,Other models do not exhibit this behavior with the same consistency(if at all),0
5866,"The vertical bar in (a), (c), and (e) indicates the perturbation level that was","For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ",0
5867,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",0
5868,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,"The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",0
5869,(f) Test error vs,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",0
5870,(f) Test error vs,"In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts",0
5871,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",(f) Test error vs,1
5872,the adversarial perturbation level adv,(f) Test error vs,1
5873,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.",0
5874,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,(f) Test error vs,1
5875,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"Then, with probability at leastner, 1996, Ch",0
5876,(f) Test error vs,This result is essentially due to Katz et al.(2017a),0
5877,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,G,0
5878,"The vertical bar in (a), (c), and (e) indicates the perturbation level that was",(f) Test error vs,1
5879,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",(f) Test error vs,1
5880,"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",(f) Test error vs,1
5881,the adversarial perturbation level adv,"Then there exists θ such that
this optimization problem is also NP-hard.",0
5882,"The vertical bar in (a), (c), and (e) indicates the perturbation level that was","arXiv:1703.11008 [cs.LG], 2017.P",0
5883,"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",(f) Test error vs,1
5884,"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",(f) Test error vs,1
5885,(f) Test error vs,(f) Test error vs,1
5886,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",0
5887,"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",See Section C.2 for the proof,0
5888,"The vertical bar in (a), (c), and (e) indicates the perturbation level that was",(f) Test error vs,1
5889,(f) Test error vs,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",0
5890,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,(f) Test error vs,1
5891,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",(f) Test error vs,1
5892,"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks","The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)",0
5893,(f) Test error vs,(f) Test error vs,1
5894,"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",Computing nonvacuous generalization bounds for deep (stochastic)neural networks with many more parameters than training data,0
5895,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,"For any distribution Q and any ρ > 0,(5)",0
5896,"The vertical bar in (a), (c), and (e) indicates the perturbation level that was","The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)",0
5897,"Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",(f) Test error vs,1
5898,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,(f) Test error vs,1
5899,the adversarial perturbation level adv,(f) Test error vs,1
5900,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.",We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,0
5901,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.",Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,0
5902,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5903,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5904,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",0
5905,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
5906,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","for all γ, ρ ≥ 0",0
5907,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",0
5908,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,0
5909,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","for t = 1, ",0
5910,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5911,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",0
5912,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,0
5913,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
5914,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,large desired robustness values) our approach becomes a heuristic just like the other approaches.,0
5915,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
5916,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,0
5917,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5918,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",0
5919,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5920,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5921,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
5922,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",0
5923,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5924,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,In constrast to f-divergences (e.g,0
5925,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5926,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.",V,0
5927,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)",0
5928,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",0
5929,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"For each z0 ∈ Z, c(·, z0) is 1-stronglyconvex with respect to the norm (cid:107)·(cid:107).",0
5930,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,0
5931,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",0
5932,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",0
5933,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5934,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5935,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","Bellemare, A",0
5936,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input",0
5937,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5938,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",0
5939,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",", K}",0
5940,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.",V,0
5941,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5942,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective",0
5943,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",0
5944,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",0
5945,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5946,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
5947,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5948,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A",0
5949,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5950,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"Graves, M",0
5951,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5952,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5953,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5954,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",0
5955,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.",All mod-els are trained in the ∞-norm,0
5956,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.",The major beneﬁt of our approach is its simplicityand wide applicability across many models and machine-learning scenarios.There remain many avenues for future investigation,0
5957,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z",0
5958,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5959,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5960,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",0
5961,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",In constrast to f-divergences (e.g,0
5962,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5963,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",0
5964,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"In most scenarios, however,it is reasonable to defend against weaker adversaries that instead perturb inﬂuential features more.We consider this setting and train against (cid:107)·(cid:107)2-norm attacks",0
5965,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5966,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",0
5967,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5968,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5969,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5970,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5971,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations",0
5972,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5973,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5974,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5975,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5976,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,0
5977,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5978,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A",0
5979,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5980,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ",0
5981,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",0
5982,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5983,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5984,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5985,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",0
5986,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c",0
5987,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",0
5988,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5989,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5990,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5991,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5992,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"For each γadv, we consider the distance to adversarial examples in the test dataset",0
5993,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5994,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,0
5995,We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5996,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,0
5997,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5998,"Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
5999,"A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",1
6000,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection","Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",0
6001,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection","Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)",0
6002,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",0
6003,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6004,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",The limitations ofdeep learning in adversarial settings,0
6005,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6006,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train","Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv",0
6007,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6008,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",with the convention that 0 · ∞ = 0,1
6009,with the convention that 0 · ∞ = 0,"Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",0
6010,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",with the convention that 0 · ∞ = 0,1
6011,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6012,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,0
6013,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6014,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6015,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",allowing us to prevent attacks on the test set,0
6016,with the convention that 0 · ∞ = 0,This result is essentially due to Katz et al.(2017a),0
6017,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6018,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train","In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",0
6019,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6020,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6021,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",0
6022,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",Theorem 7,0
6023,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",K,0
6024,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",Distillation as a defense to adversarialperturbations against deep neural networks,0
6025,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6026,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6027,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection","For example,Ben-Tal et al",0
6028,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",with the convention that 0 · ∞ = 0,1
6029,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6030,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,0
6031,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection","Due to the nonstationary, policy-dependent radius for the Wassersteinball, an analogous  for the fast-gradient method (or other variants) is not well-deﬁned",0
6032,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6033,with the convention that 0 · ∞ = 0,"In this proof, we drop the subscript on the iteration t to ease notation",0
6034,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection","Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks",0
6035,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6036,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",with the convention that 0 · ∞ = 0,1
6037,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6038,with the convention that 0 · ∞ = 0,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,0
6039,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection","Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",0
6040,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection","57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞",0
6041,with the convention that 0 · ∞ = 0,"These works provide an initial foundation for adversarial training, but itis challenging to rigorously identify the classes of attacks against which they can defend (or if theyexist)",0
6042,with the convention that 0 · ∞ = 0,"Silver, A",0
6043,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",with the convention that 0 · ∞ = 0,1
6044,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",with the convention that 0 · ∞ = 0,1
6045,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,0
6046,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6047,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6048,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",with the convention that 0 · ∞ = 0,1
6049,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6050,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection","WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.",0
6051,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6052,with the convention that 0 · ∞ = 0,M,0
6053,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train","In this regime(small γ, large ), performance between WRM and other methods diverge",0
6054,with the convention that 0 · ∞ = 0,"We note thattudes of ∇z(cid:96)(θ; z) in a neighborhood of the nominal input, which ensures stability of the model.",0
6055,with the convention that 0 · ∞ = 0,We have the following theorem.Theorem 5,0
6056,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",the adversarial perturbation level adv,0
6057,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection","Then there exists θ such that
this optimization problem is also NP-hard.",0
6058,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train","Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label",0
6059,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",with the convention that 0 · ∞ = 0,1
6060,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection","Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",0
6061,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection","Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
6062,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",with the convention that 0 · ∞ = 0,1
6063,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6064,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6065,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",with the convention that 0 · ∞ = 0,1
6066,with the convention that 0 · ∞ = 0,"arXiv:1703.11008 [cs.LG], 2017.P",0
6067,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",with the convention that 0 · ∞ = 0,1
6068,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6069,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection","AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",0
6070,with the convention that 0 · ∞ = 0,"Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks",0
6071,with the convention that 0 · ∞ = 0,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
6072,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train","On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",0
6073,with the convention that 0 · ∞ = 0,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,0
6074,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",(c) Test error vs,0
6075,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6076,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6077,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6078,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6079,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6080,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train","Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks",0
6081,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train","In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.",0
6082,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",with the convention that 0 · ∞ = 0,1
6083,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,0
6084,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6085,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6086,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,0
6087,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",with the convention that 0 · ∞ = 0,1
6088,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection","Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",0
6089,with the convention that 0 · ∞ = 0,with the convention that 0 · ∞ = 0,1
6090,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6091,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6092,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",with the convention that 0 · ∞ = 0,1
6093,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6094,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6095,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",with the convention that 0 · ∞ = 0,1
6096,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",The vertical bar indicates the achieved level,0
6097,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection","WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.",0
6098,"Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,0
6099,"Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",with the convention that 0 · ∞ = 0,1
6100,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6101,"In this regime(small γ, large ), performance between WRM and other methods diverge","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6102,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,0
6103,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:c((x, y), (x0, y0)) = (cid:107)x − x0(cid:107)2∞ + ∞ · 1{y (cid:54)= y0} .","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6104,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",the adversarial perturbation level adv,0
6105,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6106,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",Kuhn,0
6107,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,0
6108,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)","Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",0
6109,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,0
6110,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","2574–2582, 2016.",0
6111,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget","for t = 1, ",0
6112,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)",0
6113,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)","Now, let x(z) beany measurable function that is -close to attaining the supremum above",0
6114,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6115,"In this regime(small γ, large ), performance between WRM and other methods diverge","Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",0
6116,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:c((x, y), (x0, y0)) = (cid:107)x − x0(cid:107)2∞ + ∞ · 1{y (cid:54)= y0} .","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6117,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6118,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6119,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6120,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6121,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large","One natural appli-cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-pointproblem (1) and give specialized bounds for domain adaptation problems",0
6122,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,0
6123,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:c((x, y), (x0, y0)) = (cid:107)x − x0(cid:107)2∞ + ∞ · 1{y (cid:54)= y0} .","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6124,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",Figure 1,0
6125,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget","As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)",0
6126,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6127,"In this regime(small γ, large ), performance between WRM and other methods diverge","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6128,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6129,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6130,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:c((x, y), (x0, y0)) = (cid:107)x − x0(cid:107)2∞ + ∞ · 1{y (cid:54)= y0} .","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6131,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",0
6132,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6133,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6134,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:c((x, y), (x0, y0)) = (cid:107)x − x0(cid:107)2∞ + ∞ · 1{y (cid:54)= y0} .","The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",0
6135,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)","Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",0
6136,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6137,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6138,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6139,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6140,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6141,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,0
6142,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms","Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",0
6143,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",0
6144,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",We illustrate testmisclassiﬁcation error vs,0
6145,"In this regime(small γ, large ), performance between WRM and other methods diverge","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6146,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6147,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",allowing us to prevent attacks on the test set,0
6148,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",0
6149,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms","McDaniel, X",0
6150,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:c((x, y), (x0, y0)) = (cid:107)x − x0(cid:107)2∞ + ∞ · 1{y (cid:54)= y0} .","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6151,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget","To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",0
6152,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6153,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",We illustrate testmisclassiﬁcation error vs,0
6154,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",We have the following theorem.Theorem 5,0
6155,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms","We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A",0
6156,"In this regime(small γ, large ), performance between WRM and other methods diverge","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6157,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",0
6158,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",0
6159,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large","Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
6160,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large","Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations",0
6161,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",0
6162,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6163,"In this regime(small γ, large ), performance between WRM and other methods diverge","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6164,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6165,"(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms","For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0",0
6166,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6167,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6168,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",Figure 2,0
6169,"In this regime(small γ, large ), performance between WRM and other methods diverge","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6170,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,0
6171,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:c((x, y), (x0, y0)) = (cid:107)x − x0(cid:107)2∞ + ∞ · 1{y (cid:54)= y0} .","Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
6172,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),0
6173,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)","Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input",0
6174,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget","From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",0
6175,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",0
6176,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6177,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6178,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,0
6179,"In this regime(small γ, large ), performance between WRM and other methods diverge","For imperceptible perturbations, our method matches or outperformsheuristic approaches.",0
6180,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:c((x, y), (x0, y0)) = (cid:107)x − x0(cid:107)2∞ + ∞ · 1{y (cid:54)= y0} .","We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",0
6181,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,0
6182,"In this regime(small γ, large ), performance between WRM and other methods diverge","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6183,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","Thus, theoptimization problem is NP-hard.",0
6184,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,0
6185,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",0
6186,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6187,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",0
6188,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6189,"(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",Goodfellow et al,0
6190,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6191,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",0
6192,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",Dziugaite and D,0
6193,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6194,"In this regime(small γ, large ), performance between WRM and other methods diverge",Dziugaite and D,0
6195,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6196,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6197,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6198,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness","By appropriatescaling of θ, v, and w, Katz et al",0
6199,"(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",1
6200,We observe that similar trends as inSection A.5.1 hold again.,The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,0
6201,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6202,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries","The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.",0
6203,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6204,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6205,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",0
6206,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries","For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM",0
6207,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6208,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6209,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6210,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6211,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries","For any distribution Q and any ρ > 0,(5)",0
6212,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries","For example,Ben-Tal et al",0
6213,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries","Wu, S",0
6214,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6215,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
6216,We observe that similar trends as inSection A.5.1 hold again.,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6217,We observe that similar trends as inSection A.5.1 hold again.,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,0
6218,We observe that similar trends as inSection A.5.1 hold again.,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6219,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6220,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries","for all γ, ρ ≥ 0",0
6221,We observe that similar trends as inSection A.5.1 hold again.,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,0
6222,We observe that similar trends as inSection A.5.1 hold again.,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6223,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries","One natural appli-cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-pointproblem (1) and give specialized bounds for domain adaptation problems",0
6224,We observe that similar trends as inSection A.5.1 hold again.,S.-M,0
6225,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6226,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,0
6227,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,0
6228,We observe that similar trends as inSection A.5.1 hold again.,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6229,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",0
6230,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6231,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries","Thus, weonly compare with an agent trained on the nominal MDP",0
6232,We observe that similar trends as inSection A.5.1 hold again.,radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,0
6233,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,Swami,0
6234,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6235,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",0
6236,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,"Ried-miller, A",0
6237,We observe that similar trends as inSection A.5.1 hold again.,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6238,We observe that similar trends as inSection A.5.1 hold again.,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6239,We observe that similar trends as inSection A.5.1 hold again.,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6240,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6241,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,"Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",0
6242,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6243,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",Distillation as a defense to adversarialperturbations against deep neural networks,0
6244,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6245,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6246,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6247,We observe that similar trends as inSection A.5.1 hold again.,We have the following theorem.Theorem 5,0
6248,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6249,We observe that similar trends as inSection A.5.1 hold again.,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or",0
6250,We observe that similar trends as inSection A.5.1 hold again.,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
6251,We observe that similar trends as inSection A.5.1 hold again.,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6252,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,"If f is inf-compact, then ¯f is directionally differentiable with",0
6253,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,0
6254,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,0
6255,We observe that similar trends as inSection A.5.1 hold again.,(a) and (b) show test misclassiﬁcation error vs,0
6256,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries","However, WRM withReLU’s still suffers from sensitivities (e.g",0
6257,We observe that similar trends as inSection A.5.1 hold again.,The below general duality result gives Proposition 1 as an immediate specialcase,0
6258,We observe that similar trends as inSection A.5.1 hold again.,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6259,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,"Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",0
6260,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries","By appropriatescaling of θ, v, and w, Katz et al",0
6261,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries","Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.",0
6262,We observe that similar trends as inSection A.5.1 hold again.,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6263,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6264,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",We observe that similar trends as inSection A.5.1 hold again.,0
6265,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
6266,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6267,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6268,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6269,We observe that similar trends as inSection A.5.1 hold again.,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6270,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6271,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6272,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,"For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering",0
6273,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6274,We observe that similar trends as inSection A.5.1 hold again.,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6275,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,Kuhn,0
6276,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,large desired robustness values) our approach becomes a heuristic just like the other approaches.,0
6277,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,(e) Test error vs,0
6278,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6279,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries","Kavukcuoglu, D",0
6280,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",This result is essentially due to Katz et al.(2017a),0
6281,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,"Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective",0
6282,We observe that similar trends as inSection A.5.1 hold again.,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6283,We observe that similar trends as inSection A.5.1 hold again.,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6284,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6285,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6286,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",0
6287,We observe that similar trends as inSection A.5.1 hold again.,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A",0
6288,We observe that similar trends as inSection A.5.1 hold again.,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,0
6289,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6290,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,"Top row: PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",0
6291,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6292,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,0
6293,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6294,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries","Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",0
6295,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,"These results suggest advantages of networks with smooth activations
rather than ReLU’s",0
6296,"In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6297,We observe that similar trends as inSection A.5.1 hold again.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),0
6298,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,"Ostrovski, et al",0
6299,We observe that similar trends as inSection A.5.1 hold again.,(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,1
6300,the adversarial perturbation level adv,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
6301,Attacks on the MNIST dataset,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",0
6302,(e) Test error vs,"The vertical bar in (a), (c), and (e) indicates the estimated",0
6303,"Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks","Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)",0
6304,the adversarial perturbation level adv,Attacks on the MNIST dataset,0
6305,"We compare standard WRM with ∞-norm PGM, FGM,IFGM","Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",0
6306,We illustrate test misclassiﬁcation error vs,"small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",0
6307,We illustrate test misclassiﬁcation error vs,Attacks on the MNIST dataset,0
6308,"Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",It isthus important to distinguish the methods’ abilities to combat attacks,0
6309,"Left column: Euclidean-normattacks, right column: ∞-norm attacks",(e) Test error vs,1
6310,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,(e) Test error vs,1
6311,"We compare standard WRM with ∞-norm PGM, FGM,IFGM","Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",0
6312,the adversarial perturbation level adv,(e) Test error vs,1
6313,"The vertical bar in (a), (c), and (e) indicates the estimated","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
6314,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,(e) Test error vs,1
6315,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,"Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",0
6316,the adversarial perturbation level adv,(e) Test error vs,1
6317,"Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",(e) Test error vs,1
6318,the adversarial perturbation level adv,(e) Test error vs,1
6319,"Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",(e) Test error vs,1
6320,We illustrate test misclassiﬁcation error vs,(e) Test error vs,1
6321,Attacks on the MNIST dataset,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",0
6322,"Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",(e) Test error vs,1
6323,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,This result is essentially due to Katz et al.(2017a),0
6324,Attacks on the MNIST dataset,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,0
6325,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
6326,"Left column: Euclidean-normattacks, right column: ∞-norm attacks",(e) Test error vs,1
6327,Attacks on the MNIST dataset,(e) Test error vs,1
6328,(e) Test error vs,"The accuracy parameter  has a ﬁxed effect on optimization accuracy, independent ofT : approximate maximization has limited effects.Key to the convergence guarantee of Theorem 2 is that the loss (cid:96) is smooth in z: the inner supre-mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section Bfor a proof of this for ReLU’s)",0
6329,"Left column: Euclidean-normattacks, right column: ∞-norm attacks",(e) Test error vs,1
6330,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,(e) Test error vs,1
6331,the adversarial perturbation level adv,(e) Test error vs,1
6332,"We compare standard WRM with ∞-norm PGM, FGM,IFGM",(e) Test error vs,1
6333,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,(e) Test error vs,1
6334,the adversarial perturbation level adv,(e) Test error vs,1
6335,"The vertical bar in (a), (c), and (e) indicates the estimated","As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",0
6336,"Left column: Euclidean-normattacks, right column: ∞-norm attacks",(e) Test error vs,1
6337,Attacks on the MNIST dataset,(e) Test error vs,1
6338,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,(e) Test error vs,1
6339,We illustrate test misclassiﬁcation error vs,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",0
6340,Attacks on the MNIST dataset,(e) Test error vs,1
6341,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,(e) Test error vs,1
6342,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,(e) Test error vs,1
6343,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",0
6344,the adversarial perturbation level adv,See Section C.4 for its proof,0
6345,"We compare standard WRM with ∞-norm PGM, FGM,IFGM",(e) Test error vs,1
6346,(e) Test error vs,(e) Test error vs,1
6347,"The vertical bar in (a), (c), and (e) indicates the estimated",The choice of P in the robust objective (1) affectsboth the richness of the uncertainty set we wish to consider as well as the tractability of the result-ing optimization problem,0
6348,We illustrate test misclassiﬁcation error vs,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
6349,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,(e) Test error vs,1
6350,"Left column: Euclidean-normattacks, right column: ∞-norm attacks","Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",0
6351,Attacks on the MNIST dataset,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",0
6352,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,"As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)",0
6353,"We compare standard WRM with ∞-norm PGM, FGM,IFGM",(e) Test error vs,1
6354,We illustrate test misclassiﬁcation error vs,(e) Test error vs,1
6355,(e) Test error vs,"By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data",0
6356,"Left column: Euclidean-normattacks, right column: ∞-norm attacks","For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",0
6357,"Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
6358,"The vertical bar in (a), (c), and (e) indicates the estimated",(e) Test error vs,1
6359,Attacks on the MNIST dataset,S.-M,0
6360,"We compare standard WRM with ∞-norm PGM, FGM,IFGM","Replacing our current covering num-ber arguments with more intricate notions such as margin-based bounds (Bartlett et al., 2017) wouldextend the scope and usefulness of our theoretical guarantees",0
6361,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,"The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds",0
6362,"Left column: Euclidean-normattacks, right column: ∞-norm attacks","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",0
6363,"Left column: Euclidean-normattacks, right column: ∞-norm attacks","Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",0
6364,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
6365,"Left column: Euclidean-normattacks, right column: ∞-norm attacks","Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks",0
6366,"Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",We have the following theorem.Theorem 5,0
6367,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,(e) Test error vs,1
6368,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,(e) Test error vs,1
6369,"Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",0
6370,the adversarial perturbation level adv,(e) Test error vs,1
6371,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,(e) Test error vs,1
6372,"We compare standard WRM with ∞-norm PGM, FGM,IFGM",We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,0
6373,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,"For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",0
6374,"Left column: Euclidean-normattacks, right column: ∞-norm attacks","Finally, we consider a reinforcement learning problemin Section 4.3, where the Markov decision process used for training differs from that for testing.WRM enjoys the theoretical guarantees of Sections 2 and 3 for large γ, but for small γ (large adver-sarial budgets), WRM becomes a heuristic like other methods",0
6375,"The vertical bar in (a), (c), and (e) indicates the estimated","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
6376,Attacks on the MNIST dataset,"This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",0
6377,"We compare standard WRM with ∞-norm PGM, FGM,IFGM",Figure 9,0
6378,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,(e) Test error vs,1
6379,(e) Test error vs,(e) Test error vs,1
6380,"The vertical bar in (a), (c), and (e) indicates the estimated",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
6381,"We compare standard WRM with ∞-norm PGM, FGM,IFGM",(e) Test error vs,1
6382,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,(e) Test error vs,1
6383,"Left column: Euclidean-normattacks, right column: ∞-norm attacks","In most scenarios, however,it is reasonable to defend against weaker adversaries that instead perturb inﬂuential features more.We consider this setting and train against (cid:107)·(cid:107)2-norm attacks",0
6384,"Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",(e) Test error vs,1
6385,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",0
6386,Attacks on the MNIST dataset,(e) Test error vs,1
6387,Attacks on the MNIST dataset,(e) Test error vs,1
6388,"Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",(e) Test error vs,1
6389,We illustrate test misclassiﬁcation error vs,"Consider the classical supervised learning problem,in which we minimize an expected lossEP0[(cid:96)(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P0, P0 is a distribution on a space Z, and (cid:96)is a loss function",0
6390,"The vertical bar in (a), (c), and (e) indicates the estimated",Since 70% of the data are of the,0
6391,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,(e) Test error vs,1
6392,(e) Test error vs,from which it is easy to see that f is inf-compact,0
6393,the adversarial perturbation level adv,The function cx : X × X → R+ is continuous,0
6394,Attacks on the MNIST dataset,(e) Test error vs,1
6395,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,(e) Test error vs,1
6396,"The vertical bar in (a), (c), and (e) indicates the estimated",adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,0
6397,We illustrate test misclassiﬁcation error vs,(e) Test error vs,1
6398,"The vertical bar in (a), (c), and (e) indicates the estimated",(2017) attempt to mitigate this shortcoming,0
6399,Attacks on the MNIST dataset,"Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",0
6400,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,The limitations ofdeep learning in adversarial settings,0
6401,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6402,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
6403,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.",0
6404,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6405,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
6406,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6407,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6408,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,K,0
6409,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6410,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",0
6411,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6412,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",0
6413,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6414,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",0
6415,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6416,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,0
6417,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6418,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6419,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6420,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,0
6421,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6422,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,Experimental results on synthetic data,0
6423,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,0
6424,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks",0
6425,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6426,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6427,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6428,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6429,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,We omit the certiﬁcate’s error,0
6430,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6431,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,0
6432,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6433,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)",0
6434,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",0
6435,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
6436,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c",0
6437,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6438,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)",0
6439,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",0
6440,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,B,0
6441,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Fidjeland, G",0
6442,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",0
6443,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"If f is inf-compact, then ¯f is directionally differentiable with",0
6444,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,from which it is easy to see that f is inf-compact,0
6445,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6446,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"We address this problemthrough the principled lens of distributionally robust optimization, which guar-antees performance under adversarial input perturbations",0
6447,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6448,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6449,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"for t = 1, ",0
6450,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)",0
6451,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Replacing our current covering num-ber arguments with more intricate notions such as margin-based bounds (Bartlett et al., 2017) wouldextend the scope and usefulness of our theoretical guarantees",0
6452,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6453,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,K,0
6454,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,Thestatistical error term n(t) is omitted from the certiﬁcate,0
6455,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6456,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",0
6457,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)",0
6458,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6459,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6460,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",0
6461,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty",0
6462,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6463,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,The distinctions in performance between various methodsare less apparent now,0
6464,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6465,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Veness, M",0
6466,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",0
6467,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",0
6468,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",0
6469,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",0
6470,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6471,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6472,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6473,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6474,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6475,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"By suitably modifying the cost function c(z, z(cid:48))to take value ∞ outside this small region, our general formulation covers such variants.",0
6476,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6477,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6478,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,", Tadv, where Π denotes projection onto B,p(xi) := {x : (cid:107)x − xi(cid:107)p ≤ }.The adversarial training literature (e.g",0
6479,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6480,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6481,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6482,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6483,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",0
6484,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Thus, weonly compare with an agent trained on the nominal MDP",0
6485,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",0
6486,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt",0
6487,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6488,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"In the former case, since the update (22)simply modiﬁes the state-action transitions (independent of Q), standard results on convergence fortabular Q-learning (e.g",0
6489,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6490,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6491,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,The distinctions in performance between various methodsare less apparent now,0
6492,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
6493,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,It isthus important to distinguish the methods’ abilities to combat attacks,0
6494,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6495,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6496,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,0
6497,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
6498,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6499,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6500,"Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",from which it is easy to see that f is inf-compact,0
6501,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks","Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
6502,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",with the convention that 0 · ∞ = 0,0
6503,We illustrate test misclassiﬁcationerror vs,Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,0
6504,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",G,0
6505,the adversarial perturbation level adv,"Due to the nonstationary, policy-dependent radius for the Wassersteinball, an analogous  for the fast-gradient method (or other variants) is not well-deﬁned",0
6506,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"Due to the nonstationary, policy-dependent radius for the Wassersteinball, an analogous  for the fast-gradient method (or other variants) is not well-deﬁned",0
6507,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",(f) Test error vs,1
6508,The,(f) Test error vs,1
6509,the adversarial perturbation level adv,(f) Test error vs,1
6510,The,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,0
6511,adv for (cid:107) · (cid:107)2-IFGM attackFigure 12,(f) Test error vs,1
6512,The,(f) Test error vs,1
6513,adv for (cid:107) · (cid:107)2-IFGM attackFigure 12,(f) Test error vs,1
6514,"Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models",The choice of P in the robust objective (1) affectsboth the richness of the uncertainty set we wish to consider as well as the tractability of the result-ing optimization problem,0
6515,We illustrate test misclassiﬁcationerror vs,Roy,0
6516,"Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models",(f) Test error vs,1
6517,We illustrate test misclassiﬁcationerror vs,"The vertical bar in (a), (c), and (e) indicates the estimated",0
6518,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",(f) Test error vs,1
6519,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,"Recalling Rockafellar & Wets (1998, Def",0
6520,The,(f) Test error vs,1
6521,The,"Fidjeland, G",0
6522,"Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks","(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)",0
6523,the adversarial perturbation level adv,(f) Test error vs,1
6524,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks","Top row: FGM attacks, bottom row: IFGM attacks",0
6525,adv for (cid:107) · (cid:107)2-IFGM attackFigure 12,(f) Test error vs,1
6526,adv for (cid:107) · (cid:107)2-IFGM attackFigure 12,(f) Test error vs,1
6527,We illustrate test misclassiﬁcationerror vs,(f) Test error vs,1
6528,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,(f) Test error vs,1
6529,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,(f) Test error vs,1
6530,"Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks","McDaniel, X",0
6531,adv for (cid:107) · (cid:107)2-IFGM attackFigure 12,(f) Test error vs,1
6532,adv for (cid:107) · (cid:107)2-IFGM attackFigure 12,"9–10)), we have",0
6533,"Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks","In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",0
6534,The,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:c((x, y), (x0, y0)) = (cid:107)x − x0(cid:107)2∞ + ∞ · 1{y (cid:54)= y0} .",0
6535,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,(f) Test error vs,1
6536,the adversarial perturbation level adv,"(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features",0
6537,the adversarial perturbation level adv,(f) Test error vs,1
6538,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",0
6539,the adversarial perturbation level adv,See Section C.4 for its proof,0
6540,the adversarial perturbation level adv,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",0
6541,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,0
6542,The,The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),0
6543,(f) Test error vs,(f) Test error vs,1
6544,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,"4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",0
6545,the adversarial perturbation level adv,"In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",0
6546,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,(f) Test error vs,1
6547,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",0
6548,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",(f) Test error vs,1
6549,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)",0
6550,The,(f) Test error vs,1
6551,"Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models","Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",0
6552,We illustrate test misclassiﬁcationerror vs,"Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))",0
6553,"Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks","Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",0
6554,"Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models",(f) Test error vs,1
6555,The,(f) Test error vs,1
6556,The,"2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",0
6557,The,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",0
6558,adv for (cid:107) · (cid:107)2-IFGM attackFigure 12,(f) Test error vs,1
6559,We illustrate test misclassiﬁcationerror vs,(f) Test error vs,1
6560,"Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models",(f) Test error vs,1
6561,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,0
6562,"Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",(f) Test error vs,1
6563,"Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",(f) Test error vs,1
6564,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",(f) Test error vs,1
6565,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,(f) Test error vs,1
6566,the adversarial perturbation level adv,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
6567,"Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",0
6568,We illustrate test misclassiﬁcationerror vs,(f) Test error vs,1
6569,We illustrate test misclassiﬁcationerror vs,"where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",0
6570,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"Ried-miller, A",0
6571,"Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",0
6572,"Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models",(f) Test error vs,1
6573,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",N,0
6574,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,(f) Test error vs,1
6575,"Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",(f) Test error vs,1
6576,(f) Test error vs,(f) Test error vs,1
6577,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,(f) Test error vs,1
6578,"Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models",the adversarial perturbation level adv,0
6579,The,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes",0
6580,We illustrate test misclassiﬁcationerror vs,(f) Test error vs,1
6581,We illustrate test misclassiﬁcationerror vs,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",0
6582,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",0
6583,(f) Test error vs,(f) Test error vs,1
6584,adv for (cid:107) · (cid:107)2-IFGM attackFigure 12,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",0
6585,The,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",0
6586,the adversarial perturbation level adv,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",0
6587,The,"Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",0
6588,The,Distillation as a defense to adversarialperturbations against deep neural networks,0
6589,adv for (cid:107) · (cid:107)2-IFGM attackFigure 12,"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",0
6590,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",0
6591,"Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models","In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",0
6592,We illustrate test misclassiﬁcationerror vs,(f) Test error vs,1
6593,We illustrate test misclassiﬁcationerror vs,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",0
6594,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,(f) Test error vs,1
6595,adv for (cid:107) · (cid:107)2-IFGM attackFigure 12,(f) Test error vs,1
6596,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks","Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z",0
6597,"Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models","Fidjeland, G",0
6598,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,(f) Test error vs,1
6599,"Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks","This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd",0
6600,"Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",(e) Test error vs,1
6601,"The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",(e) Test error vs,1
6602,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,(e) Test error vs,1
6603,All models are trained in the ∞-norm,"By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data",0
6604,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,(e) Test error vs,1
6605,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",(e) Test error vs,1
6606,"Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",(e) Test error vs,1
6607,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",(e) Test error vs,1
6608,Attacks on the MNIST dataset,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R",0
6609,"The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for","Graves, M",0
6610,Attacks on the MNIST dataset,(e) Test error vs,1
6611,We illustrate testmisclassiﬁcation error vs,"For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM",0
6612,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",(e) Test error vs,1
6613,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
6614,All models are trained in the ∞-norm,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0",0
6615,"Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",(e) Test error vs,1
6616,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,(e) Test error vs,1
6617,We illustrate testmisclassiﬁcation error vs,the adversarial perturbation level adv,0
6618,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks","In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp",0
6619,Attacks on the MNIST dataset,G,0
6620,the adversarial perturbation level adv,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)",0
6621,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,(e) Test error vs,1
6622,All models are trained in the ∞-norm,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",0
6623,(e) Test error vs,(e) Test error vs,1
6624,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)",0
6625,"Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",(e) Test error vs,1
6626,(e) Test error vs,G,0
6627,"Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks","(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c",0
6628,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks","As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes",0
6629,the adversarial perturbation level adv,"(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave",0
6630,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",(e) Test error vs,1
6631,(e) Test error vs,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]",0
6632,Attacks on the MNIST dataset,"Wu, S",0
6633,the adversarial perturbation level adv,"WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.",0
6634,Attacks on the MNIST dataset,(e) Test error vs,1
6635,(e) Test error vs,"In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)",0
6636,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",The major beneﬁt of our approach is its simplicityand wide applicability across many models and machine-learning scenarios.There remain many avenues for future investigation,0
6637,Attacks on the MNIST dataset,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
6638,the adversarial perturbation level adv,Robust MDP’s consider an ambiguity set Psa for state-action transitions,0
6639,the adversarial perturbation level adv,(e) Test error vs,1
6640,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
6641,We illustrate testmisclassiﬁcation error vs,(e) Test error vs,1
6642,Attacks on the MNIST dataset,the adversarial perturbation level adv,0
6643,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,"The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",0
6644,(e) Test error vs,(e) Test error vs,1
6645,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4",0
6646,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",0
6647,"Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",(e) Test error vs,1
6648,"The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",(2017) attempt to mitigate this shortcoming,0
6649,(e) Test error vs,(e) Test error vs,1
6650,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
6651,Attacks on the MNIST dataset,(e) Test error vs,1
6652,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,"If f is inf-compact, then ¯f is directionally differentiable with",0
6653,Attacks on the MNIST dataset,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",0
6654,(e) Test error vs,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,0
6655,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,(e) Test error vs,1
6656,(e) Test error vs,"Consider feedforward neural networks with ReLU’s and let U := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U (cid:96)(θ; z + u) is NPO",0
6657,"Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",(e) Test error vs,1
6658,(e) Test error vs,"Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)",0
6659,"The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",(e) Test error vs,1
6660,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,(e) Test error vs,1
6661,We illustrate testmisclassiﬁcation error vs,"ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",0
6662,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,(e) Test error vs,1
6663,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,"Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
6664,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,allowing us to prevent attacks on the test set,0
6665,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks","Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",0
6666,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",(e) Test error vs,1
6667,"The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",(e) Test error vs,1
6668,Attacks on the MNIST dataset,(e) Test error vs,1
6669,"Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",(e) Test error vs,1
6670,"Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",(e) Test error vs,1
6671,We illustrate testmisclassiﬁcation error vs,"Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",0
6672,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,(e) Test error vs,1
6673,Attacks on the MNIST dataset,"WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.",0
6674,We illustrate testmisclassiﬁcation error vs,"We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R",0
6675,Attacks on the MNIST dataset,(e) Test error vs,1
6676,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,0
6677,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",(e) Test error vs,1
6678,All models are trained in the ∞-norm,(e) Test error vs,1
6679,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,(e) Test error vs,1
6680,"The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for","Left column:Euclidean-norm attacks, right column: ∞-norm attacks",0
6681,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,(e) Test error vs,1
6682,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,(e) Test error vs,1
6683,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,"Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",0
6684,We illustrate testmisclassiﬁcation error vs,(e) Test error vs,1
6685,"Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",We thank Jacob Steinhardt for valuable feedback,0
6686,the adversarial perturbation level adv,(e) Test error vs,1
6687,We illustrate testmisclassiﬁcation error vs,In Section 4.2 we consider a supervised learning problem for MNISTwhere we adversarially perturb the test data,0
6688,(e) Test error vs,(e) Test error vs,1
6689,We illustrate testmisclassiﬁcation error vs,(e) Test error vs,1
6690,All models are trained in the ∞-norm,(e) Test error vs,1
6691,"The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",Distillation as a defense to adversarialperturbations against deep neural networks,0
6692,"Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",(e) Test error vs,1
6693,All models are trained in the ∞-norm,All mod-els are trained in the ∞-norm,0
6694,"Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",(e) Test error vs,1
6695,Attacks on the MNIST dataset,(e) Test error vs,1
6696,We illustrate testmisclassiﬁcation error vs,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,0
6697,(e) Test error vs,(e) Test error vs,1
6698,adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,(e) Test error vs,1
6699,adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,(e) Test error vs,1
6700,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"For any distribution Q and any ρ > 0,(5)",0
6701,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label",0
6702,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,0
6703,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6704,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6705,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6706,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6707,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,Kuhn,0
6708,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6709,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6710,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6711,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6712,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6713,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6714,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6715,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6716,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,Attacks on the MNIST dataset,0
6717,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",0
6718,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",0
6719,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,K,0
6720,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6721,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6722,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques",0
6723,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6724,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ",0
6725,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"The smoothness of (cid:96) is essential so that a penalized version(cid:96)(θ, z) − γc(z, z0) is concave in z (which can be approximately veriﬁed by computing Hessians∇2zz(cid:96)(θ, z) for each training datapoint), allowing computation and our coming certiﬁcates of opti-mality",0
6726,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6727,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6728,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6729,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6730,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6731,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",0
6732,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6733,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,0
6734,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6735,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",0
6736,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6737,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6738,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,The distinctions in performance between various methodsare less apparent now,0
6739,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",0
6740,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6741,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,(a) and (b) show test misclassiﬁcation error vs,0
6742,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"For each γadv, we consider the distance to adversarial examples in the test dataset",0
6743,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Thus, weonly compare with an agent trained on the nominal MDP",0
6744,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6745,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6746,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6747,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6748,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,8.7.1) to obtain,0
6749,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
6750,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6751,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"For each γadv, we consider the distance to adversarial examples in the test dataset",0
6752,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5",0
6753,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6754,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6755,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument",0
6756,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6757,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6758,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6759,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6760,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",0
6761,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6762,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))",0
6763,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,Attacks on the MNIST dataset,0
6764,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6765,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6766,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6767,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6768,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
6769,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,Frossard,0
6770,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Now, let x(z) beany measurable function that is -close to attaining the supremum above",0
6771,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6772,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Silver, A",0
6773,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3",0
6774,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6775,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks",0
6776,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,0
6777,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6778,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6779,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Jha, M",0
6780,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6781,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,Figure 9,0
6782,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",0
6783,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument",0
6784,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6785,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6786,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
6787,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",0
6788,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6789,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"The decision problem Turing-reducesto the optimization problem, as the decision problem can be solved with one call to O",0
6790,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
6791,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,All models are trained in the ∞-norm,0
6792,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6793,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6794,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance",0
6795,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6796,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6797,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,1
6798,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
6799,00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,"The decision problem Turing-reducesto the optimization problem, as the decision problem can be solved with one call to O",0
6800,"Top row: PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",(f) Test error vs,1
6801,(f) Test error vs,(f) Test error vs,1
6802,We illustrate test misclassiﬁcation error vs,(f) Test error vs,1
6803,"Top row: PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks","In this regime(small γ, large ), performance between WRM and other methods diverge",0
6804,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks","Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",0
6805,We illustrate test misclassiﬁcation error vs,(f) Test error vs,1
6806,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,0
6807,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",0
6808,All mod-els are trained in the ∞-norm,(f) Test error vs,1
6809,"Top row: PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",(f) Test error vs,1
6810,the adversarial perturbationlevel adv,"We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5",0
6811,We illustrate test misclassiﬁcation error vs,(f) Test error vs,0
6812,"The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",(f) Test error vs,1
6813,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",(f) Test error vs,1
6814,(f) Test error vs,(f) Test error vs,1
6815,(f) Test error vs,"χ2- or Kullback-Leibler divergences) which are effectivewhen the support of the distribution P0 is ﬁxed, a Wasserstein ball around P0 includes distributionsQ with different support and allows (in a sense) robustness to unseen data.Many authors have studied tractable classes of uncertainty sets P and losses (cid:96)",0
6816,(f) Test error vs,"Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)",0
6817,We illustrate test misclassiﬁcation error vs,"Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",0
6818,(f) Test error vs,The distinctions in performance between various methodsare less apparent now,0
6819,We illustrate test misclassiﬁcation error vs,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A",0
6820,"The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",(f) Test error vs,1
6821,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,"By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data",0
6822,adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,"As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)",0
6823,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks","Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
6824,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,(f) Test error vs,1
6825,(f) Test error vs,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",0
6826,the adversarial perturbationlevel adv,(f) Test error vs,1
6827,adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,(f) Test error vs,1
6828,the adversarial perturbationlevel adv,(f) Test error vs,1
6829,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,We omit the certiﬁcate’s error,0
6830,We illustrate test misclassiﬁcation error vs,"Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",0
6831,(f) Test error vs,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
6832,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,"The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.",0
6833,the adversarial perturbationlevel adv,(f) Test error vs,1
6834,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,(f) Test error vs,1
6835,"The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
6836,(f) Test error vs,"In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",0
6837,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,0
6838,adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,(f) Test error vs,1
6839,adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,(f) Test error vs,1
6840,the adversarial perturbationlevel adv,(f) Test error vs,1
6841,All mod-els are trained in the ∞-norm,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,0
6842,(f) Test error vs,"Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
6843,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,"such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-ward",0
6844,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,0
6845,We illustrate test misclassiﬁcation error vs,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have",0
6846,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,K,0
6847,(f) Test error vs,(f) Test error vs,1
6848,adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),0
6849,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)",0
6850,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,"Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",0
6851,adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
6852,(f) Test error vs,(f) Test error vs,1
6853,adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,(f) Test error vs,1
6854,adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,"This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd",0
6855,We illustrate test misclassiﬁcation error vs,The choice of P in the robust objective (1) affectsboth the richness of the uncertainty set we wish to consider as well as the tractability of the result-ing optimization problem,0
6856,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,(f) Test error vs,1
6857,All mod-els are trained in the ∞-norm,"Jha, and A",0
6858,the adversarial perturbationlevel adv,"We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1",0
6859,All mod-els are trained in the ∞-norm,(f) Test error vs,1
6860,"Top row: PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",We illustrate test misclassiﬁcation error vs.theadversarial perturbation level adv,0
6861,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,(f) Test error vs,1
6862,All mod-els are trained in the ∞-norm,(f) Test error vs,1
6863,"The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",(f) Test error vs,1
6864,"The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",(f) Test error vs,1
6865,adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,(f) Test error vs,1
6866,"The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",(f) Test error vs,1
6867,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,Experimental results on synthetic data,0
6868,We illustrate test misclassiﬁcation error vs,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,0
6869,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",(f) Test error vs,1
6870,(f) Test error vs,"(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D",0
6871,We illustrate test misclassiﬁcation error vs,(f) Test error vs,1
6872,the adversarial perturbationlevel adv,"McDaniel, S",0
6873,All mod-els are trained in the ∞-norm,the adversarial perturbation level adv,0
6874,(f) Test error vs,"The vertical bar in (a), (c), and (e) indicates the estimated",0
6875,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"For any distribution Q and any ρ > 0,(5)",0
6876,the adversarial perturbationlevel adv,"Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",0
6877,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"Papernot, P",0
6878,the adversarial perturbationlevel adv,(f) Test error vs,1
6879,adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,(f) Test error vs,1
6880,All mod-els are trained in the ∞-norm,(f) Test error vs,1
6881,"The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
6882,"Top row: PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,0
6883,the adversarial perturbationlevel adv,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,0
6884,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,"Then, the bounds (11) and (12) hold with",0
6885,"The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",We illustrate test misclassiﬁcation error vs,0
6886,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,allowing us to prevent attacks on the test set,0
6887,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",(f) Test error vs,1
6888,"The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",(f) Test error vs,1
6889,All mod-els are trained in the ∞-norm,(f) Test error vs,1
6890,"Top row: PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks","From Lemma 5, our previous results (Theorems 2 and 4) follow",0
6891,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,In constrast to f-divergences (e.g,0
6892,We illustrate test misclassiﬁcation error vs,(f) Test error vs,1
6893,"Top row: PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks","Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
6894,the adversarial perturbationlevel adv,(f) Test error vs,1
6895,All mod-els are trained in the ∞-norm,(f) Test error vs,1
6896,the adversarial perturbationlevel adv,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",0
6897,All mod-els are trained in the ∞-norm,(f) Test error vs,1
6898,the adversarial perturbationlevel adv,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes",0
6899,adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,(f) Test error vs,1
6900,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime","57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞",0
6901,"Then there exists θ such that
this optimization problem is also NP-hard.",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6902,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",0
6903,This result is essentially due to Katz et al.(2017a),We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6904,"We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",", Tadv, where Π denotes projection onto B,p(xi) := {x : (cid:107)x − xi(cid:107)p ≤ }.The adversarial training literature (e.g",0
6905,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6906,"We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6907,"Then there exists θ such that
this optimization problem is also NP-hard.",adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,0
6908,"In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6909,"Then there exists θ such that
this optimization problem is also NP-hard.",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6910,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",0
6911,"Then there exists θ such that
this optimization problem is also NP-hard.",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6912,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6913,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,0
6914,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",See Section C.2 for the proof,0
6915,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6916,"In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6917,"In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",0
6918,This result is essentially due to Katz et al.(2017a),We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6919,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6920,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,"In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
6921,"Consider feedforward neural networks with ReLU’s and let U := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U (cid:96)(θ; z + u) is NPO",Dziugaite and D,0
6922,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6923,"In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",Further attacks on the MNIST dataset,0
6924,"Consider feedforward neural networks with ReLU’s and let U := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U (cid:96)(θ; z + u) is NPO","Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",0
6925,"Consider feedforward neural networks with ReLU’s and let U := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U (cid:96)(θ; z + u) is NPO",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6926,"Consider feedforward neural networks with ReLU’s and let U := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U (cid:96)(θ; z + u) is NPO",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6927,"Consider feedforward neural networks with ReLU’s and let U := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U (cid:96)(θ; z + u) is NPO",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6928,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime","For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",0
6929,This result is essentially due to Katz et al.(2017a),We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6930,This result is essentially due to Katz et al.(2017a),"Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)",0
6931,"Consider feedforward neural networks with ReLU’s and let U := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U (cid:96)(θ; z + u) is NPO",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6932,This result is essentially due to Katz et al.(2017a),JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,0
6933,This result is essentially due to Katz et al.(2017a),We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6934,This result is essentially due to Katz et al.(2017a),We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6935,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",Robust MDP’s consider an ambiguity set Psa for state-action transitions,0
6936,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6937,"We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",0
6938,"We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2","6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",0
6939,"We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2","Nature, 518(7540):529–533, 2015.",0
6940,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,We use reward r(β) := e−|β| for the angle β of the pole from the vertical,0
6941,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime","The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and",0
6942,"Then there exists θ such that
this optimization problem is also NP-hard.","The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",0
6943,"Then there exists θ such that
this optimization problem is also NP-hard.","Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
6944,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime","Moosavi-Dezfooli, A",0
6945,"We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",0
6946,"Then there exists θ such that
this optimization problem is also NP-hard.","Left column:Euclidean-norm attacks, right column: ∞-norm attacks",0
6947,"Then there exists θ such that
this optimization problem is also NP-hard.",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6948,"Consider feedforward neural networks with ReLU’s and let U := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U (cid:96)(θ; z + u) is NPO",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6949,This result is essentially due to Katz et al.(2017a),We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6950,"Consider feedforward neural networks with ReLU’s and let U := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U (cid:96)(θ; z + u) is NPO",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6951,"In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,0
6952,"Then there exists θ such that
this optimization problem is also NP-hard.",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6953,This result is essentially due to Katz et al.(2017a),"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",0
6954,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime","Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B",0
6955,"Then there exists θ such that
this optimization problem is also NP-hard.",(2013) and Namkoong & Duchi (2017) use convex optimization approaches for f-divergence balls,0
6956,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,"The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
6957,This result is essentially due to Katz et al.(2017a),"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",0
6958,This result is essentially due to Katz et al.(2017a),We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6959,This result is essentially due to Katz et al.(2017a),"Left column: Euclidean-norm attacks, rightcolumn: ∞-norm attacks",0
6960,"Then there exists θ such that
this optimization problem is also NP-hard.",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6961,"In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e","For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering",0
6962,"Then there exists θ such that
this optimization problem is also NP-hard.","The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds",0
6963,This result is essentially due to Katz et al.(2017a),We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6964,This result is essentially due to Katz et al.(2017a),We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6965,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6966,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6967,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6968,This result is essentially due to Katz et al.(2017a),We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6969,"Then there exists θ such that
this optimization problem is also NP-hard.","Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",0
6970,"We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6971,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6972,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6973,"In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6974,"We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,0
6975,This result is essentially due to Katz et al.(2017a),We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6976,"Then there exists θ such that
this optimization problem is also NP-hard.",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6977,"We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6978,"Consider feedforward neural networks with ReLU’s and let U := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U (cid:96)(θ; z + u) is NPO",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6979,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime","In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s",0
6980,"We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6981,"We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6982,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,"For any distribution Q and any ρ > 0,(5)",0
6983,"We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2","For each γadv, we consider the distance to adversarial examples in the test dataset",0
6984,"In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e","Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",0
6985,"We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,0
6986,"Then there exists θ such that
this optimization problem is also NP-hard.",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6987,This result is essentially due to Katz et al.(2017a),Weillustrate test misclassiﬁcation error vs,0
6988,"Consider feedforward neural networks with ReLU’s and let U := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U (cid:96)(θ; z + u) is NPO","Left column: Euclidean-normattacks, right column: ∞-norm attacks",0
6989,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
6990,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6991,"In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",Esfahani and D,0
6992,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime","AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",0
6993,"We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2","In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",0
6994,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6995,"In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",Let ∆F ≥ F (θ0) − inf θ F (θ),0
6996,This result is essentially due to Katz et al.(2017a),We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6997,"In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
6998,We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,"McDaniel, X",0
6999,"a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,1
7000,"By appropriatescaling of θ, v, and w, Katz et al","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7001,"The decision problem Turing-reducesto the optimization problem, as the decision problem can be solved with one call to O","Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)",0
7002,"Thus, theoptimization problem is NP-hard.","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7003,"ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7004,"ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7005,"ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b","Papernot, P",0
7006,"(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D","Veness, M",0
7007,"(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7008,"(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7009,"ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",See Section C.2 for the proof,0
7010,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7011,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7012,"By appropriatescaling of θ, v, and w, Katz et al","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",0
7013,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",B,0
7014,"The decision problem Turing-reducesto the optimization problem, as the decision problem can be solved with one call to O","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7015,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7016,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7017,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time","For large γ, we can again solve problem (22) efﬁciently using gradient descent",0
7018,"Thus, theoptimization problem is NP-hard.","WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.",0
7019,"The decision problem Turing-reducesto the optimization problem, as the decision problem can be solved with one call to O",We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,0
7020,"Thus, theoptimization problem is NP-hard.","6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",0
7021,"(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D","Bellemare, A",0
7022,"The decision problem Turing-reducesto the optimization problem, as the decision problem can be solved with one call to O","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7023,"Thus, theoptimization problem is NP-hard.","(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)",0
7024,"By appropriatescaling of θ, v, and w, Katz et al","By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives",0
7025,"ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7026,"ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b","(2017), Dziugaite & Roy (2017), andNeyshabur et al",0
7027,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem","For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM",0
7028,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,0
7029,"ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b","In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.",0
7030,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",V,0
7031,"By appropriatescaling of θ, v, and w, Katz et al","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7032,"Thus, theoptimization problem is NP-hard.",Let ∆F ≥ F (θ0) − inf θ F (θ),0
7033,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7034,"(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D","In this regime(small γ, large ), performance between WRM and other methods diverge",0
7035,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7036,"Thus, theoptimization problem is NP-hard.","In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts",0
7037,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",is closed-valued and measurable,0
7038,"The decision problem Turing-reducesto the optimization problem, as the decision problem can be solved with one call to O","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",0
7039,"Thus, theoptimization problem is NP-hard.","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7040,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem","Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
7041,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7042,"(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7043,"Thus, theoptimization problem is NP-hard.","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7044,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time",We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,0
7045,"ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7046,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7047,"Thus, theoptimization problem is NP-hard.","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7048,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time",Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,0
7049,"ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),0
7050,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7051,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7052,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time","Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.",0
7053,"(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D",G,0
7054,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",0
7055,"ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b","Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",0
7056,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7057,"ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7058,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time",G,0
7059,"Thus, theoptimization problem is NP-hard.","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7060,"The decision problem Turing-reducesto the optimization problem, as the decision problem can be solved with one call to O","Veness, M",0
7061,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7062,"By appropriatescaling of θ, v, and w, Katz et al",Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,0
7063,"By appropriatescaling of θ, v, and w, Katz et al","The decision problem Turing-reducesto the optimization problem, as the decision problem can be solved with one call to O",0
7064,"By appropriatescaling of θ, v, and w, Katz et al","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7065,"Thus, theoptimization problem is NP-hard.","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7066,"By appropriatescaling of θ, v, and w, Katz et al",Since 70% of the data are of the,0
7067,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",See Section C.2 for the proof,0
7068,"The decision problem Turing-reducesto the optimization problem, as the decision problem can be solved with one call to O","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7069,"Thus, theoptimization problem is NP-hard.","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7070,"ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7071,"By appropriatescaling of θ, v, and w, Katz et al",The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,0
7072,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7073,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time","Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)",0
7074,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7075,"By appropriatescaling of θ, v, and w, Katz et al",Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,0
7076,"Thus, theoptimization problem is NP-hard.","We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",0
7077,"The decision problem Turing-reducesto the optimization problem, as the decision problem can be solved with one call to O","From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",0
7078,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7079,"Thus, theoptimization problem is NP-hard.","Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
7080,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time","From Lemma 5, our previous results (Theorems 2 and 4) follow",0
7081,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",is closed-valued and measurable,0
7082,"The decision problem Turing-reducesto the optimization problem, as the decision problem can be solved with one call to O","For large γ, we can again solve problem (22) efﬁciently using gradient descent",0
7083,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time","Bellemare, A",0
7084,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",0
7085,"By appropriatescaling of θ, v, and w, Katz et al","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7086,"(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7087,"Thus, theoptimization problem is NP-hard.","Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",0
7088,"The decision problem Turing-reducesto the optimization problem, as the decision problem can be solved with one call to O","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",0
7089,"By appropriatescaling of θ, v, and w, Katz et al","Fawzi, and P",0
7090,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",Distillation as a defense to adversarialperturbations against deep neural networks,0
7091,"Thus, theoptimization problem is NP-hard.","To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section",0
7092,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",Roy,0
7093,"(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7094,"Thus, theoptimization problem is NP-hard.",adv for (cid:107) · (cid:107)∞-IFGM attackFigure 11,0
7095,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time",the adversarial perturbation level adv,0
7096,"The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem","We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clearthe computational beneﬁts of relaxing the strict robustness requirements of formulation (5)",0
7097,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
7098,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time",JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,0
7099,"The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",1
7100,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7101,"14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping",Explicit distributional robustness of the form (5) is intractable except in limited cases,0
7102,14.27 and Prop,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7103,The below general duality result gives Proposition 1 as an immediate specialcase,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7104,14.27 and Prop,"IEEE, 2016c.",0
7105,14.27 and Prop,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",0
7106,"Recalling Rockafellar & Wets (1998, Def","For any distribution Q and any ρ > 0,(5)",0
7107,The below general duality result gives Proposition 1 as an immediate specialcase,large desired robustness values) our approach becomes a heuristic just like the other approaches.,0
7108,14.27 and Prop,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7109,The below general duality result gives Proposition 1 as an immediate specialcase,Experimental results on synthetic data,0
7110,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7111,"Recalling Rockafellar & Wets (1998, Def","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7112,"14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping",The function c : Z×Z → R+ is continuous,0
7113,"14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping","We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A",0
7114,"14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7115,The below general duality result gives Proposition 1 as an immediate specialcase,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",0
7116,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3",0
7117,The below general duality result gives Proposition 1 as an immediate specialcase,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7118,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis","The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3",0
7119,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",0
7120,"14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping","Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.",0
7121,14.27 and Prop,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7122,"Recalling Rockafellar & Wets (1998, Def","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7123,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","Thus, we see that our adversarial-training method defends against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint untilthe model misclassiﬁes it",0
7124,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",For large adversaries(i.e,0
7125,14.27 and Prop,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7126,14.27 and Prop,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7127,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7128,14.27 and Prop,We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,0
7129,"14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping","For any distribution Q and any ρ > 0,(5)",0
7130,"14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping",Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,0
7131,"14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7132,"Recalling Rockafellar & Wets (1998, Def","Then there exists θ such that
this optimization problem is also NP-hard.",0
7133,"Recalling Rockafellar & Wets (1998, Def","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7134,The below general duality result gives Proposition 1 as an immediate specialcase,"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance",0
7135,"14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7136,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,0
7137,"14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",0
7138,The below general duality result gives Proposition 1 as an immediate specialcase,"The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",0
7139,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument",These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,0
7140,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7141,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes",0
7142,14.27 and Prop,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7143,"Recalling Rockafellar & Wets (1998, Def","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7144,14.27 and Prop,"To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section",0
7145,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7146,The below general duality result gives Proposition 1 as an immediate specialcase,We illustrate test misclassiﬁcation error vs,0
7147,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",0
7148,14.27 and Prop,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7149,14.27 and Prop,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7150,The below general duality result gives Proposition 1 as an immediate specialcase,"For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",0
7151,"Recalling Rockafellar & Wets (1998, Def","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7152,"14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7153,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",0
7154,14.27 and Prop,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
7155,The below general duality result gives Proposition 1 as an immediate specialcase,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7156,14.27 and Prop,2.2) gives the result.,0
7157,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input",0
7158,"14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping",adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
7159,The below general duality result gives Proposition 1 as an immediate specialcase,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7160,The below general duality result gives Proposition 1 as an immediate specialcase,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",0
7161,The below general duality result gives Proposition 1 as an immediate specialcase,"Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
7162,The below general duality result gives Proposition 1 as an immediate specialcase,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7163,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7164,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7165,The below general duality result gives Proposition 1 as an immediate specialcase,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7166,"14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7167,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis","The vertical bar in (a), (c), and (e) indicates the perturbation level that was",0
7168,"Recalling Rockafellar & Wets (1998, Def","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7169,14.27 and Prop,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7170,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7171,14.27 and Prop,"When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ",0
7172,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","Wu, S",0
7173,14.27 and Prop,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes",0
7174,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7175,The below general duality result gives Proposition 1 as an immediate specialcase,Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,0
7176,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",(2017) attempt to mitigate this shortcoming,0
7177,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7178,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7179,"Recalling Rockafellar & Wets (1998, Def","In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]",0
7180,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7181,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",0
7182,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7183,The below general duality result gives Proposition 1 as an immediate specialcase,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",0
7184,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7185,"Recalling Rockafellar & Wets (1998, Def","IEEE, 2016c.",0
7186,14.27 and Prop,"To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",0
7187,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis","The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and",0
7188,The below general duality result gives Proposition 1 as an immediate specialcase,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7189,"Recalling Rockafellar & Wets (1998, Def","In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",0
7190,14.27 and Prop,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7191,The below general duality result gives Proposition 1 as an immediate specialcase,Attacks on the MNIST dataset,0
7192,The below general duality result gives Proposition 1 as an immediate specialcase,This procedureprovides robustness to uncertainties in state-action transitions,0
7193,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7194,"14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7195,"Recalling Rockafellar & Wets (1998, Def",The vertical bar indicates the achieved level,0
7196,The below general duality result gives Proposition 1 as an immediate specialcase,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7197,"14.33), we say that a functiong : X × Z → R is a normal integrand if for each α, the mapping","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7198,"Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7199,"For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",1
7200,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",is closed-valued and measurable,1
7201,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
7202,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand","Left column:Euclidean-norm attacks, right column: ∞-norm attacks",0
7203,is closed-valued and measurable,"Forour approach we use γ = 2, and to make fair comparisons with FGM we use",0
7204,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",is closed-valued and measurable,1
7205,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand","Unfortunately, this approach is intractable except for specially struc-tured losses, such as the composition of a linear and simple convex function (Ben-Tal et al., 2009; Xuet al., 2009; 2012)",0
7206,is closed-valued and measurable,is closed-valued and measurable,1
7207,We have the following theorem.Theorem 5,is closed-valued and measurable,1
7208,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand","The vertical bar in (a), (c), and (e) indicates the estimated",0
7209,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand","The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time",0
7210,We have the following theorem.Theorem 5,is closed-valued and measurable,1
7211,is closed-valued and measurable,This procedureprovides robustness to uncertainties in state-action transitions,0
7212,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,0
7213,We have the following theorem.Theorem 5,Theorem 7,0
7214,"(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave",is closed-valued and measurable,1
7215,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand","Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",0
7216,We have the following theorem.Theorem 5,"We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms",0
7217,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor","We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",0
7218,We have the following theorem.Theorem 5,is closed-valued and measurable,1
7219,is closed-valued and measurable,"for all γ, ρ ≥ 0",0
7220,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",is closed-valued and measurable,1
7221,"(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave",is closed-valued and measurable,1
7222,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",is closed-valued and measurable,1
7223,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
7224,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",is closed-valued and measurable,1
7225,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",is closed-valued and measurable,1
7226,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",is closed-valued and measurable,1
7227,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",is closed-valued and measurable,1
7228,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor","Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument",0
7229,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand","On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
7230,"(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave","For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g",0
7231,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",K,0
7232,We have the following theorem.Theorem 5,is closed-valued and measurable,1
7233,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor","Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)22] ≤ σ2 and, Algorithm 1take constant stepsizes α =satisﬁes",0
7234,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,0
7235,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",is closed-valued and measurable,1
7236,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
7237,We have the following theorem.Theorem 5,"For example,Ben-Tal et al",0
7238,"(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
7239,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",0
7240,is closed-valued and measurable,is closed-valued and measurable,1
7241,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",is closed-valued and measurable,1
7242,"(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave","Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",0
7243,"(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave",is closed-valued and measurable,1
7244,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand","We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5",0
7245,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
7246,is closed-valued and measurable,"Fawzi, and P",0
7247,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",0
7248,We have the following theorem.Theorem 5,adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
7249,is closed-valued and measurable,is closed-valued and measurable,1
7250,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",is closed-valued and measurable,1
7251,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand","The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",0
7252,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",is closed-valued and measurable,1
7253,is closed-valued and measurable,is closed-valued and measurable,1
7254,is closed-valued and measurable,The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,0
7255,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",is closed-valued and measurable,1
7256,We have the following theorem.Theorem 5,is closed-valued and measurable,1
7257,is closed-valued and measurable,is closed-valued and measurable,1
7258,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",0
7259,is closed-valued and measurable,is closed-valued and measurable,1
7260,is closed-valued and measurable,is closed-valued and measurable,1
7261,We have the following theorem.Theorem 5,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
7262,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",(c) Test error vs,0
7263,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",is closed-valued and measurable,1
7264,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",is closed-valued and measurable,1
7265,We have the following theorem.Theorem 5,is closed-valued and measurable,1
7266,is closed-valued and measurable,is closed-valued and measurable,1
7267,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",is closed-valued and measurable,1
7268,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",is closed-valued and measurable,1
7269,is closed-valued and measurable,is closed-valued and measurable,1
7270,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,0
7271,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand","Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt",0
7272,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",is closed-valued and measurable,1
7273,is closed-valued and measurable,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",0
7274,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor","Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective",0
7275,We have the following theorem.Theorem 5,"Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)",0
7276,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",0
7277,"(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
7278,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor","The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",0
7279,We have the following theorem.Theorem 5,is closed-valued and measurable,1
7280,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",is closed-valued and measurable,1
7281,is closed-valued and measurable,"To that end, we present empirical evaluations on supervised and reinforcementlearning tasks where we compare performance with empirical risk minimization (ERM) and, whereappropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), itsiterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madryet al., 2017)",0
7282,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",is closed-valued and measurable,1
7283,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor","Top row: PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",0
7284,We have the following theorem.Theorem 5,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",0
7285,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",is closed-valued and measurable,1
7286,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",is closed-valued and measurable,1
7287,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",is closed-valued and measurable,1
7288,We have the following theorem.Theorem 5,All models are trained in the ∞-norm,0
7289,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",is closed-valued and measurable,1
7290,We have the following theorem.Theorem 5,is closed-valued and measurable,1
7291,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,0
7292,"(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave","Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",0
7293,"Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",Robust MDP’s consider an ambiguity set Psa for state-action transitions,0
7294,is closed-valued and measurable,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",0
7295,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",Esfahani and D,0
7296,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",Attacks on the MNIST dataset,0
7297,"(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave","Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B",0
7298,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",is closed-valued and measurable,1
7299,"We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor","Fawzi, and P",0
7300,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7301,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",by solving (7),0
7302,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7303,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7304,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7305,8.7.1) to obtain,All mod-els are trained in the ∞-norm,0
7306,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",The function cx : X × X → R+ is continuous,0
7307,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7308,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)",0
7309,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7310,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",372–387,0
7311,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",0
7312,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",0
7313,8.7.1) to obtain,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)",0
7314,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",0
7315,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,0
7316,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",A,0
7317,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7318,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7319,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp",0
7320,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,0
7321,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",0
7322,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7323,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7324,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7325,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",The function c : Z×Z → R+ is continuous,0
7326,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,0
7327,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7328,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7329,8.7.1) to obtain,Mnih et al,0
7330,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",0
7331,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5",0
7332,8.7.1) to obtain,S.-M,0
7333,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7334,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7335,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7336,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","Indeed, it is NP-hard
to ﬁnd worst-case perturbations when deep networks use ReLU activations, suggesting difﬁculties
for fast and iterated heuristics (see Lemma 2 in Appendix B)",0
7337,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7338,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7339,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,0
7340,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7341,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7342,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7343,8.7.1) to obtain,"On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",0
7344,8.7.1) to obtain,large desired robustness values) our approach becomes a heuristic just like the other approaches.,0
7345,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7346,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7347,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt",0
7348,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",0
7349,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7350,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7351,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,0
7352,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7353,8.7.1) to obtain,"The vertical bar in (b), (d), and (f) indicatesthe perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
7354,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7355,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance",0
7356,8.7.1) to obtain,Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,0
7357,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7358,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7359,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",N,0
7360,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",Figure 2,0
7361,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
7362,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",We illustrate test misclassiﬁcation error vs,0
7363,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7364,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7365,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7366,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7367,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",the adversarial perturbation level adv,0
7368,8.7.1) to obtain,Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
7369,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",0
7370,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7371,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7372,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,0
7373,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7374,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7375,8.7.1) to obtain,The,0
7376,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",0
7377,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7378,8.7.1) to obtain,"Szepesv´ari & Littman (1999)) apply under these adversarial dynamics.We test our adversarial training procedure in the cart-pole environment, where the goal is to balancea pole on a cart by moving the cart left or right",0
7379,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7380,8.7.1) to obtain,"Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv",0
7381,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",0
7382,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7383,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7384,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7385,8.7.1) to obtain,"Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)",0
7386,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",Experimental results on synthetic data,0
7387,8.7.1) to obtain,"Silver, A",0
7388,8.7.1) to obtain,with the convention that 0 · ∞ = 0,0
7389,8.7.1) to obtain,"Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",0
7390,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7391,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7392,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)",0
7393,8.7.1) to obtain,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7394,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7395,8.7.1) to obtain,"Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
7396,"First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7397,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",0
7398,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm","First, the mapping P (cid:55)→ Wc(P, Q) is convex in the space of probability measures",1
7399,8.7.1) to obtain,We assume without loss of2 − z0 (cid:54)= 0,0
7400,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
7401,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7402,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7403,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7404,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7405,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"empirical problem, concentrates uniformly around its population counterpart.Theorem 4",0
7406,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",0
7407,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",0
7408,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",0
7409,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,Deﬁne the scale parameter βt > 0 by,0
7410,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =",(24)Optimizing the Lagrangian formulation (2b) with the ∞-norm is difﬁcult since subtracting a multipleof the ∞-norm does not add (negative) curvature in all directions,0
7411,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7412,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",Attacks on the MNIST dataset,0
7413,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7414,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","McDaniel, X",0
7415,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7416,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7417,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"for t = 1, ",0
7418,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","One natural appli-cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-pointproblem (1) and give specialized bounds for domain adaptation problems",0
7419,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7420,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",0
7421,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7422,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =",the adversarial perturbationlevel adv,0
7423,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",Our optimization result (Theorem 2) appliesonly for small values of robustness ρ and to a limited class of Wasserstein costs,0
7424,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7425,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7426,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7427,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7428,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7429,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7430,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7431,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,allowing us to prevent attacks on the test set,0
7432,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,0
7433,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,Human-level control through deep reinforcementlearning,0
7434,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7435,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7436,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7437,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",0
7438,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7439,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7440,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
7441,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7442,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7443,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7444,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7445,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Consequently, stochasticgradient methods applied to problem (2) have similar convergence guarantees as for non-robustmethods (ERM)",0
7446,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =",radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,0
7447,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",0
7448,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",0
7449,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7450,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",0
7451,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Wu, S",0
7452,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7453,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7454,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =",Madry et al,0
7455,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
7456,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Jha, and A",0
7457,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",0
7458,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =",We illustrate test misclassiﬁcationerror vs,0
7459,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B",0
7460,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7461,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =",adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,0
7462,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,582–597,0
7463,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7464,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,A,0
7465,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Rusu, J",0
7466,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,We illustrate testmisclassiﬁcation error vs,0
7467,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"(2017), Dziugaite & Roy (2017), andNeyshabur et al",0
7468,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"(2017a) show that 3-SAT Turing-reduces to this decision problem:given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with apolynomial number of calls to D",0
7469,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7470,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =",Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,0
7471,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks",0
7472,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
7473,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7474,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7475,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7476,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7477,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7478,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
7479,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",0
7480,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7481,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7482,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)",0
7483,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7484,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7485,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for",0
7486,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7487,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7488,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Our proof is less general, requiring the cost function c to be continuous and convexin its ﬁrst argument",0
7489,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",0
7490,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7491,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =",A,0
7492,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",0
7493,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",The below general duality result gives Proposition 1 as an immediate specialcase,0
7494,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7495,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7496,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7497,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7498,"Using the powerful measurability results ofRockafellar & Wets (1998, Theorem 14.60), we have[f (x(z)) − γc(x(z), z)]dQ(z) =","Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7499,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,"Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each zand for each measurable A, the function z (cid:55)→ P (A | z) is measurable",1
7500,"Now, let x(z) beany measurable function that is -close to attaining the supremum above","In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp",0
7501,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7502,"Now, let x(z) beany measurable function that is -close to attaining the supremum above","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",0
7503,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",0
7504,"Then using thepreceding display, we have","because f − c is upper semi-continuous, and the latter function is measurable",1
7505,"Then using thepreceding display, we have","Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)as well as larger test adversarial budgets",0
7506,"Then using thepreceding display, we have","because f − c is upper semi-continuous, and the latter function is measurable",1
7507,"Then using thepreceding display, we have","because f − c is upper semi-continuous, and the latter function is measurable",1
7508,"because f − c is upper semi-continuous, and the latter function is measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7509,"Then using thepreceding display, we have","We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",0
7510,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or",0
7511,"because f − c is upper semi-continuous, and the latter function is measurable",AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
7512,"Then using thepreceding display, we have","because f − c is upper semi-continuous, and the latter function is measurable",1
7513,"Then using thepreceding display, we have","because f − c is upper semi-continuous, and the latter function is measurable",1
7514,"because f − c is upper semi-continuous, and the latter function is measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7515,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),0
7516,"because f − c is upper semi-continuous, and the latter function is measurable","Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
7517,"Now, let x(z) beany measurable function that is -close to attaining the supremum above",We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,0
7518,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7519,"Then using thepreceding display, we have","We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1",0
7520,"Then using thepreceding display, we have","The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",0
7521,"Now, let x(z) beany measurable function that is -close to attaining the supremum above",the adversarial perturbation level adv,0
7522,"Then using thepreceding display, we have","because f − c is upper semi-continuous, and the latter function is measurable",1
7523,"because f − c is upper semi-continuous, and the latter function is measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7524,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7525,"Now, let x(z) beany measurable function that is -close to attaining the supremum above","because f − c is upper semi-continuous, and the latter function is measurable",1
7526,"Now, let x(z) beany measurable function that is -close to attaining the supremum above",Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,0
7527,"Now, let x(z) beany measurable function that is -close to attaining the supremum above",M,0
7528,"because f − c is upper semi-continuous, and the latter function is measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7529,"Then using thepreceding display, we have","because f − c is upper semi-continuous, and the latter function is measurable",1
7530,"because f − c is upper semi-continuous, and the latter function is measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7531,"because f − c is upper semi-continuous, and the latter function is measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7532,"because f − c is upper semi-continuous, and the latter function is measurable",Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,0
7533,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",582–597,0
7534,"because f − c is upper semi-continuous, and the latter function is measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7535,"Now, let x(z) beany measurable function that is -close to attaining the supremum above","because f − c is upper semi-continuous, and the latter function is measurable",1
7536,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7537,"because f − c is upper semi-continuous, and the latter function is measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7538,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",0
7539,"Now, let x(z) beany measurable function that is -close to attaining the supremum above",the adversarial perturbation level adv,0
7540,"Now, let x(z) beany measurable function that is -close to attaining the supremum above","because f − c is upper semi-continuous, and the latter function is measurable",1
7541,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes",0
7542,"Now, let x(z) beany measurable function that is -close to attaining the supremum above","(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz (cid:107)z − z(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz (cid:107)z − z(cid:48)(cid:107) , (cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ (cid:107)θ − θ(cid:48)(cid:107) .",0
7543,"Then using thepreceding display, we have","6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",0
7544,"Now, let x(z) beany measurable function that is -close to attaining the supremum above","because f − c is upper semi-continuous, and the latter function is measurable",1
7545,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",N,0
7546,"Then using thepreceding display, we have","because f − c is upper semi-continuous, and the latter function is measurable",1
7547,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7548,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",In scenarios where the underlying environment has a continuous state-space and we representQ with a differentiable function (e.g,0
7549,"because f − c is upper semi-continuous, and the latter function is measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7550,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7551,"Now, let x(z) beany measurable function that is -close to attaining the supremum above","Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples",0
7552,"Then using thepreceding display, we have",Attacks on the MNIST dataset,0
7553,"Now, let x(z) beany measurable function that is -close to attaining the supremum above","because f − c is upper semi-continuous, and the latter function is measurable",1
7554,"because f − c is upper semi-continuous, and the latter function is measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7555,"Now, let x(z) beany measurable function that is -close to attaining the supremum above","because f − c is upper semi-continuous, and the latter function is measurable",1
7556,"Now, let x(z) beany measurable function that is -close to attaining the supremum above",Human-level control through deep reinforcementlearning,0
7557,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",0
7558,"because f − c is upper semi-continuous, and the latter function is measurable",Distillation as a defense to adversarialperturbations against deep neural networks,0
7559,"Then using thepreceding display, we have",Roy,0
7560,"because f − c is upper semi-continuous, and the latter function is measurable",2.2) gives the result.,0
7561,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7562,"Now, let x(z) beany measurable function that is -close to attaining the supremum above",In constrast to f-divergences (e.g,0
7563,"because f − c is upper semi-continuous, and the latter function is measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7564,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),0
7565,"because f − c is upper semi-continuous, and the latter function is measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7566,"because f − c is upper semi-continuous, and the latter function is measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7567,"Now, let x(z) beany measurable function that is -close to attaining the supremum above","because f − c is upper semi-continuous, and the latter function is measurable",1
7568,"Then using thepreceding display, we have","because f − c is upper semi-continuous, and the latter function is measurable",1
7569,"because f − c is upper semi-continuous, and the latter function is measurable","(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)",0
7570,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ",0
7571,"because f − c is upper semi-continuous, and the latter function is measurable","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",0
7572,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7573,"Now, let x(z) beany measurable function that is -close to attaining the supremum above","because f − c is upper semi-continuous, and the latter function is measurable",1
7574,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,0
7575,"Then using thepreceding display, we have","Of course, the certiﬁcate (15) stillholds regardless.More broadly, this work focuses on small-perturbation attacks, and our theoretical guarantees showthat it is possible to efﬁciently build models that provably guard against such attacks",0
7576,"Now, let x(z) beany measurable function that is -close to attaining the supremum above",(c) Test error vs,0
7577,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7578,"Then using thepreceding display, we have","Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",0
7579,"Now, let x(z) beany measurable function that is -close to attaining the supremum above",M,0
7580,"because f − c is upper semi-continuous, and the latter function is measurable",We typically solve the penalty problem (2) with P0 replaced,0
7581,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7582,"Now, let x(z) beany measurable function that is -close to attaining the supremum above","In Security and Privacy (SP), 2016 IEEE Symposiumon, pp",0
7583,"Now, let x(z) beany measurable function that is -close to attaining the supremum above","because f − c is upper semi-continuous, and the latter function is measurable",1
7584,"Then using thepreceding display, we have",allowing us to prevent attacks on the test set,0
7585,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,0
7586,"because f − c is upper semi-continuous, and the latter function is measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7587,"Then using thepreceding display, we have","In the former case, since the update (22)simply modiﬁes the state-action transitions (independent of Q), standard results on convergence fortabular Q-learning (e.g",0
7588,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7589,"Now, let x(z) beany measurable function that is -close to attaining the supremum above","because f − c is upper semi-continuous, and the latter function is measurable",1
7590,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7591,"because f − c is upper semi-continuous, and the latter function is measurable",The choice of P in the robust objective (1) affectsboth the richness of the uncertainty set we wish to consider as well as the tractability of the result-ing optimization problem,0
7592,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7593,"because f − c is upper semi-continuous, and the latter function is measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7594,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7595,"because f − c is upper semi-continuous, and the latter function is measurable","For each γadv, we consider the distance to adversarial examples in the test dataset",0
7596,"Now, let x(z) beany measurable function that is -close to attaining the supremum above","because f − c is upper semi-continuous, and the latter function is measurable",1
7597,"Then using thepreceding display, we have","because f − c is upper semi-continuous, and the latter function is measurable",1
7598,"Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable","because f − c is upper semi-continuous, and the latter function is measurable",1
7599,"Then using thepreceding display, we have","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",0
7600,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7601,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",0
7602,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",0
7603,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques",0
7604,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7605,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","Kavukcuoglu, D",0
7606,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.",0
7607,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",0
7608,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7609,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7610,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
7611,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7612,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7613,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.",0
7614,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",0
7615,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞",0
7616,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7617,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0",Results are shown in Figure 13 for a smalltraining adversary and Figure 14 for a large training adversary,0
7618,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7619,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","Silver, A",0
7620,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7621,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7622,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7623,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7624,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","Graves, M",0
7625,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","We now give a concrete variant of Theorem 3 for Lipschitz functions.When Θ is ﬁnite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearlywith d despite the inﬁnite-dimensional Wasserstein penalty",0
7626,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7627,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7628,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7629,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7630,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7631,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7632,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7633,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7634,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",the adversarial perturbation level adv,0
7635,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7636,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",0
7637,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7638,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)",0
7639,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7640,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",0
7641,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7642,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7643,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7644,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7645,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",0
7646,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7647,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7648,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
7649,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",0
7650,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7651,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives",We illustrate testmisclassiﬁcation error vs,0
7652,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7653,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7654,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7655,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s",0
7656,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7657,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7658,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7659,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7660,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","In this regime(small γ, large ), performance between WRM and other methods diverge",0
7661,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","In the former case, since the update (22)simply modiﬁes the state-action transitions (independent of Q), standard results on convergence fortabular Q-learning (e.g",0
7662,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",0
7663,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","McDaniel, X",0
7664,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7665,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7666,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7667,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.",0
7668,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
7669,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7670,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7671,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7672,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7673,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7674,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead onthe Lagrangian penalty problem (2) and its empirical counterpart",0
7675,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7676,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ",We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,0
7677,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7678,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0",We thank Jacob Steinhardt for valuable feedback,0
7679,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7680,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","Fawzi, and P",0
7681,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7682,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7683,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7684,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7685,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time",0
7686,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)","Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",0
7687,"For Lipschitznessof z(cid:63)(θ), we ﬁrst argue that z(cid:63)(θ) is continuous in θ","because f − c is upper semi-continuous, and the latter function is measurable",0
7688,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7689,"C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,0
7690,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7691,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","In many systems, robustness to changes in the data-generating distribution P0 isdesirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016)",0
7692,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7693,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
7694,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7695,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives",Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,0
7696,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0",K,0
7697,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives","Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)",0
7698,"By strong concavity, for any θ1, θ2 and z(cid:63)2 = z(cid:63)(θ2),we have(cid:107)z(cid:63)λ2Summing these inequalities gives",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
7699,"For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",1
7700,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","Celik, and A",0
7701,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7702,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7703,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
7704,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7705,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","Bellemare, A",0
7706,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","In Security and Privacy (SP), 2016 IEEE Symposiumon, pp",0
7707,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks",0
7708,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7709,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","A.5 MNIST ∞-NORM EXPERIMENTSWe consider training FGM, IFGM, and PGM with p = ∞",0
7710,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7711,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7712,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
7713,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",0
7714,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations",0
7715,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7716,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7717,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7718,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7719,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","We note thattudes of ∇z(cid:96)(θ; z) in a neighborhood of the nominal input, which ensures stability of the model.",0
7720,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",We have the following theorem.Theorem 5,0
7721,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
7722,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7723,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
7724,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM",0
7725,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","Then, we consider a heuristicLagrangian approach for training WRM with the squared ∞-norm cost.",0
7726,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",The limitations ofdeep learning in adversarial settings,0
7727,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7728,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7729,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7730,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7731,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7732,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",Since 70% of the data are of the,0
7733,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7734,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)",0
7735,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",0
7736,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",0
7737,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7738,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",0
7739,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",0
7740,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7741,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",(16)) in θ,0
7742,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",0
7743,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7744,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","ProofFirst, we introduce the decision reformulation of the problem: for some b, we ask whetherthere exists some u such that (cid:96)(θ; z + u) ≥ b",0
7745,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7746,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7747,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","McDaniel, S",0
7748,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
7749,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7750,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",0
7751,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",0
7752,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7753,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7754,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)",0
7755,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",0
7756,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",Swami,0
7757,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7758,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",K,0
7759,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7760,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7761,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7762,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",The below general duality result gives Proposition 1 as an immediate specialcase,0
7763,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7764,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",0
7765,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7766,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,0
7767,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7768,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",2.2) gives the result.,0
7769,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7770,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7771,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",HN was partially supported by aSamsung Fellowship,0
7772,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7773,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7774,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7775,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks",0
7776,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7777,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7778,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",0
7779,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7780,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7781,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7782,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7783,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","These results suggest advantages of networks with smooth activations
rather than ReLU’s",0
7784,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",0
7785,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.",0
7786,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7787,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7788,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7789,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",A,0
7790,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7791,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7792,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,0
7793,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7794,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7795,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",Kuhn,0
7796,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7797,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))","To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",1
7798,"To see the second inequality, we show that ¯f is differentiable with ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",We providea principled method for efﬁciently guaranteeing distributional robustness with a simple form ofadversarial data perturbation,0
7799,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",0
7800,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
7801,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7802,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","Then using thepreceding display, we have",0
7803,"If f is inf-compact, then ¯f is directionally differentiable with","6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",0
7804,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)","From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",0
7805,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z","for all γ, ρ ≥ 0",0
7806,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7807,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7808,"If f is inf-compact, then ¯f is directionally differentiable with","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7809,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7810,"If f is inf-compact, then ¯f is directionally differentiable with","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7811,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z","For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM",0
7812,"If f is inf-compact, then ¯f is directionally differentiable with",Theorem 7,0
7813,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7814,"If f is inf-compact, then ¯f is directionally differentiable with",The action space is binary: push the cartleft or right with a ﬁxed force,0
7815,"If f is inf-compact, then ¯f is directionally differentiable with",(2013) and Namkoong & Duchi (2017) use convex optimization approaches for f-divergence balls,0
7816,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7817,"If f is inf-compact, then ¯f is directionally differentiable with","AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",0
7818,"If f is inf-compact, then ¯f is directionally differentiable with",Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,0
7819,"If f is inf-compact, then ¯f is directionally differentiable with",We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,0
7820,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7821,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7822,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z","Replacing our current covering num-ber arguments with more intricate notions such as margin-based bounds (Bartlett et al., 2017) wouldextend the scope and usefulness of our theoretical guarantees",0
7823,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",8.7.1) to obtain,0
7824,"If f is inf-compact, then ¯f is directionally differentiable with","For large γ, we can again solve problem (22) efﬁciently using gradient descent",0
7825,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
7826,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7827,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","(2016) show how to convert the saddle-pointproblem (1) to a regularized ERM problem, but this is possible only for a limited class of convexlosses (cid:96) and costs c",0
7828,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z",The function c : Z×Z → R+ is continuous,0
7829,"If f is inf-compact, then ¯f is directionally differentiable with",Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,0
7830,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7831,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7832,"If f is inf-compact, then ¯f is directionally differentiable with","Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",0
7833,"If f is inf-compact, then ¯f is directionally differentiable with",In constrast to f-divergences (e.g,0
7834,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,0
7835,"If f is inf-compact, then ¯f is directionally differentiable with","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7836,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",PGM attacks on the MNIST dataset,0
7837,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7838,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",0
7839,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7840,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z",Theboundaries are shown with the training data as well as separately with the true class boundaries.,0
7841,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7842,"If f is inf-compact, then ¯f is directionally differentiable with",The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,0
7843,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7844,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7845,"If f is inf-compact, then ¯f is directionally differentiable with","The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
7846,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7847,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7848,"If f is inf-compact, then ¯f is directionally differentiable with",Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,0
7849,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),0
7850,"If f is inf-compact, then ¯f is directionally differentiable with","Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",0
7851,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",(e) Test error vs,0
7852,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)","Throughout, we assume Θ ⊆ Rd.Lemma 1",0
7853,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)","Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)",0
7854,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7855,"If f is inf-compact, then ¯f is directionally differentiable with","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7856,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z",N,0
7857,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",The,0
7858,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)","Top row: FGM attacks, bottom row: IFGM attacks",0
7859,"If f is inf-compact, then ¯f is directionally differentiable with","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7860,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z",Theboundaries are shown with the training data as well as separately with the true class boundaries.,0
7861,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z",The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,0
7862,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7863,"If f is inf-compact, then ¯f is directionally differentiable with","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7864,"If f is inf-compact, then ¯f is directionally differentiable with","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7865,"If f is inf-compact, then ¯f is directionally differentiable with","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",0
7866,"If f is inf-compact, then ¯f is directionally differentiable with","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
7867,"If f is inf-compact, then ¯f is directionally differentiable with","For imperceptible perturbations, our method matches or outperformsheuristic approaches.",0
7868,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,0
7869,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,0
7870,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",Figure 2,0
7871,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7872,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
7873,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",N,0
7874,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7875,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7876,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7877,"If f is inf-compact, then ¯f is directionally differentiable with","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7878,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)","We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5",0
7879,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B",0
7880,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z",Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,0
7881,"If f is inf-compact, then ¯f is directionally differentiable with","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7882,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",582–597,0
7883,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",0
7884,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7885,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)","Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",0
7886,"If f is inf-compact, then ¯f is directionally differentiable with","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7887,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or",0
7888,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7889,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7890,"If f is inf-compact, then ¯f is directionally differentiable with","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7891,"If f is inf-compact, then ¯f is directionally differentiable with",Dziugaite and D,0
7892,"If f is inf-compact, then ¯f is directionally differentiable with","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",0
7893,"If f is inf-compact, then ¯f is directionally differentiable with","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7894,"If f is inf-compact, then ¯f is directionally differentiable with","AS, HN, and JD were partially supported bythe SAIL-Toyota Center for AI Research",0
7895,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7896,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)","For exam-ple, for a differentiable convex h : Z → R, the Bregman divergence c(z, z0) = h(z) − h(z0) −(cid:104)∇h(z0), z − z0(cid:105) satisﬁes these conditions",0
7897,"See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,0
7898,"for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7899,"If f is inf-compact, then ¯f is directionally differentiable with","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",1
7900,from which it is easy to see that f is inf-compact,from which it is easy to see that f is inf-compact,1
7901,from which it is easy to see that f is inf-compact,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",0
7902,from which it is easy to see that f is inf-compact,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes",0
7903,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7904,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))","Top row: FGM attacks, bottom row: IFGM attacks",0
7905,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7906,from which it is easy to see that f is inf-compact,"In a learning-theoretic context,where the goal is to provide insight into convergence behavior as well as comfort that a proce-dure will “work” given enough data, such guarantees are satisfactory, but this may not be enoughin security-essential contexts",0
7907,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",from which it is easy to see that f is inf-compact,1
7908,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",from which it is easy to see that f is inf-compact,1
7909,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7910,from which it is easy to see that f is inf-compact,from which it is easy to see that f is inf-compact,1
7911,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7912,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7913,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7914,from which it is easy to see that f is inf-compact,from which it is easy to see that f is inf-compact,1
7915,from which it is easy to see that f is inf-compact,"Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",0
7916,from which it is easy to see that f is inf-compact,(e) Test error vs,0
7917,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",from which it is easy to see that f is inf-compact,1
7918,from which it is easy to see that f is inf-compact,from which it is easy to see that f is inf-compact,1
7919,from which it is easy to see that f is inf-compact,from which it is easy to see that f is inf-compact,1
7920,from which it is easy to see that f is inf-compact,Our optimization result (Theorem 2) appliesonly for small values of robustness ρ and to a limited class of Wasserstein costs,0
7921,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)","Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",0
7922,from which it is easy to see that f is inf-compact,from which it is easy to see that f is inf-compact,1
7923,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7924,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7925,from which it is easy to see that f is inf-compact,Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,0
7926,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",from which it is easy to see that f is inf-compact,1
7927,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))","Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",0
7928,from which it is easy to see that f is inf-compact,"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",0
7929,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)","Graves, M",0
7930,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7931,from which it is easy to see that f is inf-compact,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,0
7932,from which it is easy to see that f is inf-compact,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",0
7933,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7934,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)","We considerthe robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 underthe Wasserstein metric Wc(·,·) (see Section 2 for a formal deﬁnition)",0
7935,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",(f) Test error vs,0
7936,from which it is easy to see that f is inf-compact,"We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",0
7937,from which it is easy to see that f is inf-compact,"Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",0
7938,from which it is easy to see that f is inf-compact,"Previous approaches to distributional robustness have considered ﬁnite-dimensional parametrizations for P, such as constraint sets for moments, support, or directionaldeviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametricdistances for probability measures such as f-divergences (Ben-Tal et al., 2013; Bertsimas et al.,2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), andWasserstein distances (Esfahani & Kuhn, 2015; Shaﬁeezadeh-Abadeh et al., 2015; Blanchet et al.,2016)",0
7939,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))","Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models",0
7940,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))","For probability measures P and Q supported on Z,let Π(P, Q) denote their couplings, meaning measures M on Z 2 with M (A,Z) = P (A) and",0
7941,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",from which it is easy to see that f is inf-compact,1
7942,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)","Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",0
7943,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)","Then, we have",0
7944,from which it is easy to see that f is inf-compact,The function c : Z×Z → R+ is continuous,0
7945,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",from which it is easy to see that f is inf-compact,1
7946,from which it is easy to see that f is inf-compact,from which it is easy to see that f is inf-compact,1
7947,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",We typically solve the penalty problem (2) with P0 replaced,0
7948,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7949,from which it is easy to see that f is inf-compact,from which it is easy to see that f is inf-compact,1
7950,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",from which it is easy to see that f is inf-compact,1
7951,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",0
7952,from which it is easy to see that f is inf-compact,from which it is easy to see that f is inf-compact,1
7953,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7954,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7955,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)","When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",0
7956,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",from which it is easy to see that f is inf-compact,1
7957,from which it is easy to see that f is inf-compact,All of our resultssuitably generalize to this setting with minor modiﬁcations to the robust surrogate (2b) and theabove assumptions (see Section D),0
7958,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7959,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7960,from which it is easy to see that f is inf-compact,from which it is easy to see that f is inf-compact,1
7961,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,0
7962,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))","Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",0
7963,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)","a deterministicalgorithm can decide in polynomial time whether u ∈ U), and (iii) (cid:96) can be evaluated in polynomialtime",0
7964,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",from which it is easy to see that f is inf-compact,1
7965,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)","Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples",0
7966,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7967,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",from which it is easy to see that f is inf-compact,1
7968,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,0
7969,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)","arXiv:1505.05116 [math.OC],2015.",0
7970,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7971,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",(a) and (b) show test misclassiﬁcation error vs,0
7972,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",by solving (7),0
7973,from which it is easy to see that f is inf-compact,Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,0
7974,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",from which it is easy to see that f is inf-compact,1
7975,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7976,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",0
7977,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",from which it is easy to see that f is inf-compact,1
7978,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))","Then, with probability at leastner, 1996, Ch",0
7979,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7980,from which it is easy to see that f is inf-compact,from which it is easy to see that f is inf-compact,1
7981,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",0
7982,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",We illustrate test misclassiﬁcation error vs,0
7983,from which it is easy to see that f is inf-compact,from which it is easy to see that f is inf-compact,1
7984,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7985,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)","We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A",0
7986,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))","In the former case, since the update (22)simply modiﬁes the state-action transitions (independent of Q), standard results on convergence fortabular Q-learning (e.g",0
7987,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7988,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",from which it is easy to see that f is inf-compact,1
7989,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7990,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)","NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",0
7991,from which it is easy to see that f is inf-compact,from which it is easy to see that f is inf-compact,1
7992,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)","Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",0
7993,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",In scenarios where the underlying environment has a continuous state-space and we representQ with a differentiable function (e.g,0
7994,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",", K}",0
7995,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",0
7996,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
7997,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",from which it is easy to see that f is inf-compact,1
7998,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)","By appropriatescaling of θ, v, and w, Katz et al",0
7999,"Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",from which it is easy to see that f is inf-compact,1
8000,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",0
8001,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8002,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8003,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,0
8004,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8005,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8006,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",0
8007,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
8008,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,0
8009,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","With that said, the strong dualityresult, Proposition 1, still applies to any distribution",0
8010,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8011,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt",theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,0
8012,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8013,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8014,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8015,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",Explicit distributional robustness of the form (5) is intractable except in limited cases,0
8016,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8017,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8018,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8019,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8020,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",0
8021,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8022,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8023,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8024,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8025,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8026,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8027,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Throughout, we assume Θ ⊆ Rd.Lemma 1",0
8028,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8029,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8030,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets",0
8031,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",0
8032,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Moreover,(cid:107)z(cid:63)(θ1) − z(cid:63)(θ2)(cid:107) ≤ Lzθλ",0
8033,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt",adv for (cid:107) · (cid:107)2-IFGM attackFigure 10,0
8034,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8035,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothnessassumptions",0
8036,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8037,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8038,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8039,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8040,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8041,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
8042,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8043,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Mnih, K",0
8044,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)",0
8045,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",0
8046,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering",0
8047,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt",adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,0
8048,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",0
8049,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",0
8050,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8051,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",0
8052,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",0
8053,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",Let ∆F ≥ F (θ0) − inf θ F (θ),0
8054,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",0
8055,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Then there exists θ such that
this optimization problem is also NP-hard.",0
8056,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt",For large adversaries(i.e,0
8057,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8058,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8059,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",0
8060,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
8061,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",We providea principled method for efﬁciently guaranteeing distributional robustness with a simple form ofadversarial data perturbation,0
8062,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,0
8063,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8064,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",0
8065,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Speciﬁcally, we again consider WRM attacks and we decrease γadv untileach model misclassiﬁes the input",0
8066,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Jha, and A",0
8067,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8068,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8069,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8070,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8071,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",0
8072,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",The distinctions in performance between various methodsare less apparent now,0
8073,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt",the adversarial perturbation level adv,0
8074,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8075,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8076,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8077,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","All methods achieve at least 99% test-set accuracy,implying there is little test-time penalty for the robustness levels ( and γ) used for training",0
8078,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8079,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8080,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8081,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",It isthus important to distinguish the methods’ abilities to combat attacks,0
8082,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","In this context, our work is situated between these agendas: wedevelop efﬁcient procedures with rigorous guarantees for small to moderate amounts of robustness.We take the perspective of distributionally robust optimization and provide an adversarial trainingprocedure with provable guarantees on its computational and statistical performance",0
8083,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",0
8084,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8085,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt",372–387,0
8086,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",Swami,0
8087,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8088,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",0
8089,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",Swami,0
8090,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",0
8091,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,0
8092,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",2.2) gives the result.,0
8093,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",0
8094,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Kavukcuoglu, D",0
8095,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","(2015), and Blanchet et al",0
8096,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",1
8097,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",Training data are shown in blue and red,0
8098,"Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",0
8099,"Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
8100,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","McDaniel, X",0
8101,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:c((x, y), (x0, y0)) = (cid:107)x − x0(cid:107)2∞ + ∞ · 1{y (cid:54)= y0} .",0
8102,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8103,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8104,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8105,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8106,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)",0
8107,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8108,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8109,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",0
8110,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,0
8111,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
8112,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8113,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)",0
8114,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8115,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","In this regime(small γ, large ), performance between WRM and other methods diverge",0
8116,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",Theboundaries are shown with the training data as well as separately with the true class boundaries.,0
8117,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8118,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8119,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","However, WRM withReLU’s still suffers from sensitivities (e.g",0
8120,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",Other models do not exhibit this behavior with the same consistency(if at all),0
8121,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8122,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8123,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8124,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8125,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","For example,Ben-Tal et al",0
8126,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2574–2582, 2016.",0
8127,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8128,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,0
8129,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8130,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",0
8131,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8132,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8133,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",Theboundaries are shown with the training data as well as separately with the true class boundaries.,0
8134,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",(16)) in θ,0
8135,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8136,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8137,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,0
8138,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","Wu, S",0
8139,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8140,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","For example,Ben-Tal et al",0
8141,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",0
8142,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8143,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","We test performance of theﬁve methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and∞-norms",0
8144,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8145,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",0
8146,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8147,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8148,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8149,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",(2013) and Namkoong & Duchi (2017) use convex optimization approaches for f-divergence balls,0
8150,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",Kuhn,0
8151,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",0
8152,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",0
8153,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8154,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd",0
8155,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8156,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8157,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",0
8158,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","For tabular Q-learning, where werepresent Q only over a discretized covering of the underlying state-space, we can either neglect thesecond term in the update (22) and, after performing the update, round st+1 as usual, or we can per-form minimization directly over the discretized covering",0
8159,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8160,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","If f is inf-compact, then ¯f is directionally differentiable with",0
8161,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",0
8162,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",Wasserstein robustness and duality Wasserstein distances deﬁne a notion of closeness betweendistributions,0
8163,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8164,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1",0
8165,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8166,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",0
8167,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8168,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",adv for (cid:107) · (cid:107)2-IFGM attackFigure 12,0
8169,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,0
8170,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",Goodfellow et al,0
8171,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8172,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞",0
8173,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)",0
8174,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8175,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8176,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8177,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8178,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8179,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",the adversarial perturbation level adv,0
8180,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8181,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models",0
8182,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8183,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8184,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8185,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8186,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",0
8187,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
8188,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8189,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8190,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
8191,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",The choice of P inﬂuences robustness guarantees and computability; we develop robustness setsP with computationally efﬁcient relaxations that apply even when the loss (cid:96) is non-convex,0
8192,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.",0
8193,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8194,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8195,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",2.2) gives the result.,0
8196,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8197,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2",1
8198,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",0
8199,"2 + 2(cid:10)∇θφγ(θ; zt) − ∇F (θt), δt(cid:11)(cid:17)2 +(cid:13)(cid:13)∇θφγ(θ; zt) − ∇F (θt)(cid:13)(cid:13)2(cid:13)(cid:13)∇F (θt)(cid:13)(cid:13)22 + αt (1 − Lφαt)(cid:10)∇F (θt),∇F (θt) − ∇θφγ(θ; zt)(cid:11)(cid:13)(cid:13)δt(cid:13)(cid:13)2","By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",0
8200,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8201,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8202,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8203,"Then, with probability at leastner, 1996, Ch",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8204,"Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8205,"Then, with probability at leastner, 1996, Ch",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8206,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8207,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),"First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",0
8208,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8209,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8210,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8211,"Then, with probability at leastner, 1996, Ch","Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",0
8212,"Then, with probability at leastner, 1996, Ch","In this proof, we drop the subscript on the iteration t to ease notation",0
8213,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8214,"Then, with probability at leastner, 1996, Ch","Then, the bounds (11) and (12) hold with",0
8215,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),"For our method, the inner supremum is no longer strongly concave for over10% of the data, indicating that we no longer have guarantees of performance",0
8216,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),"Forour approach we use γ = 2, and to make fair comparisons with FGM we use",0
8217,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8218,"Then, with probability at leastner, 1996, Ch","We note thattudes of ∇z(cid:96)(θ; z) in a neighborhood of the nominal input, which ensures stability of the model.",0
8219,"Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8220,2.2) gives the result.,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,0
8221,"Then, with probability at leastner, 1996, Ch",", K}",0
8222,"Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8223,"Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",0
8224,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",0
8225,"Then, with probability at leastner, 1996, Ch","In Figure 7, we repeat the illustration in Figure 4(b) for more digits",0
8226,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8227,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8228,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)","14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",0
8229,"Then, with probability at leastner, 1996, Ch",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8230,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8231,"Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,0
8232,"Then, with probability at leastner, 1996, Ch",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,0
8233,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8234,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8235,"Then, with probability at leastner, 1996, Ch","The smoothness of (cid:96) is essential so that a penalized version(cid:96)(θ, z) − γc(z, z0) is concave in z (which can be approximately veriﬁed by computing Hessians∇2zz(cid:96)(θ, z) for each training datapoint), allowing computation and our coming certiﬁcates of opti-mality",0
8236,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8237,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8238,2.2) gives the result.,"Wu, S",0
8239,2.2) gives the result.,"Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” foran adversary to perturb z0 to z (we typically use c(z, z0) = (cid:107)z − z0(cid:107)2p with p ≥ 1)",0
8240,"Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8241,"Then, with probability at leastner, 1996, Ch","Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",0
8242,"Then, with probability at leastner, 1996, Ch",(f) Test error vs,0
8243,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),For large adversaries(i.e,0
8244,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8245,"Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",Weuse a tabular representation for Q with 30 discretized states for β and 15 for its time-derivative ˙β (weperform the update (22) without the Q-dependent term),0
8246,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8247,2.2) gives the result.,"Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques",0
8248,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)","Classiﬁca-tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively",0
8249,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,0
8250,"Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm","(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave",0
8251,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8252,"Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm","From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",0
8253,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8254,"Then, with probability at leastner, 1996, Ch",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8255,"Then, with probability at leastner, 1996, Ch",AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
8256,2.2) gives the result.,Neural networks are vulnerable to adversarial examples and researchers have pro-posed many heuristic attack and defense mechanisms,0
8257,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8258,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)","The following dualityresult (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result forthe problem (1)",0
8259,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)","Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞",0
8260,"Then, with probability at leastner, 1996, Ch",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8261,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8262,"Then, with probability at leastner, 1996, Ch","For any distribution Q and any ρ > 0,(5)",0
8263,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)","First, in Figure 2(b) we again illustrate the validity of our certiﬁcate of robustness (11) for the worst-case test performance for arbitrary level of robustness ρ",0
8264,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",Our bound is efﬁciently computable and hence certiﬁes a level of robustness forthe worst-case population objective,0
8265,2.2) gives the result.,The below general duality result gives Proposition 1 as an immediate specialcase,0
8266,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8267,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)","Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks",0
8268,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
8269,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),"Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks",0
8270,"Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8271,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)","The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.",0
8272,2.2) gives the result.,"(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",0
8273,2.2) gives the result.,Weillustrate test misclassiﬁcation error vs,0
8274,2.2) gives the result.,Assume |(cid:96)(θ; z)| ≤ M(cid:96) for all θ ∈ Θ and z ∈ Z,0
8275,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)","The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.",0
8276,"Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm","In this regime(small γ, large ), performance between WRM and other methods diverge",0
8277,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8278,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8279,"Then, with probability at leastner, 1996, Ch",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8280,"Then, with probability at leastner, 1996, Ch","Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",0
8281,2.2) gives the result.,"Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",0
8282,"Then, with probability at leastner, 1996, Ch",(f) Test error vs,0
8283,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8284,"Then, with probability at leastner, 1996, Ch",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8285,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8286,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8287,"Then, with probability at leastner, 1996, Ch",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8288,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8289,"Then, with probability at leastner, 1996, Ch","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",0
8290,"Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8291,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8292,"Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm","Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)",0
8293,"Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8294,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes",0
8295,2.2) gives the result.,because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8296,"Then, with probability at leastner, 1996, Ch",because −M(cid:96) ≤ (cid:96)(θ; z) ≤ φγ(θ; z) ≤ supz (cid:96)(θ; z) ≤ M(cid:96),1
8297,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)","Namely, we draw the nominal",0
8298,"Then, with probability at leastner, 1996, Ch","We note thattudes of ∇z(cid:96)(θ; z) in a neighborhood of the nominal input, which ensures stability of the model.",0
8299,"6.2), and applying standard results onRademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-To see the second result (12), we substitute ρ =(cid:98)ρn in the bound (11)",We assume without loss of2 − z0 (cid:54)= 0,0
8300,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8301,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or",The below general duality result gives Proposition 1 as an immediate specialcase,0
8302,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8303,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8304,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8305,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8306,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or",Distillation as a defense to adversarialperturbations against deep neural networks,0
8307,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","Assuming there exist θ0 ∈ Θ, Mθ0 < ∞such that |(cid:96)(θ0; z)| ≤ Mθ0 for all z ∈ Z, we have the following corollary (see proof in Section C.5).Corollary 1",0
8308,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",0
8309,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","Of course, the certiﬁcate (15) stillholds regardless.More broadly, this work focuses on small-perturbation attacks, and our theoretical guarantees showthat it is possible to efﬁciently build models that provably guard against such attacks",0
8310,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5",0
8311,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8312,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","Nevertheless, this robust approach underlies recent advances in adversarial train-ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradientsign method” (Goodfellow et al., 2015)):∆xi(θ) := argmax
(cid:107)η(cid:107)p≤",0
8313,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8314,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8315,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8316,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8317,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8318,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","As the bound (11) with η = γ suggests, even when the adversary hasmore budget than that used for training (1/γ < 1/γadv), degradation in performance is still smooth.Further, as we decrease the penalty γ, the amount of achieved robustness—measured here by testerror on adversarial perturbations with γadv—has diminishing gains; this is again consistent withour theory which says that the inner problem (2b) is not efﬁciently computable for small γ.",0
8319,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8320,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","For any distribution Q and any ρ > 0,(5)",0
8321,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8322,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8323,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8324,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",PGM attacks on the MNIST dataset,0
8325,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8326,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",with the convention that 0 · ∞ = 0,0
8327,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes",0
8328,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8329,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","In this regime(small γ, large ), performance between WRM and other methods diverge",0
8330,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8331,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8332,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8333,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8334,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8335,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time",0
8336,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",We ﬁrst compare with WRM trainedin the same manner as before—with the squared Euclidean cost,0
8337,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",Kuhn,0
8338,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
8339,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",Kuhn,0
8340,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8341,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8342,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8343,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8344,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8345,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z",0
8346,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8347,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8348,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8349,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8350,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or",HN was partially supported by aSamsung Fellowship,0
8351,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","We believe that conventional (cid:107)·(cid:107)∞-defense heuristics developed for image classiﬁ-cation do not offer much comfort in the large-perturbation/perceptible-attack setting: (cid:107)·(cid:107)∞-attackswith a large budget can render images indiscernible to human eyes, while, for example, (cid:107)·(cid:107)1-attacksallow a concerted perturbation to critical regions of the image",0
8352,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8353,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8354,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or",", K}",0
8355,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.",0
8356,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",0
8357,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8358,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8359,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","Then, the bounds (11) and (12) hold with",0
8360,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8361,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8362,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or",(a) and (b) show test misclassiﬁcation error vs,0
8363,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8364,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8365,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8366,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",WRM’s “misclassiﬁcations”appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);WRM defends against gradient-based exploits by learning a representation that makes gradientspoint towards inputs of other classes,0
8367,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",is closed-valued and measurable,0
8368,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets",0
8369,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8370,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8371,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)",0
8372,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or",Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,0
8373,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8374,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),0
8375,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞",0
8376,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8377,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8378,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8379,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8380,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",(c) Test error vs,0
8381,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8382,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,0
8383,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8384,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8385,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,0
8386,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8387,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8388,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","Then using thepreceding display, we have",0
8389,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8390,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","See Bonnans & Shapiro (2013,Theorem 4.13) for a proof of the following result.Lemma 3",0
8391,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8392,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,0
8393,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1",0
8394,", θN} of Θ guarantees thatmini |(cid:96)(θ; z) − (cid:96)(θi; z)| ≤ L for all θ, z, or","NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",0
8395,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8396,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",We providea principled method for efﬁciently guaranteeing distributional robustness with a simple form ofadversarial data perturbation,0
8397,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8398,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usualloss (cid:96)(θ; Z) by the robust surrogate φγ(θ; Z); this surrogate (2b) allows adversarial perturbations ofthe data z, modulated by the penalty γ",0
8399,"The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",1
8400,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",The goal of a reinforcement-learning agent is to maximize (discounted) cumula-t λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [(cid:96)(θ; Z)] insupervised learning,0
8401,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8402,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt",0
8403,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain",Reasonable misclassiﬁcations correspond to having learned a data representation thatmakes gradients interpretable.,0
8404,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ",0
8405,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8406,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","Our starting point is a duality result for the minimax problem (1)and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connectionsbetween distributional robustness and the “lazy” surrogate (2b) clear",0
8407,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)",0
8408,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8409,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8410,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8411,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8412,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,0
8413,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain",AS was also partially supported by a Stanford GraduateFellowship and a Fannie & John Hertz Foundation Fellowship,0
8414,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","The decision reformulation for an NPO problem isin NP, as a certiﬁcate for the decision problem can be veriﬁed in polynomial time",0
8415,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)",0
8416,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","Finally, we consider a reinforcement learning problemin Section 4.3, where the Markov decision process used for training differs from that for testing.WRM enjoys the theoretical guarantees of Sections 2 and 3 for large γ, but for small γ (large adver-sarial budgets), WRM becomes a heuristic like other methods",0
8417,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8418,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8419,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8420,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",For large adversaries(i.e,0
8421,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes",0
8422,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8423,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8424,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8425,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8426,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",(2017) attempt to mitigate this shortcoming,0
8427,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8428,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","because f − c is upper semi-continuous, and the latter function is measurable",0
8429,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and",0
8430,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8431,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s",0
8432,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8433,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞",0
8434,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8435,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8436,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,0
8437,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8438,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8439,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8440,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","The decision problem Turing-reducesto the optimization problem, as the decision problem can be solved with one call to O",0
8441,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain",Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,0
8442,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",0
8443,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
8444,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",(e) Test error vs,0
8445,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8446,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8447,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8448,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8449,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8450,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8451,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8452,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8453,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp",0
8454,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8455,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","The decision problem is thus NP-complete.Now, consider an oracle O for the optimization problem",0
8456,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","In Figure 8, we choose a ﬁxed WRM adversary (ﬁxed γadv) and perturb WRM models trained withvarious penalty parameters γ",0
8457,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and",0
8458,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain",Let ∆F ≥ F (θ0) − inf θ F (θ),0
8459,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes",0
8460,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8461,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8462,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp",0
8463,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","empirical problem, concentrates uniformly around its population counterpart.Theorem 4",0
8464,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",The function cx : X × X → R+ is continuous,0
8465,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8466,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8467,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8468,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8469,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain",Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,0
8470,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8471,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","From Lemma 5, our previous results (Theorems 2 and 4) follow",0
8472,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","These results suggest advantages of networks with smooth activations
rather than ReLU’s",0
8473,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.",0
8474,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",the adversarial perturbation level adv,0
8475,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks",0
8476,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8477,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8478,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8479,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,0
8480,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8481,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",0
8482,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8483,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","IEEE, 2016b.",0
8484,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain",B,0
8485,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",G,0
8486,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8487,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain",Attacks on the MNIST dataset,0
8488,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p","The worst-case losses on the test dataset are then given byE(cid:98)PtestAs anticipated, our certiﬁcate is almost tight near the achieved level of robustness (cid:98)ρn(θWRM) for",0
8489,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8490,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8491,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8492,"By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",(e) Test error vs,0
8493,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",0
8494,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","Top row: PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",0
8495,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",0
8496,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures","In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers morerobustness even with respect to these PGM attacks",0
8497,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8498,"57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8499,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",1
8500,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","In Appendix A.4, we compare WRMwith other methods on attacks with large adversarial budgets",0
8501,"9–10)), we have",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,0
8502,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8503,(16)) in θ,"Suppose that f (·, z) is differentiable in θ for all z ∈ Z, and f, ∇zf are continuous onΘ × Z",0
8504,"9–10)), we have","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8505,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z",0
8506,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8507,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","Top row:PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks",0
8508,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","Graves, M",0
8509,(16)) in θ,"By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",0
8510,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",0
8511,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",is closed-valued and measurable,0
8512,(16)) in θ,"Applying Lemma 3 to ¯f and noting that S(θ) isunique by strong convexity of f (θ,·), we have that ¯f is directionally differentiable with ∇ ¯f (θ) =gθ(θ, z(cid:63)(θ))",0
8513,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8514,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8515,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",The environment caps episode lengths to 400 stepsand ends the episode prematurely if the pole falls too far from the vertical or the cart translates toofar from its origin,0
8516,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8517,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
8518,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8519,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",0
8520,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8521,(16)) in θ,The major beneﬁt of our approach is its simplicityand wide applicability across many models and machine-learning scenarios.There remain many avenues for future investigation,0
8522,"9–10)), we have","In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp",0
8523,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",0
8524,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8525,"9–10)), we have","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8526,"9–10)), we have",adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
8527,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","IEEE, 2016c.",0
8528,"9–10)), we have","To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost",0
8529,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8530,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8531,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","Throughout, we assume Θ ⊆ Rd.Lemma 1",0
8532,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8533,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",0
8534,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",0
8535,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8536,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8537,"9–10)), we have",14.27 and Prop,0
8538,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8539,(16)) in θ,"Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.",0
8540,(16)) in θ,"We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",0
8541,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",The choice of P in the robust objective (1) affectsboth the richness of the uncertainty set we wish to consider as well as the tractability of the result-ing optimization problem,0
8542,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8543,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8544,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",0
8545,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",Attacks on the MNIST dataset,0
8546,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",is closed-valued and measurable,0
8547,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8548,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8549,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2",0
8550,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8551,"9–10)), we have",Figure 3,0
8552,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8553,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8554,(16)) in θ,"However, WRM withReLU’s still suffers from sensitivities (e.g",0
8555,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",0
8556,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",Figure 9,0
8557,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8558,(16)) in θ,"In classiﬁcation settings, we have Y ∈{1, ",0
8559,(16)) in θ,"Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)",0
8560,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8561,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","These works provide an initial foundation for adversarial training, but itis challenging to rigorously identify the classes of attacks against which they can defend (or if theyexist)",0
8562,"9–10)), we have","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8563,"9–10)), we have","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8564,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8565,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",Our methodbecomes another heuristic for protection against attacks with large adversarial budgets,0
8566,"9–10)), we have","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8567,(16)) in θ,Dziugaite and D,0
8568,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","Another consequence of Theorem 4 is that (cid:98)ρn(θWRM) in the
certiﬁcate (12) is positive as long as the loss (cid:96) is not completely invariant to data",0
8569,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8570,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8571,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8572,"9–10)), we have","Let f, c be such that for any γ ≥ 0, the function g(x, z) = γc(x, z)− f (x) is a normalintegrand",0
8573,"9–10)), we have","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8574,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
8575,"9–10)), we have",with the convention that 0 · ∞ = 0,0
8576,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","We note thattudes of ∇z(cid:96)(θ; z) in a neighborhood of the nominal input, which ensures stability of the model.",0
8577,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","arXiv:1505.05116 [math.OC],2015.",0
8578,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8579,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",by solving (7),0
8580,(16)) in θ,K,0
8581,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8582,(16)) in θ,"We now consider a standard benchmark—training a neural network classiﬁer on the MNIST dataset.The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional ﬁlter layers with ELU activationsfollowed by a fully connected layer and softmax output",0
8583,"9–10)), we have","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8584,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM",0
8585,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8586,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",This result is essentially due to Katz et al.(2017a),0
8587,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq","We test both models with perturbationsto the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and thestrength of gravity g by 5",0
8588,"9–10)), we have","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8589,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",G,0
8590,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8591,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8592,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,0
8593,"9–10)), we have","This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al",0
8594,"9–10)), we have","Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",0
8595,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8596,"9–10)), we have","which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8597,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",The vertical bar indicates the achieved level,0
8598,(16)) in θ,"which is unique and well-deﬁned under our strong concavity assumption that γ > Lzz, and smooth(recall Eq",1
8599,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs","The vertical bar in (a), (c), and (e) indicates the estimated",0
8600,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes","WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ",0
8601,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes",Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,0
8602,"In this section, we give an adapation ofthe results in Sections 2 and 3 (Theorems 2 and 4) to such scenarios","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8603,"In this section, we give an adapation ofthe results in Sections 2 and 3 (Theorems 2 and 4) to such scenarios","When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",0
8604,", K}","Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",0
8605,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8606,", K}","By suitably modifying the cost function c(z, z(cid:48))to take value ∞ outside this small region, our general formulation covers such variants.",0
8607,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
8608,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8609,"In this section, we give an adapation ofthe results in Sections 2 and 3 (Theorems 2 and 4) to such scenarios",We illustrate test misclassiﬁcationerror vs,0
8610,", K}","AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",0
8611,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8612,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8613,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","Letting Z = (X, Y ) where X denotes the feature vector (covariates) andY the label, this is equivalent to deﬁning the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by(8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
8614,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X",00.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-110000.050.10.150.20.2510-210-110000.050.10.150.210-210-1100Published as a conference paper at ICLR 2018,0
8615,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label",We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,0
8616,"In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8617,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8618,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label",Empirical comparison between certiﬁcate of robustness (11) (blue) and out-of-sample (test)worst-case performance (red) for the experiments on MNIST with a larger training adversary,0
8619,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8620,"In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",The function cx : X × X → R+ is continuous,0
8621,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8622,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes","Let the transportation costc : Z × Z → [0,∞) be nonnegative, lower semi-continuous, and satisfy c(z, z) = 0",0
8623,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8624,"In this section, we give an adapation ofthe results in Sections 2 and 3 (Theorems 2 and 4) to such scenarios","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8625,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X","These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:c((x, y), (x0, y0)) = (cid:107)x − x0(cid:107)2∞ + ∞ · 1{y (cid:54)= y0} .",0
8626,"In this section, we give an adapation ofthe results in Sections 2 and 3 (Theorems 2 and 4) to such scenarios","We consider Markov decision processes (MDP’s)(S,A, Psa, r) with state space S, action space A, state-action transition probabilities Psa, and re-wards r : S → R",0
8627,", K}","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",0
8628,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8629,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8630,"In classiﬁcation settings, we have Y ∈{1, ","For both activations, WRMpushes the classiﬁcation boundaries further outwards than ERM or FGM",0
8631,"In classiﬁcation settings, we have Y ∈{1, ","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8632,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8633,", K}","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8634,"In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8635,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes","Let φγ(θ; z0) =supz∈Z {(cid:96)(θ; z) − γc(z, z0)} be the robust surrogate (2b)",0
8636,"In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8637,"In classiﬁcation settings, we have Y ∈{1, ","In this regime(small γ, large ), performance between WRM and other methods diverge",0
8638,"In classiﬁcation settings, we have Y ∈{1, ","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8639,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes","Graves, M",0
8640,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In this work, we treat a much larger class of losses and costs and provide directsolution methods for a Lagrangian relaxation of the saddle-point problem (1)",0
8641,"In classiﬁcation settings, we have Y ∈{1, ",Let ∆F ≥ F (θ0) − inf θ F (θ),0
8642,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8643,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8644,"In this section, we give an adapation ofthe results in Sections 2 and 3 (Theorems 2 and 4) to such scenarios",Since 70% of the data are of the,0
8645,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8646,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8647,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8648,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8649,"In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)","Substituting the this into the progress guarantee (27), we haveF (θt+1) ≤ F (θt) − αt",0
8650,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label",We illustrate test misclassiﬁcation error vs,0
8651,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8652,"In classiﬁcation settings, we have Y ∈{1, ","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8653,"In classiﬁcation settings, we have Y ∈{1, ","C.2 PROOF OF LEMMA 1First, note that z(cid:63)(θ) is unique and well-deﬁned by the strong convexity of f (θ,·)",0
8654,", K}","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8655,", K}","We note thattudes of ∇z(cid:96)(θ; z) in a neighborhood of the nominal input, which ensures stability of the model.",0
8656,", K}","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8657,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8658,"In classiﬁcation settings, we have Y ∈{1, ","Then for any c : Z × Z →R+ ∪ {∞} 1-strongly convex in its ﬁrst argument, a Taylor expansion yields(cid:96)(θ; z(cid:48)) − γc(z(cid:48), z0) ≤ (cid:96)(θ; z) − γc(z, z0) + (cid:104)∇z((cid:96)(θ; z) − γc(z, z0)), z(cid:48) − z(cid:105) +(cid:107)z − z(cid:48)(cid:107)22 .(4)For γ ≥ L this is the ﬁrst-order condition for (γ − L)-strong concavity of z (cid:55)→ ((cid:96)(θ; z)− γc(z, z0)).Thus, whenever the loss is smooth enough in z and the penalty γ is large enough (corresponding toless robustness), computing the surrogate (2b) is a strongly-concave optimization problem.We leverage the insight (4) to show that as long as we do not require too much robustness, thisstrong concavity approach (4) provides a computationally efﬁcient and principled approach for ro-bust optimization problems (1)",0
8659,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8660,"In this section, we give an adapation ofthe results in Sections 2 and 3 (Theorems 2 and 4) to such scenarios","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8661,"In classiﬁcation settings, we have Y ∈{1, ",We show that computing worst-case perturbations supu∈U (cid:96)(θ; z + u) is NP-hard for a large classof feedforward neural networks with ReLU activations,0
8662,"In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8663,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes",We omit the certiﬁcate’s error,0
8664,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8665,"In this section, we give an adapation ofthe results in Sections 2 and 3 (Theorems 2 and 4) to such scenarios","If f is inf-compact, then ¯f is directionally differentiable with",0
8666,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes","Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",0
8667,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","Rusu, J",0
8668,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8669,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8670,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8671,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X",We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,0
8672,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8673,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8674,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8675,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8676,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label",The distinctions in performance between various methodsare less apparent now,0
8677,"In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8678,"In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)","Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",0
8679,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8680,"In this section, we give an adapation ofthe results in Sections 2 and 3 (Theorems 2 and 4) to such scenarios",N,0
8681,"In classiﬁcation settings, we have Y ∈{1, ","For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",0
8682,"In classiﬁcation settings, we have Y ∈{1, ","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8683,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes","We recall that if g is continuous, then g is a normal inte-grand (Rockafellar & Wets, 1998, Cor",0
8684,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X","Making point (i) precise, Lemma 1 shows that if γ is large enough andAssumption B holds, the surrogate φγ is still smooth",0
8685,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8686,", K}","For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",0
8687,", K}","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8688,"In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)","Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)",0
8689,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8690,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8691,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8692,"In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8693,"In classiﬁcation settings, we have Y ∈{1, ",The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions,0
8694,"In classiﬁcation settings, we have Y ∈{1, ","Jha, M",0
8695,"In this section, we give an adapation ofthe results in Sections 2 and 3 (Theorems 2 and 4) to such scenarios","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8696,"As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8697,"We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X",We typically solve the penalty problem (2) with P0 replaced,0
8698,", K}","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8699,"Let Z = (X, Y ) ∈ X × Rwhere X ∈ X is a feature vector5 and Y ∈ R is a label","In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",1
8700,"Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B","The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,defense against imperceptible adversarial perturbations—are achievable at essentially no computa-tional or statistical cost for smooth losses (cid:96)",0
8701,"The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8702,"Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8703,The function cx : X × X → R+ is continuous,"Szepesv´ari & Littman (1999)) apply under these adversarial dynamics.We test our adversarial training procedure in the cart-pole environment, where the goal is to balancea pole on a cart by moving the cart left or right",0
8704,"Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8705,"For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",0
8706,"Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))",Esfahani and D,0
8707,"Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8708,"Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8709,"For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,0
8710,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C","Our subsequent results hold uniformly over the space ofparameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-tion 2.1",0
8711,"Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8712,"Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))","Jha, and A",0
8713,"Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8714,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8715,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",We illustrate testmisclassiﬁcation error vs,0
8716,"The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds","The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3",0
8717,"The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8718,"Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",0
8719,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8720,The function cx : X × X → R+ is continuous,All mod-els are trained in the ∞-norm,0
8721,"The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds","Veness, M",0
8722,"Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8723,"Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8724,"Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8725,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8726,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8727,"The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8728,"The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8729,"Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)",N,0
8730,"The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8731,"Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ",582–597,0
8732,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8733,"Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)","Nature, 518(7540):529–533, 2015.",0
8734,"For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D","Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",0
8735,"For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D","Silver, A",0
8736,"Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))","for t = 1, ",0
8737,The function cx : X × X → R+ is continuous,Other models do not exhibit this behavior with the same consistency(if at all),0
8738,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C","The original label is 8, whereas on the adversarial examplesIFGM predicts 2, PGM predicts 0, and the other models predict 3",0
8739,"For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,0
8740,"The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds","Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",0
8741,"Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ","We note thattudes of ∇z(cid:96)(θ; z) in a neighborhood of the nominal input, which ensures stability of the model.",0
8742,"Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ","Then there exists θ such that
this optimization problem is also NP-hard.",0
8743,"Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ","Weuse Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate thebeneﬁts of certiﬁed robustness",0
8744,The function cx : X × X → R+ is continuous,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8745,"Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ",Figure 1,0
8746,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8747,"The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8748,"Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8749,"Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8750,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,"In supervised learning settings, it is often natural—for example, in classiﬁcation—to only consideradversarial perturbations to the feature vectors (covariates)",0
8751,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8752,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8753,"Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8754,"Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8755,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8756,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,"For large γ, we can again solve problem (22) efﬁciently using gradient descent",0
8757,"Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8758,"Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)",(we initialize our inner gradient ascent iterations with the sampled natural example zt).Convergence properties of Algorithm 1 depend on the loss (cid:96),0
8759,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8760,"Then ¯f is differentiable, andletting x(cid:63)(θ) = argmaxx∈X f (θ, x), we have ∇ ¯f (θ) = gθ(θ, x(cid:63)(θ))","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8761,"Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)","Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ",0
8762,"Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",0
8763,The function cx : X × X → R+ is continuous,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8764,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,"Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we abuse notation by using the same norm (cid:107)·(cid:107) on Θand Z, though the speciﬁc norm is clear from context.Assumption B",0
8765,"Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B",The below general duality result gives Proposition 1 as an immediate specialcase,0
8766,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8767,"Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ","For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",0
8768,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,"We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",0
8769,"Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8770,"Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8771,"Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8772,The function cx : X × X → R+ is continuous,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8773,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8774,"Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8775,"Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8776,"The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8777,"Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B",(e) Test error vs,0
8778,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C","We note thattudes of ∇z(cid:96)(θ; z) in a neighborhood of the nominal input, which ensures stability of the model.",0
8779,"For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8780,"Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B","In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp",0
8781,"The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds",Thestatistical error term n(t) is omitted from the certiﬁcate,0
8782,The function cx : X × X → R+ is continuous,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8783,The function cx : X × X → R+ is continuous,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8784,"Moreover,(cid:107)x(cid:63)(θ1) − x(cid:63)(θ2)(cid:107) ≤ Lxθλ","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8785,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,(e) Test error vs,0
8786,"The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds","In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",0
8787,"The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds","Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.",0
8788,"Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8789,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8790,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8791,"Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8792,"Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8793,The function cx : X × X → R+ is continuous,"Furthermore,our statistical guarantees allow us to efﬁciently certify robustness for the popu-lation loss",0
8794,The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,"Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B",0
8795,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C","In this work, we treat a much larger class of losses and costs and provide directsolution methods for a Lagrangian relaxation of the saddle-point problem (1)",0
8796,"Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",1
8797,"Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to thenorm (cid:107)·(cid:107), and deﬁne ¯f (θ) = supx∈X f (θ, x)","arXiv:1505.05116 [math.OC],2015.",0
8798,"For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",Since 70% of the data are of the,0
8799,"Let gθ(θ, x) = ∇θf (θ, x) and gx(θ, x) = ∇xf (θ, x),and assume gθ and gx satisfy the Lipschitz conditions of Assumption B","Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks",0
8800,The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),"WRM with ELU’s provides a certiﬁed level of robustness, yielding anaxisymmetric classiﬁcation boundary that hedges against adversarial perturbations in all directions.Recall that our certiﬁcates of robustness on the worst-case performance given in Theorem 3 appliesfor any level of robustness ρ",0
8801,Let ∆F ≥ F (θ0) − inf θ F (θ),"For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",0
8802,"From Lemma 5, our previous results (Theorems 2 and 4) follow","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8803,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8804,"From Lemma 5, our previous results (Theorems 2 and 4) follow","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8805,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8806,The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8807,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8808,Let ∆F ≥ F (θ0) − inf θ F (θ),adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
8809,"From Lemma 5, our previous results (Theorems 2 and 4) follow","The loss (cid:96) : Θ × Z → R satisﬁes the Lipschitzian smoothness conditions(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lθθ (cid:107)θ − θ(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lxx (cid:107)x − x(cid:48)(cid:107) ,(cid:107)∇θ(cid:96)(θ; (x, y)) − ∇θ(cid:96)(θ; (x(cid:48), y))(cid:107)∗ ≤ Lθx (cid:107)x − x(cid:48)(cid:107) , (cid:107)∇x(cid:96)(θ; (x, y)) − ∇x(cid:96)(θ(cid:48); (x, y))(cid:107)∗ ≤ Lxθ (cid:107)θ − θ(cid:48)(cid:107) .Under Assumptions C and D, an analogue of Lemma 1 still holds",0
8810,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",0
8811,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8812,"From Lemma 5, our previous results (Theorems 2 and 4) follow","4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESSiid∼ N(02, I2) withFor our ﬁrst experiment, we generate synthetic data Z = (X, Y ) ∼ P0 by Xilabels Yi = sign((cid:107)x(cid:107)2 − √2), where X ∈ R2 and I2 is the identity matrix in R2",0
8813,"From Lemma 5, our previous results (Theorems 2 and 4) follow","Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,2017; Tram`er et al., 2017)",0
8814,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes",G,0
8815,The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8816,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","(2017), Dziugaite & Roy (2017), andNeyshabur et al",0
8817,Let ∆F ≥ F (θ0) − inf θ F (θ),"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8818,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8819,"From Lemma 5, our previous results (Theorems 2 and 4) follow","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8820,"From Lemma 5, our previous results (Theorems 2 and 4) follow","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8821,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8822,"From Lemma 5, our previous results (Theorems 2 and 4) follow","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8823,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","We compare standard WRM with ∞-norm PGM, FGM,IFGM",0
8824,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,Attacks on the MNIST dataset,0
8825,"From Lemma 5, our previous results (Theorems 2 and 4) follow",(2017) attempt to mitigate this shortcoming,0
8826,"From Lemma 5, our previous results (Theorems 2 and 4) follow","In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",0
8827,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8828,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8829,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8830,Let ∆F ≥ F (θ0) − inf θ F (θ),"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8831,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8832,The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8833,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","Recently, researchers have proposed convex relaxations of the NP-hard veriﬁcationproblem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may bedifﬁcult to scale to large networks",0
8834,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8835,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8836,Let ∆F ≥ F (θ0) − inf θ F (θ),"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8837,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8838,"From Lemma 5, our previous results (Theorems 2 and 4) follow","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",0
8839,Let ∆F ≥ F (θ0) − inf θ F (θ),"IEEE, 2016b.",0
8840,Let ∆F ≥ F (θ0) − inf θ F (θ),"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8841,"From Lemma 5, our previous results (Theorems 2 and 4) follow","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8842,Let ∆F ≥ F (θ0) − inf θ F (θ),"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8843,"From Lemma 5, our previous results (Theorems 2 and 4) follow",G,0
8844,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)",0
8845,"From Lemma 5, our previous results (Theorems 2 and 4) follow","Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",0
8846,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8847,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8848,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8849,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8850,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8851,The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),"If f is inf-compact, then ¯f is directionally differentiable with",0
8852,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8853,"From Lemma 5, our previous results (Theorems 2 and 4) follow",Experimental results on synthetic data,0
8854,"From Lemma 5, our previous results (Theorems 2 and 4) follow","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8855,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8856,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",0
8857,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8858,Let ∆F ≥ F (θ0) − inf θ F (θ),"for t = 1, ",0
8859,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","small values of (cid:98)ρtest(θ), the distance to adversarial examples (20), correspond to small magni-Figure 4(a) shows that(cid:98)ρtest differs by orders of magnitude between the training methods (models",0
8860,Let ∆F ≥ F (θ0) − inf θ F (θ),"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8861,"From Lemma 5, our previous results (Theorems 2 and 4) follow","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8862,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8863,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,0
8864,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8865,"From Lemma 5, our previous results (Theorems 2 and 4) follow","On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",0
8866,Let ∆F ≥ F (θ0) − inf θ F (θ),"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8867,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8868,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","By Pro-horov’s theorem (Billingsley, 1999, Ch 1.1, p",0
8869,"From Lemma 5, our previous results (Theorems 2 and 4) follow","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8870,The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8871,Let ∆F ≥ F (θ0) − inf θ F (θ),"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8872,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"Thet λtEP [r(st)], analogous to problem (1).goal is maximizing the worst-case realization inf P∈PsaIn a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations",0
8873,"From Lemma 5, our previous results (Theorems 2 and 4) follow",For large adversaries(i.e,0
8874,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes",with the convention that 0 · ∞ = 0,0
8875,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"In Section 3, we provide a certiﬁcate of robustness for any ρ; we give an efﬁcientlyEP [(cid:96)(θ; Z)]",0
8876,"From Lemma 5, our previous results (Theorems 2 and 4) follow","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8877,The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),"Whenever γ is large enough (so that this is tight for small ρ), we may efﬁcientlycompute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-rameter θ to a particular sample and predicted labeling based on the sample.",0
8878,Let ∆F ≥ F (θ0) − inf θ F (θ),"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8879,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8880,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8881,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-ness",0
8882,The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8883,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8884,Let ∆F ≥ F (θ0) − inf θ F (θ),"For any distribution Q and any ρ > 0,(5)",0
8885,The following is an analogue ofTheorem 2 for the cost function (8).Theorem 6 (Convergence of Nonconvex SGD),"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
8886,"From Lemma 5, our previous results (Theorems 2 and 4) follow","Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models",0
8887,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8888,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8889,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8890,Let ∆F ≥ F (θ0) − inf θ F (θ),"Wecompare standard WRM with ∞-norm PGM, FGM, IFGM models",0
8891,Let ∆F ≥ F (θ0) − inf θ F (θ),"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8892,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","For large γ, we can again solve problem (22) efﬁciently using gradient descent",0
8893,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8894,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8895,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wassersteinform of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0",0
8896,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"Due to the nonstationary, policy-dependent radius for the Wassersteinball, an analogous  for the fast-gradient method (or other variants) is not well-deﬁned",0
8897,"Assume E[(cid:107)∇F (θ) − ∇θφγ(θ, Z)(cid:107)2take constant stepsizes α =, Algorithm 1satisﬁes","From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8898,Let ∆F ≥ F (θ0) − inf θ F (θ),"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8899,Let Assumptions C and D hold with the (cid:96)2-norm2] ≤ σ2 andand let Θ = Rd,"From Lemma 5, our previous results (Theorems 2 and 4) follow",1
8900,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ","From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",0
8901,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)","To make this rigorous, ﬁx γ > 0, and consider the worst-case perturbation, typicallycalled the transportation map or Monge map (Villani, 2009),0
where b1, b2 are numerical constants.
We are now ready to state the main result of this section",0
8902,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",Theorem 7,1
8903,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ","As deep networks be-come prevalent in modern performance-critical systems (perception for self-driving cars, automateddetection of tumors), model failure is increasingly costly; in these situations, it is irresponsible todeploy models whose robustness and failure modes we do not understand or cannot certify.Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-ceptible perturbations to data can lead to misbehavior of the model, such as misclassiﬁcation of theoutput (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,2016)",0
8904,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",Theorem 7,1
8905,Theorem 7,"The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.",0
8906,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",Theorem 7,1
8907,Theorem 7,"AsProoftaking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (inﬁnitedimensional) duality results (Luenberger, 1969, Thm",0
8908,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",Theorem 7,1
8909,Theorem 7,Theorem 7,1
8910,Theorem 7,Theorem 7,1
8911,Theorem 7,All of our resultssuitably generalize to this setting with minor modiﬁcations to the robust surrogate (2b) and theabove assumptions (see Section D),0
8912,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",Other models do not exhibit this behavior with the same consistency(if at all),0
8913,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ","(For example, continuity of f and closed convexity of c is sufﬁcient.) For any ρ > 0 wehave",0
8914,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ","In Security and Privacy (SP), 2016 IEEE Symposiumon, pp",0
8915,Theorem 7,Theorem 7,1
8916,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ","Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",0
8917,Theorem 7,Theorem 7,1
8918,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",Theorem 7,1
8919,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ","In most scenarios, however,it is reasonable to defend against weaker adversaries that instead perturb inﬂuential features more.We consider this setting and train against (cid:107)·(cid:107)2-norm attacks",0
8920,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",Theorem 7,1
8921,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by","McDaniel, S",0
8922,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",0
8923,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",Theorem 7,1
8924,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",Theorem 7,1
8925,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",Theorem 7,1
8926,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",Theorem 7,1
8927,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)","Left column: Euclidean-norm attacks, right column: ∞-norm attacks",0
8928,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",Theorem 7,1
8929,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",Theorem 7,1
8930,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",Theorem 7,1
8931,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ","The vertical bar in (a), (c), and (e) indicates the estimated",0
8932,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by","Similarly, our distributionally robust framework (2) is generalenough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z",0
8933,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",Theorem 7,1
8934,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",Theorem 7,1
8935,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",is closed-valued and measurable,0
8936,Theorem 7,Theorem 7,1
8937,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ","When (cid:96) is convex in θ and γ is largeenough that z (cid:55)→ ((cid:96)(θ; z) − γc(z, z0)) is concave for all (θ, z0) ∈ Θ × Z, we have a stochasticmonotone variational inequality, which is efﬁciently solvable (Juditsky et al., 2011; Chen et al.,2014) with convergence rate 1/T ",0
8938,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)","In contrast, our approachis to use the distributionally robust approach to both defend against imperceptible adversarial per-turbations and develop efﬁcient optimization procedures.",0
8939,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",Theorem 7,1
8940,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",Theorem 7,1
8941,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",Let ∆F ≥ F (θ0) − inf θ F (θ),0
8942,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)","where (cid:98)Ptest is the test distribution, c(z, z(cid:48)) := (cid:107)x − x(cid:48)(cid:107)2(20)2 + ∞ · 1{y (cid:54)= y(cid:48)} as before, andTγadv (θ, Z) = argmaxz{(cid:96)(θ; z) − γadvc(z, Z)} is the adversarial perturbation of Z (Monge map)for the model θ",0
8943,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",Recallthat F (θ) = EP0 [φγ(θ; Z)] is the robust surrogate objective for the Lagrangian relaxation (2).Theorem 2 (Convergence of Nonconvex SGD),0
8944,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",Theorem 7,1
8945,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by","By usinga variant of the envelope (or Danskin’s) theorem, we ﬁrst show directional differentiability of ¯f.Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θsuch that",0
8946,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",0
8947,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",Theorem 7,1
8948,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",We see that our certiﬁcate provides aperformance guarantee for out-of-sample worst-case performance.We now compare adversarial training techniques,0
8949,Theorem 7,"One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",0
8950,Theorem 7,"Left column:Euclidean-norm attacks, right column: ∞-norm attacks",0
8951,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ","This problem currently persists for most learning-theoretic guaran-tees in deep learning, and the recent works of Bartlett et al",0
8952,Theorem 7,Theorem 7,1
8953,Theorem 7,Theorem 7,1
8954,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)","We ﬁrst show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
Theorem 3",0
8955,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",Theorem 7,1
8956,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",Theorem 7,1
8957,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",Theorem 7,1
8958,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",Theorem 7,1
8959,Theorem 7,Human-level control through deep reinforcementlearning,0
8960,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",Theorem 7,1
8961,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)","The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.",0
8962,Theorem 7,"Using only assumptions about the smoothness of the loss function(cid:96), we prove that our method enjoys strong statistical guarantees and fast optimization rates for alarge class of problems",0
8963,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)","Similarly as in Section 2.1, we require the following two assumptions that guarantee efﬁcient com-putability of the robust surrogate φγ.Assumption C",0
8964,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",Theorem 7,1
8965,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",Theorem 7,1
8966,Theorem 7,Empirical comparison between certiﬁcate of robustness (11) (blue) and test worst-case per-formance (red) for experiments with (a) synthetic data and (b) MNIST,0
8967,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",Theorem 7,1
8968,Theorem 7,Computing nonvacuous generalization bounds for deep (stochastic)neural networks with many more parameters than training data,0
8969,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",Theorem 7,1
8970,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",", K}",0
8971,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ","Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",0
8972,Theorem 7,"To see this, note
surely, and hence for large enough n, we have(cid:98)ρn(θ) > 0 by the bound (30).
from the optimality conditions for Tγ(θ; Z) that EP0[c(Tγ(θ; Z), Z)] = 0 iff ∇z(cid:96)(θ; z) = 0 almost",0
8973,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",Theorem 7,1
8974,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",Theorem 7,1
8975,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ","In Appendix A.5, we further compareWRM—which is trained to defend against (cid:107)·(cid:107)2-adversaries—with other heuristics trained to defendagainst (cid:107)·(cid:107)∞-adversaries",0
8976,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ","Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",0
8977,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",adv for (cid:107) · (cid:107)2-IFGM attack(d) Test error vs,0
8978,Theorem 7,2.2) gives the result.,0
8979,Theorem 7,"Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",0
8980,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)","The beneﬁtsof Lagrangian relaxation become clear here: for (cid:96)(θ; z) smooth in z and γ large enough, gradient",0
8981,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",Theorem 7,1
8982,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",Theorem 7,1
8983,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",Theorem 7,1
8984,Theorem 7,Theorem 7,1
8985,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",Theorem 7,1
8986,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)","In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s",0
8987,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",Theorem 7,1
8988,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)","Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)",0
8989,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ","For large γ, we can again solve problem (22) efﬁciently using gradient descent",0
8990,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by","Instead, we considerits Lagrangian relaxation for a ﬁxed penalty parameter γ ≥ 0, resulting in the reformulation",0
8991,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",Theorem 7,1
8992,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",Theorem 7,1
8993,Theorem 7,Theorem 7,1
8994,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",Theorem 7,1
8995,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",Theorem 7,1
8996,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) cx(·,·) is Lc-Lipschitz over X with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and x (cid:55)→ (cid:96)(θ, (x, y)) is γLc-Lipschitz for all θ ∈ Θ",Theorem 7,1
8997,"For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is deﬁned by",Theorem 7,1
8998,Theorem 7,Since 70% of the data are of the,0
8999,"If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)","When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",0
9000,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9001,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,0
9002,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Silver, A",0
9003,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Aman Sinha∗,1, Hongseok Namkoong∗,2, John Duchi1,3Departments of 1Electrical Engineering, 2Management Science & Engineering, 3StatisticsStanford UniversityStanford, CA 94305{amans,hnamk,jduchi}@stanford.edu",0
9004,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",Deﬁne the scale parameter βt > 0 by,0
9005,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",S.-M,0
9006,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
9007,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9008,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Fidjeland, G",0
9009,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9010,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9011,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9012,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9013,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",Frossard,0
9014,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9015,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Due to the nonstationary, policy-dependent radius for the Wassersteinball, an analogous  for the fast-gradient method (or other variants) is not well-deﬁned",0
9016,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9017,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9018,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",The bounds (11)–(13) may be too large—because of their dependence on covering numbers anddimension—for practical use in security-critical applications,0
9019,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",Training with the Euclidean cost still providesrobustness to ∞-norm fast gradient attacks,0
9020,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",The proof of the following resultis nearly identical to that of Lemma 1; we state the full result for completeness.Lemma 5,0
9021,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9022,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","However, WRM withReLU’s still suffers from sensitivities (e.g",0
9023,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9024,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9025,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",The,0
9026,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",(16)) in θ,0
9027,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9028,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9029,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9030,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",K,0
9031,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9032,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9033,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9034,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv",0
9035,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9036,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9037,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9038,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9039,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","By appropriatescaling of θ, v, and w, Katz et al",0
9040,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","In this regime(small γ, large ), performance between WRM and other methods diverge",0
9041,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9042,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9043,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9044,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9045,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9046,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",Figure 3,0
9047,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9048,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",Data-driven distributionally robust optimization using the Wassersteinmetric: Performance guarantees and tractable reformulations,0
9049,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9050,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
9051,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","(2015)) usually considers (cid:107)·(cid:107)∞-norm at-tacks, which allow imperceptible perturbations to all input features",0
9052,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9053,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9054,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",Let ∆F ≥ F (θ0) − inf θ F (θ),0
9055,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","IEEE, 2016b.",0
9056,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9057,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9058,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Papernot, P",0
9059,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Under Assumptions A, B, φγ(·; z0)then has L = Lθθ + LθzLzθ[γ−Lzz]+",0
9060,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9061,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,0
9062,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Robust optimization and adversarial training The standard robust-optimization approach mini-mizes losses of the form supu∈U (cid:96)(θ; z + u) for some uncertainty set U (Ratliff et al., 2006; Ben-Talet al., 2009; Xu et al., 2009)",0
9063,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","In this regime(small γ, large ), performance between WRM and other methods diverge",0
9064,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9065,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9066,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.",0
9067,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9068,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9069,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Fix z0 ∈ Z and focus on the (cid:96)2-norm case where c(z, z0) satisﬁesAssumption A with (cid:107)·(cid:107)2",0
9070,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9071,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9072,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","In the former case, since the update (22)simply modiﬁes the state-action transitions (independent of Q), standard results on convergence fortabular Q-learning (e.g",0
9073,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9074,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",adv for (cid:107) · (cid:107)∞-IFGM attack(e) Test error vs,0
9075,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,0
9076,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",Let ∆F ≥ F (θ0) − inf θ F (θ),0
9077,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s",0
9078,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, we have",0
9079,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9080,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","The system’s dynamics are such that the heavy, short, and strong-gravitycases are more unstable than the original environment, whereas their counterparts are less unstable.",0
9081,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9082,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9083,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","If f is inf-compact, then ¯f is directionally differentiable with",0
9084,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",", K}",0
9085,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",Further attacks on the MNIST dataset,0
9086,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9087,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","(2017) observe that theseprocedures attempt to optimize the objective EP0[sup(cid:107)u(cid:107)p≤ (cid:96)(θ; Z + u)], a constrained version ofthe penalty problem (2)",0
9088,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9089,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
9090,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Standard adversarial training methods often train to defendagainst (cid:107)·(cid:107)∞-norm attacks, which we compare our method against in this subsection",0
9091,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Of course, the certiﬁcate (15) stillholds regardless.More broadly, this work focuses on small-perturbation attacks, and our theoretical guarantees showthat it is possible to efﬁciently build models that provably guard against such attacks",0
9092,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9093,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s",0
9094,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",0
9095,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9096,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",1
9097,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","Replacing our current covering num-ber arguments with more intricate notions such as margin-based bounds (Bartlett et al., 2017) wouldextend the scope and usefulness of our theoretical guarantees",0
9098,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞",JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.,0
9099,"Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) forthe smooth function −f (z), take a gradient descent step at the current iterate zt (zt+ 12 below) and∞, take a proximal step for the function λtα∞","The NP-hardness of certifying robustness for ReLU networks, coupled withour empirical success and theoretical certiﬁcates for smooth networks in deep learning, suggest thatusing smooth networks may be preferable if we wish to guarantee robustness",0
9100,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9101,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","Forour approach we use γ = 2, and to make fair comparisons with FGM we use",0
9102,Deﬁne the scale parameter βt > 0 by,"We be-gin with assumptions we require, which roughly quantify the amount of robustness we can provide.Assumption A",0
9103,Deﬁne the scale parameter βt > 0 by,"WRM, which provablydefends against small perturbations, outperforms other heuristics against imperceptible attacks forboth Euclidean and ∞ norms",0
9104,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9105,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","Let gθ(θ, z) = ∇θf (θ, z) and gz(θ, z) = ∇zf (θ, z), andassume gθ and gz satisfy Assumption B with (cid:96)(θ; z) replaced with f (θ, z)",0
9106,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9107,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9108,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9109,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9110,Deﬁne the scale parameter βt > 0 by,"14.34); therefore, g(x, z) = γc(x, z) − (cid:96)(θ; x) is a normalintegrand",0
9111,Deﬁne the scale parameter βt > 0 by,"Then, for a ﬁxed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 − e−t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
(11)",0
9112,Deﬁne the scale parameter βt > 0 by,"for t = 1, ",0
9113,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9114,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9115,Deﬁne the scale parameter βt > 0 by,"As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",0
9116,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","Top row: PGM attacks, middle row:FGM attacks, bottom row: IFGM attacks",0
9117,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","Both models perform similarly over easier environments, but the robust model greatly out-performs in harder environments",0
9118,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","Forexample, it may be appropriate in certain applications to hedge against adversarial perturbations to asmall ﬁxed region of an image (Brown et al., 2017)",0
9119,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","Furthermore, to√create a wide margin separating the classes, we remove data with (cid:107)X(cid:107)2 ∈ (2)",0
9120,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9121,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data",0
9122,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",0
9123,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9124,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","As before, we assumethat cx is nonnegative, continuous, convex in its ﬁrst argument and satisﬁes cx(x, x) = 0.Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empiricalcounterpart (7) becomes",0
9125,Deﬁne the scale parameter βt > 0 by,"In Security and Privacy (EuroS&P), 2016 IEEE EuropeanSymposium on, pp",0
9126,Deﬁne the scale parameter βt > 0 by,"(2017), Dziugaite & Roy (2017), andNeyshabur et al",0
9127,Deﬁne the scale parameter βt > 0 by,Deﬁne the scale parameter βt > 0 by,0
9128,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9129,Deﬁne the scale parameter βt > 0 by,Our technique for distributionally robust optimization with adversarial training extends beyond su-pervised learning,0
9130,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","Then ¯f is differentiable,and letting z(cid:63)(θ) = argmaxz∈Z f (θ, z), we have ∇ ¯f (θ) = gθ(θ, z(cid:63)(θ))",0
9131,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9132,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","Top row: FGM attacks, bottom row: IFGM attacks",0
9133,Deﬁne the scale parameter βt > 0 by,theadversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-spectively,0
9134,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9135,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","Silver, A",0
9136,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9137,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9138,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","for t = 1, ",0
9139,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","Speciﬁcally, for large enough penalty γ (by duality,small enough robustness ρ), the function z (cid:55)→ (cid:96)(θ; z) − γc(z, z0) in the robust surrogate (2b) isstrongly concave and hence easy to optimize if (cid:96)(θ, z) is smooth in z",0
9140,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9141,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9142,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9143,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9144,Deﬁne the scale parameter βt > 0 by,"Note that for F = {(cid:96)(θ;·) : θ ∈ Θ}, any (,(cid:107)·(cid:107))-covering {θ1, ",0
9145,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9146,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","Moosavi-Dezfooli, A",0
9147,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9148,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain",0
9149,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9150,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9151,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9152,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The smoothness of (cid:96) is essential so that a penalized version(cid:96)(θ, z) − γc(z, z0) is concave in z (which can be approximately veriﬁed by computing Hessians∇2zz(cid:96)(θ, z) for each training datapoint), allowing computation and our coming certiﬁcates of opti-mality",0
9153,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9154,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",radial asymmetry in the classiﬁcation surface) due to thelack of robustness guarantees,0
9155,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",0
9156,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","McDaniel, X",0
9157,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","Since the worst-(test) worst-case performance supP :Wc(P,P0)≤ρcase loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values ofγadv",0
9158,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",0
9159,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",See Section C.2 for the proof,0
9160,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9161,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9162,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8",We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,0
9163,Deﬁne the scale parameter βt > 0 by,8.7.1) to obtain,0
9164,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8",V,0
9165,Deﬁne the scale parameter βt > 0 by,"Empirical evaluationsindicate that our methods are in fact robust to perturbations in the data, and they match or outperformless-principled adversarial training techniques",0
9166,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","Throughout, we assume Θ ⊆ Rd.Lemma 1",0
9167,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8",allowing us to prevent attacks on the test set,0
9168,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9169,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9170,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8",Mnih et al,0
9171,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","When the loss (cid:96) is nonconvex in θ, the following theoremguarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz",0
9172,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9173,Deﬁne the scale parameter βt > 0 by,Further attacks on the MNIST dataset,0
9174,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8",Deepfool: a simple and accurate method to fooldeep neural networks,0
9175,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9176,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9177,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9178,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","Top row: FGM attacks, bottom row: IFGM attacks",0
9179,Deﬁne the scale parameter βt > 0 by,"Top row: PGM attacks, middle row: FGM attacks,bottom row: IFGM attacks",0
9180,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8",G,0
9181,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","If Assumptions C and D hold,then with probability at least 1 − e−t,term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms)",0
9182,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","This notion of robustness is typically intractable: the inner supremum isgenerally non-concave in u, so it is unclear whether model-ﬁtting with these techniques converges,and there are possibly worst-case perturbations these techniques do not ﬁnd",0
9183,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9184,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","In this section, we give an adapation ofthe results in Sections 2 and 3 (Theorems 2 and 4) to such scenarios",0
9185,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9186,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) usingconvex analysis",0
9187,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9188,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9189,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9190,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9191,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9192,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9193,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9194,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","By suitably modifying the cost function c(z, z(cid:48))to take value ∞ outside this small region, our general formulation covers such variants.",0
9195,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have",0
9196,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9197,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8",adv for (cid:107) · (cid:107)2-IFGM attackFigure 14,0
9198,Deﬁne the scale parameter βt > 0 by,"The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9199,"In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8","The following proposition shows that we can compute the proximal step zt+1 efﬁciently, simply bysorting the vector |zt+ 12 − z0| in decreasingorder",1
9200,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
9201,See Section E.1 for the proof of the proposition,"WRM’s “misclassiﬁcations”are consistently reasonable to the human eye, as gradient-based perturbations actually transform theoriginal image to other labels",0
9202,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9203,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9204,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,0
9205,See Section E.1 for the proof of the proposition,"Second, we show in Section 3.2 that adversarial perturbationson the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees asimilar level of robustness as directly solving its population counterpart (2).",0
9206,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9207,See Section E.1 for the proof of the proposition,"Thus, we see that our adversarial-training method defends against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint untilthe model misclassiﬁes it",0
9208,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1","Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",0
9209,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,0
9210,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9211,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9212,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1","(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",0
9213,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).","Since gθ is continuous by Assumption B and z(cid:63)(θ) is Lipschitz (26), we conclude that¯f is differentiable.Finally, we have(cid:107)gθ(θ1, z(cid:63)",0
9214,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).","Certainly (cid:107)·(cid:107)∞-attack and defensemodels have been fruitful in building a foundation for security research in deep learning, but movingbeyond them may be necessary for more advances in the large-perturbation regime.",0
9215,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9216,See Section E.1 for the proof of the proposition,"Theorem 2 shows that the stochastic gradient method achieves the rates of convergenceon the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &Lan, 2013)",0
9217,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9218,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9219,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",N,0
9220,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9221,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9222,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",We providea principled method for efﬁciently guaranteeing distributional robustness with a simple form ofadversarial data perturbation,0
9223,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1","2/1.3), distributional robustness favors pushing the classiﬁcation boundaryoutwards; intuitively, adversarial examples are most likely to come from pushing blue points out-wards across the boundary",0
9224,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9225,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9226,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9227,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9228,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9229,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9230,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9231,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9232,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1","PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",0
9233,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9234,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9235,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9236,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9237,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",adv for (cid:107)·(cid:107)∞-IFGM attackFigure 6,0
9238,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).","In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",0
9239,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).","We note thattudes of ∇z(cid:96)(θ; z) in a neighborhood of the nominal input, which ensures stability of the model.",0
9240,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9241,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1","2 PROPOSED APPROACHOur approach is based on the following simple insight: assume that the function z (cid:55)→ (cid:96)(θ; z) issmooth, meaning there is some L for which ∇z(cid:96)(θ;·) is L-Lipschitz",0
9242,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
9243,See Section E.1 for the proof of the proposition,"For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",0
9244,See Section E.1 for the proof of the proposition,"On attacks with large adversarial budgets(large adv), however, the performance of WRM is worse than that of the other methods (especiallyin the case of ∞-norm attacks)",0
9245,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9246,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9247,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9248,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9249,See Section E.1 for the proof of the proposition,"for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1Figure 1 illustrates the classiﬁcation boundaries for the three training procedures over the ReLU-activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models",0
9250,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).","Furthermore, ourstatistical guarantees (Theorems 3 and 4) use (cid:107)·(cid:107)∞-covering numbers as a measure of model com-plexity, which can become prohibitively large for deep networks",0
9251,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).","In particular, given a collection of test examples, we may interrogate possible losses under perturbations for the test examples by noting that,Z test",0
9252,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1","Wu, S",0
9253,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9254,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9255,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
9256,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",582–597,0
9257,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9258,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9259,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).","Alternative approaches that provide formal veriﬁcation of deep networks (Huang et al., 2017;Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even onsmall networks",0
9260,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1","Interestingly, as shown in Figure 5, the robust model also learnsmore efﬁciently than the nominal model in the original MDP",0
9261,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9262,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).","One form of adversarial training trains on the losses (cid:96)(θ; (xi + ∆xi(θ), yi)) (Goodfellow et al.,2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini& Wagner, 2017; Madry et al., 2017; Tram`er et al., 2017)",0
9263,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9264,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9265,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",Let (cid:96)(·; z) be L-Lipschitz with respect to some norm (cid:107)·(cid:107) for all z ∈ Z,0
9266,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9267,See Section E.1 for the proof of the proposition,HN was partially supported by aSamsung Fellowship,0
9268,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9269,See Section E.1 for the proof of the proposition,"(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be[φγ(θ; Z)] and therefore have the best guarantees.Most importantly, the certiﬁcate (14) is easy to compute via expression (10): as noted in Section 2.1,[c(T (θ, Z), Z)].",0
9270,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9271,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9272,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9273,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1","In Security and Privacy (SP), 2016 IEEE Symposiumon, pp",0
9274,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9275,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9276,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).","Deﬁne the conditionaldistribution P (· | z) to be supported on x(z), which is evidently measurable",0
9277,See Section E.1 for the proof of the proposition,"Then, we have",0
9278,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9279,See Section E.1 for the proof of the proposition,Our optimization result (Theorem 2) appliesonly for small values of robustness ρ and to a limited class of Wasserstein costs,0
9280,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1","Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞",0
9281,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9282,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9283,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9284,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9285,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",adv for (cid:107) · (cid:107)∞-IFGM attackFigure 13,0
9286,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).","These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
9287,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",See Section E.1 for the proof of the proposition,1
9288,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9289,See Section E.1 for the proof of the proposition,"By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data",0
9290,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9291,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9292,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",Let X denote the spaceof all measurable mappings z (cid:55)→ x(z) from Z to X,0
9293,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1","arXiv:1703.11008 [cs.LG], 2017.P",0
9294,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1",See Section E.1 for the proof of the proposition,1
9295,See Section E.1 for the proof of the proposition,See Section E.1 for the proof of the proposition,1
9296,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",Let Assumptions A and B hold with the (cid:96)2-normand let Θ = Rd,0
9297,"From the proposition, we obtain the proximalprocedure in Algorithm 2 that can be used to solve for the approximate maximizer of (cid:96)(θ; z) −γc(z, z0) in Algorithm 1","Moosavi-Dezfooli, A",0
9298,See Section E.1 for the proof of the proposition,"Let Z ⊂ {z ∈ Rm : (cid:107)z(cid:107) ≤ Mz} so that (cid:107)Z(cid:107) ≤ Mz almost surely and assume eitherthat (i) c(·,·) is Lc-Lipschitz over Z with respect to the norm (cid:107)·(cid:107) in each argument, or (ii) that(cid:96)(θ, z) ∈ [0, M(cid:96)] and z (cid:55)→ (cid:96)(θ, z) is γLc-Lipschitz for all θ ∈ Θ.If Assumptions A and B hold, then with probability at least 1 − e−t,that the bound (30) gives the usual(cid:112)d/n generalization rate for the distance between adversarial
perturbations and natural examples",0
9299,See Section E.1 for the proof of the proposition,In constrast to f-divergences (e.g,0
9300,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","In this proof, we drop the subscript on the iteration t to ease notation",1
9301,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",0
9302,We assume without loss of2 − z0 (cid:54)= 0,"In this proof, we drop the subscript on the iteration t to ease notation",1
9303,We assume without loss of2 − z0 (cid:54)= 0,"In this proof, we drop the subscript on the iteration t to ease notation",1
9304,We assume without loss of2 − z0 (cid:54)= 0,"In this proof, we drop the subscript on the iteration t to ease notation",1
9305,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9306,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9307,"In this proof, we drop the subscript on the iteration t to ease notation","In this proof, we drop the subscript on the iteration t to ease notation",1
9308,We assume without loss of2 − z0 (cid:54)= 0,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain",0
9309,We assume without loss of2 − z0 (cid:54)= 0,"Heuristically, ignoring the truncation term in the proximal update (34),we haveHere, we move towards the sign of zt + λt∇f (zt) − z0 modulated by the term βt, as opposed tojust the sign of ∇f (zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakinet al., 2016).",0
9310,"In this proof, we drop the subscript on the iteration t to ease notation","In this proof, we drop the subscript on the iteration t to ease notation",1
9311,We assume without loss of2 − z0 (cid:54)= 0,"In this proof, we drop the subscript on the iteration t to ease notation",1
9312,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","In this proof, we drop the subscript on the iteration t to ease notation",1
9313,"In this proof, we drop the subscript on the iteration t to ease notation",We omit the certiﬁcate’s error,0
9314,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","Recalling Rockafellar & Wets (1998, Def",0
9315,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9316,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9317,We assume without loss of2 − z0 (cid:54)= 0,"In this proof, we drop the subscript on the iteration t to ease notation",1
9318,We assume without loss of2 − z0 (cid:54)= 0,"Continuity properties ofWasserstein distances (Villani, 2009, Corollary 6.11) then imply thatk→∞ Wc(P 1/k, P0) = Wc(P ∗, P0).Combining (29) and the monotone convergence theorem, we obtain",0
9319,"In this proof, we drop the subscript on the iteration t to ease notation","In this proof, we drop the subscript on the iteration t to ease notation",1
9320,"In this proof, we drop the subscript on the iteration t to ease notation","Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",0
9321,We assume without loss of2 − z0 (cid:54)= 0,"In this proof, we drop the subscript on the iteration t to ease notation",1
9322,We assume without loss of2 − z0 (cid:54)= 0,"In this proof, we drop the subscript on the iteration t to ease notation",1
9323,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","In this proof, we drop the subscript on the iteration t to ease notation",1
9324,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","9–10)), we have",0
9325,We assume without loss of2 − z0 (cid:54)= 0,large desired robustness values) our approach becomes a heuristic just like the other approaches.,0
9326,We assume without loss of2 − z0 (cid:54)= 0,"In this proof, we drop the subscript on the iteration t to ease notation",1
9327,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","In this proof, we drop the subscript on the iteration t to ease notation",1
9328,We assume without loss of2 − z0 (cid:54)= 0,These properties guarantee both (i) the well-behavedness of the robust surrogate φγ and (ii) itsefﬁcient computability,0
9329,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9330,"In this proof, we drop the subscript on the iteration t to ease notation","In this proof, we drop the subscript on the iteration t to ease notation",1
9331,We assume without loss of2 − z0 (cid:54)= 0,"In this proof, we drop the subscript on the iteration t to ease notation",1
9332,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9333,We assume without loss of2 − z0 (cid:54)= 0,"Finally, we consider a reinforcement learning problemin Section 4.3, where the Markov decision process used for training differs from that for testing.WRM enjoys the theoretical guarantees of Sections 2 and 3 for large γ, but for small γ (large adver-sarial budgets), WRM becomes a heuristic like other methods",0
9334,We assume without loss of2 − z0 (cid:54)= 0,"Fredrikson, Z",0
9335,We assume without loss of2 − z0 (cid:54)= 0,We use reward r(β) := e−|β| for the angle β of the pole from the vertical,0
9336,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g",Our optimization result (Theorem 2) appliesonly for small values of robustness ρ and to a limited class of Wasserstein costs,0
9337,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","In this proof, we drop the subscript on the iteration t to ease notation",1
9338,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have",K,0
9339,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9340,"In this proof, we drop the subscript on the iteration t to ease notation","We train WRM with γ = 0.04E(cid:98)Pn[(cid:107)X(cid:107)2],and for the other methods we choose  as the level of robustness achieved by WRM (19).2 In theﬁgures, we scale the budgets 1/γadv and adv for the adversary with Cp := E(cid:98)Pn[(cid:107)X(cid:107)p].3",0
9341,We assume without loss of2 − z0 (cid:54)= 0,"Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to ﬁnd Lagrangian worst-case perturbations with low computational cost.",0
9342,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9343,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","Thus, the functional θ (cid:55)→ Fn(θ)satisﬁes bounded differences (Boucheron et al., 2013, Thm",0
9344,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9345,We assume without loss of2 − z0 (cid:54)= 0,Table 1 shows the performance of the trained models over the original MDP and all of the perturbedMDPs,0
9346,"In this proof, we drop the subscript on the iteration t to ease notation",PGM attacks on the MNIST dataset,0
9347,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9348,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7)",0
9349,"In this proof, we drop the subscript on the iteration t to ease notation","In this proof, we drop the subscript on the iteration t to ease notation",1
9350,"In this proof, we drop the subscript on the iteration t to ease notation","In this proof, we drop the subscript on the iteration t to ease notation",1
9351,We assume without loss of2 − z0 (cid:54)= 0,"In this proof, we drop the subscript on the iteration t to ease notation",1
9352,We assume without loss of2 − z0 (cid:54)= 0,"In this proof, we drop the subscript on the iteration t to ease notation",1
9353,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9354,We assume without loss of2 − z0 (cid:54)= 0,"In this proof, we drop the subscript on the iteration t to ease notation",1
9355,"In this proof, we drop the subscript on the iteration t to ease notation","In this proof, we drop the subscript on the iteration t to ease notation",1
9356,We assume without loss of2 − z0 (cid:54)= 0,We then show (Section 2.1)how stochastic gradient descent methods can efﬁciently ﬁnd minimizers (in the convex case) orapproximate stationary points (when (cid:96) is non-convex) for our relaxed robust problems.,0
9357,We assume without loss of2 − z0 (cid:54)= 0,"In this proof, we drop the subscript on the iteration t to ease notation",1
9358,We assume without loss of2 − z0 (cid:54)= 0,We hypothesize that a potential side-effect of robustness is that adversarial perturbations encourage better exploration of the environment.,0
9359,"In this proof, we drop the subscript on the iteration t to ease notation","In this proof, we drop the subscript on the iteration t to ease notation",1
9360,"In this proof, we drop the subscript on the iteration t to ease notation","In this proof, we drop the subscript on the iteration t to ease notation",1
9361,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have",Let ∆F ≥ F (θ0) − inf θ F (θ),0
9362,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","IEEE, 2016b.",0
9363,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","In Figure 2(a), we plot our certiﬁcate (11) against the out-of-sampleEP [(cid:96)(θ; Z)] for WRM with ELU’s",0
9364,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","In this proof, we drop the subscript on the iteration t to ease notation",1
9365,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-rem 2, making distributionally robust optimization tractable for deep learning.In supervised-learning scenarios, we are often interested in adversarial perturbations only to featurevectors (and not labels)",0
9366,We assume without loss of2 − z0 (cid:54)= 0,"In this proof, we drop the subscript on the iteration t to ease notation",1
9367,"In this proof, we drop the subscript on the iteration t to ease notation","In this proof, we drop the subscript on the iteration t to ease notation",1
9368,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9369,We assume without loss of2 − z0 (cid:54)= 0,"Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,2009, Chs",0
9370,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","In this proof, we drop the subscript on the iteration t to ease notation",1
9371,"In this proof, we drop the subscript on the iteration t to ease notation","We give an alternative proof in Appendix C.1 for convex, continuous cost functions.Proposition 1",0
9372,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
9373,"In this proof, we drop the subscript on the iteration t to ease notation","In this proof, we drop the subscript on the iteration t to ease notation",1
9374,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","Recalling the deﬁnition (2b) of φγ(θ; z0) = supz∈Z f (θ, z; z0), we deﬁne the potentially biasederrors δt = gt − ∇θφγ(θt; zt)",0
9375,"In this proof, we drop the subscript on the iteration t to ease notation","In this proof, we drop the subscript on the iteration t to ease notation",1
9376,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","For any θ, optimality of z(cid:63)(θ) implies thatgz(θ, z(cid:63)(θ))T (z−z(cid:63)(θ)) ≤ 0",0
9377,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have",Let (cid:96) : Θ × Z → R and c : Z × Z → R+ be continuous,0
9378,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have",(f) Test error vs,0
9379,"In this proof, we drop the subscript on the iteration t to ease notation",Weillustrate test misclassiﬁcation error vs,0
9380,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9381,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","Together, Figures 4(a) and (b) depict our method’s defensemechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitudeof gradients and improving their interpretability.",0
9382,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g",adv for (cid:107) · (cid:107)2-IFGM attack(f) Test error vs,0
9383,"In this proof, we drop the subscript on the iteration t to ease notation","Because 2-normadversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations arerelatively large",0
9384,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9385,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","For imperceptible perturbations, our method matches or outperformsheuristic approaches.",0
9386,We assume without loss of2 − z0 (cid:54)= 0,"Thatcomputable data-dependent upper bound on the worst-case loss supP :Wc(P,P0)≤ρis, the worst-case performance of the output of our principled adversarial training procedure is guarfor the empirical objective",0
9387,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","In this proof, we drop the subscript on the iteration t to ease notation",1
9388,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9389,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9390,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","Top row: PGM attacks,middle row: FGM attacks, bottom row: IFGM attacks",0
9391,"For some convex, lower semi-continuous function g : Rm → R, letgenerality that zt+ 1g∗(s) = sups{s(cid:62)t − g(t)} be the Fenchel conjuagte of g","In this proof, we drop the subscript on the iteration t to ease notation",1
9392,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have",from which it is easy to see that f is inf-compact,0
9393,We assume without loss of2 − z0 (cid:54)= 0,"These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
9394,We assume without loss of2 − z0 (cid:54)= 0,(e) Test error vs,0
9395,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","For deep networks and othercomplex models, this formulation of problem (1) is intractable with arbitrary ρ",0
9396,"From the Moreau decomposition (Parikh& Boyd, 2013, Section 2.5), we have","In this proof, we drop the subscript on the iteration t to ease notation",1
9397,"In this proof, we drop the subscript on the iteration t to ease notation",(2017) attempt to mitigate this shortcoming,0
9398,"In this proof, we drop the subscript on the iteration t to ease notation","In this proof, we drop the subscript on the iteration t to ease notation",1
9399,"In this proof, we drop the subscript on the iteration t to ease notation","In this proof, we drop the subscript on the iteration t to ease notation",1
9400,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9401,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9402,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9403,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9404,"Then, we have","For smooth losses,our procedure provably achieves moderate levels of robustness with little compu-tational or statistical cost relative to empirical risk minimization",0
9405,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)","If f is inf-compact, then ¯f is directionally differentiable with",0
9406,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)","For worst-case regions P formed by Wasserstein balls, Esfahani & Kuhn (2015),Shaﬁeezadeh-Abadeh et al",0
9407,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9408,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9409,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9410,"Then, we have",G,0
9411,"Then, we have",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9412,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",We postulatea class P of distributions around the data-generating distribution P0 and consider the problem,0
9413,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9414,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,"By considering a La-grangian penalty formulation of perturbing the underlying data distribution in aWasserstein ball, we provide a training procedure that augments model param-eter updates with worst-case perturbations of training data",0
9415,"Then, we have","Then there exists θ such that
this optimization problem is also NP-hard.",0
9416,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,"In the following, we use polynomial time to mean polynomial growth with respect to m,the dimension of the inputs z.An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution growspolynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e",0
9417,"Then, we have",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9418,"Then, we have","Fawzi, and P",0
9419,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)","These ﬁndings verify that WRM is a practical alternative overexisting heuristics for the moderate levels of robustness where our guarantees hold.A.5.2 COMPARISON WITH (cid:107)·(cid:107)∞-WRMOur computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-norm adversaries:(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
9420,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)","Namely, we use the squared Euclideancost for the feature vectors cx(x, x(cid:48)) := (cid:107)x − x(cid:48)(cid:107)22 and deﬁne the overall cost as the covariate-shiftadversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in allexperiments; we still test against adversarial perturbations with respect to the norms p = 2,∞",0
9421,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9422,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,"PGM augments stochastic gradient steps for the parameter θ with projected gradientascent over x (cid:55)→ (cid:96)(θ; x, y), iterating (for data point xi, yi)i, yi)T η} and xt+1",0
9423,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,"Noting that f (θ, z) := (cid:96)(θ, z) − γc(z, z0) is (γ − Lzz)-strongly concavefrom the insight (4) (with L := Lzz), let us apply Lemma 1",0
9424,"Then, we have",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9425,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",from which it is easy to see that f is inf-compact,0
9426,"Then, we have","In most scenarios, however,it is reasonable to defend against weaker adversaries that instead perturb inﬂuential features more.We consider this setting and train against (cid:107)·(cid:107)2-norm attacks",0
9427,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)","for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013)",0
9428,"Then, we have","In the proposition, we use the notation [·]+ = max(·, 0).j−1(cid:88)Proposition 8",0
9429,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9430,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9431,"Then, we have","The vertical bar in (a) and (c) indicates theperturbation level that was used for training the PGM, FGM, and IFGM models and the estimated",0
9432,"Then, we have",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9433,"Then, we have","Moosavi-Dezfooli, A",0
9434,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,"Let Z ⊂ Rm, and let (Z,A, P0) be a probability space",0
9435,"Then, we have",The function cx : X × X → R+ is continuous,0
9436,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9437,"Then, we have","Our main result in this section is a data-dependent upper bound for the worst-case population ob-n) for all θ ∈ Θ, with highjective: supP :Wc(P,P0)≤ρprobability",0
9438,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)","In Figure 11 we show the results trained with a small training adversarial budget.In this regime, (large γ, small ), WRM matches the performance of other techniques.In Figure 12 we show the results trained with a large training adversarial budget",0
9439,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9440,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",Attacks on the MNIST dataset with larger (training and test) adversarial budgets,0
9441,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9442,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)","Direct com-parison between these approaches is not immediate, as we need to determine a suitable  to train",0
9443,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)","In Appendix E, we propose aheuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).Our approach is based on a variant of proximal algorithms.We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-cedures that were trained against ∞-norm adversaries",0
9444,"Then, we have","θ = θERM, θFGM, θIFGM, θPGM, θWRM); the trend is nearly uniform over all γadv, with θWRMbeing the most stable",0
9445,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9446,"Then, we have",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9447,"Then, we have",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9448,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",The major beneﬁt of our approach is its simplicityand wide applicability across many models and machine-learning scenarios.There remain many avenues for future investigation,0
9449,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)","Our ﬁrst main result, presented in Section 3.1, gives a data-dependent upper bound on theEP [(cid:96)(θ; Z)] for any arbitrary level of robustnesspopulation worst-case objective supP :Wc(P,P0)≤ρ",0
9450,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9451,"Then, we have",the adversarial perturbationlevel adv,0
9452,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)","Forour approach we use γ = 2, and to make fair comparisons with FGM we use",0
9453,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)","Further, it outperforms other heuristics on natural images, showingthat it consistently achieves a smaller price of robustness",0
9454,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9455,"Then, we have","We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,2015), which can be easily represented in our robust formulation (2) by deﬁning the cost functionc : Z × Z → R+ ∪ {∞} as follows: for z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)), recall the covariate shift costfunction (8)where cx : X × X → R+ is the transportation cost for the feature vector X",0
9456,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,"(23)as the adversarial training budget for FGM, IFGM and PGM with (cid:107)·(cid:107)∞-norms",0
9457,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9458,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9459,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,"Distributionally robust optimization To situate the current work, we review some of the sub-stantial body of work on robustness and learning",0
9460,"Then, we have","Left column: Euclidean-normattacks, right column: ∞-norm attacks",0
9461,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,"By suitably modifying the cost function c(z, z(cid:48))to take value ∞ outside this small region, our general formulation covers such variants.",0
9462,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,(a) and (b) show test misclassiﬁcation error vs,0
9463,"Then, we have",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9464,"Then, we have",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9465,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9466,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9467,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9468,"Then, we have","As Z is compact, the collection {P 1/k}k∈N is a uniformly tight collection of measures",0
9469,"Then, we have",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9470,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9471,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9472,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)","WRM matches or outperforms other heuristics against imperceptible at-tacks, while it underperforms for attacks with large adversarial budgets.",0
9473,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9474,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",", K}",0
9475,"Then, we have",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9476,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9477,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",The action space is binary: push the cartleft or right with a ﬁxed force,0
9478,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9479,"Then, we have","57), (restricting to a subsequence if necessary), thereexists some distribution P ∗ on Z such that P 1/kd→ P ∗ as k → ∞",0
9480,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9481,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)","Let f : Θ×Z → R be differentiable and λ-strongly concave in z with respect to the norm(cid:107)·(cid:107), and deﬁne ¯f (θ) = supz∈Z f (θ, z)",0
9482,"Then, we have",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9483,"Then, we have",Esfahani and D,0
9484,"Then, we have","For each x0 ∈ X , cx(·, x0) is1-strongly convex with respect to the norm (cid:107)·(cid:107).Let (cid:107)·(cid:107)∗ be the dual norm to (cid:107)·(cid:107); we again abuse notation by using the same norm (cid:107)·(cid:107) on Θ and X ,though the speciﬁc norm is clear from context.Assumption D",0
9485,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9486,"Then, we have",We provide further evidence in Appendix A.1.Next we study stability of the loss surface with respect to perturbations to inputs,0
9487,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9488,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)","For our ﬁnal experiments, we consider distributional robustness in the context of Q-learning, amodel-free reinforcement learning technique",0
9489,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",In constrast to f-divergences (e.g,0
9490,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9491,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9492,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9493,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9494,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9495,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9496,"Then, we have","On a simple implementation in Tensorﬂow, our method takes 5–10× as longas stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for otheradversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).We show that our procedure—which learns to protect against adversarial perturbations in the trainingdataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.We brieﬂy overview our approach",0
9497,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9498,"Sincevi’s are decreasing and nonnegative, there exists j(cid:48) such that vj(cid:48) > β(cid:48) ≥ vj(cid:48)+1 (we abuse notationand let vm+1 := 0)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9499,"NotingProof of Lemmathat (cid:107)v(cid:107)1 > 0 and −αλ(cid:107)v(cid:107)∞ < 0, there exists β(cid:48) such that h(β(cid:48)) = 0 and β(cid:48) ∈ (0,(cid:107)v(cid:107)∞)",i:vi>β(vi − β) − αλβ =: h(β) is decreasing,1
9500,"Previously, the InformationBottleneck (IB) framework (Tishby et al",Yichuan Tang and Ruslan Salakhutdinov,0
9501,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"arXiv preprint arXiv:1406.1078, 2014.",0
9502,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9503,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9504,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance","As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets",0
9505,"Previously, the InformationBottleneck (IB) framework (Tishby et al","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
9506,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9507,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9508,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.",1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],0
9509,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9510,(1999)) extracts relevant information fora target variable,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9511,"Previously, the InformationBottleneck (IB) framework (Tishby et al","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9512,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.","Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
9513,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9514,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively","Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp",0
9515,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9516,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.",The MLE principle maximizes the likelihood function under theempirical data distribution,0
9517,"We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9518,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9519,"Previously, the InformationBottleneck (IB) framework (Tishby et al","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9520,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.","drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
9521,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9522,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.","(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning",0
9523,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9524,"We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level","Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",0
9525,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9526,"We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level","Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters",0
9527,"Previously, the InformationBottleneck (IB) framework (Tishby et al","drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
9528,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",0
9529,"We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level","PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",0
9530,(1999)) extracts relevant information fora target variable,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
9531,"Previously, the InformationBottleneck (IB) framework (Tishby et al","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9532,(1999)) extracts relevant information fora target variable,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9533,"We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9534,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance","The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ",0
9535,"Previously, the InformationBottleneck (IB) framework (Tishby et al","Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",0
9536,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9537,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9538,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.","The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",0
9539,"Previously, the InformationBottleneck (IB) framework (Tishby et al","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
9540,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance","For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
9541,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,Our PIB is also a standard steptowards better exploiting representational power of more expressive neural network models such asConvolutional Neural Networks (LeCun et al,0
9542,(1999)) extracts relevant information fora target variable,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9543,"Previously, the InformationBottleneck (IB) framework (Tishby et al","The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",0
9544,"Previously, the InformationBottleneck (IB) framework (Tishby et al","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9545,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively","(2014)) and game playing (e.g., Silver et al.(2016))",0
9546,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.",The neural network in this perspectiveconsists of a series of stochastic encoders that sequentially encode the original data space X intothe intermediate code spaces Zl,0
9547,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",0
9548,"We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level","In estimating mutual information, we adopted the variational method asin Alemi et al",0
9549,"We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level","Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich",0
9550,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance",The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,0
9551,(1999)) extracts relevant information fora target variable,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9552,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9553,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.",We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,0
9554,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9555,"We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9556,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",0
9557,"Previously, the InformationBottleneck (IB) framework (Tishby et al","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9558,"We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9559,"Previously, the InformationBottleneck (IB) framework (Tishby et al","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9560,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9561,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9562,(1999)) extracts relevant information fora target variable,"Fortunately,one can estimate the gradient in this case",0
9563,"We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9564,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.",This also empirically holds for the case of SFNN,0
9565,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9566,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance",(21)where |Z1| denotes the cardinality of the space of variable Z1,0
9567,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance","The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively",0
9568,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9569,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9570,"We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9571,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9572,"Previously, the InformationBottleneck (IB) framework (Tishby et al","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9573,(1999)) extracts relevant information fora target variable,"The approximate generalized IB objective in turn
presents interesting connections with the MLE principle",0
9574,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively","Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned",0
9575,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9576,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"We propose re-using the existing neural network architecture
as variational decoders for each hidden layers",0
9577,"We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9578,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"Previously, the InformationBottleneck (IB) framework (Tishby et al",0
9579,"Previously, the InformationBottleneck (IB) framework (Tishby et al","(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
9580,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",0
9581,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9582,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9583,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9584,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9585,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9586,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9587,"We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level",We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,0
9588,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,0
9589,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9590,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9591,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance","As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets",0
9592,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",0
9593,(1999)) extracts relevant information fora target variable,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9594,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",0
9595,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9596,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9597,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9598,(1999)) extracts relevant information fora target variable,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9599,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",1
9600,"(2012), Szegedy et al","arXiv preprint arXiv:1406.1078, 2014.",0
9601,"(2014), Bahdanau et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9602,"(2012), Szegedy et al","A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",0
9603,"In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,0
9604,"(2015)), natural languagetranslation (e.g., Cho et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9605,"(2014), Bahdanau et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9606,"(2014), Bahdanau et al",Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,0
9607,"In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ","In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ",0
9608,"Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al","In practice, the IB problem can be solved efﬁcientlyin the following two cases only: (1) X, Y and Z are all discrete (Tishby et al",0
9609,(1999)) whichextracts relevant information in an input variable X about a target variable Y ,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,0
9610,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9611,"The optimal representation Z is determined via theminimization of the following Lagrangian:LIB[p(z|x)] = I(Z, X) − βI(Z, Y )","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9612,"(2015)), natural languagetranslation (e.g., Cho et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9613,"Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al",The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,0
9614,"(2014)) and game playing (e.g., Silver et al.(2016))","For deterministicneural networks, we only have one sample of hidden variables given one data point",0
9615,"However, the MLE principle is very generic thatis not specially tailored for neural networks","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9616,"In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ","In estimating mutual information, we adopted the variational method asin Alemi et al",0
9617,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al","We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
9618,"(2014), Bahdanau et al",The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
9619,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",Xavier Glorot and Yoshua Bengio,0
9620,"(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",0
9621,"Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,0
9622,"Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9623,"Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9624,"(2014), Bahdanau et al","The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively",0
9625,"More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9626,"More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9627,"More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9628,"More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible","(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))",0
9629,"The optimal representation Z is determined via theminimization of the following Lagrangian:Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9630,"(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9631,"(2012), Szegedy et al","In practice, the IB problem can be solved efﬁcientlyin the following two cases only: (1) X, Y and Z are all discrete (Tishby et al",0
9632,"(2015)), natural languagetranslation (e.g., Cho et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9633,"(2015)), natural languagetranslation (e.g., Cho et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9634,"Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9635,"In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9636,"The optimal representation Z is determined via theminimization of the following Lagrangian:Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",0
9637,"(2014), Bahdanau et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9638,"(2015)), natural languagetranslation (e.g., Cho et al","In the general IB framework, each value of β speciﬁes a uniquepoint of Z in the information plane",0
9639,(1999)) whichextracts relevant information in an input variable X about a target variable Y ,Figure 1: A directed graphical representation of a PIB of two bottlenecks,0
9640,"(2012), Szegedy et al","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
9641,"However, the MLE principle is very generic thatis not specially tailored for neural networks","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9642,(1999)) whichextracts relevant information in an input variable X about a target variable Y ,"Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9643,"(2014)) and game playing (e.g., Silver et al.(2016))","335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",0
9644,"(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction","Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",0
9645,(1999)) whichextracts relevant information in an input variable X about a target variable Y ,We used three different randomly initialized neural,0
9646,"(2012), Szegedy et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9647,"The optimal representation Z is determined via theminimization of the following Lagrangian:LIB[p(z|x)] = I(Z, X) − βI(Z, Y )","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",0
9648,"(2014), Bahdanau et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9649,"(2012), Szegedy et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9650,"(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9651,"(2014)) and game playing (e.g., Silver et al.(2016))",The gradient is then propagated only throughthe deterministic term and ignored in the noise term,0
9652,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",Figure 1: A directed graphical representation of a PIB of two bottlenecks,0
9653,"(2014), Bahdanau et al","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",0
9654,"Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9655,(1999)) whichextracts relevant information in an input variable X about a target variable Y ,"Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9656,"The optimal representation Z is determined via theminimization of the following Lagrangian:LIB[p(z|x)] = I(Z, X) − βI(Z, Y )","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9657,"(2012), Szegedy et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9658,"More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
9659,"More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",The en-coder maps the input variable xxx to a single bottleneck variable zzz that is not a part of the consideredneural network architecture,0
9660,"However, the MLE principle is very generic thatis not specially tailored for neural networks","Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution ˆpD as possible (e.g., see Appendix I.B)",0
9661,"In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9662,"(2012), Szegedy et al",The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,0
9663,"In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9664,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9665,"(2014)) and game playing (e.g., Silver et al.(2016))","For PIB, we also usedβ−1l = β−1 = 10−4",0
9666,"In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9667,"(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9668,"The optimal representation Z is determined via theminimization of the following Lagrangian:LIB[p(z|x)] = I(Z, X) − βI(Z, Y )",URLhttps://doi.org/10.1109/CVPR.2016.90.,0
9669,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al","(2014), Bahdanau et al",0
9670,"In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ","CoRR, abs/1406.2989, 2014",0
9671,"(2015)), natural languagetranslation (e.g., Cho et al","(2015)), natural languagetranslation (e.g., Cho et al",0
9672,"(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9673,"However, the MLE principle is very generic thatis not specially tailored for neural networks","The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively",0
9674,"However, the MLE principle is very generic thatis not specially tailored for neural networks",The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,0
9675,"(2014), Bahdanau et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9676,"The optimal representation Z is determined via theminimization of the following Lagrangian:Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9677,"However, the MLE principle is very generic thatis not specially tailored for neural networks","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9678,"(2014)) and game playing (e.g., Silver et al.(2016))","We propose re-using the existing neural network architecture
as variational decoders for each hidden layers",0
9679,"(2012), Szegedy et al","In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",0
9680,"(2014), Bahdanau et al","The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",0
9681,"More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9682,"Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al","(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))",0
9683,"In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9684,"More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",We used three different randomly initialized neural,0
9685,"(2014)) and game playing (e.g., Silver et al.(2016))","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9686,"However, the MLE principle is very generic thatis not specially tailored for neural networks","Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T",0
9687,"(2012), Szegedy et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9688,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9689,"(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9690,(1999)) whichextracts relevant information in an input variable X about a target variable Y ,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",0
9691,"However, the MLE principle is very generic thatis not specially tailored for neural networks",A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,0
9692,"(2012), Szegedy et al","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",0
9693,"(2014)) and game playing (e.g., Silver et al.(2016))","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9694,"Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9695,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al","Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)",0
9696,(1999)) whichextracts relevant information in an input variable X about a target variable Y ,Xavier Glorot and Yoshua Bengio,0
9697,"(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",1
9698,"In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ","Speciﬁcally, the relevance decoder is determined as follows:",0
9699,"(2015)), natural languagetranslation (e.g., Cho et al","(2013) proposed to resort to reinforcement learning.However, these estimators have high variance",0
9700,"Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,0
9701,The exactsolution to the minimization problem above is found (Tishby et al,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)",0
9702,"As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9703,"We propose re-using the existing neural network architecture
as variational decoders for each hidden layers","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9704,"In the MLE principle, we only need empirical samples of
the joint distribution to maximize the likelihood function of the model given the data",The exactsolution to the minimization problem above is found (Tishby et al,0
9705,The exactsolution to the minimization problem above is found (Tishby et al,"where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9706,"where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )","Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",0
9707,"We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9708,"However in general the two principles are not obviously related.
In this work, we leverage neural networks and the IB principle by viewing neural networks as a set
of encoders that sequentially modify the original data space","In practice, the IB problem can be solved efﬁcientlyin the following two cases only: (1) X, Y and Z are all discrete (Tishby et al",0
9709,The exactsolution to the minimization problem above is found (Tishby et al,"where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9710,"The MLE principle treats
the neural network model p(xxx; θθθ) as a whole without explicitly considering the contribution of its
internal structures (e.g., hidden layers and hidden neurons)","To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",0
9711,"We propose re-using the existing neural network architecture
as variational decoders for each hidden layers","Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang",0
9712,"where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9713,"However in general the two principles are not obviously related.
In this work, we leverage neural networks and the IB principle by viewing neural networks as a set
of encoders that sequentially modify the original data space","In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",0
9714,"We propose re-using the existing neural network architecture
as variational decoders for each hidden layers","For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
9715,"As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9716,"The approximate generalized IB objective in turn
presents interesting connections with the MLE principle","Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",0
9717,"We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9718,"We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9719,"where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,0
9720,"(1999)) with the implicit selfwhere Z(x; β) is the normalization function, and DKL[.(cid:107).] is the Kullback - Leibler (KL) divergence
(Kullback & Leibler (1951))","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9721,"Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest","The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",0
9722,"We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.","Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis",0
9723,"In the MLE principle, we only need empirical samples of
the joint distribution to maximize the likelihood function of the model given the data",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
9724,"We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,0
9725,"Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution ˆpD as possible (e.g., see Appendix I.B)","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9726,"Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9727,"We then propose a new generalized
IB-based objective that takes into account the compression and relevance of all layers in the network
as an explicit goal for guiding the encodings in a beneﬁcial manner","(2013) proposed to resort to reinforcement learning.However, these estimators have high variance",0
9728,"As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets","We then propose a new generalized
IB-based objective that takes into account the compression and relevance of all layers in the network
as an explicit goal for guiding the encodings in a beneﬁcial manner",0
9729,"where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9730,The exactsolution to the minimization problem above is found (Tishby et al,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting",0
9731,"However in general the two principles are not obviously related.
In this work, we leverage neural networks and the IB principle by viewing neural networks as a set
of encoders that sequentially modify the original data space","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9732,"The MLE
principle is proved to be mathematically equivalent to the IB principle for the multinomial mixture
model for clustering problem when the input distribution X is uniform or has a large sample size
(Slonim & Weiss (2002))","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9733,"(1999)) with the implicit selfwhere Z(x; β) is the normalization function, and DKL[.(cid:107).] is the Kullback - Leibler (KL) divergence
(Kullback & Leibler (1951))","PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",0
9734,"The approximate generalized IB objective in turn
presents interesting connections with the MLE principle",The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,0
9735,"In the MLE principle, we only need empirical samples of
the joint distribution to maximize the likelihood function of the model given the data","(1999)) with the implicit selfwhere Z(x; β) is the normalization function, and DKL[.(cid:107).] is the Kullback - Leibler (KL) divergence
(Kullback & Leibler (1951))",0
9736,"We propose re-using the existing neural network architecture
as variational decoders for each hidden layers","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9737,"We then propose a new generalized
IB-based objective that takes into account the compression and relevance of all layers in the network
as an explicit goal for guiding the encodings in a beneﬁcial manner",We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,0
9738,"However in general the two principles are not obviously related.
In this work, we leverage neural networks and the IB principle by viewing neural networks as a set
of encoders that sequentially modify the original data space","In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",0
9739,"Since the objective is designed
to optimize all parameters of neural networks and is mainly motivated by the IB principle for deep
learning (Tishby & Zaslavsky (2015)), we name this method the Parametric Information Bottleneck
(PIB)","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9740,"We then propose a new generalized
IB-based objective that takes into account the compression and relevance of all layers in the network
as an explicit goal for guiding the encodings in a beneﬁcial manner","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9741,"The approximate generalized IB objective in turn
presents interesting connections with the MLE principle","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9742,"As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets","Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",0
9743,"We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.","Since the objective is designed
to optimize all parameters of neural networks and is mainly motivated by the IB principle for deep
learning (Tishby & Zaslavsky (2015)), we name this method the Parametric Information Bottleneck
(PIB)",0
9744,"As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9745,"We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9746,"The MLE
principle is proved to be mathematically equivalent to the IB principle for the multinomial mixture
model for clustering problem when the input distribution X is uniform or has a large sample size
(Slonim & Weiss (2002))","Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp",0
9747,"The MLE principle treats
the neural network model p(xxx; θθθ) as a whole without explicitly considering the contribution of its
internal structures (e.g., hidden layers and hidden neurons)","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9748,"As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets","(2014), Dauphin & Grangier(2016))",0
9749,"Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9750,"where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9751,"(1999)) with the implicit selfwhere Z(x; β) is the normalization function, and DKL[.(cid:107).] is the Kullback - Leibler (KL) divergence
(Kullback & Leibler (1951))","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9752,"As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets",Learning phrase representations using rnn encoder-decoderfor statistical machine translation,0
9753,"We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.",doi: 10.1109/CVPR.2016.90,0
9754,"Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation",The gradient is then propagated only throughthe deterministic term and ignored in the noise term,0
9755,"Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9756,"We then propose a new generalized
IB-based objective that takes into account the compression and relevance of all layers in the network
as an explicit goal for guiding the encodings in a beneﬁcial manner","Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al",0
9757,"Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
9758,The exactsolution to the minimization problem above is found (Tishby et al,(1999)) extracts relevant information fora target variable,0
9759,"Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest",URL http://arxiv.org/abs/1605.02688.,0
9760,"Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9761,"We propose re-using the existing neural network architecture
as variational decoders for each hidden layers","Since the objective is designed
to optimize all parameters of neural networks and is mainly motivated by the IB principle for deep
learning (Tishby & Zaslavsky (2015)), we name this method the Parametric Information Bottleneck
(PIB)",0
9762,The exactsolution to the minimization problem above is found (Tishby et al,"where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9763,"The MLE
principle is proved to be mathematically equivalent to the IB principle for the multinomial mixture
model for clustering problem when the input distribution X is uniform or has a large sample size
(Slonim & Weiss (2002))","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9764,"In the MLE principle, we only need empirical samples of
the joint distribution to maximize the likelihood function of the model given the data","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9765,"Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution ˆpD as possible (e.g., see Appendix I.B)","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9766,"However in general the two principles are not obviously related.
In this work, we leverage neural networks and the IB principle by viewing neural networks as a set
of encoders that sequentially modify the original data space","In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)",0
9767,"Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9768,The exactsolution to the minimization problem above is found (Tishby et al,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",0
9769,"As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets",URLhttps://doi.org/10.1109/CVPR.2016.90.,0
9770,The exactsolution to the minimization problem above is found (Tishby et al,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",0
9771,"We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9772,"Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution ˆpD as possible (e.g., see Appendix I.B)","The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)",0
9773,"where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",0
9774,"The MLE principle treats
the neural network model p(xxx; θθθ) as a whole without explicitly considering the contribution of its
internal structures (e.g., hidden layers and hidden neurons)","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9775,"Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution ˆpD as possible (e.g., see Appendix I.B)","Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks",0
9776,"The MLE principle treats
the neural network model p(xxx; θθθ) as a whole without explicitly considering the contribution of its
internal structures (e.g., hidden layers and hidden neurons)",(1999),0
9777,"In the MLE principle, we only need empirical samples of
the joint distribution to maximize the likelihood function of the model given the data","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9778,"Since the objective is designed
to optimize all parameters of neural networks and is mainly motivated by the IB principle for deep
learning (Tishby & Zaslavsky (2015)), we name this method the Parametric Information Bottleneck
(PIB)","CoRR, abs/1308.3432, 2013",0
9779,"The approximate generalized IB objective in turn
presents interesting connections with the MLE principle","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9780,"(1999)) with the implicit selfwhere Z(x; β) is the normalization function, and DKL[.(cid:107).] is the Kullback - Leibler (KL) divergence
(Kullback & Leibler (1951))","CoRR, abs/1406.2989, 2014",0
9781,"Since the objective is designed
to optimize all parameters of neural networks and is mainly motivated by the IB principle for deep
learning (Tishby & Zaslavsky (2015)), we name this method the Parametric Information Bottleneck
(PIB)","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9782,"Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9783,"where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9784,"The approximate generalized IB objective in turn
presents interesting connections with the MLE principle","Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
9785,"The MLE
principle is proved to be mathematically equivalent to the IB principle for the multinomial mixture
model for clustering problem when the input distribution X is uniform or has a large sample size
(Slonim & Weiss (2002))","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9786,"The MLE principle treats
the neural network model p(xxx; θθθ) as a whole without explicitly considering the contribution of its
internal structures (e.g., hidden layers and hidden neurons)","However, the variational approximatedistribution for the code space may be too loose while the actual marginal distribution can be sam-pled easily.Our work, on the other hand, focuses on better exploiting intermediate representations of a neuralnetwork architecture using the IB principle",0
9787,"As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets","Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",0
9788,"Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9789,"Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution ˆpD as possible (e.g., see Appendix I.B)","Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P",0
9790,"In the MLE principle, we only need empirical samples of
the joint distribution to maximize the likelihood function of the model given the data","448–456, 2015",0
9791,"In the MLE principle, we only need empirical samples of
the joint distribution to maximize the likelihood function of the model given the data","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9792,"Since the objective is designed
to optimize all parameters of neural networks and is mainly motivated by the IB principle for deep
learning (Tishby & Zaslavsky (2015)), we name this method the Parametric Information Bottleneck
(PIB)","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9793,"where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9794,"We then propose a new generalized
IB-based objective that takes into account the compression and relevance of all layers in the network
as an explicit goal for guiding the encodings in a beneﬁcial manner","drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
9795,"The approximate generalized IB objective in turn
presents interesting connections with the MLE principle","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9796,The exactsolution to the minimization problem above is found (Tishby et al,"where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9797,"The MLE
principle is proved to be mathematically equivalent to the IB principle for the multinomial mixture
model for clustering problem when the input distribution X is uniform or has a large sample size
(Slonim & Weiss (2002))","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",1
9798,"We propose re-using the existing neural network architecture
as variational decoders for each hidden layers",Understanding the difﬁculty of training deep feedforward neuralnetworks,0
9799,"As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets","David Silver, Aja Huang, Chris J",0
9800,"Originally, the general IB framework is proposed in Tishby et al","Originally, the general IB framework is proposed in Tishby et al",1
9801,"By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
9802,"(1999)); or (2) X, Yand Z are mutually joint Gaussian (Chechik et al","We scaled the images to [0, 1] and do not perform anyother data augmentation",0
9803,"Furthermore,estimating mutual information between the variables transformed by network layers and the datavariables poses several computational challenges in practice that the authors did not address in thework","Originally, the general IB framework is proposed in Tishby et al",1
9804,"(1999)); or (2) X, Yand Z are mutually joint Gaussian (Chechik et al","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",0
9805,"Thus, it is hard to analytically capture such modiﬁcations.","448–456, 2015",0
9806,"Thus, it is hard to analytically capture such modiﬁcations.","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",0
9807,"(1999)); or (2) X, Yand Z are mutually joint Gaussian (Chechik et al",Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,0
9808,"Originally, the general IB framework is proposed in Tishby et al","Originally, the general IB framework is proposed in Tishby et al",1
9809,"In practice, the IB problem can be solved efﬁcientlyin the following two cases only: (1) X, Y and Z are all discrete (Tishby et al","While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)",0
9810,"In practice, the IB problem can be solved efﬁcientlyin the following two cases only: (1) X, Y and Z are all discrete (Tishby et al","For visualization of learned ﬁlters of PIB, seeAppendix II.A.",0
9811,"By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner","Originally, the general IB framework is proposed in Tishby et al",1
9812,"In practice, the IB problem can be solved efﬁcientlyin the following two cases only: (1) X, Y and Z are all discrete (Tishby et al","Originally, the general IB framework is proposed in Tishby et al",1
9813,"Originally, the general IB framework is proposed in Tishby et al","Originally, the general IB framework is proposed in Tishby et al",1
9814,"Originally, the general IB framework is proposed in Tishby et al",Learning phrase representations using rnn encoder-decoderfor statistical machine translation,0
9815,"Thus, it is hard to analytically capture such modiﬁcations.","Originally, the general IB framework is proposed in Tishby et al",1
9816,"By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner","Originally, the general IB framework is proposed in Tishby et al",1
9817,"By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner","Originally, the general IB framework is proposed in Tishby et al",1
9818,(1999),The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
9819,(1999),"Originally, the general IB framework is proposed in Tishby et al",1
9820,"(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))","The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",0
9821,"(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))","Originally, the general IB framework is proposed in Tishby et al",1
9822,The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,"Originally, the general IB framework is proposed in Tishby et al",1
9823,"In practice, the IB problem can be solved efﬁcientlyin the following two cases only: (1) X, Y and Z are all discrete (Tishby et al","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",0
9824,"Thus, it is hard to analytically capture such modiﬁcations.","Originally, the general IB framework is proposed in Tishby et al",1
9825,A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,"Originally, the general IB framework is proposed in Tishby et al",1
9826,"Originally, the general IB framework is proposed in Tishby et al",We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,0
9827,The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,"The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively",0
9828,"Originally, the general IB framework is proposed in Tishby et al","Originally, the general IB framework is proposed in Tishby et al",1
9829,"(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))","Originally, the general IB framework is proposed in Tishby et al",1
9830,"Thus, it is hard to analytically capture such modiﬁcations.",Learning phrase representations using rnn encoder-decoderfor statistical machine translation,0
9831,"Thus, it is hard to analytically capture such modiﬁcations.",The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,0
9832,"Furthermore,estimating mutual information between the variables transformed by network layers and the datavariables poses several computational challenges in practice that the authors did not address in thework","Originally, the general IB framework is proposed in Tishby et al",1
9833,"Although the analysis offers a new perspective about optimality in neural networks,it proposes general analysis of optimality rather than a practical optimization criteria","Originally, the general IB framework is proposed in Tishby et al",1
9834,"(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))","However, the variational approximatedistribution for the code space may be too loose while the actual marginal distribution can be sam-pled easily.Our work, on the other hand, focuses on better exploiting intermediate representations of a neuralnetwork architecture using the IB principle",0
9835,(1999),"Originally, the general IB framework is proposed in Tishby et al",1
9836,"By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner","Since the objective is designed
to optimize all parameters of neural networks and is mainly motivated by the IB principle for deep
learning (Tishby & Zaslavsky (2015)), we name this method the Parametric Information Bottleneck
(PIB)",0
9837,"However, thealgorithm is not applicable to neural networks","Originally, the general IB framework is proposed in Tishby et al",1
9838,"(1999)); or (2) X, Yand Z are mutually joint Gaussian (Chechik et al",doi: 10.1109/CVPR.2015.7298594,0
9839,"Furthermore,estimating mutual information between the variables transformed by network layers and the datavariables poses several computational challenges in practice that the authors did not address in thework",1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],0
9840,"By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner","For visualization of learned ﬁlters of PIB, seeAppendix II.A.",0
9841,"Furthermore,estimating mutual information between the variables transformed by network layers and the datavariables poses several computational challenges in practice that the authors did not address in thework","Originally, the general IB framework is proposed in Tishby et al",1
9842,This workproposes using mutual information of a hidden layer with the input layer and the output layer toquantify the performance of DNNs,"Originally, the general IB framework is proposed in Tishby et al",1
9843,(1999),"Originally, the general IB framework is proposed in Tishby et al",1
9844,"Although the analysis offers a new perspective about optimality in neural networks,it proposes general analysis of optimality rather than a practical optimization criteria","The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",0
9845,"Originally, the general IB framework is proposed in Tishby et al","Fortunately,one can estimate the gradient in this case",0
9846,A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,"The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively",0
9847,"In practice, the IB problem can be solved efﬁcientlyin the following two cases only: (1) X, Y and Z are all discrete (Tishby et al","Originally, the general IB framework is proposed in Tishby et al",1
9848,"In practice, the IB problem can be solved efﬁcientlyin the following two cases only: (1) X, Y and Z are all discrete (Tishby et al","For deterministicneural networks, we only have one sample of hidden variables given one data point",0
9849,"Thus, it is hard to analytically capture such modiﬁcations.","Originally, the general IB framework is proposed in Tishby et al",1
9850,"Originally, the general IB framework is proposed in Tishby et al","The approximate generalized IB objective in turn
presents interesting connections with the MLE principle",0
9851,"However, thealgorithm is not applicable to neural networks","Originally, the general IB framework is proposed in Tishby et al",1
9852,"By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,0
9853,"Originally, the general IB framework is proposed in Tishby et al","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",0
9854,"Although the analysis offers a new perspective about optimality in neural networks,it proposes general analysis of optimality rather than a practical optimization criteria","Originally, the general IB framework is proposed in Tishby et al",1
9855,(1999),"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)",0
9856,"Although the analysis offers a new perspective about optimality in neural networks,it proposes general analysis of optimality rather than a practical optimization criteria","Originally, the general IB framework is proposed in Tishby et al",1
9857,"Originally, the general IB framework is proposed in Tishby et al","Originally, the general IB framework is proposed in Tishby et al",1
9858,The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,"We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.",0
9859,"Originally, the general IB framework is proposed in Tishby et al",The ﬁrst model (ModelA) is a deterministic neural network,0
9860,(1999),URL http://arxiv.org/abs/1605.02688.,0
9861,A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",0
9862,"Although the analysis offers a new perspective about optimality in neural networks,it proposes general analysis of optimality rather than a practical optimization criteria","Originally, the general IB framework is proposed in Tishby et al",1
9863,"(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
9864,A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,doi: 10.1109/CVPR.2015.7298594,0
9865,"Furthermore,estimating mutual information between the variables transformed by network layers and the datavariables poses several computational challenges in practice that the authors did not address in thework","Originally, the general IB framework is proposed in Tishby et al",1
9866,(1999),"The approximate generalized IB objective in turn
presents interesting connections with the MLE principle",0
9867,"Although the analysis offers a new perspective about optimality in neural networks,it proposes general analysis of optimality rather than a practical optimization criteria","Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks",0
9868,The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,"Originally, the general IB framework is proposed in Tishby et al",1
9869,A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,"(2014), Bahdanau et al",0
9870,This workproposes using mutual information of a hidden layer with the input layer and the output layer toquantify the performance of DNNs,"Originally, the general IB framework is proposed in Tishby et al",1
9871,"Furthermore,estimating mutual information between the variables transformed by network layers and the datavariables poses several computational challenges in practice that the authors did not address in thework","Originally, the general IB framework is proposed in Tishby et al",1
9872,"However, thealgorithm is not applicable to neural networks","In practice, the IB problem can be solved efﬁcientlyin the following two cases only: (1) X, Y and Z are all discrete (Tishby et al",0
9873,The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,"770–778, 2016",0
9874,"By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner","Originally, the general IB framework is proposed in Tishby et al",1
9875,"(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))","Originally, the general IB framework is proposed in Tishby et al",1
9876,A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,URLhttp://arxiv.org/abs/1308.3432.,0
9877,"However, thealgorithm is not applicable to neural networks","Originally, the general IB framework is proposed in Tishby et al",1
9878,(1999),"However, the MLE principle is very generic thatis not specially tailored for neural networks",0
9879,"Originally, the general IB framework is proposed in Tishby et al","To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",0
9880,(1999),"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",0
9881,"(1999)); or (2) X, Yand Z are mutually joint Gaussian (Chechik et al","Originally, the general IB framework is proposed in Tishby et al",1
9882,"By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner","Originally, the general IB framework is proposed in Tishby et al",1
9883,"However, thealgorithm is not applicable to neural networks","Originally, the general IB framework is proposed in Tishby et al",1
9884,"Furthermore,estimating mutual information between the variables transformed by network layers and the datavariables poses several computational challenges in practice that the authors did not address in thework","Originally, the general IB framework is proposed in Tishby et al",1
9885,"By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner","Originally, the general IB framework is proposed in Tishby et al",1
9886,"However, thealgorithm is not applicable to neural networks",(1999)) extracts relevant information fora target variable,0
9887,"However, thealgorithm is not applicable to neural networks",Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,0
9888,"(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))","Originally, the general IB framework is proposed in Tishby et al",1
9889,"In practice, the IB problem can be solved efﬁcientlyin the following two cases only: (1) X, Y and Z are all discrete (Tishby et al",Discrete-valued variables in PIBs make standard back-propagation not straightforward,0
9890,"Originally, the general IB framework is proposed in Tishby et al","Originally, the general IB framework is proposed in Tishby et al",1
9891,This workproposes using mutual information of a hidden layer with the input layer and the output layer toquantify the performance of DNNs,"Originally, the general IB framework is proposed in Tishby et al",1
9892,"Thus, it is hard to analytically capture such modiﬁcations.","More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
9893,A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,0
9894,"(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))","Journal of Machine Learning Research, 6:165–188, 2005",0
9895,"In practice, the IB problem can be solved efﬁcientlyin the following two cases only: (1) X, Y and Z are all discrete (Tishby et al",In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,0
9896,"(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))","Originally, the general IB framework is proposed in Tishby et al",1
9897,"(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
9898,"In practice, the IB problem can be solved efﬁcientlyin the following two cases only: (1) X, Y and Z are all discrete (Tishby et al","Originally, the general IB framework is proposed in Tishby et al",1
9899,The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,"(2013) proposed to resort to reinforcement learning.However, these estimators have high variance",0
9900,"More speciﬁcally, our work proposes an optimizationIB criteria for an existing neural network architecture in an effort to better learn the layers’ represen-tation to their IB optimality","Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",0
9901,The en-coder maps the input variable xxx to a single bottleneck variable zzz that is not a part of the consideredneural network architecture,The recent work Alemi et al,1
9902,"(2016) for I(Z, Y ) but use empirical estimation for I(Z, X)","335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",0
9903,"More speciﬁcally, our work proposes an optimizationIB criteria for an existing neural network architecture in an effort to better learn the layers’ represen-tation to their IB optimality",The recent work Alemi et al,1
9904,"However, the variational approximatedistribution for the code space may be too loose while the actual marginal distribution can be sam-pled easily.Our work, on the other hand, focuses on better exploiting intermediate representations of a neuralnetwork architecture using the IB principle",The recent work Alemi et al,1
9905,The en-coder maps the input variable xxx to a single bottleneck variable zzz that is not a part of the consideredneural network architecture,The recent work Alemi et al,1
9906,"(2016) for I(Z, Y ) but use empirical estimation for I(Z, X)","Speciﬁcally, the relevance decoder is determined as follows:",0
9907,"(2016) for I(Z, Y ) but use empirical estimation for I(Z, X)","Fortunately,one can estimate the gradient in this case",0
9908,"Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,0
9909,The en-coder maps the input variable xxx to a single bottleneck variable zzz that is not a part of the consideredneural network architecture,The recent work Alemi et al,1
9910,The recent work Alemi et al,The recent work Alemi et al,1
9911,"Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution",(2016)).,0
9912,The en-coder maps the input variable xxx to a single bottleneck variable zzz that is not a part of the consideredneural network architecture,The recent work Alemi et al,1
9913,"Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution",The recent work Alemi et al,1
9914,"(2016) for I(Z, Y ) but use empirical estimation for I(Z, X)",content of the original space possibly including its dimensionality and topological structure,0
9915,The recent work Alemi et al,The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,0
9916,"In estimating mutual information, we adopted the variational method asin Alemi et al","In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",0
9917,"More speciﬁcally, our work proposes an optimizationIB criteria for an existing neural network architecture in an effort to better learn the layers’ represen-tation to their IB optimality",The recent work Alemi et al,0
9918,"Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution",Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,0
9919,"Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.","Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014",0
9920,"Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution","The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively",0
9921,Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,The recent work Alemi et al,1
9922,Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,0
9923,"However, the variational approximatedistribution for the code space may be too loose while the actual marginal distribution can be sam-pled easily.Our work, on the other hand, focuses on better exploiting intermediate representations of a neuralnetwork architecture using the IB principle",This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,0
9924,"More speciﬁcally, our work proposes an optimizationIB criteria for an existing neural network architecture in an effort to better learn the layers’ represen-tation to their IB optimality",ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,0
9925,Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,"Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution",0
9926,"Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",The recent work Alemi et al,1
9927,"Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise","For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
9928,(2016) also uses variational methods to approximate the mutual infor-mation as an attempt to apply the IB principle to neural networks,The recent work Alemi et al,1
9929,"(2016) for I(Z, Y ) but use empirical estimation for I(Z, X)","We scaled the images to [0, 1] and do not perform anyother data augmentation",0
9930,"However, the variational approximatedistribution for the code space may be too loose while the actual marginal distribution can be sam-pled easily.Our work, on the other hand, focuses on better exploiting intermediate representations of a neuralnetwork architecture using the IB principle","Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",0
9931,The recent work Alemi et al,"As β varies from 0 to ∞, Z traces a concave curve, knownas information curve for representation Z, with a slope of β−1",0
9932,(2016) also uses variational methods to approximate the mutual infor-mation as an attempt to apply the IB principle to neural networks,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
9933,"Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",The recent work Alemi et al,1
9934,Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",0
9935,(2016) also uses variational methods to approximate the mutual infor-mation as an attempt to apply the IB principle to neural networks,The recent work Alemi et al,1
9936,Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,"The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",0
9937,The recent work Alemi et al,The recent work Alemi et al,1
9938,Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,"For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
9939,"In estimating mutual information, we adopted the variational method asin Alemi et al",The recent work Alemi et al,1
9940,"Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise","In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",0
9941,The recent work Alemi et al,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",0
9942,The recent work Alemi et al,The recent work Alemi et al,1
9943,"In estimating mutual information, we adopted the variational method asin Alemi et al",The recent work Alemi et al,1
9944,The recent work Alemi et al,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance",0
9945,(2016) also uses variational methods to approximate the mutual infor-mation as an attempt to apply the IB principle to neural networks,"In estimating mutual information, we adopted the variational method asin Alemi et al",0
9946,"Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",The recent work Alemi et al,1
9947,Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,"1–9, 2015",0
9948,Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,The recent work Alemi et al,1
9949,"In estimating mutual information, we adopted the variational method asin Alemi et al","In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",0
9950,"(2016) for I(Z, Y ) but use empirical estimation for I(Z, X)",The recent work Alemi et al,1
9951,"Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",0
9952,The recent work Alemi et al,The recent work Alemi et al,1
9953,"Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution",These code spaces are lossy representations of the data space as itfollows from the data-processing inequality (DPI) (Cover & Thomas (2006)) that,0
9954,"Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise","Fortunately,one can estimate the gradient in this case",0
9955,"Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.",All models areimplemented using Theano framework (Al-Rfou et al,0
9956,"More speciﬁcally, our work proposes an optimizationIB criteria for an existing neural network architecture in an effort to better learn the layers’ represen-tation to their IB optimality",The recent work Alemi et al,1
9957,"(2016) for I(Z, Y ) but use empirical estimation for I(Z, X)",The recent work Alemi et al,1
9958,"Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution","Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
9959,"Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution",All models areimplemented using Theano framework (Al-Rfou et al,0
9960,"Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution","Here we preferto this extreme, i.e., the 0th level, as the super level",0
9961,"Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution","335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",0
9962,"(2016) for I(Z, Y ) but use empirical estimation for I(Z, X)","The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)",0
9963,"Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution",The recent work Alemi et al,1
9964,"However, the variational approximatedistribution for the code space may be too loose while the actual marginal distribution can be sam-pled easily.Our work, on the other hand, focuses on better exploiting intermediate representations of a neuralnetwork architecture using the IB principle",The recent work Alemi et al,1
9965,"Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.",The recent work Alemi et al,1
9966,"However, the variational approximatedistribution for the code space may be too loose while the actual marginal distribution can be sam-pled easily.Our work, on the other hand, focuses on better exploiting intermediate representations of a neuralnetwork architecture using the IB principle",The recent work Alemi et al,1
9967,(2016) also uses variational methods to approximate the mutual infor-mation as an attempt to apply the IB principle to neural networks,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,0
9968,"Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",The recent work Alemi et al,1
9969,"Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.",The recent work Alemi et al,1
9970,"In estimating mutual information, we adopted the variational method asin Alemi et al","Onaverage, 2H(X|Z) elements of X are mapped to the same code in Z",0
9971,"More speciﬁcally, our work proposes an optimizationIB criteria for an existing neural network architecture in an effort to better learn the layers’ represen-tation to their IB optimality",The recent work Alemi et al,1
9972,"More speciﬁcally, our work proposes an optimizationIB criteria for an existing neural network architecture in an effort to better learn the layers’ represen-tation to their IB optimality",The recent work Alemi et al,1
9973,"However, the variational approximatedistribution for the code space may be too loose while the actual marginal distribution can be sam-pled easily.Our work, on the other hand, focuses on better exploiting intermediate representations of a neuralnetwork architecture using the IB principle","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",0
9974,"More speciﬁcally, our work proposes an optimizationIB criteria for an existing neural network architecture in an effort to better learn the layers’ represen-tation to their IB optimality","(2014)) and game playing (e.g., Silver et al.(2016))",0
9975,The en-coder maps the input variable xxx to a single bottleneck variable zzz that is not a part of the consideredneural network architecture,"In this work, we use the gradient estimator inspiredby Raiko et al",0
9976,"More speciﬁcally, our work proposes an optimizationIB criteria for an existing neural network architecture in an effort to better learn the layers’ represen-tation to their IB optimality","To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)",0
9977,"Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",The recent work Alemi et al,1
9978,"More speciﬁcally, our work proposes an optimizationIB criteria for an existing neural network architecture in an effort to better learn the layers’ represen-tation to their IB optimality","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",0
9979,"In estimating mutual information, we adopted the variational method asin Alemi et al",The recent work Alemi et al,1
9980,"Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",The recent work Alemi et al,1
9981,Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,The recent work Alemi et al,1
9982,"Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution",The recent work Alemi et al,1
9983,"Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.",The recent work Alemi et al,1
9984,"Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.",The recent work Alemi et al,1
9985,"Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.","For PIB, we also usedβ−1l = β−1 = 10−4",0
9986,"In estimating mutual information, we adopted the variational method asin Alemi et al","PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",0
9987,"(2016) for I(Z, Y ) but use empirical estimation for I(Z, X)",The recent work Alemi et al,1
9988,Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,0
9989,"Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise","770–778, 2016",0
9990,Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )",0
9991,"Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.","For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
9992,(2016) also uses variational methods to approximate the mutual infor-mation as an attempt to apply the IB principle to neural networks,The recent work Alemi et al,1
9993,"(2016) for I(Z, Y ) but use empirical estimation for I(Z, X)",URL https://doi.org/10.1038/nature16961.,0
9994,"Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution",We used three different randomly initialized neural,0
9995,"Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",The recent work Alemi et al,1
9996,The recent work Alemi et al,The recent work Alemi et al,1
9997,"In estimating mutual information, we adopted the variational method asin Alemi et al",The recent work Alemi et al,1
9998,Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
9999,"In estimating mutual information, we adopted the variational method asin Alemi et al",The recent work Alemi et al,1
10000,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10001,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,0
10002,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10003,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10004,"Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10005,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10006,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10007,"We extend the notations of Zl by using the convention Z0 := X and Z−1 := ∅.The space of X, Y and Zl are denoted as X ,Y and Zl, respectively",A detail of gradient-based training of PIB ispresented in Algorithm 1,0
10008,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10009,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
10010,"Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10011,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10012,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers","Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution ˆpD as possible (e.g., see Appendix I.B)",0
10013,"In this work,we focus on binary bottlenecks where Zl ∈ {0, 1}nl and ni is the dimensionality of the bottleneckspace.",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10014,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10015,"Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10016,"Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10017,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions","(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))",0
10018,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions","Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
10019,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,0
10020,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions","PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",0
10021,"We extend the notations of Zl by using the convention Z0 := X and Z−1 := ∅.The space of X, Y and Zl are denoted as X ,Y and Zl, respectively",A,0
10022,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
10023,"Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,0
10024,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers","530–538, 2013",0
10025,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10026,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers","Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.",0
10027,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10028,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10029,"Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10030,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10031,"We extend the notations of Zl by using the convention Z0 := X and Z−1 := ∅.The space of X, Y and Zl are denoted as X ,Y and Zl, respectively",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10032,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10033,"In this work,we focus on binary bottlenecks where Zl ∈ {0, 1}nl and ni is the dimensionality of the bottleneckspace.",(1999),0
10034,"We extend the notations of Zl by using the convention Z0 := X and Z−1 := ∅.The space of X, Y and Zl are denoted as X ,Y and Zl, respectively",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10035,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers","Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)",0
10036,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,"informative yet compressed representation, which is supported by qualitative empirical results",0
10037,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10038,"We extend the notations of Zl by using the convention Z0 := X and Z−1 := ∅.The space of X, Y and Zl are denoted as X ,Y and Zl, respectively",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10039,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,"For PIB, we also usedβ−1l = β−1 = 10−4",0
10040,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10041,"Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network","(2014)) and game playing (e.g., Silver et al.(2016))",0
10042,"We extend the notations of Zl by using the convention Z0 := X and Z−1 := ∅.The space of X, Y and Zl are denoted as X ,Y and Zl, respectively","(2014)) and game playing (e.g., Silver et al.(2016))",0
10043,"In this work,we focus on binary bottlenecks where Zl ∈ {0, 1}nl and ni is the dimensionality of the bottleneckspace.","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
10044,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),"In estimating mutual information, we adopted the variational method asin Alemi et al",0
10045,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10046,"Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10047,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10048,"Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10049,"In this work,we focus on binary bottlenecks where Zl ∈ {0, 1}nl and ni is the dimensionality of the bottleneckspace.",Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,0
10050,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),"For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
10051,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,"Fortunately,one can estimate the gradient in this case",0
10052,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10053,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10054,"We extend the notations of Zl by using the convention Z0 := X and Z−1 := ∅.The space of X, Y and Zl are denoted as X ,Y and Zl, respectively","Thus, it is hard to analytically capture such modiﬁcations.",0
10055,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,0
10056,"In this work,we focus on binary bottlenecks where Zl ∈ {0, 1}nl and ni is the dimensionality of the bottleneckspace.","In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",0
10057,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10058,"We extend the notations of Zl by using the convention Z0 := X and Z−1 := ∅.The space of X, Y and Zl are denoted as X ,Y and Zl, respectively",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10059,"We extend the notations of Zl by using the convention Z0 := X and Z−1 := ∅.The space of X, Y and Zl are denoted as X ,Y and Zl, respectively","David Silver, Aja Huang, Chris J",0
10060,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10061,"In this work,we focus on binary bottlenecks where Zl ∈ {0, 1}nl and ni is the dimensionality of the bottleneckspace.",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10062,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10063,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10064,"We extend the notations of Zl by using the convention Z0 := X and Z−1 := ∅.The space of X, Y and Zl are denoted as X ,Y and Zl, respectively","Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T",0
10065,"In this work,we focus on binary bottlenecks where Zl ∈ {0, 1}nl and ni is the dimensionality of the bottleneckspace.",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10066,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions","Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution ˆpD as possible (e.g., see Appendix I.B)",0
10067,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
10068,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10069,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10070,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10071,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,0
10072,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10073,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J",0
10074,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10075,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,0
10076,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,"More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
10077,"In this work,we focus on binary bottlenecks where Zl ∈ {0, 1}nl and ni is the dimensionality of the bottleneckspace.",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10078,"We extend the notations of Zl by using the convention Z0 := X and Z−1 := ∅.The space of X, Y and Zl are denoted as X ,Y and Zl, respectively","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",0
10079,"In this work,we focus on binary bottlenecks where Zl ∈ {0, 1}nl and ni is the dimensionality of the bottleneckspace.",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10080,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10081,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",doi: 10.1109/CVPR.2016.90,0
10082,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10083,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions","Journal of Machine Learning Research, 6:165–188, 2005",0
10084,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10085,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions","Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
10086,"Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,0
10087,"Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",URLhttp://arxiv.org/abs/1308.3432.,0
10088,"Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",The ﬁrst model (ModelA) is a deterministic neural network,0
10089,"In this work,we focus on binary bottlenecks where Zl ∈ {0, 1}nl and ni is the dimensionality of the bottleneckspace.",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10090,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,0
10091,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10092,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions","(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
10093,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",0
10094,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers","The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively",0
10095,"Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10096,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10097,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10098,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",0
10099,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,1
10100,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",content of the original space possibly including its dimensionality and topological structure,1
10101,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)","We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level",0
10102,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
10103,"Onaverage, 2H(X|Z) elements of X are mapped to the same code in Z",content of the original space possibly including its dimensionality and topological structure,1
10104,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",doi: 10.1109/CVPR.2016.90,0
10105,These code spaces are lossy representations of the data space as itfollows from the data-processing inequality (DPI) (Cover & Thomas (2006)) that,content of the original space possibly including its dimensionality and topological structure,1
10106,"The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)",content of the original space possibly including its dimensionality and topological structure,1
10107,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",content of the original space possibly including its dimensionality and topological structure,1
10108,"Onaverage, 2H(X|Z) elements of X are mapped to the same code in Z",Techniques for learningbinary stochastic feedforward neural networks,0
10109,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space","It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
10110,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)","It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
10111,"The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)",(2016)).,0
10112,content of the original space possibly including its dimensionality and topological structure,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P",0
10113,These code spaces are lossy representations of the data space as itfollows from the data-processing inequality (DPI) (Cover & Thomas (2006)) that,content of the original space possibly including its dimensionality and topological structure,1
10114,content of the original space possibly including its dimensionality and topological structure,content of the original space possibly including its dimensionality and topological structure,1
10115,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",content of the original space possibly including its dimensionality and topological structure,1
10116,"Therefore, lossy representation is often more helpful (and of course moreefﬁcient) than a precise representation",content of the original space possibly including its dimensionality and topological structure,1
10117,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",Figure 1: A directed graphical representation of a PIB of two bottlenecks,0
10118,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",URL http://www.jmlr.org/papers/v6/chechik05a.html.,0
10119,"Therefore, lossy representation is often more helpful (and of course moreefﬁcient) than a precise representation",content of the original space possibly including its dimensionality and topological structure,1
10120,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",content of the original space possibly including its dimensionality and topological structure,1
10121,"Therefore, lossy representation is often more helpful (and of course moreefﬁcient) than a precise representation","The approximate generalized IB objective in turn
presents interesting connections with the MLE principle",0
10122,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)","We then propose a new generalized
IB-based objective that takes into account the compression and relevance of all layers in the network
as an explicit goal for guiding the encodings in a beneﬁcial manner",0
10123,"The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)",content of the original space possibly including its dimensionality and topological structure,1
10124,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",A detail of gradient-based training of PIB ispresented in Algorithm 1,0
10125,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)","By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner",0
10126,"The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)","In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ",0
10127,content of the original space possibly including its dimensionality and topological structure,content of the original space possibly including its dimensionality and topological structure,1
10128,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,0
10129,content of the original space possibly including its dimensionality and topological structure,content of the original space possibly including its dimensionality and topological structure,1
10130,"Therefore, lossy representation is often more helpful (and of course moreefﬁcient) than a precise representation",content of the original space possibly including its dimensionality and topological structure,1
10131,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space","We propose re-using the existing neural network architecture
as variational decoders for each hidden layers",0
10132,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",content of the original space possibly including its dimensionality and topological structure,1
10133,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",content of the original space possibly including its dimensionality and topological structure,1
10134,The neural network in this perspectiveconsists of a series of stochastic encoders that sequentially encode the original data space X intothe intermediate code spaces Zl,content of the original space possibly including its dimensionality and topological structure,1
10135,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,0
10136,These code spaces are lossy representations of the data space as itfollows from the data-processing inequality (DPI) (Cover & Thomas (2006)) that,content of the original space possibly including its dimensionality and topological structure,1
10137,The neural network in this perspectiveconsists of a series of stochastic encoders that sequentially encode the original data space X intothe intermediate code spaces Zl,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",0
10138,content of the original space possibly including its dimensionality and topological structure,"Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",0
10139,"Therefore, lossy representation is often more helpful (and of course moreefﬁcient) than a precise representation",content of the original space possibly including its dimensionality and topological structure,1
10140,"Therefore, lossy representation is often more helpful (and of course moreefﬁcient) than a precise representation",content of the original space possibly including its dimensionality and topological structure,1
10141,The neural network in this perspectiveconsists of a series of stochastic encoders that sequentially encode the original data space X intothe intermediate code spaces Zl,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",0
10142,These code spaces are lossy representations of the data space as itfollows from the data-processing inequality (DPI) (Cover & Thomas (2006)) that,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",0
10143,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",content of the original space possibly including its dimensionality and topological structure,1
10144,These code spaces are lossy representations of the data space as itfollows from the data-processing inequality (DPI) (Cover & Thomas (2006)) that,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,0
10145,"Onaverage, 2H(X|Z) elements of X are mapped to the same code in Z",content of the original space possibly including its dimensionality and topological structure,1
10146,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",content of the original space possibly including its dimensionality and topological structure,1
10147,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",Learning phrase representations using rnn encoder-decoderfor statistical machine translation,0
10148,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space","arXiv preprint arXiv:1406.1078, 2014.",0
10149,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,0
10150,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",content of the original space possibly including its dimensionality and topological structure,1
10151,The neural network in this perspectiveconsists of a series of stochastic encoders that sequentially encode the original data space X intothe intermediate code spaces Zl,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
10152,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",content of the original space possibly including its dimensionality and topological structure,1
10153,"The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)","In the general IB framework, each value of β speciﬁes a uniquepoint of Z in the information plane",0
10154,"Onaverage, 2H(X|Z) elements of X are mapped to the same code in Z",content of the original space possibly including its dimensionality and topological structure,1
10155,"Onaverage, 2H(X|Z) elements of X are mapped to the same code in Z","Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",0
10156,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",content of the original space possibly including its dimensionality and topological structure,1
10157,"Therefore, lossy representation is often more helpful (and of course moreefﬁcient) than a precise representation",content of the original space possibly including its dimensionality and topological structure,1
10158,content of the original space possibly including its dimensionality and topological structure,URLhttp://arxiv.org/abs/1308.3432.,0
10159,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",content of the original space possibly including its dimensionality and topological structure,1
10160,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",content of the original space possibly including its dimensionality and topological structure,1
10161,content of the original space possibly including its dimensionality and topological structure,content of the original space possibly including its dimensionality and topological structure,1
10162,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task","In the following section, we presentour approximation to LP IB which fully utilizes the existing architecture without resorting to anymodel that is not part of the considered neural network",0
10163,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",content of the original space possibly including its dimensionality and topological structure,1
10164,content of the original space possibly including its dimensionality and topological structure,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",0
10165,The neural network in this perspectiveconsists of a series of stochastic encoders that sequentially encode the original data space X intothe intermediate code spaces Zl,content of the original space possibly including its dimensionality and topological structure,1
10166,These code spaces are lossy representations of the data space as itfollows from the data-processing inequality (DPI) (Cover & Thomas (2006)) that,"(2014), Bahdanau et al",0
10167,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)","In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",0
10168,"Therefore, lossy representation is often more helpful (and of course moreefﬁcient) than a precise representation",content of the original space possibly including its dimensionality and topological structure,1
10169,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",(2016) also uses variational methods to approximate the mutual infor-mation as an attempt to apply the IB principle to neural networks,0
10170,"Therefore, lossy representation is often more helpful (and of course moreefﬁcient) than a precise representation",We trained themodels in the full training set of 60000 images and tested in the test set,0
10171,"The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)","We propose re-using the existing neural network architecture
as variational decoders for each hidden layers",0
10172,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",content of the original space possibly including its dimensionality and topological structure,1
10173,content of the original space possibly including its dimensionality and topological structure,content of the original space possibly including its dimensionality and topological structure,1
10174,"Therefore, lossy representation is often more helpful (and of course moreefﬁcient) than a precise representation",content of the original space possibly including its dimensionality and topological structure,1
10175,"Onaverage, 2H(X|Z) elements of X are mapped to the same code in Z",content of the original space possibly including its dimensionality and topological structure,1
10176,These code spaces are lossy representations of the data space as itfollows from the data-processing inequality (DPI) (Cover & Thomas (2006)) that,"Therefore, lossy representation is often more helpful (and of course moreefﬁcient) than a precise representation",0
10177,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space","(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning",0
10178,"Onaverage, 2H(X|Z) elements of X are mapped to the same code in Z",ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,0
10179,These code spaces are lossy representations of the data space as itfollows from the data-processing inequality (DPI) (Cover & Thomas (2006)) that,Masteringthe game of go with deep neural networks and tree search,0
10180,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",content of the original space possibly including its dimensionality and topological structure,1
10181,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],0
10182,The neural network in this perspectiveconsists of a series of stochastic encoders that sequentially encode the original data space X intothe intermediate code spaces Zl,"Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.",0
10183,"The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)",content of the original space possibly including its dimensionality and topological structure,1
10184,These code spaces are lossy representations of the data space as itfollows from the data-processing inequality (DPI) (Cover & Thomas (2006)) that,Figure 1: A directed graphical representation of a PIB of two bottlenecks,0
10185,These code spaces are lossy representations of the data space as itfollows from the data-processing inequality (DPI) (Cover & Thomas (2006)) that,content of the original space possibly including its dimensionality and topological structure,1
10186,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
10187,The neural network in this perspectiveconsists of a series of stochastic encoders that sequentially encode the original data space X intothe intermediate code spaces Zl,content of the original space possibly including its dimensionality and topological structure,1
10188,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",content of the original space possibly including its dimensionality and topological structure,1
10189,content of the original space possibly including its dimensionality and topological structure,"(2015)), natural languagetranslation (e.g., Cho et al",0
10190,"Onaverage, 2H(X|Z) elements of X are mapped to the same code in Z","Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned",0
10191,"Therefore, lossy representation is often more helpful (and of course moreefﬁcient) than a precise representation",Batch normalization: Accelerating deep network training byreducing internal covariate shift,0
10192,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",0
10193,"Therefore, lossy representation is often more helpful (and of course moreefﬁcient) than a precise representation",Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,0
10194,content of the original space possibly including its dimensionality and topological structure,content of the original space possibly including its dimensionality and topological structure,1
10195,"The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",0
10196,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",content of the original space possibly including its dimensionality and topological structure,1
10197,"Therefore, lossy representation is often more helpful (and of course moreefﬁcient) than a precise representation","This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al",0
10198,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space","However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)",0
10199,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",content of the original space possibly including its dimensionality and topological structure,1
10200,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10201,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10202,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable","Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks",0
10203,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10204,Figure 1: A directed graphical representation of a PIB of two bottlenecks,"The optimal representation Z is determined via theminimization of the following Lagrangian:Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",0
10205,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",0
10206,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10207,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10208,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10209,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10210,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",0
10211,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",0
10212,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10213,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,"The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ",0
10214,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",0
10215,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,0
10216,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10217,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10218,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)","In optimization’s aspect, the minimization ofLP IB is much harder than the minimization of LIB since LP IB involves inter-dependent terms thateven the self-consistent equations of the IB framework are not applicable to this case",0
10219,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,0
10220,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",0
10221,Figure 1: A directed graphical representation of a PIB of two bottlenecks,"We propose re-using the existing neural network architecture
as variational decoders for each hidden layers",0
10222,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,0
10223,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,"However, thealgorithm is not applicable to neural networks",0
10224,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",Estimating or propagating gradientsthrough stochastic neurons for conditional computation,0
10225,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10226,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10227,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,0
10228,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10229,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,Courville,0
10230,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,0
10231,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,URL https://doi.org/10.1109/CVPR.2015.7298594.,0
10232,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10233,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10234,Figure 1: A directed graphical representation of a PIB of two bottlenecks,"(2014), Bahdanau et al",0
10235,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10236,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable","Previously, the InformationBottleneck (IB) framework (Tishby et al",0
10237,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",0
10238,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,0
10239,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10240,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10241,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,0
10242,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable","In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",0
10243,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10244,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,URL http://dl.acm.org/citation.cfm?id=2670313.,0
10245,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10246,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10247,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10248,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,0
10249,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10250,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10251,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10252,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",0
10253,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,0
10254,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10255,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10256,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10257,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable","Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al",0
10258,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",URLhttp://arxiv.org/abs/1308.3432.,0
10259,Figure 1: A directed graphical representation of a PIB of two bottlenecks,"(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
10260,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,0
10261,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)",0
10262,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10263,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable","In this work,we focus on binary bottlenecks where Zl ∈ {0, 1}nl and ni is the dimensionality of the bottleneckspace.",0
10264,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable","Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters",0
10265,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10266,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10267,Figure 1: A directed graphical representation of a PIB of two bottlenecks,"Speciﬁcally, the relevance decoder is determined as follows:",0
10268,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,0
10269,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10270,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)","For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
10271,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10272,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10273,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
10274,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10275,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10276,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10277,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10278,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
10279,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10280,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10281,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",0
10282,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10283,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,Each hidden layer in SFNNs is also considered as a stochastic variable,0
10284,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10285,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,"Thus, our framework does not depend on a speciﬁc stochastic model",0
10286,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable","For visualization of learned ﬁlters of PIB, seeAppendix II.A.",0
10287,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10288,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10289,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10290,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)","The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)",0
10291,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10292,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10293,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10294,Figure 1: A directed graphical representation of a PIB of two bottlenecks,"For visualization of learned ﬁlters of PIB, seeAppendix II.A.",0
10295,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)","770–778, 2016",0
10296,Figure 1: A directed graphical representation of a PIB of two bottlenecks,Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10297,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,"Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",0
10298,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable","Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
10299,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",Figure 1: A directed graphical representation of a PIB of two bottlenecks,1
10300,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,"We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
10301,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,"We scaled the images to [0, 1] and do not perform anyother data augmentation",0
10302,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10303,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",0
10304,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",0
10305,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10306,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,"The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",0
10307,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961",0
10308,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10309,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10310,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10311,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,"We extend the notations of Zl by using the convention Z0 := X and Z−1 := ∅.The space of X, Y and Zl are denoted as X ,Y and Zl, respectively",0
10312,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","CoRR, abs/1406.2989, 2014",0
10313,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10314,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","For PIBs, we set βl = β,∀1 ≤ l ≤ L",0
10315,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest",0
10316,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10317,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance",0
10318,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",0
10319,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10320,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014",0
10321,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,All models areimplemented using Theano framework (Al-Rfou et al,0
10322,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10323,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",0
10324,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,"(1999)) with the implicit selfwhere Z(x; β) is the normalization function, and DKL[.(cid:107).] is the Kullback - Leibler (KL) divergence
(Kullback & Leibler (1951))",0
10325,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10326,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10327,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10328,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,0
10329,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","arXiv preprint arXiv:1406.1078, 2014.",0
10330,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,This also empirically holds for the case of SFNN,0
10331,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","448–456, 2015",0
10332,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10333,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10334,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10335,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",Sergey Ioffe and Christian Szegedy,0
10336,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10337,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10338,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",0
10339,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ",0
10340,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))",0
10341,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10342,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",This also empirically holds for the case of SFNN,0
10343,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10344,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,0
10345,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10346,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10347,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","(1999)) with the implicit selfwhere Z(x; β) is the normalization function, and DKL[.(cid:107).] is the Kullback - Leibler (KL) divergence
(Kullback & Leibler (1951))",0
10348,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10349,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10350,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","Originally, the general IB framework is proposed in Tishby et al",0
10351,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,"Thus, our framework does not depend on a speciﬁc stochastic model",0
10352,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10353,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,"Although the analysis offers a new perspective about optimality in neural networks,it proposes general analysis of optimality rather than a practical optimization criteria",0
10354,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10355,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,0
10356,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10357,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10358,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",The en-coder maps the input variable xxx to a single bottleneck variable zzz that is not a part of the consideredneural network architecture,0
10359,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10360,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",0
10361,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
10362,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,"For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",0
10363,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","(2014)) and game playing (e.g., Silver et al.(2016))",0
10364,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10365,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10366,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10367,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","(2014), Bahdanau et al",0
10368,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10369,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10370,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10371,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2",0
10372,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,"CoRR, abs/1406.2989, 2014",0
10373,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
10374,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10375,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
10376,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,0
10377,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10378,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10379,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10380,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",0
10381,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,0
10382,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10383,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,0
10384,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10385,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10386,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,0
10387,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al",0
10388,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.","(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
10389,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10390,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,"In optimization’s aspect, the minimization ofLP IB is much harder than the minimization of LIB since LP IB involves inter-dependent terms thateven the self-consistent equations of the IB framework are not applicable to this case",0
10391,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,Information bottleneck for gaus-sian variables,0
10392,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",URLhttps://doi.org/10.1109/CVPR.2016.90.,0
10393,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
10394,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10395,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10396,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10397,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10398,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10399,"In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,1
10400,"In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck","(2014)) and game playing (e.g., Silver et al.(2016))",0
10401,(2016)).,Discrete-valued variables in PIBs make standard back-propagation not straightforward,0
10402,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space","Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.",0
10403,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10404,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10405,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10406,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10407,Our PIB is also a standard steptowards better exploiting representational power of more expressive neural network models such asConvolutional Neural Networks (LeCun et al,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10408,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10409,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10410,(2016)).,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10411,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10412,(1998)) and ResNet (He et al,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10413,"In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck","As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets",0
10414,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,"For stochastic sampling, we generate M = 16 samples per point during training andM = 32 samples per point during testing",0
10415,(1998)) and ResNet (He et al,"(2014)) and game playing (e.g., Silver et al.(2016))",0
10416,(1998)) and ResNet (He et al,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10417,"In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10418,"In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck","530–538, 2013",0
10419,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
10420,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),0
10421,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",0
10422,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10423,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J",0
10424,(2016)).,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10425,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,URLhttps://doi.org/10.1109/CVPR.2016.90.,0
10426,"In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10427,"In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10428,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10429,(1998)) and ResNet (He et al,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10430,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,URL http://arxiv.org/abs/1605.02688.,0
10431,(2016)).,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10432,"In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10433,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
10434,(1998)) and ResNet (He et al,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10435,(2016)).,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10436,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10437,Our PIB is also a standard steptowards better exploiting representational power of more expressive neural network models such asConvolutional Neural Networks (LeCun et al,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10438,"In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck","The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)",0
10439,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,"By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner",0
10440,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,The approximation then leads to effectivegradient-based training of PIBs.,0
10441,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10442,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",0
10443,(1998)) and ResNet (He et al,"As β varies from 0 to ∞, Z traces a concave curve, knownas information curve for representation Z, with a slope of β−1",0
10444,Our PIB is also a standard steptowards better exploiting representational power of more expressive neural network models such asConvolutional Neural Networks (LeCun et al,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10445,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10446,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",0
10447,(1998)) and ResNet (He et al,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10448,"In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck","For PIBs, we set βl = β,∀1 ≤ l ≤ L",0
10449,(1998)) and ResNet (He et al,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10450,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,"The optimal representation Z is determined via theminimization of the following Lagrangian:LIB[p(z|x)] = I(Z, X) − βI(Z, Y )",0
10451,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10452,(2016)).,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer",0
10453,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10454,(1998)) and ResNet (He et al,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10455,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space","Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest",0
10456,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10457,"In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",0
10458,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10459,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",Our PIB is also a standard steptowards better exploiting representational power of more expressive neural network models such asConvolutional Neural Networks (LeCun et al,0
10460,(1998)) and ResNet (He et al,All models areimplemented using Theano framework (Al-Rfou et al,0
10461,(2016)).,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10462,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10463,(1998)) and ResNet (He et al,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",0
10464,"In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10465,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,0
10466,Our PIB is also a standard steptowards better exploiting representational power of more expressive neural network models such asConvolutional Neural Networks (LeCun et al,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)",0
10467,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10468,Our PIB is also a standard steptowards better exploiting representational power of more expressive neural network models such asConvolutional Neural Networks (LeCun et al,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10469,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10470,Our PIB is also a standard steptowards better exploiting representational power of more expressive neural network models such asConvolutional Neural Networks (LeCun et al,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10471,"In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10472,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10473,(2016)).,"Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.",0
10474,"In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10475,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10476,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10477,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
10478,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",0
10479,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10480,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10481,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space","We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
10482,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10483,(1998)) and ResNet (He et al,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10484,(1998)) and ResNet (He et al,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
10485,(1998)) and ResNet (He et al,A,0
10486,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,"Previously, the InformationBottleneck (IB) framework (Tishby et al",0
10487,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10488,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10489,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",0
10490,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
10491,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,"The MLE principle treats
the neural network model p(xxx; θθθ) as a whole without explicitly considering the contribution of its
internal structures (e.g., hidden layers and hidden neurons)",0
10492,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10493,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",0
10494,"In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,0
10495,"In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10496,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10497,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10498,"In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,1
10499,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,"However, the variational approximatedistribution for the code space may be too loose while the actual marginal distribution can be sam-pled easily.Our work, on the other hand, focuses on better exploiting intermediate representations of a neuralnetwork architecture using the IB principle",0
10500,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10501,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned",(1998)) and ResNet (He et al,0
10502,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10503,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10504,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)",0
10505,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer","The plane has I(Z, X) and I(Z, Y ) as its horizontal axisand its vertical axis, respectively",0
10506,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,0
10507,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,0
10508,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
10509,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10510,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer","For deterministicneural networks, we only have one sample of hidden variables given one data point",0
10511,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer",0
10512,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10513,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10514,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10515,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10516,"Speciﬁcally, the relevance decoder is determined as follows:",(1998)) and ResNet (He et al,0
10517,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10518,"Speciﬁcally, the relevance decoder is determined as follows:","Here we preferto this extreme, i.e., the 0th level, as the super level",0
10519,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10520,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10521,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10522,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",Figure 1: A directed graphical representation of a PIB of two bottlenecks,0
10523,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10524,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",0
10525,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",0
10526,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10527,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10528,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10529,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10530,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",0
10531,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
10532,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",(1999)) extracts relevant information fora target variable,0
10533,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer","Previously, the InformationBottleneck (IB) framework (Tishby et al",0
10534,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer","The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)",0
10535,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10536,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned",Sergey Ioffe and Christian Szegedy,0
10537,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","David Silver, Aja Huang, Chris J",0
10538,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,0
10539,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer","The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",0
10540,"Speciﬁcally, the relevance decoder is determined as follows:","(1999)) with the implicit selfwhere Z(x; β) is the normalization function, and DKL[.(cid:107).] is the Kullback - Leibler (KL) divergence
(Kullback & Leibler (1951))",0
10541,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10542,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","As can be seen from the table, PIB and Model C givesnearly the same performance which outperform deterministic neural networks and SFNNs, and theirstochastic and deterministic version.It is interesting to empirically see that the deterministic version of PIB at test time (Model C) gives aslightly better result than PIB",0
10543,"Speciﬁcally, the relevance decoder is determined as follows:","In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ",0
10544,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10545,"Speciﬁcally, the relevance decoder is determined as follows:",URL http://www.jmlr.org/papers/v6/chechik05a.html.,0
10546,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","In estimating mutual information, we adopted the variational method asin Alemi et al",0
10547,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10548,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10549,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10550,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned",(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
10551,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10552,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10553,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network",A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,0
10554,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10555,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",0
10556,"Speciﬁcally, the relevance decoder is determined as follows:","It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
10557,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))",0
10558,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","249–256,2010",0
10559,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10560,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10561,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10562,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","(2014)) and game playing (e.g., Silver et al.(2016))",0
10563,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned",Our PIB is also a standard steptowards better exploiting representational power of more expressive neural network models such asConvolutional Neural Networks (LeCun et al,0
10564,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10565,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10566,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","In optimization’s aspect, the minimization ofLP IB is much harder than the minimization of LIB since LP IB involves inter-dependent terms thateven the self-consistent equations of the IB framework are not applicable to this case",0
10567,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","However, thealgorithm is not applicable to neural networks",0
10568,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",0
10569,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10570,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned",We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,0
10571,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","(2016) for I(Z, Y ) but use empirical estimation for I(Z, X)",0
10572,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
10573,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",0
10574,"Speciﬁcally, the relevance decoder is determined as follows:",This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,0
10575,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10576,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.",0
10577,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10578,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10579,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
10580,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10581,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",0
10582,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10583,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10584,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network",A,0
10585,"Speciﬁcally, the relevance decoder is determined as follows:","Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation",0
10586,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10587,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10588,"Speciﬁcally, the relevance decoder is determined as follows:","The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ",0
10589,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10590,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10591,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","Fortunately,one can estimate the gradient in this case",0
10592,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10593,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10594,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","1–9, 2015",0
10595,"Speciﬁcally, the relevance decoder is determined as follows:","In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",0
10596,"Speciﬁcally, the relevance decoder is determined as follows:","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10597,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",0
10598,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network","However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)",0
10599,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",1
10600,The generated bottleneck samples are then used to estimate mutual in-formation,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10601,"(2014), Dauphin & Grangier(2016))","In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",0
10602,"Thus, our framework does not depend on a speciﬁc stochastic model","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10603,"(2014), Dauphin & Grangier(2016))","(2013) proposed to resort to reinforcement learning.However, these estimators have high variance",0
10604,The generated bottleneck samples are then used to estimate mutual in-formation,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),0
10605,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )","Nitish Srivastava, Geoffrey E",0
10606,"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10607,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al","Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang",0
10608,"For deterministicneural networks, we only have one sample of hidden variables given one data point","For the compression, we
decompose the mutual information as follows:",0
10609,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )","Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
10610,The generated bottleneck samples are then used to estimate mutual in-formation,"For simplicity, we present the derivation of I(Z1, Z0) only 3",0
10611,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10612,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )","In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)",0
10613,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )","More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
10614,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )",Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,0
10615,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )","In this work, we use the gradient estimator inspiredby Raiko et al",0
10616,"Thus, our framework does not depend on a speciﬁc stochastic model","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10617,"Thus, our framework does not depend on a speciﬁc stochastic model","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10618,"Thus, our framework does not depend on a speciﬁc stochastic model","arXiv preprint arXiv:1406.1078, 2014.",0
10619,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al","448–456, 2015",0
10620,"Thus, our framework does not depend on a speciﬁc stochastic model","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10621,"Thus, our framework does not depend on a speciﬁc stochastic model",Model E is SFNN and Model B is Model C with deterministicprediction during test phase,0
10622,"Thus, our framework does not depend on a speciﬁc stochastic model",We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,0
10623,"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.","Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation",0
10624,"Thus, our framework does not depend on a speciﬁc stochastic model","However in general the two principles are not obviously related.
In this work, we leverage neural networks and the IB principle by viewing neural networks as a set
of encoders that sequentially modify the original data space",0
10625,"Thus, our framework does not depend on a speciﬁc stochastic model","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10626,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",The neural network in this perspectiveconsists of a series of stochastic encoders that sequentially encode the original data space X intothe intermediate code spaces Zl,0
10627,"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10628,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )","1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",0
10629,"Thus, our framework does not depend on a speciﬁc stochastic model","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10630,"For deterministicneural networks, we only have one sample of hidden variables given one data point","1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",0
10631,The generated bottleneck samples are then used to estimate mutual in-formation,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10632,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )","However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",0
10633,"(2014), Dauphin & Grangier(2016))","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10634,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10635,"Thus, our framework does not depend on a speciﬁc stochastic model","The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)",0
10636,"(2014), Dauphin & Grangier(2016))","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10637,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )","Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",0
10638,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )","Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp",0
10639,The generated bottleneck samples are then used to estimate mutual in-formation,"Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al",0
10640,"Thus, our framework does not depend on a speciﬁc stochastic model",doi: 10.1109/CVPR.2016.90,0
10641,"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10642,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10643,The generated bottleneck samples are then used to estimate mutual in-formation,The neural network in this perspectiveconsists of a series of stochastic encoders that sequentially encode the original data space X intothe intermediate code spaces Zl,0
10644,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10645,"For deterministicneural networks, we only have one sample of hidden variables given one data point","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10646,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,0
10647,The generated bottleneck samples are then used to estimate mutual in-formation,We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,0
10648,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10649,"For deterministicneural networks, we only have one sample of hidden variables given one data point","For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
10650,The generated bottleneck samples are then used to estimate mutual in-formation,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10651,"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",0
10652,"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10653,"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10654,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10655,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al","The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)",0
10656,"For deterministicneural networks, we only have one sample of hidden variables given one data point","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10657,"Thus, our framework does not depend on a speciﬁc stochastic model","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10658,"(2014), Dauphin & Grangier(2016))","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10659,The generated bottleneck samples are then used to estimate mutual in-formation,"We extend the notations of Zl by using the convention Z0 := X and Z−1 := ∅.The space of X, Y and Zl are denoted as X ,Y and Zl, respectively",0
10660,"For deterministicneural networks, we only have one sample of hidden variables given one data point","Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",0
10661,"(2014), Dauphin & Grangier(2016))","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10662,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al","Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis",0
10663,"(2014), Dauphin & Grangier(2016))",A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,0
10664,"Thus, our framework does not depend on a speciﬁc stochastic model","Yoshua Bengio, Nicholas L´eonard, and Aaron C",0
10665,The generated bottleneck samples are then used to estimate mutual in-formation,"The optimal representation Z is determined via theminimization of the following Lagrangian:Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",0
10666,"For deterministicneural networks, we only have one sample of hidden variables given one data point","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10667,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10668,"For deterministicneural networks, we only have one sample of hidden variables given one data point","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10669,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,0
10670,The generated bottleneck samples are then used to estimate mutual in-formation,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL",0
10671,"For deterministicneural networks, we only have one sample of hidden variables given one data point","In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",0
10672,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",0
10673,"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.",The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
10674,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10675,"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.","The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)",0
10676,"For deterministicneural networks, we only have one sample of hidden variables given one data point","For comparisons, we trained PIBs and ﬁve additional models",0
10677,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10678,"Thus, our framework does not depend on a speciﬁc stochastic model",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,0
10679,"Thus, our framework does not depend on a speciﬁc stochastic model","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10680,"(2014), Dauphin & Grangier(2016))","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10681,"(2014), Dauphin & Grangier(2016))","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10682,"(2014), Dauphin & Grangier(2016))","In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",0
10683,The generated bottleneck samples are then used to estimate mutual in-formation,URL https://doi.org/10.1109/CVPR.2015.7298594.,0
10684,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10685,The generated bottleneck samples are then used to estimate mutual in-formation,"(2014), Dauphin & Grangier(2016))",0
10686,"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.",Techniques for learningbinary stochastic feedforward neural networks,0
10687,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )","However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",0
10688,"For deterministicneural networks, we only have one sample of hidden variables given one data point","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10689,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10690,"(2014), Dauphin & Grangier(2016))","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10691,"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10692,"Thus, our framework does not depend on a speciﬁc stochastic model","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10693,"(2014), Dauphin & Grangier(2016))","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10694,"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
10695,"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.","As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time",0
10696,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10697,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al","Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned",0
10698,"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10699,"Thus, our framework does not depend on a speciﬁc stochastic model","It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",1
10700,"We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10701,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10702,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10703,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",URLhttps://doi.org/10.1109/CVPR.2016.90.,0
10704,"We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",URL https://doi.org/10.1038/nature16961.,0
10705,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10706,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
10707,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)",0
10708,"We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.",0
10709,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10710,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10711,"We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10712,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks",0
10713,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10714,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",All models areimplemented using Theano framework (Al-Rfou et al,0
10715,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10716,"We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.",0
10717,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
10718,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10719,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures",0
10720,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10721,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,0
10722,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",URL https://doi.org/10.1038/nature16961.,0
10723,We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10724,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10725,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",0
10726,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","In the following section, we presentour approximation to LP IB which fully utilizes the existing architecture without resorting to anymodel that is not part of the considered neural network",0
10727,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",0
10728,We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,"Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp",0
10729,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis",0
10730,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10731,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",0
10732,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10733,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10734,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10735,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)",0
10736,"We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10737,We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10738,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10739,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10740,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",URL https://doi.org/10.1038/nature16961.,0
10741,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10742,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",0
10743,"We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),0
10744,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10745,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10746,We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10747,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10748,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer",0
10749,We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,"For visualization of learned ﬁlters of PIB, seeAppendix II.A.",0
10750,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",Techniques for learningbinary stochastic feedforward neural networks,0
10751,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10752,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable",0
10753,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10754,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Previously, the InformationBottleneck (IB) framework (Tishby et al",0
10755,We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,0
10756,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",0
10757,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)",0
10758,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10759,We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10760,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10761,We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2",0
10762,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",URLhttps://doi.org/10.1109/CVPR.2016.90.,0
10763,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10764,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,0
10765,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10766,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,0
10767,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","(2012), Szegedy et al",0
10768,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
10769,"We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",Sergey Ioffe and Christian Szegedy,0
10770,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10771,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer",0
10772,"We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10773,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
10774,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","(2012), Szegedy et al",0
10775,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10776,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",The exactsolution to the minimization problem above is found (Tishby et al,0
10777,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10778,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),0
10779,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
10780,We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,"Since the objective is designed
to optimize all parameters of neural networks and is mainly motivated by the IB principle for deep
learning (Tishby & Zaslavsky (2015)), we name this method the Parametric Information Bottleneck
(PIB)",0
10781,"We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time",0
10782,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",Discrete-valued variables in PIBs make standard back-propagation not straightforward,0
10783,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",0
10784,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","In the MLE principle, we only need empirical samples of
the joint distribution to maximize the likelihood function of the model given the data",0
10785,We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,This workproposes using mutual information of a hidden layer with the input layer and the output layer toquantify the performance of DNNs,0
10786,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",0
10787,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10788,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10789,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10790,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10791,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10792,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10793,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",The generated bottleneck samples are then used to estimate mutual in-formation,0
10794,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10795,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","For PIB, we also usedβ−1l = β−1 = 10−4",0
10796,We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10797,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,0
10798,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ","We tuned β from{0} ∪ {10−i : 1 ≤ i ≤ 7}, and found β−1 = 10−4 works best.Table 1 provides the results in the MNIST classiﬁcation error in the test set for PIB and the com-parative models (A), (B), (C), (D), and (E)",0
10799,"We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",1
10800,The approximation then leads to effectivegradient-based training of PIBs.,"For PIB, we also usedβ−1l = β−1 = 10−4",0
10801,"In optimization’s aspect, the minimization ofLP IB is much harder than the minimization of LIB since LP IB involves inter-dependent terms thateven the self-consistent equations of the IB framework are not applicable to this case","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10802,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10803,The approximation then leads to effectivegradient-based training of PIBs.,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10804,The approximation then leads to effectivegradient-based training of PIBs.,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10805,"Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10806,"Here we preferto this extreme, i.e., the 0th level, as the super level","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10807,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10808,"The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ","For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
10809,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10810,"In optimization’s aspect, the minimization ofLP IB is much harder than the minimization of LIB since LP IB involves inter-dependent terms thateven the self-consistent equations of the IB framework are not applicable to this case","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10811,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10812,"However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10813,The approximation then leads to effectivegradient-based training of PIBs.,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10814,"Optimizing PIBs now becomes the minimization of LP IB(Z) which at-tempts to decrease I(Zl, Zl−1) and increase I(Zl, Y ) simultaneously","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10815,"However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10816,"Here we preferto this extreme, i.e., the 0th level, as the super level",Yichuan Tang and Ruslan Salakhutdinov,0
10817,"Optimizing PIBs now becomes the minimization of LP IB(Z) which at-tempts to decrease I(Zl, Zl−1) and increase I(Zl, Y ) simultaneously","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10818,"Optimizing PIBs now becomes the minimization of LP IB(Z) which at-tempts to decrease I(Zl, Zl−1) and increase I(Zl, Y ) simultaneously","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10819,"Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable","The optimal representation Z is determined via theminimization of the following Lagrangian:LIB[p(z|x)] = I(Z, X) − βI(Z, Y )",0
10820,The approximation then leads to effectivegradient-based training of PIBs.,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10821,"In optimization’s aspect, the minimization ofLP IB is much harder than the minimization of LIB since LP IB involves inter-dependent terms thateven the self-consistent equations of the IB framework are not applicable to this case","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10822,The approximation then leads to effectivegradient-based training of PIBs.,"Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest",0
10823,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)","The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",0
10824,"Optimizing PIBs now becomes the minimization of LP IB(Z) which at-tempts to decrease I(Zl, Zl−1) and increase I(Zl, Y ) simultaneously","CoRR, abs/1308.3432, 2013",0
10825,"Here we preferto this extreme, i.e., the 0th level, as the super level","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10826,"Optimizing PIBs now becomes the minimization of LP IB(Z) which at-tempts to decrease I(Zl, Zl−1) and increase I(Zl, Y ) simultaneously","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10827,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10828,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)","Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",0
10829,"In optimization’s aspect, the minimization ofLP IB is much harder than the minimization of LIB since LP IB involves inter-dependent terms thateven the self-consistent equations of the IB framework are not applicable to this case","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10830,"In optimization’s aspect, the minimization ofLP IB is much harder than the minimization of LIB since LP IB involves inter-dependent terms thateven the self-consistent equations of the IB framework are not applicable to this case","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10831,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)","However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)",0
10832,"Optimizing PIBs now becomes the minimization of LP IB(Z) which at-tempts to decrease I(Zl, Zl−1) and increase I(Zl, Y ) simultaneously","The MLE
principle is proved to be mathematically equivalent to the IB principle for the multinomial mixture
model for clustering problem when the input distribution X is uniform or has a large sample size
(Slonim & Weiss (2002))",0
10833,"Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10834,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10835,"However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10836,The approximation then leads to effectivegradient-based training of PIBs.,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10837,"Here we preferto this extreme, i.e., the 0th level, as the super level","Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",0
10838,"Optimizing PIBs now becomes the minimization of LP IB(Z) which at-tempts to decrease I(Zl, Zl−1) and increase I(Zl, Y ) simultaneously","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10839,"Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable","The MLE principle treats
the neural network model p(xxx; θθθ) as a whole without explicitly considering the contribution of its
internal structures (e.g., hidden layers and hidden neurons)",0
10840,The approximation then leads to effectivegradient-based training of PIBs.,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10841,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)","By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner",0
10842,"Optimizing PIBs now becomes the minimization of LP IB(Z) which at-tempts to decrease I(Zl, Zl−1) and increase I(Zl, Y ) simultaneously",Model E is SFNN and Model B is Model C with deterministicprediction during test phase,0
10843,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,0
10844,"Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10845,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)","While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)",0
10846,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10847,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)","Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",0
10848,The approximation then leads to effectivegradient-based training of PIBs.,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10849,"However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10850,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10851,"The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10852,"Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",0
10853,"However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10854,The approximation then leads to effectivegradient-based training of PIBs.,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10855,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10856,"However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10857,"In the following section, we presentour approximation to LP IB which fully utilizes the existing architecture without resorting to anymodel that is not part of the considered neural network","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10858,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10859,"The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ","(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
10860,"In the following section, we presentour approximation to LP IB which fully utilizes the existing architecture without resorting to anymodel that is not part of the considered neural network","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10861,"However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10862,"The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ",This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,0
10863,"However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)","drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
10864,"Here we preferto this extreme, i.e., the 0th level, as the super level","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",0
10865,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10866,"Here we preferto this extreme, i.e., the 0th level, as the super level","drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
10867,"Optimizing PIBs now becomes the minimization of LP IB(Z) which at-tempts to decrease I(Zl, Zl−1) and increase I(Zl, Y ) simultaneously","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10868,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10869,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10870,"However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10871,"Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10872,"The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ","The MLE principle treats
the neural network model p(xxx; θθθ) as a whole without explicitly considering the contribution of its
internal structures (e.g., hidden layers and hidden neurons)",0
10873,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)","drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
10874,"However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10875,"The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10876,"Here we preferto this extreme, i.e., the 0th level, as the super level","Thus, our framework does not depend on a speciﬁc stochastic model",0
10877,"Optimizing PIBs now becomes the minimization of LP IB(Z) which at-tempts to decrease I(Zl, Zl−1) and increase I(Zl, Y ) simultaneously","Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al",0
10878,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)",URL https://doi.org/10.1109/CVPR.2015.7298594.,0
10879,"Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable","The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ",0
10880,"Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10881,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)","(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
10882,"Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable",The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,0
10883,"Here we preferto this extreme, i.e., the 0th level, as the super level",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,0
10884,The approximation then leads to effectivegradient-based training of PIBs.,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10885,"The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ",Understanding the difﬁculty of training deep feedforward neuralnetworks,0
10886,"In optimization’s aspect, the minimization ofLP IB is much harder than the minimization of LIB since LP IB involves inter-dependent terms thateven the self-consistent equations of the IB framework are not applicable to this case","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10887,"Optimizing PIBs now becomes the minimization of LP IB(Z) which at-tempts to decrease I(Zl, Zl−1) and increase I(Zl, Y ) simultaneously","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10888,"Here we preferto this extreme, i.e., the 0th level, as the super level","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
10889,"However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)",The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,0
10890,"The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ","While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)",0
10891,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)","In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",0
10892,"However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10893,The approximation then leads to effectivegradient-based training of PIBs.,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10894,"Optimizing PIBs now becomes the minimization of LP IB(Z) which at-tempts to decrease I(Zl, Zl−1) and increase I(Zl, Y ) simultaneously","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10895,"Here we preferto this extreme, i.e., the 0th level, as the super level","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10896,"The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10897,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)","We then propose a new generalized
IB-based objective that takes into account the compression and relevance of all layers in the network
as an explicit goal for guiding the encodings in a beneﬁcial manner",0
10898,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10899,"In optimization’s aspect, the minimization ofLP IB is much harder than the minimization of LIB since LP IB involves inter-dependent terms thateven the self-consistent equations of the IB framework are not applicable to this case","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",1
10900,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10901,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10902,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10903,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10904,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",A detail of gradient-based training of PIB ispresented in Algorithm 1,0
10905,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,0
10906,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10907,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
10908,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10909,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned",0
10910,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10911,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10912,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10913,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10914,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10915,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
10916,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",Each hidden layer in SFNNs is also considered as a stochastic variable,0
10917,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",Each hidden layer in SFNNs is also considered as a stochastic variable,0
10918,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10919,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10920,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10921,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10922,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",0
10923,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10924,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,0
10925,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10926,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","(1999), Slonim (2003))",0
10927,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10928,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,0
10929,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
10930,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10931,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10932,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",0
10933,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)",0
10934,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10935,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10936,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10937,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10938,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",0
10939,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
10940,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10941,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10942,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)",Learning phrase representations using rnn encoder-decoderfor statistical machine translation,0
10943,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10944,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)",0
10945,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","More speciﬁcally, our work proposes an optimizationIB criteria for an existing neural network architecture in an effort to better learn the layers’ represen-tation to their IB optimality",0
10946,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","Nitish Srivastava, Geoffrey E",0
10947,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10948,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)",URL http://dl.acm.org/citation.cfm?id=2670313.,0
10949,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10950,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10951,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","CoRR, abs/1308.3432, 2013",0
10952,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10953,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10954,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10955,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable",0
10956,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10957,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10958,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",0
10959,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10960,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network",0
10961,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10962,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10963,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",Sergey Ioffe and Christian Szegedy,0
10964,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10965,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",The MLE principle maximizes the likelihood function under theempirical data distribution,0
10966,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10967,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",0
10968,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner",0
10969,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P",0
10970,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,0
10971,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",0
10972,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.",0
10973,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10974,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10975,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10976,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",0
10977,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)",Sergey Ioffe and Christian Szegedy,0
10978,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10979,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10980,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,0
10981,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","For the compression, we
decompose the mutual information as follows:",0
10982,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis",0
10983,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10984,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10985,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","The MLE
principle is proved to be mathematically equivalent to the IB principle for the multinomial mixture
model for clustering problem when the input distribution X is uniform or has a large sample size
(Slonim & Weiss (2002))",0
10986,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10987,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","The approximate generalized IB objective in turn
presents interesting connections with the MLE principle",0
10988,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10989,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10990,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,0
10991,"Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10992,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)",(1998)) and ResNet (He et al,0
10993,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10994,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10995,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
10996,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )",0
10997,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
10998,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","(1999)); or (2) X, Yand Z are mutually joint Gaussian (Chechik et al",0
10999,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",1
11000,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11001,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively",0
11002,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","However, the MLE principle is very generic thatis not specially tailored for neural networks",0
11003,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,0
11004,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",0
11005,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
11006,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11007,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,0
11008,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",0
11009,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11010,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11011,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11012,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",0
11013,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11014,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",Theano: A Pythonframework for fast computation of mathematical expressions,0
11015,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","However, the variational approximatedistribution for the code space may be too loose while the actual marginal distribution can be sam-pled easily.Our work, on the other hand, focuses on better exploiting intermediate representations of a neuralnetwork architecture using the IB principle",0
11016,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11017,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11018,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11019,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11020,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",(2016) also uses variational methods to approximate the mutual infor-mation as an attempt to apply the IB principle to neural networks,0
11021,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11022,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11023,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11024,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al",0
11025,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","Nitish Srivastava, Geoffrey E",0
11026,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)",0
11027,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",A detail of gradient-based training of PIB ispresented in Algorithm 1,0
11028,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",(21)where |Z1| denotes the cardinality of the space of variable Z1,0
11029,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11030,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11031,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11032,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11033,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11034,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11035,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","The MLE
principle is proved to be mathematically equivalent to the IB principle for the multinomial mixture
model for clustering problem when the input distribution X is uniform or has a large sample size
(Slonim & Weiss (2002))",0
11036,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11037,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11038,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",0
11039,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961",0
11040,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11041,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",The MLE principle maximizes the likelihood function under theempirical data distribution,0
11042,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11043,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11044,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11045,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11046,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11047,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","In the MLE principle, we only need empirical samples of
the joint distribution to maximize the likelihood function of the model given the data",0
11048,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11049,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11050,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11051,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11052,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11053,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11054,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11055,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11056,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11057,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11058,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,0
11059,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11060,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","(2014), Dauphin & Grangier(2016))",0
11061,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",0
11062,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","448–456, 2015",0
11063,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",0
11064,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11065,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11066,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11067,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",URL http://www.jmlr.org/papers/v6/chechik05a.html.,0
11068,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11069,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11070,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",0
11071,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11072,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11073,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11074,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11075,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11076,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11077,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution ˆpD as possible (e.g., see Appendix I.B)",0
11078,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11079,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11080,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",0
11081,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11082,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",Techniques for learningbinary stochastic feedforward neural networks,0
11083,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11084,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","However, the MLE principle is very generic thatis not specially tailored for neural networks",0
11085,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11086,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11087,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","Onaverage, 2H(X|Z) elements of X are mapped to the same code in Z",0
11088,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",0
11089,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",URL https://doi.org/10.1109/CVPR.2015.7298594.,0
11090,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11091,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11092,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11093,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","(2014), Bahdanau et al",0
11094,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11095,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11096,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11097,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",(1999)) extracts relevant information fora target variable,0
11098,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",1
11099,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",A detail of gradient-based training of PIB ispresented in Algorithm 1,0
11100,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =","We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
11101,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),The neural network in this perspectiveconsists of a series of stochastic encoders that sequentially encode the original data space X intothe intermediate code spaces Zl,0
11102,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)","335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",0
11103,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11104,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11105,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),"In Model D, we used the weight trained in Model A to performstochastic prediction at test time",0
11106,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11107,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11108,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.",0
11109,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture","We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
11110,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)","(2012), Szegedy et al",0
11111,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =","530–538, 2013",0
11112,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11113,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,0
11114,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,0
11115,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture","In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",0
11116,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11117,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11118,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11119,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =","More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
11120,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =","We then propose a new generalized
IB-based objective that takes into account the compression and relevance of all layers in the network
as an explicit goal for guiding the encodings in a beneﬁcial manner",0
11121,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11122,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11123,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11124,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11125,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11126,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11127,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",0
11128,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11129,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11130,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11131,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11132,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),"informative yet compressed representation, which is supported by qualitative empirical results",0
11133,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11134,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture","Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance",0
11135,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =","More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
11136,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11137,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture","For the compression, we
decompose the mutual information as follows:",0
11138,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11139,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11140,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11141,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",0
11142,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11143,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11144,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,0
11145,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture","The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",0
11146,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",0
11147,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11148,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11149,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture","In this work, we use the gradient estimator inspiredby Raiko et al",0
11150,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =","In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)",0
11151,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11152,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)","Onaverage, 2H(X|Z) elements of X are mapped to the same code in Z",0
11153,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)","Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang",0
11154,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11155,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11156,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture","Fortunately,one can estimate the gradient in this case",0
11157,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =","(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning",0
11158,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =","For the compression, we
decompose the mutual information as follows:",0
11159,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)","informative yet compressed representation, which is supported by qualitative empirical results",0
11160,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11161,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11162,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,0
11163,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture","For stochastic sampling, we generate M = 16 samples per point during training andM = 32 samples per point during testing",0
11164,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)","In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",0
11165,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture","As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets",0
11166,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,0
11167,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture","For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
11168,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,0
11169,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture","In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",0
11170,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =","For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
11171,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
11172,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11173,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11174,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)",This workproposes using mutual information of a hidden layer with the input layer and the output layer toquantify the performance of DNNs,0
11175,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",0
11176,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11177,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture","For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",0
11178,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11179,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11180,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11181,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =","530–538, 2013",0
11182,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11183,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)","In the following section, we presentour approximation to LP IB which fully utilizes the existing architecture without resorting to anymodel that is not part of the considered neural network",0
11184,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)","In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)",0
11185,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11186,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture","Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
11187,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture","The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",0
11188,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11189,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11190,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
11191,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11192,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11193,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =","For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",0
11194,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11195,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture","For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",0
11196,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),1
11197,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),"For comparisons, we trained PIBs and ﬁve additional models",0
11198,"In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)","Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich",0
11199,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),The exactsolution to the minimization problem above is found (Tishby et al,0
11200,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",Learning stochastic feedforward neural networks,0
11201,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2","In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ",0
11202,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL","For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
11203,"For simplicity, we present the derivation of I(Z1, Z0) only 3","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11204,"For the compression, we
decompose the mutual information as follows:","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11205,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL","Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp",0
11206,"For simplicity, we present the derivation of I(Z1, Z0) only 3","However, thealgorithm is not applicable to neural networks",0
11207,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11208,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2","Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J",0
11209,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks",Batch normalization: Accelerating deep network training byreducing internal covariate shift,0
11210,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11211,"For simplicity, we present the derivation of I(Z1, Z0) only 3","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11212,"For the compression, we
decompose the mutual information as follows:","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11213,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks","Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting",0
11214,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11215,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11216,"For simplicity, we present the derivation of I(Z1, Z0) only 3","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11217,"For the compression, we
decompose the mutual information as follows:","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11218,"For the compression, we
decompose the mutual information as follows:","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11219,"For the compression, we
decompose the mutual information as follows:","In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",0
11220,"For simplicity, we present the derivation of I(Z1, Z0) only 3","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11221,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11222,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2","For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",0
11223,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11224,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2","For deterministicneural networks, we only have one sample of hidden variables given one data point",0
11225,"For simplicity, we present the derivation of I(Z1, Z0) only 3",The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,0
11226,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11227,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11228,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11229,"For simplicity, we present the derivation of I(Z1, Z0) only 3",The en-coder maps the input variable xxx to a single bottleneck variable zzz that is not a part of the consideredneural network architecture,0
11230,"For simplicity, we present the derivation of I(Z1, Z0) only 3","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11231,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11232,"For simplicity, we present the derivation of I(Z1, Z0) only 3","530–538, 2013",0
11233,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2",We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,0
11234,"For simplicity, we present the derivation of I(Z1, Z0) only 3",doi: 10.1109/CVPR.2015.7298594,0
11235,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",0
11236,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11237,"For simplicity, we present the derivation of I(Z1, Z0) only 3","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11238,"For the compression, we
decompose the mutual information as follows:","We propose re-using the existing neural network architecture
as variational decoders for each hidden layers",0
11239,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
11240,"For simplicity, we present the derivation of I(Z1, Z0) only 3","Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",0
11241,"For simplicity, we present the derivation of I(Z1, Z0) only 3","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11242,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2","(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))",0
11243,"For simplicity, we present the derivation of I(Z1, Z0) only 3","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11244,"For the compression, we
decompose the mutual information as follows:",We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,0
11245,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11246,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL","For visualization of learned ﬁlters of PIB, seeAppendix II.A.",0
11247,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1","In the next subsection, we describe in details how a sequentialseries of encoders, compression and relevance are deﬁned in a neural network.",0
11248,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11249,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",0
11250,"For the compression, we
decompose the mutual information as follows:","However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )",0
11251,"For the compression, we
decompose the mutual information as follows:","335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",0
11252,"For simplicity, we present the derivation of I(Z1, Z0) only 3","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11253,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11254,"For simplicity, we present the derivation of I(Z1, Z0) only 3","For PIB, we used β−1l = β−1 = 10−4",0
11255,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL",(1999)) extracts relevant information fora target variable,0
11256,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11257,"For simplicity, we present the derivation of I(Z1, Z0) only 3",networks and averaged the mutual informations,0
11258,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,0
11259,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11260,"For the compression, we
decompose the mutual information as follows:","The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",0
11261,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2",In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,0
11262,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1","In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",0
11263,"For simplicity, we present the derivation of I(Z1, Z0) only 3","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11264,"For the compression, we
decompose the mutual information as follows:","(2012), Szegedy et al",0
11265,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11266,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11267,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL","The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)",0
11268,"For the compression, we
decompose the mutual information as follows:","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11269,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11270,"For simplicity, we present the derivation of I(Z1, Z0) only 3","In the following section, we presentour approximation to LP IB which fully utilizes the existing architecture without resorting to anymodel that is not part of the considered neural network",0
11271,"For simplicity, we present the derivation of I(Z1, Z0) only 3","Journal of Machine Learning Research, 6:165–188, 2005",0
11272,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL","(2014)) and game playing (e.g., Silver et al.(2016))",0
11273,"For the compression, we
decompose the mutual information as follows:",Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,0
11274,"For the compression, we
decompose the mutual information as follows:","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11275,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11276,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11277,"For simplicity, we present the derivation of I(Z1, Z0) only 3","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11278,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11279,"For the compression, we
decompose the mutual information as follows:","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11280,"For the compression, we
decompose the mutual information as follows:","Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
11281,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL","Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",0
11282,"For simplicity, we present the derivation of I(Z1, Z0) only 3","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11283,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11284,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11285,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",0
11286,"For the compression, we
decompose the mutual information as follows:","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11287,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",URL http://dl.acm.org/citation.cfm?id=2670313.,0
11288,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11289,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11290,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
11291,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11292,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks",Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,0
11293,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11294,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11295,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks",(2016) also uses variational methods to approximate the mutual infor-mation as an attempt to apply the IB principle to neural networks,0
11296,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,0
11297,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11298,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",1
11299,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",0
11300,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",0
11301,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11302,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11303,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11304,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
11305,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11306,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11307,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
11308,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11309,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","Originally, the general IB framework is proposed in Tishby et al",0
11310,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11311,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11312,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",0
11313,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,0
11314,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11315,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11316,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
11317,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11318,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","(1999), Slonim (2003))",0
11319,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11320,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","For the compression, we
decompose the mutual information as follows:",0
11321,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11322,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","The optimal representation Z is determined via theminimization of the following Lagrangian:Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",0
11323,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11324,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11325,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner",0
11326,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,0
11327,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","For deterministicneural networks, we only have one sample of hidden variables given one data point",0
11328,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",networks and averaged the mutual informations,0
11329,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",0
11330,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11331,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","For PIB, we also usedβ−1l = β−1 = 10−4",0
11332,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11333,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11334,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
11335,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",Information bottleneck for gaus-sian variables,0
11336,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11337,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11338,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11339,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11340,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11341,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",0
11342,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","(1999), Slonim (2003))",0
11343,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11344,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11345,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",0
11346,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11347,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ",0
11348,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11349,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","We extend the notations of Zl by using the convention Z0 := X and Z−1 := ∅.The space of X, Y and Zl are denoted as X ,Y and Zl, respectively",0
11350,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",0
11351,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",Learning stochastic feedforward neural networks,0
11352,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11353,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11354,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11355,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",0
11356,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11357,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11358,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),0
11359,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11360,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,0
11361,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
11362,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11363,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11364,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","(2015)), natural languagetranslation (e.g., Cho et al",0
11365,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang",0
11366,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11367,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11368,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","In this work, we use the gradient estimator inspiredby Raiko et al",0
11369,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,0
11370,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)",0
11371,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",0
11372,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",(21)where |Z1| denotes the cardinality of the space of variable Z1,0
11373,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11374,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest",0
11375,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,0
11376,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","Therefore, lossy representation is often more helpful (and of course moreefﬁcient) than a precise representation",0
11377,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11378,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11379,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.",0
11380,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11381,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11382,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11383,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","Furthermore,estimating mutual information between the variables transformed by network layers and the datavariables poses several computational challenges in practice that the authors did not address in thework",0
11384,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11385,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),0
11386,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11387,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014",0
11388,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11389,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11390,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11391,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",URLhttps://doi.org/10.1109/CVPR.2016.90.,0
11392,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al",0
11393,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11394,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",1
11395,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","The MLE principle treats
the neural network model p(xxx; θθθ) as a whole without explicitly considering the contribution of its
internal structures (e.g., hidden layers and hidden neurons)",0
11396,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,0
11397,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",Information bottleneck for gaus-sian variables,0
11398,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)","Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T",0
11399,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1","For stochastic sampling, we generate M = 16 samples per point during training andM = 32 samples per point during testing",0
11400,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
11401,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11402,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),"(2015)), natural languagetranslation (e.g., Cho et al",0
11403,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],Masteringthe game of go with deep neural networks and tree search,0
11404,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",0
11405,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
11406,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11407,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11408,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
11409,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],The ﬁrst model (ModelA) is a deterministic neural network,0
11410,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],"For the compression, we
decompose the mutual information as follows:",0
11411,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11412,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11413,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11414,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,0
11415,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11416,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),"Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution",0
11417,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],(21)where |Z1| denotes the cardinality of the space of variable Z1,0
11418,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],URL https://doi.org/10.1109/CVPR.2015.7298594.,0
11419,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11420,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11421,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),"In estimating mutual information, we adopted the variational method asin Alemi et al",0
11422,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11423,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],"770–778, 2016",0
11424,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer",0
11425,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",0
11426,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11427,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),0
11428,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11429,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11430,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,0
11431,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11432,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,0
11433,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),"Nitish Srivastava, Geoffrey E",0
11434,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],"In the general IB framework, each value of β speciﬁes a uniquepoint of Z in the information plane",0
11435,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11436,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11437,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11438,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],where the equality in Equation 12 holds due to the Markov assumption (Equation 4),0
11439,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",0
11440,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11441,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,0
11442,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),"The optimal representation Z is determined via theminimization of the following Lagrangian:LIB[p(z|x)] = I(Z, X) − βI(Z, Y )",0
11443,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11444,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],"Previously, the InformationBottleneck (IB) framework (Tishby et al",0
11445,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11446,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11447,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11448,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11449,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11450,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,0
11451,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11452,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11453,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11454,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11455,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11456,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),Information bottleneck for gaus-sian variables,0
11457,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
11458,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",0
11459,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11460,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",0
11461,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11462,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11463,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11464,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11465,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,0
11466,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11467,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,0
11468,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11469,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),content of the original space possibly including its dimensionality and topological structure,0
11470,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
11471,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11472,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11473,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),"We tuned β from{0} ∪ {10−i : 1 ≤ i ≤ 7}, and found β−1 = 10−4 works best.Table 1 provides the results in the MNIST classiﬁcation error in the test set for PIB and the com-parative models (A), (B), (C), (D), and (E)",0
11474,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
11475,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11476,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11477,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11478,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11479,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11480,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer",0
11481,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11482,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11483,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11484,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11485,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],doi: 10.1109/CVPR.2016.90,0
11486,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),"More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
11487,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11488,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11489,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11490,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",0
11491,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11492,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11493,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11494,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
11495,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),"530–538, 2013",0
11496,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11497,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),(1999),0
11498,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11499,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],1
11500,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11501,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",0
11502,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable","Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al",0
11503,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable","Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al",0
11504,(21)where |Z1| denotes the cardinality of the space of variable Z1,"We scaled the images to [0, 1] and do not perform anyother data augmentation",0
11505,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",Courville,0
11506,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11507,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|","Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",0
11508,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11509,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11510,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11511,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11512,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11513,(21)where |Z1| denotes the cardinality of the space of variable Z1,We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,0
11514,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11515,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11516,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11517,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11518,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11519,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11520,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11521,(21)where |Z1| denotes the cardinality of the space of variable Z1,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned",0
11522,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)","448–456, 2015",0
11523,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11524,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11525,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11526,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11527,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,0
11528,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,0
11529,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11530,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
11531,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,0
11532,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11533,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,0
11534,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)","(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))",0
11535,(21)where |Z1| denotes the cardinality of the space of variable Z1,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",0
11536,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11537,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11538,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11539,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11540,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11541,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11542,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11543,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11544,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11545,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|","For visualization of learned ﬁlters of PIB, seeAppendix II.A.",0
11546,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable","Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation",0
11547,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|","Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P",0
11548,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable","(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
11549,(21)where |Z1| denotes the cardinality of the space of variable Z1,"The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ",0
11550,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11551,(21)where |Z1| denotes the cardinality of the space of variable Z1,"The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",0
11552,(21)where |Z1| denotes the cardinality of the space of variable Z1,Sergey Ioffe and Christian Szegedy,0
11553,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11554,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",0
11555,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11556,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11557,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11558,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",0
11559,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11560,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)","Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",0
11561,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable","In this work, we use the gradient estimator inspiredby Raiko et al",0
11562,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11563,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,0
11564,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11565,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable","More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
11566,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|","We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
11567,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)","1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",0
11568,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11569,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",0
11570,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|","In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",0
11571,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",0
11572,(21)where |Z1| denotes the cardinality of the space of variable Z1,"249–256,2010",0
11573,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11574,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11575,(21)where |Z1| denotes the cardinality of the space of variable Z1,Information bottleneck for gaus-sian variables,0
11576,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,0
11577,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,0
11578,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11579,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",This also empirically holds for the case of SFNN,0
11580,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11581,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11582,(21)where |Z1| denotes the cardinality of the space of variable Z1,(2016)).,0
11583,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11584,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|","The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",0
11585,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11586,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11587,(21)where |Z1| denotes the cardinality of the space of variable Z1,(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11588,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)","Originally, the general IB framework is proposed in Tishby et al",0
11589,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",(1998)) and ResNet (He et al,0
11590,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",Our PIB is also a standard steptowards better exploiting representational power of more expressive neural network models such asConvolutional Neural Networks (LeCun et al,0
11591,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
11592,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|","In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",0
11593,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",0
11594,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,0
11595,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11596,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)","arXiv preprint arXiv:1406.1078, 2014.",0
11597,(21)where |Z1| denotes the cardinality of the space of variable Z1,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",0
11598,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",(21)where |Z1| denotes the cardinality of the space of variable Z1,1
11599,(21)where |Z1| denotes the cardinality of the space of variable Z1,"In the following section, we presentour approximation to LP IB which fully utilizes the existing architecture without resorting to anymodel that is not part of the considered neural network",0
11600,Discrete-valued variables in PIBs make standard back-propagation not straightforward,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
11601,"In this work, we use the gradient estimator inspiredby Raiko et al",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11602,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,Figure 1: A directed graphical representation of a PIB of two bottlenecks,0
11603,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11604,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11605,Discrete-valued variables in PIBs make standard back-propagation not straightforward,"For stochastic sampling, we generate M = 16 samples per point during training andM = 32 samples per point during testing",0
11606,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where","The optimal representation Z is determined via theminimization of the following Lagrangian:LIB[p(z|x)] = I(Z, X) − βI(Z, Y )",0
11607,"Fortunately,one can estimate the gradient in this case",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11608,Discrete-valued variables in PIBs make standard back-propagation not straightforward,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11609,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
11610,"In this work, we use the gradient estimator inspiredby Raiko et al","In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",0
11611,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11612,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where","For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",0
11613,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11614,"In this work, we use the gradient estimator inspiredby Raiko et al",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11615,"In this work, we use the gradient estimator inspiredby Raiko et al",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11616,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11617,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11618,Discrete-valued variables in PIBs make standard back-propagation not straightforward,Yichuan Tang and Ruslan Salakhutdinov,0
11619,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where","Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting",0
11620,"(2013) proposed to resort to reinforcement learning.However, these estimators have high variance","Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",0
11621,Discrete-valued variables in PIBs make standard back-propagation not straightforward,"Previously, the InformationBottleneck (IB) framework (Tishby et al",0
11622,"In this work, we use the gradient estimator inspiredby Raiko et al","arXiv e-prints, abs/1605.02688,May 2016",0
11623,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11624,Discrete-valued variables in PIBs make standard back-propagation not straightforward,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11625,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,"McGibbon, Roland Memisevic, Bart van Merri¨enboer, Vincent Michalski, MehdiMirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel,Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Sal-vatier, Franc¸ois Savard, Jan Schl¨uter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,Dmitriy Serdyuk, Samira Shabanian, ´Etienne Simon, Sigurd Spieckermann, S",0
11626,Discrete-valued variables in PIBs make standard back-propagation not straightforward,"(2014), Bahdanau et al",0
11627,"(2013) proposed to resort to reinforcement learning.However, these estimators have high variance","(1999), Slonim (2003))",0
11628,"Fortunately,one can estimate the gradient in this case","For simplicity, we present the derivation of I(Z1, Z0) only 3",0
11629,"In this work, we use the gradient estimator inspiredby Raiko et al","For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
11630,Discrete-valued variables in PIBs make standard back-propagation not straightforward,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11631,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where","In this work, we use the gradient estimator inspiredby Raiko et al",0
11632,Discrete-valued variables in PIBs make standard back-propagation not straightforward,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11633,"In this work, we use the gradient estimator inspiredby Raiko et al","Nitish Srivastava, Geoffrey E",0
11634,"In this work, we use the gradient estimator inspiredby Raiko et al",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11635,Discrete-valued variables in PIBs make standard back-propagation not straightforward,"(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))",0
11636,"In this work, we use the gradient estimator inspiredby Raiko et al",The en-coder maps the input variable xxx to a single bottleneck variable zzz that is not a part of the consideredneural network architecture,0
11637,"(2013) proposed to resort to reinforcement learning.However, these estimators have high variance",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11638,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp",0
11639,Discrete-valued variables in PIBs make standard back-propagation not straightforward,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11640,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",0
11641,"In this work, we use the gradient estimator inspiredby Raiko et al",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11642,"Fortunately,one can estimate the gradient in this case",Courville,0
11643,"(2013) proposed to resort to reinforcement learning.However, these estimators have high variance",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11644,"(2013) proposed to resort to reinforcement learning.However, these estimators have high variance",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11645,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11646,"Fortunately,one can estimate the gradient in this case",The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,0
11647,"In this work, we use the gradient estimator inspiredby Raiko et al","For PIB, we used β−1l = β−1 = 10−4",0
11648,Discrete-valued variables in PIBs make standard back-propagation not straightforward,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11649,"Fortunately,one can estimate the gradient in this case","Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)",0
11650,"Fortunately,one can estimate the gradient in this case","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",0
11651,"In this work, we use the gradient estimator inspiredby Raiko et al",URL https://doi.org/10.1109/CVPR.2015.7298594.,0
11652,"(2013) proposed to resort to reinforcement learning.However, these estimators have high variance","Thus, our framework does not depend on a speciﬁc stochastic model",0
11653,Discrete-valued variables in PIBs make standard back-propagation not straightforward,We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,0
11654,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,Techniques for learningbinary stochastic feedforward neural networks,0
11655,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11656,Discrete-valued variables in PIBs make standard back-propagation not straightforward,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11657,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where","Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
11658,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11659,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)",0
11660,"Fortunately,one can estimate the gradient in this case",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11661,"Fortunately,one can estimate the gradient in this case",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11662,Discrete-valued variables in PIBs make standard back-propagation not straightforward,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11663,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",Information bottleneck for gaus-sian variables,0
11664,"(2013) proposed to resort to reinforcement learning.However, these estimators have high variance","Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014",0
11665,"Fortunately,one can estimate the gradient in this case",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11666,"In this work, we use the gradient estimator inspiredby Raiko et al",content of the original space possibly including its dimensionality and topological structure,0
11667,"Fortunately,one can estimate the gradient in this case",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,0
11668,"In this work, we use the gradient estimator inspiredby Raiko et al",Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,0
11669,"Fortunately,one can estimate the gradient in this case","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",0
11670,Discrete-valued variables in PIBs make standard back-propagation not straightforward,These code spaces are lossy representations of the data space as itfollows from the data-processing inequality (DPI) (Cover & Thomas (2006)) that,0
11671,"Fortunately,one can estimate the gradient in this case",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11672,"In this work, we use the gradient estimator inspiredby Raiko et al","(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
11673,"(2013) proposed to resort to reinforcement learning.However, these estimators have high variance",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11674,"In this work, we use the gradient estimator inspiredby Raiko et al",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11675,"Fortunately,one can estimate the gradient in this case","Furthermore,estimating mutual information between the variables transformed by network layers and the datavariables poses several computational challenges in practice that the authors did not address in thework",0
11676,"(2013) proposed to resort to reinforcement learning.However, these estimators have high variance","1–9, 2015",0
11677,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11678,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11679,Discrete-valued variables in PIBs make standard back-propagation not straightforward,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11680,"(2013) proposed to resort to reinforcement learning.However, these estimators have high variance","770–778, 2016",0
11681,"Fortunately,one can estimate the gradient in this case",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11682,Discrete-valued variables in PIBs make standard back-propagation not straightforward,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11683,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where","For simplicity, we present the derivation of I(Z1, Z0) only 3",0
11684,Discrete-valued variables in PIBs make standard back-propagation not straightforward,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),0
11685,"(2013) proposed to resort to reinforcement learning.However, these estimators have high variance",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11686,"In this work, we use the gradient estimator inspiredby Raiko et al",The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
11687,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,"arXiv preprint arXiv:1406.1078, 2014.",0
11688,Discrete-valued variables in PIBs make standard back-propagation not straightforward,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11689,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11690,"Fortunately,one can estimate the gradient in this case",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11691,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,0
11692,"In this work, we use the gradient estimator inspiredby Raiko et al","Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.",0
11693,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),0
11694,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11695,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,0
11696,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11697,"Fortunately,one can estimate the gradient in this case",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11698,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11699,"Fortunately,one can estimate the gradient in this case",Discrete-valued variables in PIBs make standard back-propagation not straightforward,1
11700,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,Discrete-valued variables in PIBs make standard back-propagation not straightforward,0
11701,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11702,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11703,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,These code spaces are lossy representations of the data space as itfollows from the data-processing inequality (DPI) (Cover & Thomas (2006)) that,0
11704,A detail of gradient-based training of PIB ispresented in Algorithm 1,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
11705,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11706,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11707,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,"For PIB, we used β−1l = β−1 = 10−4",0
11708,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time",0
11709,A detail of gradient-based training of PIB ispresented in Algorithm 1,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
11710,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11711,A detail of gradient-based training of PIB ispresented in Algorithm 1,doi: 10.1109/CVPR.2015.7298594,0
11712,A detail of gradient-based training of PIB ispresented in Algorithm 1,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11713,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11714,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11715,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,0
11716,A detail of gradient-based training of PIB ispresented in Algorithm 1,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11717,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11718,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,Techniques for learningbinary stochastic feedforward neural networks,0
11719,A detail of gradient-based training of PIB ispresented in Algorithm 1,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11720,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11721,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,"We scaled the images to [0, 1] and do not perform anyother data augmentation",0
11722,A detail of gradient-based training of PIB ispresented in Algorithm 1,URL http://arxiv.org/abs/1605.02688.,0
11723,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,"More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
11724,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11725,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",0
11726,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,"Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang",0
11727,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,"Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest",0
11728,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,"arXiv e-prints, abs/1605.02688,May 2016",0
11729,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,"Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution ˆpD as possible (e.g., see Appendix I.B)",0
11730,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11731,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp",0
11732,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,Techniques for learningbinary stochastic feedforward neural networks,0
11733,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",0
11734,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11735,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11736,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",0
11737,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11738,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11739,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)",0
11740,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,0
11741,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11742,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11743,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",0
11744,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
11745,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11746,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,URL https://doi.org/10.1109/CVPR.2015.7298594.,0
11747,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,networks and averaged the mutual informations,0
11748,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11749,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,"We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
11750,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,0
11751,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,"(2012), Szegedy et al",0
11752,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11753,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,A,0
11754,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11755,A detail of gradient-based training of PIB ispresented in Algorithm 1,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
11756,A detail of gradient-based training of PIB ispresented in Algorithm 1,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11757,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11758,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,"However, the MLE principle is very generic thatis not specially tailored for neural networks",0
11759,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",0
11760,A detail of gradient-based training of PIB ispresented in Algorithm 1,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11761,A detail of gradient-based training of PIB ispresented in Algorithm 1,"530–538, 2013",0
11762,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,networks and averaged the mutual informations,0
11763,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11764,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,Figure 1: A directed graphical representation of a PIB of two bottlenecks,0
11765,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11766,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,This also empirically holds for the case of SFNN,0
11767,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
11768,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11769,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11770,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance",0
11771,A detail of gradient-based training of PIB ispresented in Algorithm 1,"Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.",0
11772,A detail of gradient-based training of PIB ispresented in Algorithm 1,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",0
11773,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11774,A detail of gradient-based training of PIB ispresented in Algorithm 1,(1999),0
11775,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11776,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11777,A detail of gradient-based training of PIB ispresented in Algorithm 1,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11778,A detail of gradient-based training of PIB ispresented in Algorithm 1,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",0
11779,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,0
11780,A detail of gradient-based training of PIB ispresented in Algorithm 1,"(2015)), natural languagetranslation (e.g., Cho et al",0
11781,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11782,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11783,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11784,A detail of gradient-based training of PIB ispresented in Algorithm 1,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11785,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11786,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11787,A detail of gradient-based training of PIB ispresented in Algorithm 1,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11788,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
11789,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11790,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,"More speciﬁcally, our work proposes an optimizationIB criteria for an existing neural network architecture in an effort to better learn the layers’ represen-tation to their IB optimality",0
11791,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",0
11792,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,0
11793,A detail of gradient-based training of PIB ispresented in Algorithm 1,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11794,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11795,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,A detail of gradient-based training of PIB ispresented in Algorithm 1,0
11796,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
11797,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11798,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,1
11799,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",0
11800,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",(1999),0
11801,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J",0
11802,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11803,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",0
11804,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11805,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11806,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11807,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))",0
11808,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","For PIB, we used β−1l = β−1 = 10−4",0
11809,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11810,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","For PIBs, we set βl = β,∀1 ≤ l ≤ L",0
11811,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11812,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",A,0
11813,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11814,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","(2014)) and game playing (e.g., Silver et al.(2016))",0
11815,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",We used three different randomly initialized neural,0
11816,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).",0
11817,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",(1999)) extracts relevant information fora target variable,0
11818,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",Our PIB is also a standard steptowards better exploiting representational power of more expressive neural network models such asConvolutional Neural Networks (LeCun et al,0
11819,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
11820,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11821,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.",0
11822,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",0
11823,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11824,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11825,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,0
11826,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11827,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11828,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11829,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","1–9, 2015",0
11830,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11831,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Since the objective is designed
to optimize all parameters of neural networks and is mainly motivated by the IB principle for deep
learning (Tishby & Zaslavsky (2015)), we name this method the Parametric Information Bottleneck
(PIB)",0
11832,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11833,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11834,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11835,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
11836,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution ˆpD as possible (e.g., see Appendix I.B)",0
11837,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.",0
11838,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11839,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11840,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11841,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","The MLE principle treats
the neural network model p(xxx; θθθ) as a whole without explicitly considering the contribution of its
internal structures (e.g., hidden layers and hidden neurons)",0
11842,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,0
11843,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Although the analysis offers a new perspective about optimality in neural networks,it proposes general analysis of optimality rather than a practical optimization criteria",0
11844,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Optimizing PIBs now becomes the minimization of LP IB(Z) which at-tempts to decrease I(Zl, Zl−1) and increase I(Zl, Y ) simultaneously",0
11845,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
11846,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11847,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",0
11848,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","However, the MLE principle is very generic thatis not specially tailored for neural networks",0
11849,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11850,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11851,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11852,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,0
11853,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,0
11854,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11855,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11856,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
11857,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
11858,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.",0
11859,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,0
11860,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11861,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)",0
11862,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time",0
11863,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11864,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11865,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,0
11866,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)",0
11867,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",0
11868,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11869,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11870,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11871,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11872,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),0
11873,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","530–538, 2013",0
11874,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11875,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","We then propose a new generalized
IB-based objective that takes into account the compression and relevance of all layers in the network
as an explicit goal for guiding the encodings in a beneﬁcial manner",0
11876,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11877,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Previously, the InformationBottleneck (IB) framework (Tishby et al",0
11878,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11879,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
11880,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
11881,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11882,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11883,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",Techniques for learningbinary stochastic feedforward neural networks,0
11884,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11885,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.",0
11886,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11887,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Originally, the general IB framework is proposed in Tishby et al",0
11888,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","For comparisons, we trained PIBs and ﬁve additional models",0
11889,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11890,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,0
11891,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11892,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11893,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11894,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11895,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","In the following section, we presentour approximation to LP IB which fully utilizes the existing architecture without resorting to anymodel that is not part of the considered neural network",0
11896,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
11897,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",1
11898,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
11899,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD","For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
11900,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
11901,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11902,"The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11903,"(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11904,(2016)).,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11905,"(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11906,(2016)).,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",0
11907,All models areimplemented using Theano framework (Al-Rfou et al,"For PIBs, we set βl = β,∀1 ≤ l ≤ L",0
11908,(2016)).,The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,0
11909,"(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11910,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting",0
11911,All models areimplemented using Theano framework (Al-Rfou et al,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",0
11912,All models areimplemented using Theano framework (Al-Rfou et al,The MLE principle maximizes the likelihood function under theempirical data distribution,0
11913,(2016)).,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11914,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11915,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11916,All models areimplemented using Theano framework (Al-Rfou et al,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11917,(2016)).,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11918,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.",0
11919,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11920,"(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11921,All models areimplemented using Theano framework (Al-Rfou et al,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11922,"The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively","1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",0
11923,"(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11924,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11925,Each hidden layer in SFNNs is also considered as a stochastic variable,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",0
11926,"(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning","Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network",0
11927,All models areimplemented using Theano framework (Al-Rfou et al,URLhttps://doi.org/10.1109/CVPR.2016.90.,0
11928,All models areimplemented using Theano framework (Al-Rfou et al,"By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner",0
11929,All models areimplemented using Theano framework (Al-Rfou et al,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11930,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters",We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,0
11931,Each hidden layer in SFNNs is also considered as a stochastic variable,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11932,"The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11933,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11934,(2016)).,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11935,Each hidden layer in SFNNs is also considered as a stochastic variable,The ﬁrst model (ModelA) is a deterministic neural network,0
11936,Each hidden layer in SFNNs is also considered as a stochastic variable,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",0
11937,Each hidden layer in SFNNs is also considered as a stochastic variable,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
11938,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters","In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ",0
11939,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",Estimating or propagating gradientsthrough stochastic neurons for conditional computation,0
11940,(2016)).,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2",0
11941,(2016)).,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11942,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters",A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,0
11943,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11944,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","In the MLE principle, we only need empirical samples of
the joint distribution to maximize the likelihood function of the model given the data",0
11945,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11946,"The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively",Theano: A Pythonframework for fast computation of mathematical expressions,0
11947,"(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning","As can be seen from the table, PIB and Model C givesnearly the same performance which outperform deterministic neural networks and SFNNs, and theirstochastic and deterministic version.It is interesting to empirically see that the deterministic version of PIB at test time (Model C) gives aslightly better result than PIB",0
11948,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11949,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11950,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","In estimating mutual information, we adopted the variational method asin Alemi et al",0
11951,All models areimplemented using Theano framework (Al-Rfou et al,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11952,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp",0
11953,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11954,(2016)).,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11955,Each hidden layer in SFNNs is also considered as a stochastic variable,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",0
11956,"The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11957,"(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11958,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters","Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution",0
11959,Each hidden layer in SFNNs is also considered as a stochastic variable,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11960,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","Since the objective is designed
to optimize all parameters of neural networks and is mainly motivated by the IB principle for deep
learning (Tishby & Zaslavsky (2015)), we name this method the Parametric Information Bottleneck
(PIB)",0
11961,All models areimplemented using Theano framework (Al-Rfou et al,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11962,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11963,All models areimplemented using Theano framework (Al-Rfou et al,"The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively",0
11964,All models areimplemented using Theano framework (Al-Rfou et al,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11965,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11966,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11967,"The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11968,Each hidden layer in SFNNs is also considered as a stochastic variable,"The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).",0
11969,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11970,Each hidden layer in SFNNs is also considered as a stochastic variable,"We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
11971,Each hidden layer in SFNNs is also considered as a stochastic variable,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters",0
11972,Each hidden layer in SFNNs is also considered as a stochastic variable,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11973,(2016)).,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11974,"The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively","Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp",0
11975,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner",0
11976,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11977,"The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively",The ﬁrst model (ModelA) is a deterministic neural network,0
11978,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","(2014)) and game playing (e.g., Silver et al.(2016))",0
11979,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11980,(2016)).,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,0
11981,(2016)).,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11982,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11983,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11984,All models areimplemented using Theano framework (Al-Rfou et al,"informative yet compressed representation, which is supported by qualitative empirical results",0
11985,(2016)).,"For PIB, we used β−1l = β−1 = 10−4",0
11986,"(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning",The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,0
11987,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
11988,"The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively","The MLE principle treats
the neural network model p(xxx; θθθ) as a whole without explicitly considering the contribution of its
internal structures (e.g., hidden layers and hidden neurons)",0
11989,All models areimplemented using Theano framework (Al-Rfou et al,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11990,Each hidden layer in SFNNs is also considered as a stochastic variable,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11991,All models areimplemented using Theano framework (Al-Rfou et al,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11992,All models areimplemented using Theano framework (Al-Rfou et al,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11993,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",0
11994,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al","As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets",0
11995,All models areimplemented using Theano framework (Al-Rfou et al,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11996,"(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning",This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,0
11997,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters","These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer",0
11998,Each hidden layer in SFNNs is also considered as a stochastic variable,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",1
11999,(2016)).,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",0
12000,"For comparisons, we trained PIBs and ﬁve additional models","For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",0
12001,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12002,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time",Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,0
12003,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task","Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014",0
12004,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12005,"For comparisons, we trained PIBs and ﬁve additional models","CoRR, abs/1406.2989, 2014",0
12006,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12007,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12008,The ﬁrst model (ModelA) is a deterministic neural network,(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
12009,"For comparisons, we trained PIBs and ﬁve additional models","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12010,The ﬁrst model (ModelA) is a deterministic neural network,"The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)function.Proposition 3.2",0
12011,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12012,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12013,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12014,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12015,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,0
12016,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",0
12017,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,Yichuan Tang and Ruslan Salakhutdinov,0
12018,"For comparisons, we trained PIBs and ﬁve additional models","Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters",0
12019,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12020,The ﬁrst model (ModelA) is a deterministic neural network,Information bottleneck for gaus-sian variables,0
12021,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12022,The ﬁrst model (ModelA) is a deterministic neural network,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",0
12023,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,0
12024,The ﬁrst model (ModelA) is a deterministic neural network,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12025,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12026,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task","We scaled the images to [0, 1] and do not perform anyother data augmentation",0
12027,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,The generated bottleneck samples are then used to estimate mutual in-formation,0
12028,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12029,The ﬁrst model (ModelA) is a deterministic neural network,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12030,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12031,The ﬁrst model (ModelA) is a deterministic neural network,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12032,The ﬁrst model (ModelA) is a deterministic neural network,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12033,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task","The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)",0
12034,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,"CoRR, abs/1308.3432, 2013",0
12035,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",(1999),0
12036,"For comparisons, we trained PIBs and ﬁve additional models","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12037,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12038,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",0
12039,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time",Courville,0
12040,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12041,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,content of the original space possibly including its dimensionality and topological structure,0
12042,"For comparisons, we trained PIBs and ﬁve additional models","The optimal representation Z is determined via theminimization of the following Lagrangian:Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",0
12043,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12044,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",0
12045,"For comparisons, we trained PIBs and ﬁve additional models","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12046,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12047,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12048,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12049,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,"The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively",0
12050,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,0
12051,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time",A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,0
12052,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12053,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12054,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",URL https://doi.org/10.1109/CVPR.2015.7298594.,0
12055,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12056,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",0
12057,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12058,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time","We propose re-using the existing neural network architecture
as variational decoders for each hidden layers",0
12059,"For comparisons, we trained PIBs and ﬁve additional models","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12060,"For comparisons, we trained PIBs and ﬁve additional models","Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",0
12061,"For comparisons, we trained PIBs and ﬁve additional models","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12062,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12063,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12064,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12065,"For comparisons, we trained PIBs and ﬁve additional models","drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
12066,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J",0
12067,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12068,The ﬁrst model (ModelA) is a deterministic neural network,"Since the objective is designed
to optimize all parameters of neural networks and is mainly motivated by the IB principle for deep
learning (Tishby & Zaslavsky (2015)), we name this method the Parametric Information Bottleneck
(PIB)",0
12069,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,"(1999)); or (2) X, Yand Z are mutually joint Gaussian (Chechik et al",0
12070,"For comparisons, we trained PIBs and ﬁve additional models","By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner",0
12071,"For comparisons, we trained PIBs and ﬁve additional models","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12072,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,0
12073,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12074,"For comparisons, we trained PIBs and ﬁve additional models","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12075,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,"(2014)) and game playing (e.g., Silver et al.(2016))",0
12076,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,"where H(Y ) = constant can be ignored in the minimization of L(Z), andptrue(yyy|zzzl)p(zzzl) log ptrue(yyy|zzzl)dyyydzzzlpD(xxx, yyy)p(zzzl|xxx) log ptrue(yyy|zzzl)dzzzldxxxdyyypD(xxx, yyy)p(zzzl|xxx) log pv(yyy|zzzl)dzzzldxxxdyyy",0
12077,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12078,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12079,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12080,The ﬁrst model (ModelA) is a deterministic neural network,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12081,"For comparisons, we trained PIBs and ﬁve additional models",The MLE principle maximizes the likelihood function under theempirical data distribution,0
12082,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12083,"For comparisons, we trained PIBs and ﬁve additional models","Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting",0
12084,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12085,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12086,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,"However, thealgorithm is not applicable to neural networks",0
12087,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",(1998)) and ResNet (He et al,0
12088,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",0
12089,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12090,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12091,The ﬁrst model (ModelA) is a deterministic neural network,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12092,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12093,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task","However, the MLE principle is very generic thatis not specially tailored for neural networks",0
12094,"For comparisons, we trained PIBs and ﬁve additional models","In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",0
12095,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12096,"In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",1
12097,The ﬁrst model (ModelA) is a deterministic neural network,"Here we preferto this extreme, i.e., the 0th level, as the super level",0
12098,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",0
12099,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",0
12100,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12101,We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12102,We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12103,"We scaled the images to [0, 1] and do not perform anyother data augmentation","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12104,The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,"249–256,2010",0
12105,The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",0
12106,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer","Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)",0
12107,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12108,The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
12109,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12110,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)","The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).",0
12111,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)",URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,0
12112,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,Our PIB is also a standard steptowards better exploiting representational power of more expressive neural network models such asConvolutional Neural Networks (LeCun et al,0
12113,We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12114,"We scaled the images to [0, 1] and do not perform anyother data augmentation",doi: 10.1109/CVPR.2015.7298594,0
12115,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)","(2014), Dauphin & Grangier(2016))",0
12116,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,0
12117,The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",0
12118,The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,Learning stochastic feedforward neural networks,0
12119,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)","For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",0
12120,We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12121,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer","However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",0
12122,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12123,We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12124,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer","Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
12125,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12126,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",doi: 10.1109/CVPR.2015.7298594,0
12127,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,0
12128,We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12129,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12130,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer","For PIB, we also usedβ−1l = β−1 = 10−4",0
12131,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12132,The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,Courville,0
12133,The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,URL http://arxiv.org/abs/1605.02688.,0
12134,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12135,"We scaled the images to [0, 1] and do not perform anyother data augmentation","Thus, our framework does not depend on a speciﬁc stochastic model",0
12136,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",0
12137,"We scaled the images to [0, 1] and do not perform anyother data augmentation","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12138,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer",In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,0
12139,We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,"Here we preferto this extreme, i.e., the 0th level, as the super level",0
12140,"We scaled the images to [0, 1] and do not perform anyother data augmentation","1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",0
12141,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",0
12142,We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,The bottleneck component zi deﬁned as above still gets value of either 0 or 1 but it is decomposedinto the sum of a deterministic term and a noise term,0
12143,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)","We then propose a new generalized
IB-based objective that takes into account the compression and relevance of all layers in the network
as an explicit goal for guiding the encodings in a beneﬁcial manner",0
12144,The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12145,The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12146,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12147,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12148,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,"For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",0
12149,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12150,We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,"In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ",0
12151,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels","However, the variational approximatedistribution for the code space may be too loose while the actual marginal distribution can be sam-pled easily.Our work, on the other hand, focuses on better exploiting intermediate representations of a neuralnetwork architecture using the IB principle",0
12152,"We scaled the images to [0, 1] and do not perform anyother data augmentation","(2015)), natural languagetranslation (e.g., Cho et al",0
12153,The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,"(2014)) and game playing (e.g., Silver et al.(2016))",0
12154,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12155,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,"For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",0
12156,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12157,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)","1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",0
12158,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12159,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,0
12160,"We scaled the images to [0, 1] and do not perform anyother data augmentation","However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)",0
12161,We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12162,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12163,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,0
12164,"We scaled the images to [0, 1] and do not perform anyother data augmentation","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12165,"We scaled the images to [0, 1] and do not perform anyother data augmentation","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12166,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",0
12167,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12168,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,0
12169,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12170,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12171,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12172,The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12173,The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,"However, the MLE principle is very generic thatis not specially tailored for neural networks",0
12174,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12175,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12176,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12177,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12178,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12179,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12180,The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,"For deterministicneural networks, we only have one sample of hidden variables given one data point",0
12181,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels","(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
12182,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,URL https://doi.org/10.1038/nature16961.,0
12183,The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12184,We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",0
12185,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12186,"We scaled the images to [0, 1] and do not perform anyother data augmentation","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12187,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12188,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,0
12189,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12190,"We scaled the images to [0, 1] and do not perform anyother data augmentation","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12191,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12192,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12193,"We scaled the images to [0, 1] and do not perform anyother data augmentation","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12194,The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12195,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12196,We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,"Since the objective is designed
to optimize all parameters of neural networks and is mainly motivated by the IB principle for deep
learning (Tishby & Zaslavsky (2015)), we name this method the Parametric Information Bottleneck
(PIB)",0
12197,"We scaled the images to [0, 1] and do not perform anyother data augmentation","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12198,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12199,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,"The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",1
12200,"For stochastic sampling, we generate M = 16 samples per point during training andM = 32 samples per point during testing",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12201,"For PIBs, we set βl = β,∀1 ≤ l ≤ L",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12202,"For visualization of learned ﬁlters of PIB, seeAppendix II.A.",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12203,"We tuned β from{0} ∪ {10−i : 1 ≤ i ≤ 7}, and found β−1 = 10−4 works best.Table 1 provides the results in the MNIST classiﬁcation error in the test set for PIB and the com-parative models (A), (B), (C), (D), and (E)","Speciﬁcally, the relevance decoder is determined as follows:",0
12204,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,Learning stochastic feedforward neural networks,0
12205,"As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time","For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
12206,"For stochastic sampling, we generate M = 16 samples per point during training andM = 32 samples per point during testing","Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",0
12207,"We tuned β from{0} ∪ {10−i : 1 ≤ i ≤ 7}, and found β−1 = 10−4 works best.Table 1 provides the results in the MNIST classiﬁcation error in the test set for PIB and the com-parative models (A), (B), (C), (D), and (E)",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12208,"For PIBs, we set βl = β,∀1 ≤ l ≤ L",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12209,"For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard","Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned",0
12210,"For visualization of learned ﬁlters of PIB, seeAppendix II.A.",(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
12211,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12212,"We tuned β from{0} ∪ {10−i : 1 ≤ i ≤ 7}, and found β−1 = 10−4 works best.Table 1 provides the results in the MNIST classiﬁcation error in the test set for PIB and the com-parative models (A), (B), (C), (D), and (E)","The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",0
12213,"As can be seen from the table, PIB and Model C givesnearly the same performance which outperform deterministic neural networks and SFNNs, and theirstochastic and deterministic version.It is interesting to empirically see that the deterministic version of PIB at test time (Model C) gives aslightly better result than PIB",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12214,"As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time","The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",0
12215,"As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
12216,"For stochastic sampling, we generate M = 16 samples per point during training andM = 32 samples per point during testing",(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,0
12217,"For visualization of learned ﬁlters of PIB, seeAppendix II.A.",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12218,"As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12219,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12220,"For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12221,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12222,This also empirically holds for the case of SFNN,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12223,"We tuned β from{0} ∪ {10−i : 1 ≤ i ≤ 7}, and found β−1 = 10−4 works best.Table 1 provides the results in the MNIST classiﬁcation error in the test set for PIB and the com-parative models (A), (B), (C), (D), and (E)",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12224,This also empirically holds for the case of SFNN,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12225,"For stochastic sampling, we generate M = 16 samples per point during training andM = 32 samples per point during testing",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12226,"To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)","For PIB, we also usedβ−1l = β−1 = 10−4",0
12227,"For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12228,"As can be seen from the table, PIB and Model C givesnearly the same performance which outperform deterministic neural networks and SFNNs, and theirstochastic and deterministic version.It is interesting to empirically see that the deterministic version of PIB at test time (Model C) gives aslightly better result than PIB",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12229,"For PIBs, we set βl = β,∀1 ≤ l ≤ L",The generated bottleneck samples are then used to estimate mutual in-formation,0
12230,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",0
12231,"As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12232,"As can be seen from the table, PIB and Model C givesnearly the same performance which outperform deterministic neural networks and SFNNs, and theirstochastic and deterministic version.It is interesting to empirically see that the deterministic version of PIB at test time (Model C) gives aslightly better result than PIB",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
12233,"For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard","In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ",0
12234,"For visualization of learned ﬁlters of PIB, seeAppendix II.A.",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12235,This also empirically holds for the case of SFNN,"More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
12236,"As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12237,"To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12238,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks",0
12239,This also empirically holds for the case of SFNN,"CoRR, abs/1308.3432, 2013",0
12240,"For stochastic sampling, we generate M = 16 samples per point during training andM = 32 samples per point during testing",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12241,"For PIBs, we set βl = β,∀1 ≤ l ≤ L","For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",0
12242,"For PIBs, we set βl = β,∀1 ≤ l ≤ L",(1998)) and ResNet (He et al,0
12243,"To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12244,"As can be seen from the table, PIB and Model C givesnearly the same performance which outperform deterministic neural networks and SFNNs, and theirstochastic and deterministic version.It is interesting to empirically see that the deterministic version of PIB at test time (Model C) gives aslightly better result than PIB","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",0
12245,"To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12246,"For visualization of learned ﬁlters of PIB, seeAppendix II.A.",A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,0
12247,"For stochastic sampling, we generate M = 16 samples per point during training andM = 32 samples per point during testing","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",0
12248,"For stochastic sampling, we generate M = 16 samples per point during training andM = 32 samples per point during testing",The exactsolution to the minimization problem above is found (Tishby et al,0
12249,"To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)","For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",0
12250,This also empirically holds for the case of SFNN,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12251,"To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)","1–9, 2015",0
12252,"For PIBs, we set βl = β,∀1 ≤ l ≤ L",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12253,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
12254,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12255,"For stochastic sampling, we generate M = 16 samples per point during training andM = 32 samples per point during testing",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12256,"For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12257,"For visualization of learned ﬁlters of PIB, seeAppendix II.A.",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12258,"We tuned β from{0} ∪ {10−i : 1 ≤ i ≤ 7}, and found β−1 = 10−4 works best.Table 1 provides the results in the MNIST classiﬁcation error in the test set for PIB and the com-parative models (A), (B), (C), (D), and (E)","For simplicity, we present the derivation of I(Z1, Z0) only 3",0
12259,"To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)","Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
12260,"As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12261,"For PIBs, we set βl = β,∀1 ≤ l ≤ L",The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,0
12262,"For visualization of learned ﬁlters of PIB, seeAppendix II.A.",(21)where |Z1| denotes the cardinality of the space of variable Z1,0
12263,"For visualization of learned ﬁlters of PIB, seeAppendix II.A.",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12264,"As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12265,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12266,"To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)","Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.",0
12267,"As can be seen from the table, PIB and Model C givesnearly the same performance which outperform deterministic neural networks and SFNNs, and theirstochastic and deterministic version.It is interesting to empirically see that the deterministic version of PIB at test time (Model C) gives aslightly better result than PIB",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12268,"As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time",doi: 10.1109/CVPR.2016.90,0
12269,"For PIBs, we set βl = β,∀1 ≤ l ≤ L",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12270,"For PIBs, we set βl = β,∀1 ≤ l ≤ L",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12271,"As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time","Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
12272,"For PIBs, we set βl = β,∀1 ≤ l ≤ L",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12273,"For visualization of learned ﬁlters of PIB, seeAppendix II.A.","Speciﬁcally, the relevance decoder is determined as follows:",0
12274,"To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12275,"For PIBs, we set βl = β,∀1 ≤ l ≤ L","However, the MLE principle is very generic thatis not specially tailored for neural networks",0
12276,This also empirically holds for the case of SFNN,"(2014)) and game playing (e.g., Silver et al.(2016))",0
12277,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,Discrete-valued variables in PIBs make standard back-propagation not straightforward,0
12278,"We tuned β from{0} ∪ {10−i : 1 ≤ i ≤ 7}, and found β−1 = 10−4 works best.Table 1 provides the results in the MNIST classiﬁcation error in the test set for PIB and the com-parative models (A), (B), (C), (D), and (E)",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12279,"To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)","Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.",0
12280,"For PIBs, we set βl = β,∀1 ≤ l ≤ L",Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,0
12281,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,"Furthermore,estimating mutual information between the variables transformed by network layers and the datavariables poses several computational challenges in practice that the authors did not address in thework",0
12282,"For stochastic sampling, we generate M = 16 samples per point during training andM = 32 samples per point during testing","In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",0
12283,"For PIBs, we set βl = β,∀1 ≤ l ≤ L",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12284,"As can be seen from the table, PIB and Model C givesnearly the same performance which outperform deterministic neural networks and SFNNs, and theirstochastic and deterministic version.It is interesting to empirically see that the deterministic version of PIB at test time (Model C) gives aslightly better result than PIB","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",0
12285,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,0
12286,"For visualization of learned ﬁlters of PIB, seeAppendix II.A.","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",0
12287,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12288,"We tuned β from{0} ∪ {10−i : 1 ≤ i ≤ 7}, and found β−1 = 10−4 works best.Table 1 provides the results in the MNIST classiﬁcation error in the test set for PIB and the com-parative models (A), (B), (C), (D), and (E)","For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",0
12289,"To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12290,"For stochastic sampling, we generate M = 16 samples per point during training andM = 32 samples per point during testing",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12291,"To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12292,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12293,This also empirically holds for the case of SFNN,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12294,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
12295,"We tuned β from{0} ∪ {10−i : 1 ≤ i ≤ 7}, and found β−1 = 10−4 works best.Table 1 provides the results in the MNIST classiﬁcation error in the test set for PIB and the com-parative models (A), (B), (C), (D), and (E)",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,1
12296,"As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time","(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning",0
12297,This also empirically holds for the case of SFNN,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
12298,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,"(2012), Szegedy et al",0
12299,"As can be seen from the table, PIB and Model C givesnearly the same performance which outperform deterministic neural networks and SFNNs, and theirstochastic and deterministic version.It is interesting to empirically see that the deterministic version of PIB at test time (Model C) gives aslightly better result than PIB","However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )",0
12300,"In the general IB framework, each value of β speciﬁes a uniquepoint of Z in the information plane",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12301,"(1999), Slonim (2003))",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12302,"The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )","(2014), Bahdanau et al",0
12303,"(1999), Slonim (2003))",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12304,"(1999), Slonim (2003))",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12305,We used three different randomly initialized neural,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance",0
12306,"In the general IB framework, each value of β speciﬁes a uniquepoint of Z in the information plane",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12307,"For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,0
12308,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,0
12309,"(1999), Slonim (2003))",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12310,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,0
12311,"As β varies from 0 to ∞, Z traces a concave curve, knownas information curve for representation Z, with a slope of β−1",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12312,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,0
12313,The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,"The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively",0
12314,"The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12315,"(1999), Slonim (2003))",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12316,"(1999), Slonim (2003))","However, the MLE principle is very generic thatis not specially tailored for neural networks",0
12317,"As β varies from 0 to ∞, Z traces a concave curve, knownas information curve for representation Z, with a slope of β−1","Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network",0
12318,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12319,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12320,"In the general IB framework, each value of β speciﬁes a uniquepoint of Z in the information plane",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12321,"The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",Our PIB is also a standard steptowards better exploiting representational power of more expressive neural network models such asConvolutional Neural Networks (LeCun et al,0
12322,"The plane has I(Z, X) and I(Z, Y ) as its horizontal axisand its vertical axis, respectively","770–778, 2016",0
12323,"The plane has I(Z, X) and I(Z, Y ) as its horizontal axisand its vertical axis, respectively",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12324,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,0
12325,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,"For deterministicneural networks, we only have one sample of hidden variables given one data point",0
12326,"As β varies from 0 to ∞, Z traces a concave curve, knownas information curve for representation Z, with a slope of β−1",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12327,"The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,0
12328,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,(21)where |Z1| denotes the cardinality of the space of variable Z1,0
12329,"(1999), Slonim (2003))",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12330,"The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )","(2014), Bahdanau et al",0
12331,"As β varies from 0 to ∞, Z traces a concave curve, knownas information curve for representation Z, with a slope of β−1",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12332,"As β varies from 0 to ∞, Z traces a concave curve, knownas information curve for representation Z, with a slope of β−1",URLhttp://arxiv.org/abs/1308.3432.,0
12333,"In the general IB framework, each value of β speciﬁes a uniquepoint of Z in the information plane","249–256,2010",0
12334,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12335,"(1999), Slonim (2003))",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12336,"The plane has I(Z, X) and I(Z, Y ) as its horizontal axisand its vertical axis, respectively",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12337,"For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12338,The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12339,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12340,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12341,We used three different randomly initialized neural,"Thus, it is hard to analytically capture such modiﬁcations.",0
12342,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12343,The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12344,"As β varies from 0 to ∞, Z traces a concave curve, knownas information curve for representation Z, with a slope of β−1","David Silver, Aja Huang, Chris J",0
12345,"The plane has I(Z, X) and I(Z, Y ) as its horizontal axisand its vertical axis, respectively",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12346,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,"(2014), Bahdanau et al",0
12347,"The plane has I(Z, X) and I(Z, Y ) as its horizontal axisand its vertical axis, respectively",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12348,"As β varies from 0 to ∞, Z traces a concave curve, knownas information curve for representation Z, with a slope of β−1",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12349,We used three different randomly initialized neural,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
12350,"The plane has I(Z, X) and I(Z, Y ) as its horizontal axisand its vertical axis, respectively","We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.",0
12351,We used three different randomly initialized neural,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12352,"(1999), Slonim (2003))",Sergey Ioffe and Christian Szegedy,0
12353,"As β varies from 0 to ∞, Z traces a concave curve, knownas information curve for representation Z, with a slope of β−1",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12354,We used three different randomly initialized neural,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12355,"In the general IB framework, each value of β speciﬁes a uniquepoint of Z in the information plane","where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",0
12356,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,"(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning",0
12357,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,0
12358,"For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12359,"The plane has I(Z, X) and I(Z, Y ) as its horizontal axisand its vertical axis, respectively",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12360,"For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12361,"The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12362,"(1999), Slonim (2003))",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12363,"In the general IB framework, each value of β speciﬁes a uniquepoint of Z in the information plane",(1999)) extracts relevant information fora target variable,0
12364,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
12365,"For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,0
12366,The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,"The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",0
12367,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,URL https://doi.org/10.1038/nature16961.,0
12368,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)",0
12369,"In the general IB framework, each value of β speciﬁes a uniquepoint of Z in the information plane",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12370,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12371,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12372,"The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12373,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12374,"The plane has I(Z, X) and I(Z, Y ) as its horizontal axisand its vertical axis, respectively",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12375,"In the general IB framework, each value of β speciﬁes a uniquepoint of Z in the information plane","The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",0
12376,We used three different randomly initialized neural,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,0
12377,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12378,"In the general IB framework, each value of β speciﬁes a uniquepoint of Z in the information plane",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12379,"For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12380,"For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12381,We used three different randomly initialized neural,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12382,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,"For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
12383,"In the general IB framework, each value of β speciﬁes a uniquepoint of Z in the information plane","However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )",0
12384,We used three different randomly initialized neural,"In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ",0
12385,"For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even","In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",0
12386,"(1999), Slonim (2003))","(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
12387,We used three different randomly initialized neural,"For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",0
12388,We used three different randomly initialized neural,The approximation then leads to effectivegradient-based training of PIBs.,0
12389,We used three different randomly initialized neural,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12390,"For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12391,The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",0
12392,"The plane has I(Z, X) and I(Z, Y ) as its horizontal axisand its vertical axis, respectively","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
12393,"The plane has I(Z, X) and I(Z, Y ) as its horizontal axisand its vertical axis, respectively",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12394,"The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12395,"(1999), Slonim (2003))",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12396,"The plane has I(Z, X) and I(Z, Y ) as its horizontal axisand its vertical axis, respectively","For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
12397,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12398,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,"(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
12399,The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,1
12400,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12401,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12402,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12403,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"We then propose a new generalized
IB-based objective that takes into account the compression and relevance of all layers in the network
as an explicit goal for guiding the encodings in a beneﬁcial manner",0
12404,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12405,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,URL http://dl.acm.org/citation.cfm?id=2670313.,0
12406,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12407,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],0
12408,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12409,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12410,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,0
12411,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12412,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",0
12413,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12414,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
12415,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Batch normalization: Accelerating deep network training byreducing internal covariate shift,0
12416,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12417,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12418,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level",0
12419,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).","Fortunately,one can estimate the gradient in this case",0
12420,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).","(2012), Szegedy et al",0
12421,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"In the following section, we presentour approximation to LP IB which fully utilizes the existing architecture without resorting to anymodel that is not part of the considered neural network",0
12422,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).","448–456, 2015",0
12423,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P",0
12424,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,0
12425,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable",0
12426,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12427,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12428,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12429,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,0
12430,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
12431,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,(1999)) extracts relevant information fora target variable,0
12432,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12433,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).","(2015)), natural languagetranslation (e.g., Cho et al",0
12434,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12435,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).","A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",0
12436,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"Originally, the general IB framework is proposed in Tishby et al",0
12437,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",doi: 10.1109/CVPR.2016.90,0
12438,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12439,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",doi: 10.1109/CVPR.2016.90,0
12440,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12441,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),0
12442,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12443,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12444,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",URL https://doi.org/10.1038/nature16961.,0
12445,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"249–256,2010",0
12446,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12447,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",0
12448,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
12449,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12450,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
12451,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12452,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12453,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"We scaled the images to [0, 1] and do not perform anyother data augmentation",0
12454,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Techniques for learningbinary stochastic feedforward neural networks,0
12455,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).","In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",0
12456,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12457,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12458,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12459,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12460,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
12461,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
12462,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12463,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12464,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
12465,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12466,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,0
12467,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12468,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).","However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )",0
12469,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12470,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,0
12471,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12472,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12473,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).","Speciﬁcally, the relevance decoder is determined as follows:",0
12474,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12475,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12476,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12477,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",0
12478,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,0
12479,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.",0
12480,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,0
12481,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).",0
12482,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",0
12483,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Sergey Ioffe and Christian Szegedy,0
12484,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"(2014), Bahdanau et al",0
12485,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",The recent work Alemi et al,0
12486,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).","(2014), Bahdanau et al",0
12487,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12488,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12489,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",0
12490,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
12491,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Techniques for learningbinary stochastic feedforward neural networks,0
12492,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12493,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12494,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12495,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12496,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,0
12497,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,"The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)",0
12498,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,1
12499,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,Techniques for learningbinary stochastic feedforward neural networks,0
12500,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",networks and averaged the mutual informations,1
12501,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",networks and averaged the mutual informations,1
12502,"The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution","Here we preferto this extreme, i.e., the 0th level, as the super level",0
12503,"As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.",doi: 10.1109/CVPR.2016.90,0
12504,networks and averaged the mutual informations,networks and averaged the mutual informations,1
12505,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs","The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).",0
12506,"The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",networks and averaged the mutual informations,1
12507,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs","Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation",0
12508,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",networks and averaged the mutual informations,1
12509,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases","Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",0
12510,"As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.","Speciﬁcally, the relevance decoder is determined as follows:",0
12511,"For PIB, we used β−1l = β−1 = 10−4",networks and averaged the mutual informations,1
12512,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",networks and averaged the mutual informations,1
12513,"As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.",networks and averaged the mutual informations,1
12514,"As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.","Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation",0
12515,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs","Yoshua Bengio, Nicholas L´eonard, and Aaron C",0
12516,"As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.","However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)",0
12517,"As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
12518,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs","Previously, the InformationBottleneck (IB) framework (Tishby et al",0
12519,"For PIB, we used β−1l = β−1 = 10−4",networks and averaged the mutual informations,1
12520,"For PIB, we used β−1l = β−1 = 10−4",networks and averaged the mutual informations,1
12521,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",networks and averaged the mutual informations,1
12522,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",0
12523,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",doi: 10.1109/CVPR.2015.7298594,0
12524,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",0
12525,"The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",networks and averaged the mutual informations,1
12526,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,0
12527,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs","(2015)), natural languagetranslation (e.g., Cho et al",0
12528,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",networks and averaged the mutual informations,1
12529,"For PIB, we used β−1l = β−1 = 10−4",networks and averaged the mutual informations,1
12530,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",(2016)).,0
12531,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",Courville,0
12532,"The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",networks and averaged the mutual informations,1
12533,networks and averaged the mutual informations,"David Silver, Aja Huang, Chris J",0
12534,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",networks and averaged the mutual informations,1
12535,networks and averaged the mutual informations,networks and averaged the mutual informations,1
12536,networks and averaged the mutual informations,networks and averaged the mutual informations,1
12537,networks and averaged the mutual informations,Learning stochastic feedforward neural networks,0
12538,networks and averaged the mutual informations,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],0
12539,networks and averaged the mutual informations,networks and averaged the mutual informations,1
12540,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",networks and averaged the mutual informations,1
12541,networks and averaged the mutual informations,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time",0
12542,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",networks and averaged the mutual informations,1
12543,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well","However, thealgorithm is not applicable to neural networks",0
12544,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",networks and averaged the mutual informations,1
12545,"The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution","The optimal representation Z is determined via theminimization of the following Lagrangian:LIB[p(z|x)] = I(Z, X) − βI(Z, Y )",0
12546,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",networks and averaged the mutual informations,1
12547,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",networks and averaged the mutual informations,1
12548,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well","Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp",0
12549,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well","Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
12550,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",networks and averaged the mutual informations,1
12551,networks and averaged the mutual informations,networks and averaged the mutual informations,1
12552,networks and averaged the mutual informations,networks and averaged the mutual informations,1
12553,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",networks and averaged the mutual informations,1
12554,"The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution","1–9, 2015",0
12555,"The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",networks and averaged the mutual informations,1
12556,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,0
12557,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well","However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",0
12558,networks and averaged the mutual informations,networks and averaged the mutual informations,1
12559,"As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.",Masteringthe game of go with deep neural networks and tree search,0
12560,networks and averaged the mutual informations,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",0
12561,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",networks and averaged the mutual informations,1
12562,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",networks and averaged the mutual informations,1
12563,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases","Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T",0
12564,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",networks and averaged the mutual informations,1
12565,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",networks and averaged the mutual informations,1
12566,networks and averaged the mutual informations,networks and averaged the mutual informations,1
12567,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",networks and averaged the mutual informations,1
12568,"As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.","Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich",0
12569,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases","σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",0
12570,"The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",networks and averaged the mutual informations,1
12571,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases","Originally, the general IB framework is proposed in Tishby et al",0
12572,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",networks and averaged the mutual informations,1
12573,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well","For PIB, we also usedβ−1l = β−1 = 10−4",0
12574,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases","informative yet compressed representation, which is supported by qualitative empirical results",0
12575,"For PIB, we used β−1l = β−1 = 10−4",The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,0
12576,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",networks and averaged the mutual informations,1
12577,"For PIB, we used β−1l = β−1 = 10−4",networks and averaged the mutual informations,1
12578,"For PIB, we used β−1l = β−1 = 10−4",networks and averaged the mutual informations,1
12579,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,0
12580,"As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.","(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning",0
12581,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,0
12582,"For PIB, we used β−1l = β−1 = 10−4",networks and averaged the mutual informations,1
12583,networks and averaged the mutual informations,networks and averaged the mutual informations,1
12584,networks and averaged the mutual informations,"However, thealgorithm is not applicable to neural networks",0
12585,"The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",networks and averaged the mutual informations,1
12586,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases","Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest",0
12587,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",networks and averaged the mutual informations,1
12588,"As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.",URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,0
12589,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases","Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)",0
12590,"The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",The authors in Tang & Salakhutdinov (2013) used aGeneralized EM algorithm while Bengio et al,0
12591,"For PIB, we used β−1l = β−1 = 10−4",networks and averaged the mutual informations,1
12592,"The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",networks and averaged the mutual informations,1
12593,"As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.",networks and averaged the mutual informations,1
12594,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",networks and averaged the mutual informations,1
12595,"As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.",networks and averaged the mutual informations,1
12596,"The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well","For the compression, we
decompose the mutual information as follows:",0
12597,networks and averaged the mutual informations,"We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.",0
12598,"As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.",networks and averaged the mutual informations,1
12599,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs","Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest",0
12600,"For PIB, we also usedβ−1l = β−1 = 10−4","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12601,"For PIB, we also usedβ−1l = β−1 = 10−4","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12602,We trained themodels in the full training set of 60000 images and tested in the test set,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12603,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,"Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters",0
12604,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",0
12605,The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12606,(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),0
12607,We trained themodels in the full training set of 60000 images and tested in the test set,"The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).",0
12608,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required","More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
12609,"This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12610,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,The approximation then leads to effectivegradient-based training of PIBs.,0
12611,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,We trained themodels in the full training set of 60000 images and tested in the test set,0
12612,(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12613,"This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al",Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,0
12614,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required","Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance",0
12615,"For PIB, we also usedβ−1l = β−1 = 10−4",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,0
12616,(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,"Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",0
12617,"This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",0
12618,We trained themodels in the full training set of 60000 images and tested in the test set,"(2015)), natural languagetranslation (e.g., Cho et al",0
12619,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12620,"This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al","As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time",0
12621,We trained themodels in the full training set of 60000 images and tested in the test set,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12622,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,"As can be seen from the table, PIB and Model C givesnearly the same performance which outperform deterministic neural networks and SFNNs, and theirstochastic and deterministic version.It is interesting to empirically see that the deterministic version of PIB at test time (Model C) gives aslightly better result than PIB",0
12623,"This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al",URLhttps://doi.org/10.1109/CVPR.2016.90.,0
12624,We trained themodels in the full training set of 60000 images and tested in the test set,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,0
12625,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,0
12626,We trained themodels in the full training set of 60000 images and tested in the test set,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12627,"This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12628,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,"(2014)) and game playing (e.g., Silver et al.(2016))",0
12629,We trained themodels in the full training set of 60000 images and tested in the test set,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12630,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
12631,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,Masteringthe game of go with deep neural networks and tree search,0
12632,The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,"For stochastic sampling, we generate M = 16 samples per point during training andM = 32 samples per point during testing",0
12633,We trained themodels in the full training set of 60000 images and tested in the test set,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",0
12634,The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12635,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12636,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12637,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12638,"For PIB, we also usedβ−1l = β−1 = 10−4","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12639,"For PIB, we also usedβ−1l = β−1 = 10−4","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12640,The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12641,We trained themodels in the full training set of 60000 images and tested in the test set,"More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
12642,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12643,We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12644,We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
12645,We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12646,"This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al","In this work, we use the gradient estimator inspiredby Raiko et al",0
12647,We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
12648,(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12649,(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,0
12650,We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12651,"For PIB, we also usedβ−1l = β−1 = 10−4",All models areimplemented using Theano framework (Al-Rfou et al,0
12652,We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,"In the MLE principle, we only need empirical samples of
the joint distribution to maximize the likelihood function of the model given the data",0
12653,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12654,"For PIB, we also usedβ−1l = β−1 = 10−4","As can be seen from the table, PIB and Model C givesnearly the same performance which outperform deterministic neural networks and SFNNs, and theirstochastic and deterministic version.It is interesting to empirically see that the deterministic version of PIB at test time (Model C) gives aslightly better result than PIB",0
12655,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",URL http://arxiv.org/abs/1605.02688.,0
12656,"This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al",Figure 1: A directed graphical representation of a PIB of two bottlenecks,0
12657,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,"Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest",0
12658,"The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12659,We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,"For deterministicneural networks, we only have one sample of hidden variables given one data point",0
12660,"The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12661,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required","In estimating mutual information, we adopted the variational method asin Alemi et al",0
12662,"The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12663,(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12664,"This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al","For simplicity, we present the derivation of I(Z1, Z0) only 3",0
12665,"For PIB, we also usedβ−1l = β−1 = 10−4","More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
12666,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12667,(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,"Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp",0
12668,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12669,"The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).",(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
12670,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presentedin the information plane,0
12671,(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,"530–538, 2013",0
12672,"For PIB, we also usedβ−1l = β−1 = 10−4","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12673,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required","Yoshua Bengio, Nicholas L´eonard, and Aaron C",0
12674,The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,0
12675,"The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).",Learning stochastic feedforward neural networks,0
12676,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12677,(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,"Here we preferto this extreme, i.e., the 0th level, as the super level",0
12678,(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,0
12679,(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",0
12680,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",0
12681,We trained themodels in the full training set of 60000 images and tested in the test set,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12682,"This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12683,"The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).",Sergey Ioffe and Christian Szegedy,0
12684,"This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12685,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12686,The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,"(2014)) and game playing (e.g., Silver et al.(2016))",0
12687,The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12688,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,"arXiv preprint arXiv:1406.1078, 2014.",0
12689,"The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).","We scaled the images to [0, 1] and do not perform anyother data augmentation",0
12690,We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12691,"The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).",Batch normalization: Accelerating deep network training byreducing internal covariate shift,0
12692,We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,doi: 10.1109/CVPR.2016.90,0
12693,"The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",0
12694,The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12695,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12696,"The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12697,The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
12698,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",0
12699,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",1
12700,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12701,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,URLhttps://doi.org/10.1109/CVPR.2016.90.,0
12702,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12703,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",0
12704,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12705,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12706,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,0
12707,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12708,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12709,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12710,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,networks and averaged the mutual informations,0
12711,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12712,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,"Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution",0
12713,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,0
12714,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12715,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,"For PIB, we also usedβ−1l = β−1 = 10−4",0
12716,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12717,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12718,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,"Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",0
12719,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12720,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12721,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,0
12722,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,"The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
12723,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12724,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12725,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12726,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12727,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,"In estimating mutual information, we adopted the variational method asin Alemi et al",0
12728,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,"In estimating mutual information, we adopted the variational method asin Alemi et al",0
12729,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12730,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,"For the compression, we
decompose the mutual information as follows:",0
12731,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12732,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,0
12733,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",0
12734,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12735,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,"(1999)) with the implicit selfwhere Z(x; β) is the normalization function, and DKL[.(cid:107).] is the Kullback - Leibler (KL) divergence
(Kullback & Leibler (1951))",0
12736,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12737,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12738,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12739,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",0
12740,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,"The MLE principle treats
the neural network model p(xxx; θθθ) as a whole without explicitly considering the contribution of its
internal structures (e.g., hidden layers and hidden neurons)",0
12741,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,"To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)",0
12742,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12743,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12744,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12745,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12746,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,Each hidden layer in SFNNs is also considered as a stochastic variable,0
12747,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,"More speciﬁcally, our work proposes an optimizationIB criteria for an existing neural network architecture in an effort to better learn the layers’ represen-tation to their IB optimality",0
12748,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
12749,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,URLhttps://doi.org/10.1109/CVPR.2016.90.,0
12750,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12751,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12752,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
12753,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,"More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
12754,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",0
12755,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12756,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,"informative yet compressed representation, which is supported by qualitative empirical results",0
12757,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12758,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",0
12759,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12760,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,"More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
12761,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12762,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12763,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12764,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12765,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12766,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance",0
12767,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12768,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,"Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",0
12769,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",0
12770,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12771,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
12772,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,Xavier Glorot and Yoshua Bengio,0
12773,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12774,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12775,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12776,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12777,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12778,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12779,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
12780,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12781,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12782,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,"In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",0
12783,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12784,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12785,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12786,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,"Although the analysis offers a new perspective about optimality in neural networks,it proposes general analysis of optimality rather than a practical optimization criteria",0
12787,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12788,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,"However in general the two principles are not obviously related.
In this work, we leverage neural networks and the IB principle by viewing neural networks as a set
of encoders that sequentially modify the original data space",0
12789,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J",0
12790,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12791,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12792,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12793,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12794,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,"Note that
for a given joint distribution pD(xxx, yyy), the relevance decoder ptrue(yyy|zzzl) is uniquely determined if an
encoding function p(zzzl|xxx) is deﬁned",0
12795,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,"Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution ˆpD as possible (e.g., see Appendix I.B)",0
12796,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",0
12797,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12798,We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,"(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning",0
12799,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,1
12800,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,Learning stochastic feedforward neural networks,0
12801,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning",0
12802,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classiﬁ-cation task",0
12803,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","The MLE
principle is proved to be mathematically equivalent to the IB principle for the multinomial mixture
model for clustering problem when the input distribution X is uniform or has a large sample size
(Slonim & Weiss (2002))",0
12804,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",0
12805,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12806,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12807,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12808,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12809,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",0
12810,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12811,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"For PIB, we used β−1l = β−1 = 10−4",0
12812,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12813,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12814,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12815,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"CoRR, abs/1406.2989, 2014",0
12816,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
12817,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,(1999),0
12818,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,0
12819,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12820,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,URL https://doi.org/10.1038/nature16961.,0
12821,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
12822,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12823,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12824,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,0
12825,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12826,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12827,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"informative yet compressed representation, which is supported by qualitative empirical results",0
12828,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",0
12829,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12830,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,Techniques for learningbinary stochastic feedforward neural networks,0
12831,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12832,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",0
12833,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12834,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12835,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12836,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)",0
12837,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12838,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,Each hidden layer in SFNNs is also considered as a stochastic variable,0
12839,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,The gradient is then propagated only throughthe deterministic term and ignored in the noise term,0
12840,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",Learning stochastic feedforward neural networks,0
12841,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12842,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",0
12843,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12844,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12845,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12846,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12847,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12848,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,URL http://arxiv.org/abs/1406.2989.,0
12849,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Speciﬁcally, the relevance decoder is determined as follows:",0
12850,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12851,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation",0
12852,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12853,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,URL https://doi.org/10.1109/CVPR.2015.7298594.,0
12854,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12855,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",URLhttp://arxiv.org/abs/1308.3432.,0
12856,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",0
12857,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12858,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12859,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12860,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation",0
12861,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12862,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","770–778, 2016",0
12863,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12864,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12865,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12866,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,Model E is SFNN and Model B is Model C with deterministicprediction during test phase,0
12867,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","770–778, 2016",0
12868,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12869,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",0
12870,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
12871,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12872,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12873,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12874,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12875,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12876,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12877,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),0
12878,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12879,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL",0
12880,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",URL http://www.jmlr.org/papers/v6/chechik05a.html.,0
12881,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"arXiv e-prints, abs/1605.02688,May 2016",0
12882,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12883,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.",0
12884,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,0
12885,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12886,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12887,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12888,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12889,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12890,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12891,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","However in general the two principles are not obviously related.
In this work, we leverage neural networks and the IB principle by viewing neural networks as a set
of encoders that sequentially modify the original data space",0
12892,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12893,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
12894,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,The ﬁrst model (ModelA) is a deterministic neural network,0
12895,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12896,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J",0
12897,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",1
12898,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",0
12899,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,"The MLE principle treats
the neural network model p(xxx; θθθ) as a whole without explicitly considering the contribution of its
internal structures (e.g., hidden layers and hidden neurons)",0
12900,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.",0
12901,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)",0
12902,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",0
12903,"informative yet compressed representation, which is supported by qualitative empirical results","Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",0
12904,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,Xavier Glorot and Yoshua Bengio,0
12905,"informative yet compressed representation, which is supported by qualitative empirical results","The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",0
12906,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","informative yet compressed representation, which is supported by qualitative empirical results",1
12907,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"informative yet compressed representation, which is supported by qualitative empirical results",1
12908,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",0
12909,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
12910,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","informative yet compressed representation, which is supported by qualitative empirical results",1
12911,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)",0
12912,"informative yet compressed representation, which is supported by qualitative empirical results","informative yet compressed representation, which is supported by qualitative empirical results",1
12913,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp",0
12914,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution ˆpD as possible (e.g., see Appendix I.B)",0
12915,"informative yet compressed representation, which is supported by qualitative empirical results",This also empirically holds for the case of SFNN,0
12916,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"informative yet compressed representation, which is supported by qualitative empirical results",1
12917,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","informative yet compressed representation, which is supported by qualitative empirical results",1
12918,"informative yet compressed representation, which is supported by qualitative empirical results","informative yet compressed representation, which is supported by qualitative empirical results",1
12919,"informative yet compressed representation, which is supported by qualitative empirical results","However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)",0
12920,"informative yet compressed representation, which is supported by qualitative empirical results","informative yet compressed representation, which is supported by qualitative empirical results",1
12921,"informative yet compressed representation, which is supported by qualitative empirical results","informative yet compressed representation, which is supported by qualitative empirical results",1
12922,"informative yet compressed representation, which is supported by qualitative empirical results","informative yet compressed representation, which is supported by qualitative empirical results",1
12923,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","informative yet compressed representation, which is supported by qualitative empirical results",1
12924,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","informative yet compressed representation, which is supported by qualitative empirical results",1
12925,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,The MLE principle maximizes the likelihood function under theempirical data distribution,0
12926,"informative yet compressed representation, which is supported by qualitative empirical results","In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",0
12927,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"informative yet compressed representation, which is supported by qualitative empirical results",1
12928,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","informative yet compressed representation, which is supported by qualitative empirical results",1
12929,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",0
12930,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"770–778, 2016",0
12931,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","informative yet compressed representation, which is supported by qualitative empirical results",1
12932,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"However, thealgorithm is not applicable to neural networks",0
12933,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",0
12934,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"informative yet compressed representation, which is supported by qualitative empirical results",1
12935,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","informative yet compressed representation, which is supported by qualitative empirical results",1
12936,"informative yet compressed representation, which is supported by qualitative empirical results","informative yet compressed representation, which is supported by qualitative empirical results",1
12937,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"informative yet compressed representation, which is supported by qualitative empirical results",1
12938,"informative yet compressed representation, which is supported by qualitative empirical results",We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,0
12939,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"informative yet compressed representation, which is supported by qualitative empirical results",1
12940,"informative yet compressed representation, which is supported by qualitative empirical results","informative yet compressed representation, which is supported by qualitative empirical results",1
12941,"informative yet compressed representation, which is supported by qualitative empirical results","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
12942,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"informative yet compressed representation, which is supported by qualitative empirical results",1
12943,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","informative yet compressed representation, which is supported by qualitative empirical results",1
12944,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","Speciﬁcally, the relevance decoder is determined as follows:",0
12945,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"informative yet compressed representation, which is supported by qualitative empirical results",1
12946,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"informative yet compressed representation, which is supported by qualitative empirical results",1
12947,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"informative yet compressed representation, which is supported by qualitative empirical results",1
12948,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ",0
12949,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",0
12950,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"informative yet compressed representation, which is supported by qualitative empirical results",1
12951,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures",This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),0
12952,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","informative yet compressed representation, which is supported by qualitative empirical results",1
12953,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","informative yet compressed representation, which is supported by qualitative empirical results",1
12954,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures",Yichuan Tang and Ruslan Salakhutdinov,0
12955,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"informative yet compressed representation, which is supported by qualitative empirical results",1
12956,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"informative yet compressed representation, which is supported by qualitative empirical results",1
12957,"informative yet compressed representation, which is supported by qualitative empirical results","Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",0
12958,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
12959,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"σ(.) is the sigmoid function, and W (l) is the weights connecting the lth layer to the (l + 1)thlayer",0
12960,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",0
12961,"informative yet compressed representation, which is supported by qualitative empirical results",Learning phrase representations using rnn encoder-decoderfor statistical machine translation,0
12962,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,0
12963,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"informative yet compressed representation, which is supported by qualitative empirical results",1
12964,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"informative yet compressed representation, which is supported by qualitative empirical results",1
12965,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"informative yet compressed representation, which is supported by qualitative empirical results",1
12966,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"informative yet compressed representation, which is supported by qualitative empirical results",1
12967,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"informative yet compressed representation, which is supported by qualitative empirical results",1
12968,"informative yet compressed representation, which is supported by qualitative empirical results","For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
12969,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"informative yet compressed representation, which is supported by qualitative empirical results",1
12970,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","informative yet compressed representation, which is supported by qualitative empirical results",1
12971,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures",We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,0
12972,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable",0
12973,"informative yet compressed representation, which is supported by qualitative empirical results","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",0
12974,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"informative yet compressed representation, which is supported by qualitative empirical results",1
12975,"informative yet compressed representation, which is supported by qualitative empirical results","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",0
12976,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"informative yet compressed representation, which is supported by qualitative empirical results",1
12977,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","informative yet compressed representation, which is supported by qualitative empirical results",1
12978,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"informative yet compressed representation, which is supported by qualitative empirical results",1
12979,"informative yet compressed representation, which is supported by qualitative empirical results","Fortunately,one can estimate the gradient in this case",0
12980,"informative yet compressed representation, which is supported by qualitative empirical results","A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",0
12981,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",0
12982,"informative yet compressed representation, which is supported by qualitative empirical results","informative yet compressed representation, which is supported by qualitative empirical results",1
12983,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,0
12984,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"informative yet compressed representation, which is supported by qualitative empirical results",1
12985,"informative yet compressed representation, which is supported by qualitative empirical results","We extend the notations of Zl by using the convention Z0 := X and Z−1 := ∅.The space of X, Y and Zl are denoted as X ,Y and Zl, respectively",0
12986,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"informative yet compressed representation, which is supported by qualitative empirical results",1
12987,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"informative yet compressed representation, which is supported by qualitative empirical results",1
12988,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,"informative yet compressed representation, which is supported by qualitative empirical results",1
12989,"informative yet compressed representation, which is supported by qualitative empirical results","(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))",0
12990,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"In practice, the IB problem can be solved efﬁcientlyin the following two cases only: (1) X, Y and Z are all discrete (Tishby et al",0
12991,"informative yet compressed representation, which is supported by qualitative empirical results","informative yet compressed representation, which is supported by qualitative empirical results",1
12992,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),0
12993,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively",0
12994,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,This workproposes using mutual information of a hidden layer with the input layer and the output layer toquantify the performance of DNNs,0
12995,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),0
12996,Onelimitation is that we consider here fully-connected feed-forward architecture with binary hiddenlayers,"Originally, the general IB framework is proposed in Tishby et al",0
12997,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures","informative yet compressed representation, which is supported by qualitative empirical results",1
12998,"informative yet compressed representation, which is supported by qualitative empirical results","Yoshua Bengio, Nicholas L´eonard, and Aaron C",0
12999,"informative yet compressed representation, which is supported by qualitative empirical results","We then propose a new generalized
IB-based objective that takes into account the compression and relevance of all layers in the network
as an explicit goal for guiding the encodings in a beneﬁcial manner",0
13000,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13001,"arXiv e-prints, abs/1605.02688,May 2016","530–538, 2013",0
13002,"arXiv e-prints, abs/1605.02688,May 2016","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",0
13003,"arXiv e-prints, abs/1605.02688,May 2016","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13004,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13005,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","David Silver, Aja Huang, Chris J",0
13006,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13007,"McGibbon, Roland Memisevic, Bart van Merri¨enboer, Vincent Michalski, MehdiMirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel,Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Sal-vatier, Franc¸ois Savard, Jan Schl¨uter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,Dmitriy Serdyuk, Samira Shabanian, ´Etienne Simon, Sigurd Spieckermann, S",Model E is SFNN and Model B is Model C with deterministicprediction during test phase,0
13008,"Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13009,URL http://arxiv.org/abs/1605.02688.,"Onaverage, 2H(X|Z) elements of X are mapped to the same code in Z",0
13010,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
13011,"arXiv e-prints, abs/1605.02688,May 2016","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13012,"McGibbon, Roland Memisevic, Bart van Merri¨enboer, Vincent Michalski, MehdiMirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel,Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Sal-vatier, Franc¸ois Savard, Jan Schl¨uter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,Dmitriy Serdyuk, Samira Shabanian, ´Etienne Simon, Sigurd Spieckermann, S","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13013,URL http://arxiv.org/abs/1605.02688.,"To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)",0
13014,"arXiv e-prints, abs/1605.02688,May 2016","Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution ˆpD as possible (e.g., see Appendix I.B)",0
13015,URL http://arxiv.org/abs/1605.02688.,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer",0
13016,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J",(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
13017,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J",We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,0
13018,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","Firstly, we decompose the mutual information into a differenceof two entropies:3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
13019,"Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13020,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13021,Theano: A Pythonframework for fast computation of mathematical expressions,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)",0
13022,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13023,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J","For deterministicneural networks, we only have one sample of hidden variables given one data point",0
13024,URL http://arxiv.org/abs/1605.02688.,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13025,"Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13026,"Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13027,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13028,"Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13029,"arXiv e-prints, abs/1605.02688,May 2016","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13030,"Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang","This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al",0
13031,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13032,"Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang",(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
13033,Theano: A Pythonframework for fast computation of mathematical expressions,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",0
13034,"McGibbon, Roland Memisevic, Bart van Merri¨enboer, Vincent Michalski, MehdiMirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel,Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Sal-vatier, Franc¸ois Savard, Jan Schl¨uter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,Dmitriy Serdyuk, Samira Shabanian, ´Etienne Simon, Sigurd Spieckermann, S",This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,0
13035,"arXiv e-prints, abs/1605.02688,May 2016","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13036,"arXiv e-prints, abs/1605.02688,May 2016","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13037,Theano: A Pythonframework for fast computation of mathematical expressions,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
13038,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",The ﬁrst model (ModelA) is a deterministic neural network,0
13039,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,0
13040,Theano: A Pythonframework for fast computation of mathematical expressions,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13041,URL http://arxiv.org/abs/1605.02688.,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks",0
13042,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13043,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","448–456, 2015",0
13044,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
13045,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13046,"arXiv e-prints, abs/1605.02688,May 2016","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13047,Theano: A Pythonframework for fast computation of mathematical expressions,"Nitish Srivastava, Geoffrey E",0
13048,"McGibbon, Roland Memisevic, Bart van Merri¨enboer, Vincent Michalski, MehdiMirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel,Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Sal-vatier, Franc¸ois Savard, Jan Schl¨uter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,Dmitriy Serdyuk, Samira Shabanian, ´Etienne Simon, Sigurd Spieckermann, S","In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",0
13049,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13050,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J","arXiv e-prints, abs/1605.02688,May 2016",0
13051,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13052,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13053,"arXiv e-prints, abs/1605.02688,May 2016",We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,0
13054,Theano: A Pythonframework for fast computation of mathematical expressions,"For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",0
13055,"Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang",This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,0
13056,"McGibbon, Roland Memisevic, Bart van Merri¨enboer, Vincent Michalski, MehdiMirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel,Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Sal-vatier, Franc¸ois Savard, Jan Schl¨uter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,Dmitriy Serdyuk, Samira Shabanian, ´Etienne Simon, Sigurd Spieckermann, S",Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,0
13057,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","For deterministicneural networks, we only have one sample of hidden variables given one data point",0
13058,"Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13059,"arXiv e-prints, abs/1605.02688,May 2016","However, the MLE principle is very generic thatis not specially tailored for neural networks",0
13060,URL http://arxiv.org/abs/1605.02688.,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",0
13061,URL http://arxiv.org/abs/1605.02688.,"For comparisons, we trained PIBs and ﬁve additional models",0
13062,URL http://arxiv.org/abs/1605.02688.,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp",0
13063,URL http://arxiv.org/abs/1605.02688.,"The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
13064,URL http://arxiv.org/abs/1605.02688.,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13065,Theano: A Pythonframework for fast computation of mathematical expressions,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)",0
13066,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J","More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
13067,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J",A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,0
13068,"arXiv e-prints, abs/1605.02688,May 2016","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13069,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13070,"McGibbon, Roland Memisevic, Bart van Merri¨enboer, Vincent Michalski, MehdiMirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel,Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Sal-vatier, Franc¸ois Savard, Jan Schl¨uter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,Dmitriy Serdyuk, Samira Shabanian, ´Etienne Simon, Sigurd Spieckermann, S",(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
13071,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N","Optimizing PIBs now becomes the minimization of LP IB(Z) which at-tempts to decrease I(Zl, Zl−1) and increase I(Zl, Y ) simultaneously",0
13072,"Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang",This workproposes using mutual information of a hidden layer with the input layer and the output layer toquantify the performance of DNNs,0
13073,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13074,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J","The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",0
13075,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J","The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",0
13076,URL http://arxiv.org/abs/1605.02688.,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13077,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T",This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),0
13078,"McGibbon, Roland Memisevic, Bart van Merri¨enboer, Vincent Michalski, MehdiMirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel,Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Sal-vatier, Franc¸ois Savard, Jan Schl¨uter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,Dmitriy Serdyuk, Samira Shabanian, ´Etienne Simon, Sigurd Spieckermann, S","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13079,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13080,URL http://arxiv.org/abs/1605.02688.,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13081,URL http://arxiv.org/abs/1605.02688.,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13082,Theano: A Pythonframework for fast computation of mathematical expressions,"The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",0
13083,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13084,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J","Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al",0
13085,"McGibbon, Roland Memisevic, Bart van Merri¨enboer, Vincent Michalski, MehdiMirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel,Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Sal-vatier, Franc¸ois Savard, Jan Schl¨uter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,Dmitriy Serdyuk, Samira Shabanian, ´Etienne Simon, Sigurd Spieckermann, S",We trained themodels in the full training set of 60000 images and tested in the test set,0
13086,Theano: A Pythonframework for fast computation of mathematical expressions,The MLE principle maximizes the likelihood function under theempirical data distribution,0
13087,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13088,"arXiv e-prints, abs/1605.02688,May 2016","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13089,Theano: A Pythonframework for fast computation of mathematical expressions,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13090,"McGibbon, Roland Memisevic, Bart van Merri¨enboer, Vincent Michalski, MehdiMirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel,Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Sal-vatier, Franc¸ois Savard, Jan Schl¨uter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,Dmitriy Serdyuk, Samira Shabanian, ´Etienne Simon, Sigurd Spieckermann, S","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13091,"Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13092,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",1
13093,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","By analyzing these measures with the IB principle, the authorsestablish an information-theoretic learning principle for DNNs.In theory, one can optimize theneural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-wise manner",0
13094,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)",0
13095,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","The optimal representation Z is determined via theminimization of the following Lagrangian:LIB[p(z|x)] = I(Z, X) − βI(Z, Y )",0
13096,"Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang","Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",0
13097,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T","Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang",0
13098,"McGibbon, Roland Memisevic, Bart van Merri¨enboer, Vincent Michalski, MehdiMirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel,Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Sal-vatier, Franc¸ois Savard, Jan Schl¨uter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,Dmitriy Serdyuk, Samira Shabanian, ´Etienne Simon, Sigurd Spieckermann, S","drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
13099,"Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang",In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,0
13100,Courville,"McGibbon, Roland Memisevic, Bart van Merri¨enboer, Vincent Michalski, MehdiMirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel,Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Sal-vatier, Franc¸ois Savard, Jan Schl¨uter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,Dmitriy Serdyuk, Samira Shabanian, ´Etienne Simon, Sigurd Spieckermann, S",0
13101,Courville,The recent work Alemi et al,0
13102,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
13103,URLhttp://arxiv.org/abs/1308.3432.,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13104,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13105,Courville,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13106,URLhttp://arxiv.org/abs/1308.3432.,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13107,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13108,URLhttp://arxiv.org/abs/1308.3432.,"Previously, the InformationBottleneck (IB) framework (Tishby et al",0
13109,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",All models areimplemented using Theano framework (Al-Rfou et al,0
13110,URLhttp://arxiv.org/abs/1308.3432.,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13111,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13112,URLhttp://arxiv.org/abs/1308.3432.,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13113,URLhttp://arxiv.org/abs/1308.3432.,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13114,Courville,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13115,URLhttp://arxiv.org/abs/1308.3432.,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
13116,"Yoshua Bengio, Nicholas L´eonard, and Aaron C","Ramana Subra-manyam, Jakub Sygnowski, J´er´emie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J",0
13117,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13118,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13119,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"448–456, 2015",0
13120,URLhttp://arxiv.org/abs/1308.3432.,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)",0
13121,"Yoshua Bengio, Nicholas L´eonard, and Aaron C","Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis",0
13122,"Yoshua Bengio, Nicholas L´eonard, and Aaron C","The optimal representation Z is determined via theminimization of the following Lagrangian:Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",0
13123,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,0
13124,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13125,"Yoshua Bengio, Nicholas L´eonard, and Aaron C","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13126,URLhttp://arxiv.org/abs/1308.3432.,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13127,"Yoshua Bengio, Nicholas L´eonard, and Aaron C","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13128,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13129,"CoRR, abs/1308.3432, 2013",The neural network in this perspectiveconsists of a series of stochastic encoders that sequentially encode the original data space X intothe intermediate code spaces Zl,0
13130,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",0
13131,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13132,Courville,(2016)).,0
13133,Courville,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13134,Courville,"As β varies from 0 to ∞, Z traces a concave curve, knownas information curve for representation Z, with a slope of β−1",0
13135,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13136,Courville,"It is also important to note that many stochastic neural networks have been proposed before (e.g.,Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al",0
13137,Courville,"530–538, 2013",0
13138,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13139,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",Information bottleneck for gaus-sian variables,0
13140,"Yoshua Bengio, Nicholas L´eonard, and Aaron C","For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
13141,Courville,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13142,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13143,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",URLhttp://arxiv.org/abs/1308.3432.,0
13144,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13145,URLhttp://arxiv.org/abs/1308.3432.,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13146,URLhttp://arxiv.org/abs/1308.3432.,"Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation",0
13147,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"For deterministicneural networks, we only have one sample of hidden variables given one data point",0
13148,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13149,"Yoshua Bengio, Nicholas L´eonard, and Aaron C","In the general IB framework, each value of β speciﬁes a uniquepoint of Z in the information plane",0
13150,Courville,"Furthermore,estimating mutual information between the variables transformed by network layers and the datavariables poses several computational challenges in practice that the authors did not address in thework",0
13151,Courville,"The MLE principle treats
the neural network model p(xxx; θθθ) as a whole without explicitly considering the contribution of its
internal structures (e.g., hidden layers and hidden neurons)",0
13152,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13153,URLhttp://arxiv.org/abs/1308.3432.,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13154,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13155,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13156,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,0
13157,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13158,"Yoshua Bengio, Nicholas L´eonard, and Aaron C","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13159,Courville,"Here we preferto this extreme, i.e., the 0th level, as the super level",0
13160,"CoRR, abs/1308.3432, 2013",Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,0
13161,Courville,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13162,URLhttp://arxiv.org/abs/1308.3432.,"In estimating mutual information, we adopted the variational method asin Alemi et al",0
13163,Courville,"As can be seen from the table, PIB and Model C givesnearly the same performance which outperform deterministic neural networks and SFNNs, and theirstochastic and deterministic version.It is interesting to empirically see that the deterministic version of PIB at test time (Model C) gives aslightly better result than PIB",0
13164,"Yoshua Bengio, Nicholas L´eonard, and Aaron C","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13165,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13166,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13167,"Yoshua Bengio, Nicholas L´eonard, and Aaron C","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13168,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13169,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,0
13170,Courville,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13171,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13172,"CoRR, abs/1308.3432, 2013","1–9, 2015",0
13173,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13174,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13175,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13176,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13177,"CoRR, abs/1308.3432, 2013","However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )",0
13178,"CoRR, abs/1308.3432, 2013",Estimating or propagating gradientsthrough stochastic neurons for conditional computation,0
13179,"CoRR, abs/1308.3432, 2013","For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
13180,Courville,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13181,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13182,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13183,URLhttp://arxiv.org/abs/1308.3432.,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13184,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13185,"Yoshua Bengio, Nicholas L´eonard, and Aaron C","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13186,"Yoshua Bengio, Nicholas L´eonard, and Aaron C","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13187,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13188,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13189,"Yoshua Bengio, Nicholas L´eonard, and Aaron C","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13190,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,0
13191,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable",0
13192,"Yoshua Bengio, Nicholas L´eonard, and Aaron C","Fortunately,one can estimate the gradient in this case",0
13193,Courville,"Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",0
13194,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
13195,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13196,"CoRR, abs/1308.3432, 2013","Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13197,"CoRR, abs/1308.3432, 2013","We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.",0
13198,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",1
13199,Estimating or propagating gradientsthrough stochastic neurons for conditional computation,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",0
13200,Information bottleneck for gaus-sian variables,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",0
13201,Information bottleneck for gaus-sian variables,"In this work, we use the gradient estimator inspiredby Raiko et al",0
13202,"Journal of Machine Learning Research, 6:165–188, 2005","Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting",0
13203,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13204,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13205,Information bottleneck for gaus-sian variables,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13206,URL http://www.jmlr.org/papers/v6/chechik05a.html.,(1999),0
13207,"Journal of Machine Learning Research, 6:165–188, 2005","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13208,"Journal of Machine Learning Research, 6:165–188, 2005","For PIBs, we set βl = β,∀1 ≤ l ≤ L",0
13209,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13210,Information bottleneck for gaus-sian variables,We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,0
13211,URL http://www.jmlr.org/papers/v6/chechik05a.html.,URLhttps://doi.org/10.1109/CVPR.2016.90.,0
13212,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13213,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13214,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",This workproposes using mutual information of a hidden layer with the input layer and the output layer toquantify the performance of DNNs,0
13215,Information bottleneck for gaus-sian variables,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,0
13216,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","(2012), Szegedy et al",0
13217,Information bottleneck for gaus-sian variables,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13218,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","For simplicity, we present the derivation of I(Z1, Z0) only 3",0
13219,"Journal of Machine Learning Research, 6:165–188, 2005","CoRR, abs/1308.3432, 2013",0
13220,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13221,Information bottleneck for gaus-sian variables,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13222,"Journal of Machine Learning Research, 6:165–188, 2005","The MLE
principle is proved to be mathematically equivalent to the IB principle for the multinomial mixture
model for clustering problem when the input distribution X is uniform or has a large sample size
(Slonim & Weiss (2002))",0
13223,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13224,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"In this work, we use the gradient estimator inspiredby Raiko et al",0
13225,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","(1999)); or (2) X, Yand Z are mutually joint Gaussian (Chechik et al",0
13226,"Journal of Machine Learning Research, 6:165–188, 2005","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13227,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13228,URL http://www.jmlr.org/papers/v6/chechik05a.html.,(1999),0
13229,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13230,Information bottleneck for gaus-sian variables,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13231,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",URLhttps://doi.org/10.1109/CVPR.2016.90.,0
13232,Information bottleneck for gaus-sian variables,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
13233,Information bottleneck for gaus-sian variables,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13234,Information bottleneck for gaus-sian variables,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13235,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",0
13236,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13237,Information bottleneck for gaus-sian variables,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13238,"Journal of Machine Learning Research, 6:165–188, 2005",Estimating or propagating gradientsthrough stochastic neurons for conditional computation,0
13239,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13240,"Journal of Machine Learning Research, 6:165–188, 2005","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13241,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13242,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13243,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13244,"Journal of Machine Learning Research, 6:165–188, 2005","For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
13245,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",0
13246,Information bottleneck for gaus-sian variables,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13247,Information bottleneck for gaus-sian variables,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13248,"Journal of Machine Learning Research, 6:165–188, 2005","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13249,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13250,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
13251,Information bottleneck for gaus-sian variables,"Since the objective is designed
to optimize all parameters of neural networks and is mainly motivated by the IB principle for deep
learning (Tishby & Zaslavsky (2015)), we name this method the Parametric Information Bottleneck
(PIB)",0
13252,"Journal of Machine Learning Research, 6:165–188, 2005","For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:i=1",0
13253,"Journal of Machine Learning Research, 6:165–188, 2005","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13254,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",0
13255,URL http://www.jmlr.org/papers/v6/chechik05a.html.,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
13256,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13257,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time",0
13258,"Journal of Machine Learning Research, 6:165–188, 2005","We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level",0
13259,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"Since we used generated samples to estimate mutual information, we can potentially extendthe learning framework to larger and more complicated neural network architectures",0
13260,"Journal of Machine Learning Research, 6:165–188, 2005","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13261,Information bottleneck for gaus-sian variables,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13262,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13263,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"In the following section, we presentour approximation to LP IB which fully utilizes the existing architecture without resorting to anymodel that is not part of the considered neural network",0
13264,Information bottleneck for gaus-sian variables,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13265,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","1–9, 2015",0
13266,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer",0
13267,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,0
13268,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",0
13269,"Journal of Machine Learning Research, 6:165–188, 2005","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13270,"Journal of Machine Learning Research, 6:165–188, 2005","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13271,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","The plane has I(Z, X) and I(Z, Y ) as its horizontal axisand its vertical axis, respectively",0
13272,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13273,"Journal of Machine Learning Research, 6:165–188, 2005","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13274,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13275,"Journal of Machine Learning Research, 6:165–188, 2005","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13276,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",0
13277,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13278,Information bottleneck for gaus-sian variables,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13279,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13280,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13281,"Journal of Machine Learning Research, 6:165–188, 2005","PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",0
13282,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
13283,Information bottleneck for gaus-sian variables,This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,0
13284,"Journal of Machine Learning Research, 6:165–188, 2005","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13285,Information bottleneck for gaus-sian variables,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13286,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13287,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",0
13288,"Journal of Machine Learning Research, 6:165–188, 2005",content of the original space possibly including its dimensionality and topological structure,0
13289,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13290,Information bottleneck for gaus-sian variables,Each hidden layer in SFNNs is also considered as a stochastic variable,0
13291,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",0
13292,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13293,URL http://www.jmlr.org/papers/v6/chechik05a.html.,The dashed blue arrows do not denote variable dependencies butthe relevance decoders for each bottleneck,0
13294,Information bottleneck for gaus-sian variables,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,0
13295,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13296,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss","Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting",0
13297,"Journal of Machine Learning Research, 6:165–188, 2005","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13298,URL http://www.jmlr.org/papers/v6/chechik05a.html.,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13299,Information bottleneck for gaus-sian variables,"Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",1
13300,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13301,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,Our PIB is also a standard steptowards better exploiting representational power of more expressive neural network models such asConvolutional Neural Networks (LeCun et al,0
13302,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13303,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)",0
13304,"arXiv preprint arXiv:1406.1078, 2014.",Courville,0
13305,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",Estimating or propagating gradientsthrough stochastic neurons for conditional computation,0
13306,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
13307,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13308,"arXiv preprint arXiv:1406.1078, 2014.","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13309,"arXiv preprint arXiv:1406.1078, 2014.","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13310,"arXiv preprint arXiv:1406.1078, 2014.","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13311,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,0
13312,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,0
13313,"arXiv preprint arXiv:1406.1078, 2014.","For comparisons, we trained PIBs and ﬁve additional models",0
13314,"arXiv preprint arXiv:1406.1078, 2014.","drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
13315,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,URL http://arxiv.org/abs/1605.02688.,0
13316,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable",0
13317,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13318,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich",0
13319,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13320,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
13321,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
13322,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13323,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13324,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13325,"arXiv preprint arXiv:1406.1078, 2014.","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13326,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",Xavier Glorot and Yoshua Bengio,0
13327,"arXiv preprint arXiv:1406.1078, 2014.","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13328,"arXiv preprint arXiv:1406.1078, 2014.",We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,0
13329,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13330,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","For PIB, we used β−1l = β−1 = 10−4",0
13331,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks",0
13332,"arXiv preprint arXiv:1406.1078, 2014.","335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",0
13333,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13334,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",Courville,0
13335,"arXiv preprint arXiv:1406.1078, 2014.","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13336,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",Sergey Ioffe and Christian Szegedy,0
13337,"arXiv preprint arXiv:1406.1078, 2014.","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13338,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13339,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13340,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13341,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13342,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time",0
13343,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",0
13344,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13345,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,URLhttps://doi.org/10.1109/CVPR.2016.90.,0
13346,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","770–778, 2016",0
13347,"arXiv preprint arXiv:1406.1078, 2014.","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13348,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"However, the variational approximatedistribution for the code space may be too loose while the actual marginal distribution can be sam-pled easily.Our work, on the other hand, focuses on better exploiting intermediate representations of a neuralnetwork architecture using the IB principle",0
13349,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ",0
13350,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",0
13351,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",0
13352,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13353,"arXiv preprint arXiv:1406.1078, 2014.","(2014), Dauphin & Grangier(2016))",0
13354,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,0
13355,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"However, the MLE principle is very generic thatis not specially tailored for neural networks",0
13356,"arXiv preprint arXiv:1406.1078, 2014.",URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,0
13357,"arXiv preprint arXiv:1406.1078, 2014.","The optimal representation Z is determined via theminimization of the following Lagrangian:Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",0
13358,"arXiv preprint arXiv:1406.1078, 2014.","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13359,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13360,"arXiv preprint arXiv:1406.1078, 2014.","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13361,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
13362,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13363,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
13364,"arXiv preprint arXiv:1406.1078, 2014.","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13365,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"(2015)), natural languagetranslation (e.g., Cho et al",0
13366,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13367,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
13368,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,(2014) and predicted the lower half of the MNIST digitsusing the upper half as inputs,0
13369,"arXiv preprint arXiv:1406.1078, 2014.",This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),0
13370,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,The approximation then leads to effectivegradient-based training of PIBs.,0
13371,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13372,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",Techniques for learningbinary stochastic feedforward neural networks,0
13373,"arXiv preprint arXiv:1406.1078, 2014.","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13374,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",The MLE principle maximizes the likelihood function under theempirical data distribution,0
13375,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13376,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13377,"arXiv preprint arXiv:1406.1078, 2014.",We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,0
13378,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13379,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13380,"arXiv preprint arXiv:1406.1078, 2014.","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",0
13381,"arXiv preprint arXiv:1406.1078, 2014.",Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,0
13382,"arXiv preprint arXiv:1406.1078, 2014.","These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer",0
13383,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13384,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","More speciﬁcally, our work proposes an optimizationIB criteria for an existing neural network architecture in an effort to better learn the layers’ represen-tation to their IB optimality",0
13385,"arXiv preprint arXiv:1406.1078, 2014.","Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
13386,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13387,"arXiv preprint arXiv:1406.1078, 2014.","Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance",0
13388,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",(1998)) and ResNet (He et al,0
13389,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,Figure 1: A directed graphical representation of a PIB of two bottlenecks,0
13390,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Thus, it is hard to analytically capture such modiﬁcations.",0
13391,"arXiv preprint arXiv:1406.1078, 2014.","Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp",0
13392,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Thus, it is hard to analytically capture such modiﬁcations.",0
13393,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13394,"arXiv preprint arXiv:1406.1078, 2014.","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13395,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13396,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",1
13397,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio","In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",0
13398,"arXiv preprint arXiv:1406.1078, 2014.","In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ",0
13399,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",Masteringthe game of go with deep neural networks and tree search,0
13400,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,Xavier Glorot and Yoshua Bengio,1
13401,Xavier Glorot and Yoshua Bengio,"We tuned β from{0} ∪ {10−i : 1 ≤ i ≤ 7}, and found β−1 = 10−4 works best.Table 1 provides the results in the MNIST classiﬁcation error in the test set for PIB and the com-parative models (A), (B), (C), (D), and (E)",0
13402,Xavier Glorot and Yoshua Bengio,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",0
13403,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,Xavier Glorot and Yoshua Bengio,1
13404,Xavier Glorot and Yoshua Bengio,Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,0
13405,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,Xavier Glorot and Yoshua Bengio,1
13406,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",Xavier Glorot and Yoshua Bengio,1
13407,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,Xavier Glorot and Yoshua Bengio,1
13408,Understanding the difﬁculty of training deep feedforward neuralnetworks,Xavier Glorot and Yoshua Bengio,1
13409,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,"However, minimizing LP IB has an intuitive interpretation as tight-ening the “information knots” of a neural network architecture simultaneously at every layer level(including the super level)",0
13410,"249–256,2010",Xavier Glorot and Yoshua Bengio,1
13411,"249–256,2010",Xavier Glorot and Yoshua Bengio,1
13412,Understanding the difﬁculty of training deep feedforward neuralnetworks,Discrete-valued variables in PIBs make standard back-propagation not straightforward,0
13413,Understanding the difﬁculty of training deep feedforward neuralnetworks,Xavier Glorot and Yoshua Bengio,1
13414,Xavier Glorot and Yoshua Bengio,Xavier Glorot and Yoshua Bengio,1
13415,Xavier Glorot and Yoshua Bengio,Xavier Glorot and Yoshua Bengio,1
13416,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",Xavier Glorot and Yoshua Bengio,1
13417,"249–256,2010","In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",0
13418,Xavier Glorot and Yoshua Bengio,"(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
13419,Xavier Glorot and Yoshua Bengio,Xavier Glorot and Yoshua Bengio,1
13420,Understanding the difﬁculty of training deep feedforward neuralnetworks,Xavier Glorot and Yoshua Bengio,1
13421,"249–256,2010","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",0
13422,Understanding the difﬁculty of training deep feedforward neuralnetworks,Xavier Glorot and Yoshua Bengio,1
13423,"249–256,2010",The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
13424,Xavier Glorot and Yoshua Bengio,Xavier Glorot and Yoshua Bengio,1
13425,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,0
13426,Xavier Glorot and Yoshua Bengio,Xavier Glorot and Yoshua Bengio,1
13427,Xavier Glorot and Yoshua Bengio,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)",0
13428,Understanding the difﬁculty of training deep feedforward neuralnetworks,"For simplicity, we present the derivation of I(Z1, Z0) only 3",0
13429,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,We used three different randomly initialized neural,0
13430,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
13431,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,0
13432,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,Xavier Glorot and Yoshua Bengio,1
13433,"249–256,2010",URL http://arxiv.org/abs/1605.02688.,0
13434,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,Xavier Glorot and Yoshua Bengio,1
13435,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
13436,Understanding the difﬁculty of training deep feedforward neuralnetworks,Xavier Glorot and Yoshua Bengio,1
13437,Understanding the difﬁculty of training deep feedforward neuralnetworks,"In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",0
13438,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp","530–538, 2013",0
13439,Understanding the difﬁculty of training deep feedforward neuralnetworks,Xavier Glorot and Yoshua Bengio,1
13440,Xavier Glorot and Yoshua Bengio,Xavier Glorot and Yoshua Bengio,1
13441,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,"We scaled the images to [0, 1] and do not perform anyother data augmentation",0
13442,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp","where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",0
13443,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,0
13444,Understanding the difﬁculty of training deep feedforward neuralnetworks,The neural network in this perspectiveconsists of a series of stochastic encoders that sequentially encode the original data space X intothe intermediate code spaces Zl,0
13445,Xavier Glorot and Yoshua Bengio,"We tuned β from{0} ∪ {10−i : 1 ≤ i ≤ 7}, and found β−1 = 10−4 works best.Table 1 provides the results in the MNIST classiﬁcation error in the test set for PIB and the com-parative models (A), (B), (C), (D), and (E)",0
13446,"249–256,2010",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
13447,Understanding the difﬁculty of training deep feedforward neuralnetworks,Xavier Glorot and Yoshua Bengio,1
13448,"249–256,2010",Xavier Glorot and Yoshua Bengio,1
13449,Understanding the difﬁculty of training deep feedforward neuralnetworks,Xavier Glorot and Yoshua Bengio,1
13450,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,0
13451,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P",0
13452,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",Xavier Glorot and Yoshua Bengio,1
13453,"249–256,2010",Xavier Glorot and Yoshua Bengio,1
13454,Xavier Glorot and Yoshua Bengio,Xavier Glorot and Yoshua Bengio,1
13455,Understanding the difﬁculty of training deep feedforward neuralnetworks,Xavier Glorot and Yoshua Bengio,1
13456,Xavier Glorot and Yoshua Bengio,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",0
13457,Xavier Glorot and Yoshua Bengio,Xavier Glorot and Yoshua Bengio,1
13458,"249–256,2010","PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",0
13459,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",Xavier Glorot and Yoshua Bengio,1
13460,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,0
13461,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.",0
13462,Xavier Glorot and Yoshua Bengio,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",0
13463,Understanding the difﬁculty of training deep feedforward neuralnetworks,"However, the variational approximatedistribution for the code space may be too loose while the actual marginal distribution can be sam-pled easily.Our work, on the other hand, focuses on better exploiting intermediate representations of a neuralnetwork architecture using the IB principle",0
13464,Xavier Glorot and Yoshua Bengio,Xavier Glorot and Yoshua Bengio,1
13465,Understanding the difﬁculty of training deep feedforward neuralnetworks,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,0
13466,"249–256,2010","Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",0
13467,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp","CoRR, abs/1308.3432, 2013",0
13468,"249–256,2010","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",0
13469,Xavier Glorot and Yoshua Bengio,The approximation then leads to effectivegradient-based training of PIBs.,0
13470,Understanding the difﬁculty of training deep feedforward neuralnetworks,Xavier Glorot and Yoshua Bengio,1
13471,"249–256,2010",Xavier Glorot and Yoshua Bengio,1
13472,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp","arXiv e-prints, abs/1605.02688,May 2016",0
13473,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp","We propose re-using the existing neural network architecture
as variational decoders for each hidden layers",0
13474,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",Xavier Glorot and Yoshua Bengio,1
13475,Understanding the difﬁculty of training deep feedforward neuralnetworks,"3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
13476,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",Xavier Glorot and Yoshua Bengio,1
13477,Xavier Glorot and Yoshua Bengio,"Since the objective is designed
to optimize all parameters of neural networks and is mainly motivated by the IB principle for deep
learning (Tishby & Zaslavsky (2015)), we name this method the Parametric Information Bottleneck
(PIB)",0
13478,Xavier Glorot and Yoshua Bengio,Xavier Glorot and Yoshua Bengio,1
13479,Understanding the difﬁculty of training deep feedforward neuralnetworks,Xavier Glorot and Yoshua Bengio,1
13480,"249–256,2010",Xavier Glorot and Yoshua Bengio,1
13481,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",Xavier Glorot and Yoshua Bengio,1
13482,Understanding the difﬁculty of training deep feedforward neuralnetworks,Xavier Glorot and Yoshua Bengio,1
13483,"249–256,2010",We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,0
13484,Understanding the difﬁculty of training deep feedforward neuralnetworks,"However, the MLE principle is very generic thatis not specially tailored for neural networks",0
13485,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp","Thus, our framework does not depend on a speciﬁc stochastic model",0
13486,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,Xavier Glorot and Yoshua Bengio,1
13487,Xavier Glorot and Yoshua Bengio,Xavier Glorot and Yoshua Bengio,1
13488,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,Xavier Glorot and Yoshua Bengio,1
13489,"249–256,2010",URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,0
13490,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,Xavier Glorot and Yoshua Bengio,1
13491,Xavier Glorot and Yoshua Bengio,Xavier Glorot and Yoshua Bengio,1
13492,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,Xavier Glorot and Yoshua Bengio,1
13493,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp","In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ",0
13494,Xavier Glorot and Yoshua Bengio,Yichuan Tang and Ruslan Salakhutdinov,0
13495,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,0
13496,Xavier Glorot and Yoshua Bengio,"(2015)), natural languagetranslation (e.g., Cho et al",0
13497,Understanding the difﬁculty of training deep feedforward neuralnetworks,Xavier Glorot and Yoshua Bengio,1
13498,"249–256,2010",Xavier Glorot and Yoshua Bengio,1
13499,URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",0
13500,"770–778, 2016","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13501,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13502,doi: 10.1109/CVPR.2016.90,"For PIB, we used β−1l = β−1 = 10−4",0
13503,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13504,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp",The gradient is then propagated only throughthe deterministic term and ignored in the noise term,0
13505,"770–778, 2016","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13506,URLhttps://doi.org/10.1109/CVPR.2016.90.,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
13507,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13508,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","(2014), Dauphin & Grangier(2016))",0
13509,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13510,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13511,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13512,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",0
13513,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13514,URLhttps://doi.org/10.1109/CVPR.2016.90.,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13515,doi: 10.1109/CVPR.2016.90,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13516,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
13517,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13518,URLhttps://doi.org/10.1109/CVPR.2016.90.,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13519,"770–778, 2016",In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,0
13520,"770–778, 2016","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13521,URLhttps://doi.org/10.1109/CVPR.2016.90.,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13522,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13523,"770–778, 2016","Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters",0
13524,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13525,doi: 10.1109/CVPR.2016.90,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13526,"770–778, 2016","These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer",0
13527,URLhttps://doi.org/10.1109/CVPR.2016.90.,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,0
13528,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13529,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
13530,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.",0
13531,URLhttps://doi.org/10.1109/CVPR.2016.90.,"(2014), Dauphin & Grangier(2016))",0
13532,URLhttps://doi.org/10.1109/CVPR.2016.90.,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13533,doi: 10.1109/CVPR.2016.90,URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,0
13534,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",0
13535,URLhttps://doi.org/10.1109/CVPR.2016.90.,"In Model D, we used the weight trained in Model A to performstochastic prediction at test time",0
13536,URLhttps://doi.org/10.1109/CVPR.2016.90.,The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,0
13537,doi: 10.1109/CVPR.2016.90,"Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",0
13538,URLhttps://doi.org/10.1109/CVPR.2016.90.,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13539,URLhttps://doi.org/10.1109/CVPR.2016.90.,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13540,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13541,"770–778, 2016","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
13542,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13543,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",0
13544,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13545,URLhttps://doi.org/10.1109/CVPR.2016.90.,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,0
13546,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","(1999)); or (2) X, Yand Z are mutually joint Gaussian (Chechik et al",0
13547,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
13548,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",0
13549,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13550,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","Fortunately,one can estimate the gradient in this case",0
13551,"770–778, 2016","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13552,doi: 10.1109/CVPR.2016.90,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
13553,doi: 10.1109/CVPR.2016.90,Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,0
13554,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13555,doi: 10.1109/CVPR.2016.90,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13556,"770–778, 2016","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13557,URLhttps://doi.org/10.1109/CVPR.2016.90.,"In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",0
13558,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",0
13559,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13560,doi: 10.1109/CVPR.2016.90,Yichuan Tang and Ruslan Salakhutdinov,0
13561,URLhttps://doi.org/10.1109/CVPR.2016.90.,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13562,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",0
13563,"770–778, 2016","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13564,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",0
13565,URLhttps://doi.org/10.1109/CVPR.2016.90.,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13566,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13567,URLhttps://doi.org/10.1109/CVPR.2016.90.,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13568,URLhttps://doi.org/10.1109/CVPR.2016.90.,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13569,URLhttps://doi.org/10.1109/CVPR.2016.90.,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13570,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13571,URLhttps://doi.org/10.1109/CVPR.2016.90.,We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,0
13572,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13573,"770–778, 2016","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13574,"770–778, 2016","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13575,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13576,"770–778, 2016","(1999)) with the implicit selfwhere Z(x; β) is the normalization function, and DKL[.(cid:107).] is the Kullback - Leibler (KL) divergence
(Kullback & Leibler (1951))",0
13577,"770–778, 2016","Here we preferto this extreme, i.e., the 0th level, as the super level",0
13578,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
13579,"770–778, 2016","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13580,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13581,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13582,doi: 10.1109/CVPR.2016.90,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",0
13583,URLhttps://doi.org/10.1109/CVPR.2016.90.,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",0
13584,doi: 10.1109/CVPR.2016.90,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13585,doi: 10.1109/CVPR.2016.90,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13586,doi: 10.1109/CVPR.2016.90,"CoRR, abs/1308.3432, 2013",0
13587,doi: 10.1109/CVPR.2016.90,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13588,URLhttps://doi.org/10.1109/CVPR.2016.90.,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),0
13589,URLhttps://doi.org/10.1109/CVPR.2016.90.,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13590,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",doi: 10.1109/CVPR.2016.90,0
13591,doi: 10.1109/CVPR.2016.90,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",0
13592,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13593,"770–778, 2016","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13594,"770–778, 2016","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13595,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",0
13596,"770–778, 2016","For visualization of learned ﬁlters of PIB, seeAppendix II.A.",0
13597,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13598,doi: 10.1109/CVPR.2016.90,"770–778, 2016",0
13599,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",1
13600,Sergey Ioffe and Christian Szegedy,Sergey Ioffe and Christian Szegedy,1
13601,"448–456, 2015",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,0
13602,"448–456, 2015","Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)",0
13603,Sergey Ioffe and Christian Szegedy,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",0
13604,Batch normalization: Accelerating deep network training byreducing internal covariate shift,Sergey Ioffe and Christian Szegedy,1
13605,Sergey Ioffe and Christian Szegedy,"Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.",0
13606,"448–456, 2015","As PIB and SFNN are stochastic neural networks, they can model structured output space in whicha one-to-many mapping is required",0
13607,Batch normalization: Accelerating deep network training byreducing internal covariate shift,"We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",0
13608,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",Sergey Ioffe and Christian Szegedy,1
13609,Batch normalization: Accelerating deep network training byreducing internal covariate shift,"The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)",0
13610,"448–456, 2015","Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
13611,"448–456, 2015",Xavier Glorot and Yoshua Bengio,0
13612,Batch normalization: Accelerating deep network training byreducing internal covariate shift,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,0
13613,Batch normalization: Accelerating deep network training byreducing internal covariate shift,Sergey Ioffe and Christian Szegedy,1
13614,Batch normalization: Accelerating deep network training byreducing internal covariate shift,Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,0
13615,"448–456, 2015","Thus, our framework does not depend on a speciﬁc stochastic model",0
13616,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp","Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
13617,"448–456, 2015",Sergey Ioffe and Christian Szegedy,1
13618,Sergey Ioffe and Christian Szegedy,Sergey Ioffe and Christian Szegedy,1
13619,Batch normalization: Accelerating deep network training byreducing internal covariate shift,Sergey Ioffe and Christian Szegedy,1
13620,Batch normalization: Accelerating deep network training byreducing internal covariate shift,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)",0
13621,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",Estimating or propagating gradientsthrough stochastic neurons for conditional computation,0
13622,Sergey Ioffe and Christian Szegedy,"249–256,2010",0
13623,"448–456, 2015",Sergey Ioffe and Christian Szegedy,1
13624,"448–456, 2015","The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL",0
13625,Sergey Ioffe and Christian Szegedy,"This perspective paves a way for the soundness of constraining the compression-relevance trade-off into a neural network.We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the numberof hidden layers",0
13626,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",Sergey Ioffe and Christian Szegedy,1
13627,"448–456, 2015","Since the objective is designed
to optimize all parameters of neural networks and is mainly motivated by the IB principle for deep
learning (Tishby & Zaslavsky (2015)), we name this method the Parametric Information Bottleneck
(PIB)",0
13628,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp","For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
13629,"448–456, 2015",Sergey Ioffe and Christian Szegedy,1
13630,URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,"(2012), Szegedy et al",0
13631,"448–456, 2015",The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
13632,Batch normalization: Accelerating deep network training byreducing internal covariate shift,Sergey Ioffe and Christian Szegedy,1
13633,Batch normalization: Accelerating deep network training byreducing internal covariate shift,Sergey Ioffe and Christian Szegedy,1
13634,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",Sergey Ioffe and Christian Szegedy,1
13635,"448–456, 2015",Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-works,0
13636,Batch normalization: Accelerating deep network training byreducing internal covariate shift,Sergey Ioffe and Christian Szegedy,1
13637,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",Sergey Ioffe and Christian Szegedy,1
13638,URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,0
13639,Batch normalization: Accelerating deep network training byreducing internal covariate shift,Sergey Ioffe and Christian Szegedy,1
13640,"448–456, 2015",Sergey Ioffe and Christian Szegedy,1
13641,URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,Sergey Ioffe and Christian Szegedy,1
13642,Batch normalization: Accelerating deep network training byreducing internal covariate shift,Sergey Ioffe and Christian Szegedy,1
13643,Sergey Ioffe and Christian Szegedy,"Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",0
13644,"448–456, 2015",Sergey Ioffe and Christian Szegedy,1
13645,"448–456, 2015",The ﬁrst model (ModelA) is a deterministic neural network,0
13646,Batch normalization: Accelerating deep network training byreducing internal covariate shift,Sergey Ioffe and Christian Szegedy,1
13647,"448–456, 2015","Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich",0
13648,URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",0
13649,Sergey Ioffe and Christian Szegedy,"In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",0
13650,Sergey Ioffe and Christian Szegedy,Sergey Ioffe and Christian Szegedy,1
13651,Sergey Ioffe and Christian Szegedy,Sergey Ioffe and Christian Szegedy,1
13652,Sergey Ioffe and Christian Szegedy,Sergey Ioffe and Christian Szegedy,1
13653,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",Sergey Ioffe and Christian Szegedy,1
13654,URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,"Thus, it is hard to analytically capture such modiﬁcations.",0
13655,Sergey Ioffe and Christian Szegedy,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,0
13656,"448–456, 2015","In the following section, we presentour approximation to LP IB which fully utilizes the existing architecture without resorting to anymodel that is not part of the considered neural network",0
13657,Sergey Ioffe and Christian Szegedy,Sergey Ioffe and Christian Szegedy,1
13658,URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,Sergey Ioffe and Christian Szegedy,1
13659,"448–456, 2015","Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution ˆpD as possible (e.g., see Appendix I.B)",0
13660,URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,"1–9, 2015",0
13661,"448–456, 2015",Sergey Ioffe and Christian Szegedy,1
13662,Batch normalization: Accelerating deep network training byreducing internal covariate shift,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
13663,Batch normalization: Accelerating deep network training byreducing internal covariate shift,URL https://doi.org/10.1038/nature16961.,0
13664,Sergey Ioffe and Christian Szegedy,Sergey Ioffe and Christian Szegedy,1
13665,URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,Sergey Ioffe and Christian Szegedy,1
13666,Sergey Ioffe and Christian Szegedy,Sergey Ioffe and Christian Szegedy,1
13667,Sergey Ioffe and Christian Szegedy,Sergey Ioffe and Christian Szegedy,1
13668,"448–456, 2015",Sergey Ioffe and Christian Szegedy,1
13669,"448–456, 2015",Sergey Ioffe and Christian Szegedy,1
13670,URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,"We propose re-using the existing neural network architecture
as variational decoders for each hidden layers",0
13671,"448–456, 2015","To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)",0
13672,Batch normalization: Accelerating deep network training byreducing internal covariate shift,Sergey Ioffe and Christian Szegedy,1
13673,Batch normalization: Accelerating deep network training byreducing internal covariate shift,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp",0
13674,"448–456, 2015",Sergey Ioffe and Christian Szegedy,1
13675,URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,Sergey Ioffe and Christian Szegedy,1
13676,Sergey Ioffe and Christian Szegedy,Sergey Ioffe and Christian Szegedy,1
13677,Sergey Ioffe and Christian Szegedy,Sergey Ioffe and Christian Szegedy,1
13678,Batch normalization: Accelerating deep network training byreducing internal covariate shift,Sergey Ioffe and Christian Szegedy,1
13679,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",Sergey Ioffe and Christian Szegedy,1
13680,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",These code spaces are lossy representations of the data space as itfollows from the data-processing inequality (DPI) (Cover & Thomas (2006)) that,0
13681,Batch normalization: Accelerating deep network training byreducing internal covariate shift,Sergey Ioffe and Christian Szegedy,1
13682,Batch normalization: Accelerating deep network training byreducing internal covariate shift,"Originally, the general IB framework is proposed in Tishby et al",0
13683,Sergey Ioffe and Christian Szegedy,Sergey Ioffe and Christian Szegedy,1
13684,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp","335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",0
13685,Batch normalization: Accelerating deep network training byreducing internal covariate shift,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,0
13686,URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014",0
13687,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,0
13688,Batch normalization: Accelerating deep network training byreducing internal covariate shift,Sergey Ioffe and Christian Szegedy,1
13689,URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,"The optimal representation Z is determined via theminimization of the following Lagrangian:Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",0
13690,Sergey Ioffe and Christian Szegedy,Sergey Ioffe and Christian Szegedy,1
13691,Batch normalization: Accelerating deep network training byreducing internal covariate shift,"We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",0
13692,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",Sergey Ioffe and Christian Szegedy,1
13693,"448–456, 2015",Sergey Ioffe and Christian Szegedy,1
13694,URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014",0
13695,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",Sergey Ioffe and Christian Szegedy,1
13696,Batch normalization: Accelerating deep network training byreducing internal covariate shift,Sergey Ioffe and Christian Szegedy,1
13697,Sergey Ioffe and Christian Szegedy,Sergey Ioffe and Christian Szegedy,1
13698,Sergey Ioffe and Christian Szegedy,(1999),0
13699,Sergey Ioffe and Christian Szegedy,Sergey Ioffe and Christian Szegedy,1
13700,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13701,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).,0
13702,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",0
13703,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.","Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P",0
13704,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13705,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp","Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters",0
13706,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13707,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp","Journal of Machine Learning Research, 6:165–188, 2005",0
13708,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13709,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13710,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,"Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang",0
13711,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,0
13712,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp","(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))",0
13713,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13714,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13715,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13716,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13717,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13718,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13719,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.","However, the MLE principle is very generic thatis not specially tailored for neural networks",0
13720,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp","We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.",0
13721,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,"Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",0
13722,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.","Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",0
13723,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,"McGibbon, Roland Memisevic, Bart van Merri¨enboer, Vincent Michalski, MehdiMirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel,Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Sal-vatier, Franc¸ois Savard, Jan Schl¨uter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,Dmitriy Serdyuk, Samira Shabanian, ´Etienne Simon, Sigurd Spieckermann, S",0
13724,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,"For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
13725,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.","(2015)), natural languagetranslation (e.g., Cho et al",0
13726,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13727,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13728,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",0
13729,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.","(1999)); or (2) X, Yand Z are mutually joint Gaussian (Chechik et al",0
13730,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp","We will refer to ˜H(Y |Zl) as the variational conditional relevance (VCR) for the lth-level bottleneckvariable Zl for the rest of this work.In the following, we present two important results whichindicate that the relevance terms in our objective is closely and mutually related to the concept ofthe MLE principle.Proposition 3.1",0
13731,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13732,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.","More speciﬁcally, our work proposes an optimizationIB criteria for an existing neural network architecture in an effort to better learn the layers’ represen-tation to their IB optimality",0
13733,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13734,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13735,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13736,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",(2016)).,0
13737,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",content of the original space possibly including its dimensionality and topological structure,0
13738,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,0
13739,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,"Previously, the InformationBottleneck (IB) framework (Tishby et al",0
13740,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,"The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ",0
13741,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
13742,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13743,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13744,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13745,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13746,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13747,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13748,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13749,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,0
13750,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13751,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,The recent work Alemi et al,0
13752,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13753,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13754,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13755,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.","As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets",0
13756,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13757,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13758,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13759,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.","The encoded information atthe beginning also contains some relevant information for the target variable as I(Zi, Y ) increasesas well",0
13760,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13761,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13762,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13763,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13764,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,0
13765,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.","Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",0
13766,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,"For simplicity, we present the derivation of I(Z1, Z0) only 3",0
13767,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",The MLE principle maximizes the likelihood function under theempirical data distribution,0
13768,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],0
13769,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13770,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.","In this work,we focus on binary bottlenecks where Zl ∈ {0, 1}nl and ni is the dimensionality of the bottleneckspace.",0
13771,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Information bottleneck for gaus-sian variables,0
13772,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.","1–9, 2015",0
13773,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",The ﬁrst model (ModelA) is a deterministic neural network,0
13774,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13775,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp","David Silver, Aja Huang, Chris J",0
13776,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13777,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,networks and averaged the mutual informations,0
13778,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,"As β varies from 0 to ∞, Z traces a concave curve, knownas information curve for representation Z, with a slope of β−1",0
13779,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13780,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13781,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",This workproposes using mutual information of a hidden layer with the input layer and the output layer toquantify the performance of DNNs,0
13782,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13783,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13784,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp","For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
13785,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13786,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,"Thus, our framework does not depend on a speciﬁc stochastic model",0
13787,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.","Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang",0
13788,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,0
13789,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
13790,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,"Thus, it is hard to analytically capture such modiﬁcations.",0
13791,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13792,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13793,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13794,"1106–1114, 2012.URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.",Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13795,"Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,United States., pp",A learning principle should compress irrelevant information and preserve relevant information inthe lossy intermediate code spaces,0
13796,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13797,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",0
13798,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,URL http://dl.acm.org/citation.cfm?id=2670313.,0
13799,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,Imagenet classiﬁcation withInformation Process-deep convolutional neural networks.Information Processing Sys-ing Systems 25:tems 2012,1
13800,URL http://arxiv.org/abs/1406.2989.,One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,0
13801,"CoRR, abs/1406.2989, 2014",We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,0
13802,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,0
13803,"CoRR, abs/1406.2989, 2014","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
13804,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13805,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13806,Techniques for learningbinary stochastic feedforward neural networks,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13807,URL http://arxiv.org/abs/1406.2989.,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13808,Techniques for learningbinary stochastic feedforward neural networks,Understanding the difﬁculty of training deep feedforward neuralnetworks,0
13809,"CoRR, abs/1406.2989, 2014",The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,0
13810,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13811,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","Each point represents a hidden layer while the color indicate epochs.Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) andI(X, Y ) ≥ I(Zl, Y ) ≥ I(Zl+1, Y ).",0
13812,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13813,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13814,URL http://arxiv.org/abs/1406.2989.,networks and averaged the mutual informations,0
13815,Techniques for learningbinary stochastic feedforward neural networks,"(1999)); or (2) X, Yand Z are mutually joint Gaussian (Chechik et al",0
13816,URL http://arxiv.org/abs/1406.2989.,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13817,Techniques for learningbinary stochastic feedforward neural networks,"(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",0
13818,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13819,Techniques for learningbinary stochastic feedforward neural networks,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)",0
13820,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13821,Techniques for learningbinary stochastic feedforward neural networks,"Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest",0
13822,Techniques for learningbinary stochastic feedforward neural networks,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13823,URL http://arxiv.org/abs/1406.2989.,"= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
13824,"CoRR, abs/1406.2989, 2014","(2012), Szegedy et al",0
13825,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13826,URL http://arxiv.org/abs/1406.2989.,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13827,Techniques for learningbinary stochastic feedforward neural networks,(2016) also uses variational methods to approximate the mutual infor-mation as an attempt to apply the IB principle to neural networks,0
13828,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13829,URL http://arxiv.org/abs/1406.2989.,Learning stochastic feedforward neural networks,0
13830,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13831,URL http://arxiv.org/abs/1406.2989.,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13832,Techniques for learningbinary stochastic feedforward neural networks,Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,0
13833,Techniques for learningbinary stochastic feedforward neural networks,"Dauphin, Olivier Delal-leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, M´elanie Ducoffe,Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, MathieuGermain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, IbanHarlouchet, Jean-Philippe Heng, Bal´azs Hidasi, Sina Honari, Arjun Jain, S´ebastien Jean, KaiJia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, C´esar Lau-rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas L´eonard, Zhouhan Lin, Jesse A.Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-etro, Robert T",0
13834,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",0
13835,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13836,URL http://arxiv.org/abs/1406.2989.,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13837,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",Xavier Glorot and Yoshua Bengio,0
13838,URL http://arxiv.org/abs/1406.2989.,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13839,"CoRR, abs/1406.2989, 2014","The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",0
13840,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13841,Techniques for learningbinary stochastic feedforward neural networks,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13842,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",The MLE principle maximizes the likelihood function under theempirical data distribution,0
13843,"CoRR, abs/1406.2989, 2014","Onaverage, 2H(X|Z) elements of X are mapped to the same code in Z",0
13844,"CoRR, abs/1406.2989, 2014","(2014)) and game playing (e.g., Silver et al.(2016))",0
13845,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13846,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13847,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
13848,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13849,URL http://arxiv.org/abs/1406.2989.,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13850,Techniques for learningbinary stochastic feedforward neural networks,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13851,Techniques for learningbinary stochastic feedforward neural networks,"530–538, 2013",0
13852,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",0
13853,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13854,URL http://arxiv.org/abs/1406.2989.,A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,0
13855,"CoRR, abs/1406.2989, 2014","The approximate generalized IB objective in turn
presents interesting connections with the MLE principle",0
13856,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13857,Techniques for learningbinary stochastic feedforward neural networks,"Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network",0
13858,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13859,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13860,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",0
13861,Techniques for learningbinary stochastic feedforward neural networks,"Firstly, we decompose the mutual information into a differenceof two entropies:I(Zl, Y ) = H(Y ) − H(Y |Zl)",0
13862,URL http://arxiv.org/abs/1406.2989.,We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,0
13863,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
13864,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13865,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13866,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13867,URL http://arxiv.org/abs/1406.2989.,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13868,"CoRR, abs/1406.2989, 2014","In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",0
13869,Techniques for learningbinary stochastic feedforward neural networks,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P",0
13870,Techniques for learningbinary stochastic feedforward neural networks,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13871,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance",0
13872,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13873,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13874,Techniques for learningbinary stochastic feedforward neural networks,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13875,"CoRR, abs/1406.2989, 2014",URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,0
13876,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13877,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13878,URL http://arxiv.org/abs/1406.2989.,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13879,URL http://arxiv.org/abs/1406.2989.,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13880,URL http://arxiv.org/abs/1406.2989.,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13881,URL http://arxiv.org/abs/1406.2989.,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13882,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
13883,"CoRR, abs/1406.2989, 2014","335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",0
13884,Techniques for learningbinary stochastic feedforward neural networks,"We also show that, as compared to the MLE principle, PIB : (i)improves the generalization of neural networks in classiﬁcation tasks, (ii) is moreefﬁcient to exploit a neural network’s representation by pushing it closer to theoptimal information-theoretical representation in a faster manner.",0
13885,URL http://arxiv.org/abs/1406.2989.,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13886,Techniques for learningbinary stochastic feedforward neural networks,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13887,URL http://arxiv.org/abs/1406.2989.,Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,0
13888,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13889,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
13890,URL http://arxiv.org/abs/1406.2989.,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
13891,Techniques for learningbinary stochastic feedforward neural networks,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,0
13892,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh","To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)",0
13893,"CoRR, abs/1406.2989, 2014","Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",0
13894,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13895,Techniques for learningbinary stochastic feedforward neural networks,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13896,URL http://arxiv.org/abs/1406.2989.,"Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",0
13897,URL http://arxiv.org/abs/1406.2989.,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13898,Techniques for learningbinary stochastic feedforward neural networks,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13899,"CoRR, abs/1406.2989, 2014","Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",1
13900,"Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961","David Silver, Aja Huang, Chris J",1
13901,Masteringthe game of go with deep neural networks and tree search,"David Silver, Aja Huang, Chris J",1
13902,"Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961","Thus, it is hard to analytically capture such modiﬁcations.",0
13903,"Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961","David Silver, Aja Huang, Chris J",1
13904,"Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis","David Silver, Aja Huang, Chris J",1
13905,"Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961","David Silver, Aja Huang, Chris J",1
13906,"David Silver, Aja Huang, Chris J","However, the MLE principle is very generic thatis not specially tailored for neural networks",0
13907,"David Silver, Aja Huang, Chris J","To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",0
13908,"David Silver, Aja Huang, Chris J","In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",0
13909,URL https://doi.org/10.1038/nature16961.,"David Silver, Aja Huang, Chris J",1
13910,"David Silver, Aja Huang, Chris J",(2016) also uses variational methods to approximate the mutual infor-mation as an attempt to apply the IB principle to neural networks,0
13911,Masteringthe game of go with deep neural networks and tree search,"Furthermore,LP IB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-coders ptrue(yyy|zzzl) (computed by Equation 8) are intractable",0
13912,URL https://doi.org/10.1038/nature16961.,A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,0
13913,URL https://doi.org/10.1038/nature16961.,"Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation",0
13914,URL https://doi.org/10.1038/nature16961.,"David Silver, Aja Huang, Chris J",1
13915,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P","David Silver, Aja Huang, Chris J",1
13916,"David Silver, Aja Huang, Chris J","David Silver, Aja Huang, Chris J",1
13917,"David Silver, Aja Huang, Chris J","David Silver, Aja Huang, Chris J",1
13918,"David Silver, Aja Huang, Chris J","David Silver, Aja Huang, Chris J",1
13919,URL https://doi.org/10.1038/nature16961.,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",0
13920,"Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis","David Silver, Aja Huang, Chris J",1
13921,"David Silver, Aja Huang, Chris J","David Silver, Aja Huang, Chris J",1
13922,"David Silver, Aja Huang, Chris J","As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets",0
13923,Masteringthe game of go with deep neural networks and tree search,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich",0
13924,"Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis","We then propose a new generalized
IB-based objective that takes into account the compression and relevance of all layers in the network
as an explicit goal for guiding the encodings in a beneﬁcial manner",0
13925,Masteringthe game of go with deep neural networks and tree search,"In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",0
13926,Masteringthe game of go with deep neural networks and tree search,"Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",0
13927,"Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis","In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",0
13928,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P","However, the variational approximatedistribution for the code space may be too loose while the actual marginal distribution can be sam-pled easily.Our work, on the other hand, focuses on better exploiting intermediate representations of a neuralnetwork architecture using the IB principle",0
13929,Masteringthe game of go with deep neural networks and tree search,"David Silver, Aja Huang, Chris J",1
13930,Masteringthe game of go with deep neural networks and tree search,"David Silver, Aja Huang, Chris J",1
13931,"David Silver, Aja Huang, Chris J",URL http://arxiv.org/abs/1605.02688.,0
13932,"Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961",We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,0
13933,"Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis",Their approach however considersone single bottleneck and parameterizes the encoder p(zzz|xxx; θθθ) by an entire neural network,0
13934,"David Silver, Aja Huang, Chris J","The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL",0
13935,"David Silver, Aja Huang, Chris J",The en-coder maps the input variable xxx to a single bottleneck variable zzz that is not a part of the consideredneural network architecture,0
13936,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P","In the MLE principle, we only need empirical samples of
the joint distribution to maximize the likelihood function of the model given the data",0
13937,"Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis","David Silver, Aja Huang, Chris J",1
13938,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P","David Silver, Aja Huang, Chris J",1
13939,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P","David Silver, Aja Huang, Chris J",1
13940,URL https://doi.org/10.1038/nature16961.,"David Silver, Aja Huang, Chris J",1
13941,"Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis","David Silver, Aja Huang, Chris J",1
13942,"David Silver, Aja Huang, Chris J","David Silver, Aja Huang, Chris J",1
13943,"Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961","David Silver, Aja Huang, Chris J",1
13944,"David Silver, Aja Huang, Chris J",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
13945,Masteringthe game of go with deep neural networks and tree search,Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,0
13946,URL https://doi.org/10.1038/nature16961.,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,0
13947,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P",This workis our ﬁrst step toward exploiting expressive power of large neural networks using information-theoretic perspective that is not yet fully utilized.,0
13948,"David Silver, Aja Huang, Chris J","David Silver, Aja Huang, Chris J",1
13949,"Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961","Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
13950,"David Silver, Aja Huang, Chris J","As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets",0
13951,"David Silver, Aja Huang, Chris J","In this work,we focus on binary bottlenecks where Zl ∈ {0, 1}nl and ni is the dimensionality of the bottleneckspace.",0
13952,"Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis","David Silver, Aja Huang, Chris J",1
13953,"Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961",Our PIB is also a standard steptowards better exploiting representational power of more expressive neural network models such asConvolutional Neural Networks (LeCun et al,0
13954,URL https://doi.org/10.1038/nature16961.,"(2014), Bahdanau et al",0
13955,Masteringthe game of go with deep neural networks and tree search,"David Silver, Aja Huang, Chris J",1
13956,Masteringthe game of go with deep neural networks and tree search,"David Silver, Aja Huang, Chris J",1
13957,URL https://doi.org/10.1038/nature16961.,"David Silver, Aja Huang, Chris J",1
13958,"Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis","Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang",0
13959,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P","(2016) for I(Z, Y ) but use empirical estimation for I(Z, X)",0
13960,Masteringthe game of go with deep neural networks and tree search,This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),0
13961,Masteringthe game of go with deep neural networks and tree search,"David Silver, Aja Huang, Chris J",1
13962,Masteringthe game of go with deep neural networks and tree search,"David Silver, Aja Huang, Chris J",1
13963,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P",URL http://dl.acm.org/citation.cfm?id=2670313.,0
13964,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P","David Silver, Aja Huang, Chris J",1
13965,Masteringthe game of go with deep neural networks and tree search,The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
13966,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P",(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
13967,"Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis","David Silver, Aja Huang, Chris J",1
13968,"David Silver, Aja Huang, Chris J","David Silver, Aja Huang, Chris J",1
13969,"David Silver, Aja Huang, Chris J","David Silver, Aja Huang, Chris J",1
13970,URL https://doi.org/10.1038/nature16961.,"David Silver, Aja Huang, Chris J",1
13971,"Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis","In Model D, we used the weight trained in Model A to performstochastic prediction at test time",0
13972,"Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961","Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp",0
13973,"David Silver, Aja Huang, Chris J","As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets",0
13974,"Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis",The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,0
13975,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P","David Silver, Aja Huang, Chris J",1
13976,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P","As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets",0
13977,"Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961",This estimator is also known as the maximum likelihoodwhere zzz(k)estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)),0
13978,Masteringthe game of go with deep neural networks and tree search,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",0
13979,Masteringthe game of go with deep neural networks and tree search,"David Silver, Aja Huang, Chris J",1
13980,URL https://doi.org/10.1038/nature16961.,"David Silver, Aja Huang, Chris J",1
13981,URL https://doi.org/10.1038/nature16961.,"David Silver, Aja Huang, Chris J",1
13982,"David Silver, Aja Huang, Chris J","David Silver, Aja Huang, Chris J",1
13983,"David Silver, Aja Huang, Chris J","David Silver, Aja Huang, Chris J",1
13984,"Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis","David Silver, Aja Huang, Chris J",1
13985,"Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961","David Silver, Aja Huang, Chris J",1
13986,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",0
13987,"Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961","David Silver, Aja Huang, Chris J",1
13988,Masteringthe game of go with deep neural networks and tree search,"Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al",0
13989,"Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis","David Silver, Aja Huang, Chris J",1
13990,URL https://doi.org/10.1038/nature16961.,"David Silver, Aja Huang, Chris J",1
13991,"David Silver, Aja Huang, Chris J","CoRR, abs/1308.3432, 2013",0
13992,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P","David Silver, Aja Huang, Chris J",1
13993,"David Silver, Aja Huang, Chris J",This also empirically holds for the case of SFNN,0
13994,"Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961",ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,0
13995,"David Silver, Aja Huang, Chris J","David Silver, Aja Huang, Chris J",1
13996,Masteringthe game of go with deep neural networks and tree search,One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,0
13997,Masteringthe game of go with deep neural networks and tree search,"where the layer-speciﬁc Lagrangian multiplier β−1controls the tradeoff between relevance and com-pression in each bottleneck, and the concept of compression and relevance is taken to the extremewhen l = 0 (with convention that I(Z0, Z−1) = I(X,∅) = H(X) = constant)",0
13998,"David Silver, Aja Huang, Chris J","(2014), Bahdanau et al",0
13999,"David Silver, Aja Huang, Chris J","However in general the two principles are not obviously related.
In this work, we leverage neural networks and the IB principle by viewing neural networks as a set
of encoders that sequentially modify the original data space",0
14000,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,0
14001,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","For comparisons, we trained PIBs and ﬁve additional models",0
14002,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14003,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14004,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",Model E is SFNN and Model B is Model C with deterministicprediction during test phase,0
14005,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14006,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",0
14007,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","(2015)), natural languagetranslation (e.g., Cho et al",0
14008,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",URLhttps://doi.org/10.1109/CVPR.2016.90.,0
14009,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",Theano: A Pythonframework for fast computation of mathematical expressions,0
14010,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14011,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in theinformation plane (Tishby et al,0
14012,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","Thus, a reasonable question is does the MLE principleeffectively and sufﬁciently exploit a neural network’s representative power and is there any betteralternative? As an attempt to address this important question, this work investigates the learning ofDNNs from the information-theoretic perspective.An alternative principle is the Information Bottleneck (IB) framework (Tishby et al",0
14013,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and abatch size of 8,0
14014,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",This section presents an information-theoretic perspective of neural networks and then deﬁnes ourPIB framework,0
14015,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14016,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14017,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14018,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","The result in the test set is then reported (for stochastic prediction,we report mean and standard deviation)",0
14019,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,0
14020,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","Webb, MatthewWillson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang",0
14021,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
14022,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14023,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","(1999)); or (2) X, Yand Z are mutually joint Gaussian (Chechik et al",0
14024,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","The plane has I(Z, X) and I(Z, Y ) as its horizontal axisand its vertical axis, respectively",0
14025,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",0
14026,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014",0
14027,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,0
14028,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14029,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","1–9, 2015",0
14030,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","For PIBs, we set βl = β,∀1 ≤ l ≤ L",0
14031,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14032,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14033,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
14034,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","We tuned β from{0} ∪ {10−i : 1 ≤ i ≤ 7}, and found β−1 = 10−4 works best.Table 1 provides the results in the MNIST classiﬁcation error in the test set for PIB and the com-parative models (A), (B), (C), (D), and (E)",0
14035,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",Each hidden layer in SFNNs is also considered as a stochastic variable,0
14036,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14037,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14038,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
14039,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","530–538, 2013",0
14040,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14041,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",Wejointly optimize the compression and the relevance of all parameters in an SNNto better exploit the neural network’s representation,0
14042,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14043,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ",0
14044,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","The MLE
principle is proved to be mathematically equivalent to the IB principle for the multinomial mixture
model for clustering problem when the input distribution X is uniform or has a large sample size
(Slonim & Weiss (2002))",0
14045,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","In the large space of Z1, the probability of a singlepoint p(zzz1) may become very small that log p(zzz1) becomes numerically unstable",0
14046,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","(2015)), natural languagetranslation (e.g., Cho et al",0
14047,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14048,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","However, the variational approximatedistribution for the code space may be too loose while the actual marginal distribution can be sam-pled easily.Our work, on the other hand, focuses on better exploiting intermediate representations of a neuralnetwork architecture using the IB principle",0
14049,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,0
14050,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14051,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
14052,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","Yoshua Bengio, Nicholas L´eonard, and Aaron C",0
14053,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14054,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14055,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",0
14056,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14057,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",URL http://arxiv.org/abs/1605.02688.,0
14058,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",0
14059,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network",0
14060,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",This workproposes using mutual information of a hidden layer with the input layer and the output layer toquantify the performance of DNNs,0
14061,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning",0
14062,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,0
14063,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14064,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14065,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
14066,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14067,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","The samples generated by PIBis totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuseswith number 6, digit 8 and 9 are unrecognizable).",0
14068,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14069,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
14070,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","(2005)) where Z is a bottleneck variable.Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015))",0
14071,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14072,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14073,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",Discrete-valued variables in PIBs make standard back-propagation not straightforward,0
14074,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",0
14075,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14076,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14077,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,0
14078,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.",(2016)).,0
14079,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14080,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14081,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14082,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","The afore-mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learningdynamics, and capability of modeling complicated output structures, respectively",0
14083,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",0
14084,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","In neural networks, intermediate representations represent a hierarchy of information bottle-necks that sequentially extract relevant information for a target from the input data space",0
14085,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14086,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14087,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14088,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14089,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","arXiv preprint arXiv:1406.1078, 2014.",0
14090,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",where the equality in Equation 12 holds due to the Markov assumption (Equation 4),0
14091,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14092,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14093,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",0
14094,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14095,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14096,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14097,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14098,"information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14099,"335–342, 2002.URL http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",1
14100,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Nitish Srivastava, Geoffrey E",1
14101,URL http://dl.acm.org/citation.cfm?id=2670313.,Batch normalization: Accelerating deep network training byreducing internal covariate shift,0
14102,"Nitish Srivastava, Geoffrey E","We propose re-using the existing neural network architecture
as variational decoders for each hidden layers",0
14103,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Nitish Srivastava, Geoffrey E",1
14104,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Nitish Srivastava, Geoffrey E",1
14105,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Nitish Srivastava, Geoffrey E",1
14106,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Nitish Srivastava, Geoffrey E",1
14107,"Nitish Srivastava, Geoffrey E",Courville,0
14108,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Nitish Srivastava, Geoffrey E",1
14109,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Nitish Srivastava, Geoffrey E",1
14110,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Nitish Srivastava, Geoffrey E",1
14111,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Nitish Srivastava, Geoffrey E",1
14112,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","In practice, the IB problem can be solved efﬁcientlyin the following two cases only: (1) X, Y and Z are all discrete (Tishby et al",0
14113,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Nitish Srivastava, Geoffrey E",1
14114,URL http://dl.acm.org/citation.cfm?id=2670313.,Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,0
14115,"Nitish Srivastava, Geoffrey E","Nitish Srivastava, Geoffrey E",1
14116,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014",The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,0
14117,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Nitish Srivastava, Geoffrey E",1
14118,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Nitish Srivastava, Geoffrey E",1
14119,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014",Sergey Ioffe and Christian Szegedy,0
14120,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting",We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,0
14121,"Nitish Srivastava, Geoffrey E","Nitish Srivastava, Geoffrey E",1
14122,URL http://dl.acm.org/citation.cfm?id=2670313.,where the equality in Equation 12 holds due to the Markov assumption (Equation 4),0
14123,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting",In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,0
14124,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
14125,URL http://dl.acm.org/citation.cfm?id=2670313.,"Nitish Srivastava, Geoffrey E",1
14126,URL http://dl.acm.org/citation.cfm?id=2670313.,"(1999)); or (2) X, Yand Z are mutually joint Gaussian (Chechik et al",0
14127,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","CoRR, abs/1406.2989, 2014",0
14128,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Nitish Srivastava, Geoffrey E",1
14129,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",0
14130,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Furthermore, we exploitthe existing network architecture as variational decoders rather than resort to variational decodersthat are not part of the neural network architecture.",0
14131,URL http://dl.acm.org/citation.cfm?id=2670313.,"Nitish Srivastava, Geoffrey E",1
14132,URL http://dl.acm.org/citation.cfm?id=2670313.,"We tuned β from{0} ∪ {10−i : 1 ≤ i ≤ 7}, and found β−1 = 10−4 works best.Table 1 provides the results in the MNIST classiﬁcation error in the test set for PIB and the com-parative models (A), (B), (C), (D), and (E)",0
14133,URL http://dl.acm.org/citation.cfm?id=2670313.,"Nitish Srivastava, Geoffrey E",1
14134,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Nitish Srivastava, Geoffrey E",1
14135,URL http://dl.acm.org/citation.cfm?id=2670313.,Learning phrase representations using rnn encoder-decoderfor statistical machine translation,0
14136,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Nitish Srivastava, Geoffrey E",1
14137,URL http://dl.acm.org/citation.cfm?id=2670313.,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
14138,URL http://dl.acm.org/citation.cfm?id=2670313.,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL",0
14139,URL http://dl.acm.org/citation.cfm?id=2670313.,The en-coder maps the input variable xxx to a single bottleneck variable zzz that is not a part of the consideredneural network architecture,0
14140,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Nitish Srivastava, Geoffrey E",1
14141,URL http://dl.acm.org/citation.cfm?id=2670313.,"(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning",0
14142,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Here we preferto this extreme, i.e., the 0th level, as the super level",0
14143,URL http://dl.acm.org/citation.cfm?id=2670313.,"Nitish Srivastava, Geoffrey E",1
14144,URL http://dl.acm.org/citation.cfm?id=2670313.,"CoRR, abs/1406.2989, 2014",0
14145,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","(2012), Szegedy et al",0
14146,URL http://dl.acm.org/citation.cfm?id=2670313.,The approximation then leads to effectivegradient-based training of PIBs.,0
14147,"Nitish Srivastava, Geoffrey E","Nitish Srivastava, Geoffrey E",1
14148,"Nitish Srivastava, Geoffrey E","Nitish Srivastava, Geoffrey E",1
14149,URL http://dl.acm.org/citation.cfm?id=2670313.,Xavier Glorot and Yoshua Bengio,0
14150,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich",0
14151,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Nitish Srivastava, Geoffrey E",1
14152,URL http://dl.acm.org/citation.cfm?id=2670313.,"Nitish Srivastava, Geoffrey E",1
14153,URL http://dl.acm.org/citation.cfm?id=2670313.,"Thus, estimat-ing mutual information for hidden variables in this case is as hard as estimating mutual informationfor the data variables themselves.",0
14154,"Nitish Srivastava, Geoffrey E","Nitish Srivastava, Geoffrey E",1
14155,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Nitish Srivastava, Geoffrey E",1
14156,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Nitish Srivastava, Geoffrey E",1
14157,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","For visualization of learned ﬁlters of PIB, seeAppendix II.A.",0
14158,URL http://dl.acm.org/citation.cfm?id=2670313.,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
14159,URL http://dl.acm.org/citation.cfm?id=2670313.,"Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
14160,"Nitish Srivastava, Geoffrey E","Nitish Srivastava, Geoffrey E",1
14161,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Nitish Srivastava, Geoffrey E",1
14162,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Nitish Srivastava, Geoffrey E",1
14163,URL http://dl.acm.org/citation.cfm?id=2670313.,Figure 1: A directed graphical representation of a PIB of two bottlenecks,0
14164,"Nitish Srivastava, Geoffrey E",The MLE principle maximizes the likelihood function under theempirical data distribution,0
14165,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
14166,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Nitish Srivastava, Geoffrey E",1
14167,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
14168,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014",The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,0
14169,URL http://dl.acm.org/citation.cfm?id=2670313.,"Nitish Srivastava, Geoffrey E",1
14170,URL http://dl.acm.org/citation.cfm?id=2670313.,"Nitish Srivastava, Geoffrey E",1
14171,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting",URL http://arxiv.org/abs/1605.02688.,0
14172,"Nitish Srivastava, Geoffrey E","Nitish Srivastava, Geoffrey E",1
14173,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Nitish Srivastava, Geoffrey E",1
14174,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting",One advantage of GRAD-P IB algorithm is that it requires only a singleforward pass to estimate all the information terms in ˜LP IB since the generated samples are re-usedto compute the information terms at each layer level.,0
14175,URL http://dl.acm.org/citation.cfm?id=2670313.,"Maddison, Arthur Guez, Laurent Sifre, George van den Driess-che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, SanderDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P",0
14176,"Nitish Srivastava, Geoffrey E","(2014), Bahdanau et al",0
14177,"Nitish Srivastava, Geoffrey E","Nitish Srivastava, Geoffrey E",1
14178,"Nitish Srivastava, Geoffrey E","Nitish Srivastava, Geoffrey E",1
14179,URL http://dl.acm.org/citation.cfm?id=2670313.,URL http://dl.acm.org/citation.cfm?id=2670313.,0
14180,URL http://dl.acm.org/citation.cfm?id=2670313.,"Nitish Srivastava, Geoffrey E",1
14181,URL http://dl.acm.org/citation.cfm?id=2670313.,The generated bottleneck samples are then used to estimate mutual in-formation,0
14182,URL http://dl.acm.org/citation.cfm?id=2670313.,"530–538, 2013",0
14183,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",0
14184,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014",Theano: A Pythonframework for fast computation of mathematical expressions,0
14185,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",0
14186,"Nitish Srivastava, Geoffrey E","Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters",0
14187,"Nitish Srivastava, Geoffrey E","(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
14188,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Nitish Srivastava, Geoffrey E",1
14189,URL http://dl.acm.org/citation.cfm?id=2670313.,"While the lth level for 1 ≤ l ≤ L indicates aspeciﬁc hidden layer l, the super level represents the entire neural network as a whole.The objective LP IB can be considered as a joint version of the theoretical IB analysis for DNNsin Tishby & Zaslavsky (2015)",0
14190,URL http://dl.acm.org/citation.cfm?id=2670313.,"Nitish Srivastava, Geoffrey E",1
14191,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","448–456, 2015",0
14192,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Nitish Srivastava, Geoffrey E",1
14193,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","Nitish Srivastava, Geoffrey E",1
14194,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Nitish Srivastava, Geoffrey E",1
14195,"Nitish Srivastava, Geoffrey E","(2012), Szegedy et al",0
14196,"Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014","This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al",0
14197,"Nitish Srivastava, Geoffrey E","Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp",0
14198,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Nitish Srivastava, Geoffrey E",1
14199,"Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting","Nitish Srivastava, Geoffrey E",1
14200,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation",0
14201,"Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp","In practice, the IB problem can be solved efﬁcientlyin the following two cases only: (1) X, Y and Z are all discrete (Tishby et al",0
14202,"Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14203,doi: 10.1109/CVPR.2015.7298594,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14204,URL https://doi.org/10.1109/CVPR.2015.7298594.,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14205,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","information bottle-Infor-neck.mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, BritishColumbia, Canada], pp",0
14206,"Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp",URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,0
14207,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","3.3.1 APPROXIMATE RELEVANCESince the relevance decoder ptrue(yyy|zzzl) (Equation 8) is intractable, we use a variational relevancedecoder pv(yyy|zzzl) to approximate it",0
14208,"Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp","Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014",0
14209,doi: 10.1109/CVPR.2015.7298594,"(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
14210,"Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp","In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ",0
14211,doi: 10.1109/CVPR.2015.7298594,"(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
14212,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
14213,"1–9, 2015","(2015)), natural languagetranslation (e.g., Cho et al",0
14214,"1–9, 2015","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14215,URL https://doi.org/10.1109/CVPR.2015.7298594.,"where β is the positive Lagrangian multiplier that controls the trade-off between the complexityof the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y )",0
14216,URL https://doi.org/10.1109/CVPR.2015.7298594.,A binary stochastic variable zzzl of dimensionality nl can take on2nl different states each of which would give a different yyy,0
14217,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","These base conﬁgurations are applied to all six models we use in thisexperiment.The base architecture is a fully-connected, sigmoid activation neural network with two hidden layersand 512 units per layer",0
14218,URL https://doi.org/10.1109/CVPR.2015.7298594.,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14219,doi: 10.1109/CVPR.2015.7298594,Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),0
14220,"1–9, 2015","(2014)) and game playing (e.g., Silver et al.(2016))",0
14221,doi: 10.1109/CVPR.2015.7298594,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14222,URL https://doi.org/10.1109/CVPR.2015.7298594.,(1999)) extracts relevant information fora target variable,0
14223,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14224,doi: 10.1109/CVPR.2015.7298594,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14225,doi: 10.1109/CVPR.2015.7298594,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14226,doi: 10.1109/CVPR.2015.7298594,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14227,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14228,"1–9, 2015","David Silver, Aja Huang, Chris J",0
14229,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14230,doi: 10.1109/CVPR.2015.7298594,"Deep residual learning for image recog-In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,nition.Las Vegas, NV, USA, June 27-30, 2016, pp",0
14231,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",0
14232,URL https://doi.org/10.1109/CVPR.2015.7298594.,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14233,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14234,"Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp",The en-coder maps the input variable xxx to a single bottleneck variable zzz that is not a part of the consideredneural network architecture,0
14235,URL https://doi.org/10.1109/CVPR.2015.7298594.,The generated bottleneck samples are then used to estimate mutual in-formation,0
14236,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14237,doi: 10.1109/CVPR.2015.7298594,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14238,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",(1998)) and ResNet (He et al,0
14239,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14240,URL https://doi.org/10.1109/CVPR.2015.7298594.,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14241,"1–9, 2015",We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,0
14242,doi: 10.1109/CVPR.2015.7298594,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14243,"Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",0
14244,doi: 10.1109/CVPR.2015.7298594,URL http://arxiv.org/abs/1406.2989.,0
14245,doi: 10.1109/CVPR.2015.7298594,The larger number of samples Mguarantees the better plug-in entropy by the following bias bound (Paninski (2003)),0
14246,"Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14247,URL https://doi.org/10.1109/CVPR.2015.7298594.,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14248,doi: 10.1109/CVPR.2015.7298594,"For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
14249,"1–9, 2015","For visualization of learned ﬁlters of PIB, seeAppendix II.A.",0
14250,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14251,URL https://doi.org/10.1109/CVPR.2015.7298594.,"Although the analysis offers a new perspective about optimality in neural networks,it proposes general analysis of optimality rather than a practical optimization criteria",0
14252,"1–9, 2015","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14253,"Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14254,"1–9, 2015","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14255,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14256,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14257,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich","For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",0
14258,doi: 10.1109/CVPR.2015.7298594,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14259,"Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14260,doi: 10.1109/CVPR.2015.7298594,doi: 10.1109/CVPR.2015.7298594,0
14261,"1–9, 2015",In this paper we introduced an information-theoretic learning framework to better exploit a neuralnetwork’s representation,0
14262,"Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp","Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014",0
14263,doi: 10.1109/CVPR.2015.7298594,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",0
14264,URL https://doi.org/10.1109/CVPR.2015.7298594.,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14265,doi: 10.1109/CVPR.2015.7298594,We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,0
14266,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich","In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",0
14267,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14268,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14269,doi: 10.1109/CVPR.2015.7298594,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14270,doi: 10.1109/CVPR.2015.7298594,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,0
14271,URL https://doi.org/10.1109/CVPR.2015.7298594.,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14272,"Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp",content of the original space possibly including its dimensionality and topological structure,0
14273,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","Furthermore,estimating mutual information between the variables transformed by network layers and the datavariables poses several computational challenges in practice that the authors did not address in thework",0
14274,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",The exactsolution to the minimization problem above is found (Tishby et al,0
14275,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich","As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets",0
14276,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14277,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich","The MLE
principle is proved to be mathematically equivalent to the IB principle for the multinomial mixture
model for clustering problem when the input distribution X is uniform or has a large sample size
(Slonim & Weiss (2002))",0
14278,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14279,URL https://doi.org/10.1109/CVPR.2015.7298594.,"Sincethe network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=I(Zi, Y ) precisely and plot them over training epochs.As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-mation into their hidden layers at the beginning as I(Zi, X) increases",0
14280,"1–9, 2015","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14281,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14282,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14283,"1–9, 2015",We trained themodels in the full training set of 60000 images and tested in the test set,0
14284,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14285,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","For deterministicneural networks, we only have one sample of hidden variables given one data point",0
14286,"1–9, 2015",The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,0
14287,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","(2013) proposed to resort to reinforcement learning.However, these estimators have high variance",0
14288,doi: 10.1109/CVPR.2015.7298594,The generated bottleneck samples are then used to estimate mutual in-formation,0
14289,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14290,doi: 10.1109/CVPR.2015.7298594,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14291,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich",Therightmost column is obtained by averaging over all generated samples of bottlenecks drawn fromthe prediction,0
14292,"Going deeper with convolutions.In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,USA, June 7-12, 2015, pp","Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14293,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E","Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961",0
14294,URL https://doi.org/10.1109/CVPR.2015.7298594.,The en-coder maps the input variable xxx to a single bottleneck variable zzz that is not a part of the consideredneural network architecture,0
14295,doi: 10.1109/CVPR.2015.7298594,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14296,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich","In Model D, we used the weight trained in Model A to performstochastic prediction at test time",0
14297,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich",We used the last 10000 images of the training set as a holdout set for tuning hyper-parameters,0
14298,URL https://doi.org/10.1109/CVPR.2015.7298594.,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E",1
14299,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich","Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
14300,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,"Thatis,3.3.2 APPROXIMATE COMPRESSION
The compression terms in LP IB involve computing mutual information between two consecutive
bottlenecks",0
14301,Learning stochastic feedforward neural networks,Yichuan Tang and Ruslan Salakhutdinov,1
14302,"530–538, 2013",Information bottleneck for gaus-sian variables,0
14303,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,Yichuan Tang and Ruslan Salakhutdinov,1
14304,Learning stochastic feedforward neural networks,"Yoshua Bengio, Nicholas L´eonard, and Aaron C",0
14305,Learning stochastic feedforward neural networks,Yichuan Tang and Ruslan Salakhutdinov,1
14306,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,A,0
14307,Learning stochastic feedforward neural networks,"REFERENCESRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,Nicolas Ballas, Fr´ed´eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, YoshuaBengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, NicolasBouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Br´ebisson, OlivierBreuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-mans, Marc-Alexandre Cˆot´e, Myriam Cˆot´e, Aaron Courville, Yann N",0
14308,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,Yichuan Tang and Ruslan Salakhutdinov,1
14309,"530–538, 2013","(2014)),and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).The learning principle in DNNs has therefore attributed to the MLE principle as a standard one forguiding the learning toward a beneﬁcial direction",0
14310,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,Techniques for learningbinary stochastic feedforward neural networks,0
14311,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,"Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
14312,Learning stochastic feedforward neural networks,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio",0
14313,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,"The SFNN, on the other hand, encodes information in a way that matchesthe model distribution to the empirical data distribution",0
14314,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp",Yichuan Tang and Ruslan Salakhutdinov,1
14315,Yichuan Tang and Ruslan Salakhutdinov,Yichuan Tang and Ruslan Salakhutdinov,1
14316,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,Yichuan Tang and Ruslan Salakhutdinov,1
14317,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,"However, the MLE principle is very generic thatis not specially tailored for neural networks",0
14318,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,Yichuan Tang and Ruslan Salakhutdinov,1
14319,"530–538, 2013","CoRR, abs/1406.2989, 2014",0
14320,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp",Yichuan Tang and Ruslan Salakhutdinov,1
14321,Yichuan Tang and Ruslan Salakhutdinov,"(2014)) and game playing (e.g., Silver et al.(2016))",0
14322,Learning stochastic feedforward neural networks,"However, our motivation for this stochasticity is that it enables bottleneck sampling giventhe data variables (X, Y )",0
14323,"530–538, 2013",Yichuan Tang and Ruslan Salakhutdinov,1
14324,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,The en-coder maps the input variable xxx to a single bottleneck variable zzz that is not a part of the consideredneural network architecture,0
14325,Learning stochastic feedforward neural networks,Yichuan Tang and Ruslan Salakhutdinov,1
14326,Yichuan Tang and Ruslan Salakhutdinov,Yichuan Tang and Ruslan Salakhutdinov,1
14327,"530–538, 2013","As can be seen from the table, PIB and Model C givesnearly the same performance which outperform deterministic neural networks and SFNNs, and theirstochastic and deterministic version.It is interesting to empirically see that the deterministic version of PIB at test time (Model C) gives aslightly better result than PIB",0
14328,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp",Yichuan Tang and Ruslan Salakhutdinov,1
14329,Yichuan Tang and Ruslan Salakhutdinov,The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
14330,Learning stochastic feedforward neural networks,ExistingIB framework for DNNs speciﬁes a single bottleneck while our PIB preserves hierarchical represen-tations which a neural network’s expressiveness comes from,0
14331,Yichuan Tang and Ruslan Salakhutdinov,"Here we preferto this extreme, i.e., the 0th level, as the super level",0
14332,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp","David Silver, Aja Huang, Chris J",0
14333,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,This workproposes using mutual information of a hidden layer with the input layer and the output layer toquantify the performance of DNNs,0
14334,"530–538, 2013","(2013) proposed to resort to reinforcement learning.However, these estimators have high variance",0
14335,Yichuan Tang and Ruslan Salakhutdinov,Yichuan Tang and Ruslan Salakhutdinov,1
14336,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,"We propose re-using the existing neural network architecture
as variational decoders for each hidden layers",0
14337,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp","Therefore, lossy representation is often more helpful (and of course moreefﬁcient) than a precise representation",0
14338,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,Yichuan Tang and Ruslan Salakhutdinov,1
14339,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,Yichuan Tang and Ruslan Salakhutdinov,1
14340,Yichuan Tang and Ruslan Salakhutdinov,Yichuan Tang and Ruslan Salakhutdinov,1
14341,Yichuan Tang and Ruslan Salakhutdinov,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,0
14342,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,"Speciﬁcally in PIBs, we propose to jointly compressthe network’s intermediate spaces and preserve relevant information simultaneously at all layers ofthe network",0
14343,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,Yichuan Tang and Ruslan Salakhutdinov,1
14344,"530–538, 2013","However, the variational approximatedistribution for the code space may be too loose while the actual marginal distribution can be sam-pled easily.Our work, on the other hand, focuses on better exploiting intermediate representations of a neuralnetwork architecture using the IB principle",0
14345,Yichuan Tang and Ruslan Salakhutdinov,Yichuan Tang and Ruslan Salakhutdinov,1
14346,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,"Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation",0
14347,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp","The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ",0
14348,Learning stochastic feedforward neural networks,The MLE principle maximizes the likelihood function under theempirical data distribution,0
14349,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,Yichuan Tang and Ruslan Salakhutdinov,1
14350,"530–538, 2013","Thus, it is hard to analytically capture such modiﬁcations.",0
14351,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp",Yichuan Tang and Ruslan Salakhutdinov,1
14352,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,Yichuan Tang and Ruslan Salakhutdinov,1
14353,Yichuan Tang and Ruslan Salakhutdinov,Yichuan Tang and Ruslan Salakhutdinov,1
14354,"530–538, 2013","(2014), Bahdanau et al",0
14355,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,Yichuan Tang and Ruslan Salakhutdinov,1
14356,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,"The MLE principle treats
the neural network model p(xxx; θθθ) as a whole without explicitly considering the contribution of its
internal structures (e.g., hidden layers and hidden neurons)",0
14357,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,"Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation",0
14358,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,Yichuan Tang and Ruslan Salakhutdinov,1
14359,"530–538, 2013",Yichuan Tang and Ruslan Salakhutdinov,1
14360,Learning stochastic feedforward neural networks,Yichuan Tang and Ruslan Salakhutdinov,1
14361,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,"However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",0
14362,Learning stochastic feedforward neural networks,Yichuan Tang and Ruslan Salakhutdinov,1
14363,Yichuan Tang and Ruslan Salakhutdinov,"In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence andStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp",0
14364,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp",Yichuan Tang and Ruslan Salakhutdinov,1
14365,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp",Yichuan Tang and Ruslan Salakhutdinov,1
14366,Learning stochastic feedforward neural networks,Yichuan Tang and Ruslan Salakhutdinov,1
14367,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp",Yichuan Tang and Ruslan Salakhutdinov,1
14368,Yichuan Tang and Ruslan Salakhutdinov,The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
14369,"530–538, 2013","Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
14370,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp",Yichuan Tang and Ruslan Salakhutdinov,1
14371,"530–538, 2013","The relevance decoder ptrue(yyy|zzzi), which is uniquelydetermined given the encoder pθθθ(zzzi|xxx) and the joint distribution pD(xxx, yyy), is intractable",0
14372,Learning stochastic feedforward neural networks,"Thus, the average volume of apartitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z)",0
14373,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,The exactsolution to the minimization problem above is found (Tishby et al,0
14374,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,Yichuan Tang and Ruslan Salakhutdinov,1
14375,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp","In this paper, we present a layer-wise learning of stochastic neural networks(SNNs) in an information-theoretic perspective.In each layer of an SNN, thecompression and the relevance are deﬁned to quantify the amount of informationthat the layer contains about the input space and the target space, respectively",0
14376,Learning stochastic feedforward neural networks,Yichuan Tang and Ruslan Salakhutdinov,1
14377,"530–538, 2013","Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss",0
14378,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,Yichuan Tang and Ruslan Salakhutdinov,1
14379,"530–538, 2013","David Silver, Aja Huang, Chris J",0
14380,Learning stochastic feedforward neural networks,Masteringthe game of go with deep neural networks and tree search,0
14381,"530–538, 2013",Yichuan Tang and Ruslan Salakhutdinov,1
14382,"530–538, 2013",Yichuan Tang and Ruslan Salakhutdinov,1
14383,Learning stochastic feedforward neural networks,Yichuan Tang and Ruslan Salakhutdinov,1
14384,"530–538, 2013","(2014), Bahdanau et al",0
14385,Learning stochastic feedforward neural networks,Yichuan Tang and Ruslan Salakhutdinov,1
14386,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",0
14387,"Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp",Yichuan Tang and Ruslan Salakhutdinov,1
14388,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,URLhttp://arxiv.org/abs/1308.3432.,0
14389,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,"As can be seen from the table, PIB and Model C givesnearly the same performance which outperform deterministic neural networks and SFNNs, and theirstochastic and deterministic version.It is interesting to empirically see that the deterministic version of PIB at test time (Model C) gives aslightly better result than PIB",0
14390,Yichuan Tang and Ruslan Salakhutdinov,Yichuan Tang and Ruslan Salakhutdinov,1
14391,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",0
14392,"530–538, 2013",Yichuan Tang and Ruslan Salakhutdinov,1
14393,"530–538, 2013",Yichuan Tang and Ruslan Salakhutdinov,1
14394,"530–538, 2013",Yichuan Tang and Ruslan Salakhutdinov,1
14395,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,"In estimating mutual information, we adopted the variational method asin Alemi et al",0
14396,In Ad-vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-mation Processing Systems 2013,"Originally, the general IB framework is proposed in Tishby et al",0
14397,Learning stochastic feedforward neural networks,Yichuan Tang and Ruslan Salakhutdinov,1
14398,"530–538, 2013","1–9, 2015",0
14399,URL http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.,Yichuan Tang and Ruslan Salakhutdinov,1
14400,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14401,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",We used the same neural networkarchitecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learningrate of 0.01 in the ﬁrst 50000 training samples,0
14402,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14403,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",0
14404,A,Theano: A Pythonframework for fast computation of mathematical expressions,0
14405,A,A,1
14406,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14407,A,"However, thealgorithm is not applicable to neural networks",0
14408,A,content of the original space possibly including its dimensionality and topological structure,0
14409,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",0
14410,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14411,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",The recent work Alemi et al,0
14412,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14413,A,"To investigate morein this, we compute the test error for various values of the number of samples used for Monte-Carloaveraging, M (Figure 2)",0
14414,A,The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,0
14415,A,A,1
14416,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","Depending on the structure of the target space Y, we can use an appropriate model foroutput distributions as follows: (1) For classiﬁcation, we model the output distribution with soft-max function, p(Y = i|zzzL) = softmax(W (L+1)); (2) For binary output vectors Y ,i p(yyyi|zzzL) where p(Yi = 1|zzzL) =); (3) For real-valued output vectors Y , we use Gaussian distribution,where zzz = (zzz1, zzz2, ..., zzzL) is the entire sequence of hidden layers in the neural network",0
14417,A,(2016)).,0
14418,A,"We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level",0
14419,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14420,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits basedon the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs)",0
14421,A,"In this work, we use the gradient estimator inspiredby Raiko et al",0
14422,A,"In the general IB framework, each value of β speciﬁes a uniquepoint of Z in the information plane",0
14423,A,A,1
14424,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
14425,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14426,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",Theano: A Pythonframework for fast computation of mathematical expressions,0
14427,A,A,1
14428,A,"530–538, 2013",0
14429,A,A,1
14430,A,"The mutual information I(Z, X) which measuresthe amount of information that Z contains about X can therefore quantify the quality of the encod-ing p(zzz|xxx)",0
14431,A,"As a result, it may encode irrelevant infor-mation that hurts the generalization.For additional visualization, an empirical architecture analysis of PIB and SFNN is presented inAppendix II.B.",0
14432,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al",0
14433,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",The MLE principle maximizes the likelihood function under theempirical data distribution,0
14434,A,A,1
14435,A,A,1
14436,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",(1999)) whichextracts relevant information in an input variable X about a target variable Y ,0
14437,A,A,1
14438,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14439,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14440,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14441,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14442,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14443,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14444,A,A,1
14445,A,The visualization in Figure 4 indicates that PIB models the structured outputspace better and faster (using lesser number of epochs) than SFNN,0
14446,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","770–778, 2016",0
14447,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","McGibbon, Roland Memisevic, Bart van Merri¨enboer, Vincent Michalski, MehdiMirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel,Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Sal-vatier, Franc¸ois Savard, Jan Schl¨uter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,Dmitriy Serdyuk, Samira Shabanian, ´Etienne Simon, Sigurd Spieckermann, S",0
14448,A,The ﬁgures illustrate the capability of modeling structured output space using PIBand SFNN.,0
14449,A,A,1
14450,A,A,1
14451,A,We explicitly deﬁne the learning objective for PIB as:LP IB(Z) := LP IB(θθθ) :=,0
14452,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","For the compression, we
decompose the mutual information as follows:",0
14453,A,A,1
14454,A,A,1
14455,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",Our learning framework offers aprincipled way of interpreting and learning all layers of neural networks and encourages a more,0
14456,A,A,1
14457,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14458,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14459,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14460,A,A,1
14461,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,0
14462,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14463,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","Each respective space is asso-ciated with the corresponding probability measures pD(xxx), pD(yyy) and p(zzzl) where pD(.) indicatesthe underlying probability distribution of the data and p(.) denotes model distributions",0
14464,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",Model E is SFNN and Model B is Model C with deterministicprediction during test phase,0
14465,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","In this case, we have:pv(yyy|zzzl) = p(yyy|zzzl) =",0
14466,A,A,1
14467,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","Although the analysis offers a new perspective about optimality in neural networks,it proposes general analysis of optimality rather than a practical optimization criteria",0
14468,A,"Fortunately,one can estimate the gradient in this case",0
14469,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","For the lth-level bottleneck Zl, the compression is deﬁned as the mutual informationbetween Zl and the previous-level bottleneck Zl−1 while the relevance is speciﬁed as its mutualinformation with the target variable Y ",0
14470,A,A,1
14471,A,"More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
14472,A,"Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh",0
14473,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",These code spaces are lossy representations of the data space as itfollows from the data-processing inequality (DPI) (Cover & Thomas (2006)) that,0
14474,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","As we can see from the ﬁgure, the Monte-Carlo averaging of PIB obtainsits good approximation around M = 30 and the deterministic prediction roughly places a lowerbound on the Monte-Carlo averaging at test time",0
14475,A,A,1
14476,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","(2014), Dauphin & Grangier(2016))",0
14477,A,A,1
14478,A,URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,0
14479,A,A,1
14480,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14481,A,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL",0
14482,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution ˆpD as possible (e.g., see Appendix I.B)",0
14483,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","Speciﬁcally, the relevance decoder is determined as follows:",0
14484,A,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance",0
14485,A,"For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
14486,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14487,A,A,1
14488,A,A,1
14489,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14490,A,A,1
14491,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","For simplicity, we present the derivation of I(Z1, Z0) only 3",0
14492,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",The MLE principle maximizes the likelihood function under theempirical data distribution,0
14493,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14494,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14495,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",A,1
14496,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz",The information-theoretic goal oflearning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimalpoint in the information curve as possible,0
14497,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","Here, a SFNN sim-ply prefers to feed-forward neural network models following the MLE principle for learning modelparameters",0
14498,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","Lilli-crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis",0
14499,"PROOF OF THE PREPOSITIONSProof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(yyy|zzz) =p(yyy|zzzL, zzzL−1, ..., zzz1) = p(yyy|zzzL) and from Jensen’s inequality thatp(zzz|xxx)p(yyy|zzz)dzzz","Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States., pp",0
14500,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14501,The MLE principle maximizes the likelihood function under theempirical data distribution,"Therefore, their approach still treats a neural network as a whole ratherthan optimizing it layer-wise",0
14502,The MLE principle maximizes the likelihood function under theempirical data distribution,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14503,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)","For simplicity, we present the derivation of I(Z1, Z0) only 3",0
14504,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",0
14505,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14506,The MLE principle maximizes the likelihood function under theempirical data distribution,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14507,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14508,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14509,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d","We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al",0
14510,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14511,The MLE principle maximizes the likelihood function under theempirical data distribution,"(2015)), natural languagetranslation (e.g., Cho et al",0
14512,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14513,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d","1–9, 2015",0
14514,The MLE principle maximizes the likelihood function under theempirical data distribution,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14515,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,"Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich",0
14516,The MLE principle maximizes the likelihood function under theempirical data distribution,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14517,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d","In Proceedings of the 32nd International Conference on MachineLearning, ICML 2015, Lille, France, 6-11 July 2015, pp",0
14518,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)","Speciﬁcally, the relevance decoder is determined as follows:",0
14519,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,"In practice, log p(zzz1) may be nu-merically unstable for large cardinality |Z1|",0
14520,The MLE principle maximizes the likelihood function under theempirical data distribution,"A smaller mutual information I(Z, X) implies a more compressed representation Z interms of X.Since the original data space is continuous, it requires inﬁnite precision to represent it precisely.However, only some set of underlying explanatory factors in the the data space would be beneﬁ-cial for a certain task",0
14521,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,"However, the MLE principle is very generic thatis not specially tailored for neural networks",0
14522,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14523,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,"Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance",0
14524,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",Learning stochastic feedforward neural networks,0
14525,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14526,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",Our PIB is also a standard steptowards better exploiting representational power of more expressive neural network models such asConvolutional Neural Networks (LeCun et al,0
14527,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14528,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)","Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overﬁtting",0
14529,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14530,The MLE principle maximizes the likelihood function under theempirical data distribution,"Although the analysis offers a new perspective about optimality in neural networks,it proposes general analysis of optimality rather than a practical optimization criteria",0
14531,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,0
14532,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14533,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14534,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14535,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14536,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14537,The MLE principle maximizes the likelihood function under theempirical data distribution,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14538,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,Courville,0
14539,The MLE principle maximizes the likelihood function under theempirical data distribution,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14540,The MLE principle maximizes the likelihood function under theempirical data distribution,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14541,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)","Furthermore, the work imposes a variational prior distribution in thecode space to approximate its actual marginal distribution",0
14542,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14543,The MLE principle maximizes the likelihood function under theempirical data distribution,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14544,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14545,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,(2016)).,0
14546,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14547,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14548,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)","Thus, it is hard to analytically capture such modiﬁcations.",0
14549,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14550,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14551,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d","In this aspect, we view the hidden layers of a multi-layeredneural network as a lossy representation of the data space",0
14552,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)","(2014), Bahdanau et al",0
14553,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14554,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d","More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
14555,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",The leftmostcolumn is the original MNIST test digit followed by the masked out digits and nine samples,0
14556,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,Model C uses the weighted trained in PIB but we report deterministicprediction instead of stochastic prediction for test performance.,0
14557,The MLE principle maximizes the likelihood function under theempirical data distribution,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14558,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14559,The MLE principle maximizes the likelihood function under theempirical data distribution,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14560,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)","Nature, 529(7587):484–489, 2016.doi: 10.1038/nature16961",0
14561,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",The MLE principle maximizes the likelihood function under theempirical data distribution,0
14562,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14563,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14564,The MLE principle maximizes the likelihood function under theempirical data distribution,"Originally, the general IB framework is proposed in Tishby et al",0
14565,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",We have also proposed an approximation that fully utilizes all parametersin a neural network and does not resort to any extra models,0
14566,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14567,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14568,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14569,The MLE principle maximizes the likelihood function under theempirical data distribution,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14570,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14571,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)","Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
14572,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14573,The MLE principle maximizes the likelihood function under theempirical data distribution,"To overcome thisproblem, we propose an upper bound on the entropy using Jensen’s inequality:log p(zzz1) = log Ep(zzz0)[p(zzz1|zzz0)] ≥ Ep(zzz0) [log p(zzz1|zzz0)]H(Z1) ≤ −Ep(zzz1)",0
14574,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14575,The MLE principle maximizes the likelihood function under theempirical data distribution,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14576,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",URL https://doi.org/10.1038/nature16961.,0
14577,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",We used the same neural network architecture of 392-512-512-392for PIB and SFNN and trained them with SGD with constant learning rate of 0.01,0
14578,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",0
14579,The MLE principle maximizes the likelihood function under theempirical data distribution,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14580,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,"In the general IB framework, each value of β speciﬁes a uniquepoint of Z in the information plane",0
14581,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",Understanding the difﬁculty of training deep feedforward neuralnetworks,0
14582,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14583,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14584,The MLE principle maximizes the likelihood function under theempirical data distribution,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14585,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14586,The MLE principle maximizes the likelihood function under theempirical data distribution,"The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",0
14587,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d","In estimating mutual information, we adopted the variational method asin Alemi et al",0
14588,"Rigorously,given a set of samples X = {xxx1, xxx2, ..., xxxN} i.i.d",Sergey Ioffe and Christian Szegedy,0
14589,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14590,The MLE principle maximizes the likelihood function under theempirical data distribution,Techniques for learningbinary stochastic feedforward neural networks,0
14591,The MLE principle maximizes the likelihood function under theempirical data distribution,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14592,"drawn from some underlying data distributionpD(xxx), a parametric model pmodel(xxx; θθθ) attempts to map any data sample xxx to a real number that es-timates the true probability pD(xxx)",The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14593,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,"The MLE
principle is proved to be mathematically equivalent to the IB principle for the multinomial mixture
model for clustering problem when the input distribution X is uniform or has a large sample size
(Slonim & Weiss (2002))",0
14594,The MLE principle maximizes the likelihood function under theempirical data distribution,"We explicitly deﬁne the learning objective for PIB as:Since the neural network is a lossy representation of the original data space, a learning principleshould make this loss in a beneﬁcial manner",0
14595,The MLE principle maximizes the likelihood function under theempirical data distribution,"The VCR at the highest-level bottleneck variable ZL equals the VCR for the entirecompositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL",0
14596,The MLE principle maximizes the likelihood function under theempirical data distribution,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14597,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,"For multi-layered neural networks, each hidden layer Zlis a representation that can also be quantiﬁed in the information plane.In this experiment, we considered an odd-even decision problem in the MNIST dataset in whichthe task is to determine if the digit in an image is odd or even",0
14598,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14599,This in turn can be interpreted as matching the model distribution pmodelwith the data distribution pD by minimizing their KL divergence to ﬁnd the maximum likelihood(point) estimator for θθθ:θθθM L = arg max,The purpose of the MLE principle can be interpreted as matching the model distribution to the em-pirical data distribution using the KL divergence as a measure of their discrepancy,1
14600,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14601,"The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14602,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)",The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
14603,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)","For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
14604,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The optimal representation Z is determined via theminimization of the following Lagrangian:Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasksincluding image recognition (e.g., Krizhevsky et al",0
14605,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","For stochastic sampling, we generate M = 16 samples per point during training andM = 32 samples per point during testing",0
14606,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",The framework providesa principled way of extracting the relevant information in one variable X about another variable Y .The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-tions and propose the iterative Blahut Arimoto algorithm to optimize the objective,0
14607,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14608,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14609,"The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",All models areimplemented using Theano framework (Al-Rfou et al,0
14610,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14611,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)","For stochastic prediction, we run the prediction 10 timesand report its mean and deviation standard",0
14612,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)",We usepθθθ(yyy|zzzi) as a variational approximation to each intractable relevance decoder ptrue(yyy|zzzi).,0
14613,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14614,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)","The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples ofhandwritten digit images for training and test, respectively in which each image is grayscale of size28 × 28 pixels",0
14615,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14616,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","Previously, the InformationBottleneck (IB) framework (Tishby et al",0
14617,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)","This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al",0
14618,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)","Reed, Dragomir Anguelov,Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich",0
14619,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14620,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14621,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","In this information-theoreticperspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about Xand I(Z, Y ) represents the relevance of Z to Y ",0
14622,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14623,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","Although the analysis offers a new perspective about optimality in neural networks,it proposes general analysis of optimality rather than a practical optimization criteria",0
14624,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14625,"The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14626,"The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","More speciﬁcally,the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of Xbut preserves as much relevant information in X about Y as possible",0
14627,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)","(1998))for image classiﬁcation, odd-even decision problem and multi-modal learning",0
14628,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","CoRR, abs/1406.2989, 2014",0
14629,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14630,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14631,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14632,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",The en-coder maps the input variable xxx to a single bottleneck variable zzz that is not a part of the consideredneural network architecture,0
14633,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",The approximation then leads to effectivegradient-based training of PIBs.,0
14634,"The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14635,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14636,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14637,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The decrease of I(Zl, Zl−1)makes the representation at the lth-level more compressed while the increase of I(Zl, Y ) promotesthe preservation of relevant information in Zl about Y ",0
14638,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","Journal of Machine Learning Research, 6:165–188, 2005",0
14639,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","Journal of Machine Learn-ing Research, 15(1):1929–1958, 2014",0
14640,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",1 ∼ p(zzz1) = Ep(zzz0)[p(zzz1|zzz0)],0
14641,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","However, information encoding in the PIB is more selective as it quickly encodes more rel-evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layersconcise at higher epochs",0
14642,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","We tuned β from{0} ∪ {10−i : 1 ≤ i ≤ 7}, and found β−1 = 10−4 works best.Table 1 provides the results in the MNIST classiﬁcation error in the test set for PIB and the com-parative models (A), (B), (C), (D), and (E)",0
14643,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14644,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14645,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","448–456, 2015",0
14646,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","Here, we propose Parametric Information Bottleneck (PIB) for aneural network by utilizing (only) its model parameters explicitly to approximatethe compression and the relevance",0
14647,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14648,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The information plane is an information-theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))given the joint distribution I(X, Y )",0
14649,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14650,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14651,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14652,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","Wename Zl, 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network",0
14653,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14654,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",URL http://jmlr.org/proceedings/papers/v37/ioffe15.html.,0
14655,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","1–9, 2015",0
14656,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",The best conﬁguration chosen from the holdout set is used to retrain the models fromscratch in the full training set,0
14657,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14658,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14659,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","(2014) for binary bottlenecks because it has low variance despite of being biased.Speciﬁcally, a bottleneck zzz = (z1, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +i where",0
14660,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14661,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)","The neural networkparameters θθθ = (θθθ1, θθθ2, θθθ3)",0
14662,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","We show that, the PIB framework can be con-sidered as an extension of the maximum likelihood estimate (MLE) principle toevery layer level",0
14663,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 fortesting).1: procedure GRAD-PIB2: Input: Labeled training dataset SD3: θθθ ← Initialize parameters4: repeat:5:6:7:i=1 ← Random minibatch of N samples drawn from SD
(xxxi, yyyi)N
Generate M samples of zzzi per each sample of zzzi−1 for 1 ≤ i ≤ L
Use the generated samples above and Equations 15 and 23 to approximate ˜LP IB(θθθ)
ggg ← ∂
∂θθθ
θθθ ← Update parameters using the approximate gradients ggg and SGD",0
14664,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14665,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14666,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14667,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14668,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14669,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14670,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)",A,0
14671,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","In this case, we have:where the equality in Equation 12 holds due to the Markov assumption (Equation 4)",0
14672,"The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",Each Zl isstochastically mapped from the previous stochastic variable Zl−1 via an encoder p(zzzl|zzzl−1),0
14673,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14674,"The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14675,"The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","In PIBs, weutilize only neural network parameters θθθ for deﬁning encoders and variational relevance decoders atevery level, therefore the name Parametric Information Bottleneck",0
14676,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)","In PIBs, wepropose to use the higher-level part of the existing network architecture at each layer to deﬁne thevariational relevance encoder for that layer, i.e., pv(yyy|zzzl) = p(yyy|zzzl) where p(yyy|zzzl) is determined bythe network architecture",0
14677,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","We scaled the images to [0, 1] and do not perform anyother data augmentation",0
14678,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14679,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14680,"The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14681,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14682,"The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14683,"The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14684,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14685,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",Understanding the difﬁculty of training deep feedforward neuralnetworks,0
14686,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14687,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","Speciﬁcally in supervised learning contexts, a common practice to achieve good perfor-mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with varioustechniques such as data-speciﬁc design of network architecture (e.g., convolutional neural networkarchitecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al",0
14688,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","This is the reason why the conditionaldistribution p(yyy|xxx) in stochastic neural networks is multi-modal.In this experiment, we followed Raiko et al",0
14689,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",A small change in a multi-layered neural network could greatly modify the entropy of theinput variables,0
14690,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)",0
14691,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)","For the entropy term H(Z1), we resort to empirical samples of zzz1 generated by MonteCarlo sampling to estimate the entropy:= Ep(zzz0)(19)i=1 and H(Z1,i|Z0 = zzz0) = −q log q − (1 − q) log(1 − q) where q = p(Z1,i =where Z1 = (Z1,i)N11|Z0 = zzz0)",0
14692,"The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14693,"Indeed, without loss of generality, we assume bivariate targetvariable yyy = (y1, y2)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14694,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",Courville,0
14695,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14696,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14697,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14698,"It follows from the fact that the neurons within a layer are independent givensome previous layer that we have:(cid:2)Ep(zzzl|xxx) [log p(y1, y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(y1|zzzl) + log p(y2|zzzl)](cid:3)
(cid:2)Ep(zzzl|xxx) [log p(yi|zzzl)](cid:3)","The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",1
14699,"The VCR at level l (deﬁned by (15), (16)) for a multivariate variable yyy can be decomposed into theVCRs for each of its components",Our PIB also gives neural networksan information-theoretic interpretation both in network structure and model learning,0
