	cdate	content	ddate	forum	id	invitation	nonreaders	number	original	readers	replyto	signatures	tcdate	tddate	tmdate	writers
0	1542459638461	{'title': 'Combining Long Short Term Memory and Convolutional Neural Network for Cross-Sentence n-ary Relation Extraction', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper25/Authors'], 'keywords': ['n-ary relation extraction', 'information extraction'], 'abstract': 'A combined Long Short Term Memory and Convolutional Neural Networks  (lstm_cnn+wf+pf) that exploits word embeddings and positional embeddings is proposed for cross-sentence n-ary relation extraction. The proposed model brings together the properties of lstms and cnns, to simultaneously exploit long-range sequential information and capture the most informative features, essential for cross-sentence n-ary relation extraction. The  lstm_cnn+wf+pf model was evaluated using standard datasets for cross-sentence n-ary relation extraction, where the model significantly outperforms baseline cnn and lstm model and a combined cnn_lstm model. The paper also shows that the lstm_cnn model outperforms the current state-of-the-art methods on cross-sentence n-ary relation extraction.', 'pdf': '/pdf/c49ec54d1c4c4909e73f88a0279bbec2c9e86d6d.pdf', 'archival status': 'Non-Archival', 'subject areas': ['Information Extraction', 'Applications: Biomedicine'], 'paperhash': 'anonymous|combining_long_short_term_memory_and_convolutional_neural_network_for_crosssentence_nary_relation_extraction', '_bibtex': '@inproceedings{    \nanonymous2019combining,    \ntitle={Combining Long Short Term Memory and Convolutional Neural Network for Cross-Sentence n-ary Relation Extraction},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=Sye0lZqp6Q},    \nnote={under review}    \n}'}		Sye0lZqp6Q	Sye0lZqp6Q	AKBC.ws/2019/Conference/-/Blind_Submission	[]	25	SJxC65snpQ	['everyone']		['AKBC.ws/2019/Conference']	1542459638461		1549019825572	['AKBC.ws/2019/Conference']
1	1549019663120	{'title': 'Answers to comments', 'comment': 'We thank the reviewer for the comments. The following is our response.\n\nQ1: In general, I think that wording can be tighter, and some repetitive information can be omitted. For example, Section 4 could be condensed to highlight the main findings, instead of splitting them across subsections.\nAns: Since we wanted to discuss various aspects of the model, we have used different sub-sections. The title of the sub-section indicates the key aspect of the model, that we wanted to highlight and discuss. We feel that condensing it into one section, can result in difficulty in reading the paper.\n\nQ2: I think that Section 3.1.2 (“Position Features”) would benefit from an example showing an input encoding.\nAns: We have updated Section 3.1.2 to provide an example of position embedding.\n\nQ3: Table 5 shows up in the references. \nAns: Corrected. Table 5 is moved to the next page\n\nQ4: Typos and margins\nAns: The typos and margins are corrected.\n\n'}		Sye0lZqp6Q	HJxPMcj-4E	AKBC.ws/2019/Conference/-/Paper25/Official_Comment	['AKBC.ws/2019/Conference/Paper25/Reviewers/Unsubmitted']	3		['everyone']	B1eBHYjFgV	['AKBC.ws/2019/Conference/Paper25/Authors']	1549019663120		1549019663120	['AKBC.ws/2019/Conference/Paper25/Authors', 'AKBC.ws/2019/Conference']
2	1549016732657	"{'title': 'Clarification for some unclear issues', 'comment': 'We thank the reviewer for the comments.\n\nQ1. - ""The use of multiple filters facilitates selection of the most important feature for each feature map"": What do you mean with this sentence? Don\'t you get another feature map for each filter? Isn\'t the use of multiple filters rather to capture different semantics within the sentence?\nAns: It is true that the use of multiple features facilitates capturing different semantics within the sentence. We have corrected this section on max-pooling in the paper.\n\nQ2.  ""The task of predicting n-ary relations is modeled both as a binary and multi-class classification problem"": How do you do that? Are there different softmax layers? And if yes, how do you decide which one to use?\nAns: The task of predicting n-ary relations as a binary and multi-classification problem is specific to the datasets that are used in the paper. While using the Peng et al. (2016) dataset, we have a multi-class classification problem. However, when we are working with Chemcial-induced dataset, we have a binary classification problem, to predict whether their exists a binary relation between the entities both in single sentence and across sentences. Therefore, when working with Peng et al dataset, we use softmax function with categorical cross entropy loss to output probability over the five output classes (resistance; resistance or no-response; response; sensitivity; and none) and employ softmax layer with binary cross-entropy loss to predict probability over two classes.\n\nQ3:  Table 1/2: How can you draw conclusions about the performance on binary and ternary relations from these tables? I can only see the distinction of single sentence and cross sentence there.\nAns: As indicated in the caption for Tables 1 and 2, while Table 1 specifically deals with ternary relations involving ternary relations between drug-gene-mutation, Table 2 deals with binary relations involving drug-gene entities. However, these binary and ternary relations exists both in single sentences and across sentences. Thus the distinction between single and across sentences is made. Given this aspect, the conclusions are drawn for the performance of binary and ternary relations in single sentences and across sentences.\n\nQ4: - Table 3: The numbers for short distance spans (mostly 20.0) look suspicious to me. What is the frequency of short/medium/long distance spans in the datasets? Are they big enough to be able to draw any conclusions from them?\nAns: The reviewer is right in noting that there are very few instances for short distance spans. This is also the reason why the performance of different models are lower for short distance spans, compared to medium and long distance spans.\n\nQ5: - You say that CNN_LSTM does not work because after applying the CNN all sequential information is lost. But how can you apply an LSTM afterwards then? Is there any recurrence at all? (The sequential information would not be lost after the CNN if you didn\'t apply pooling. Have you tried that?)\nAns: We had conducted experiments removing the max-pooling layer and passing the features to an LSTM layer. However, this did not help in improving the performance.\n\nQ6: Your observation that more than two positional embeddings decrease the performance is interesting (and unexpected). Do you have any insights on this? Does the model pay attention at all to the second of three entities? What would happen if you simply deleted this entity or even some context around this entity (i.e., perform an adversarial attack on your model)?\nAns: Given the experimental results, we observe that adding positional embeddings for more than two entities results in a decrease in the performance. We have not conducted further experiments such as removing the second entity or context around the second entity. Removing the second entity or the context around it would certainly result in a poor performance as we would be disturbing the sequential information. However, it is worthwhile to conduct such experiments and intend to do it in the future.\n\nQ7: Typos\nAns: Thanks for identifying the typos. The typos are corrected.'}"		Sye0lZqp6Q	SyeSjCqW4E	AKBC.ws/2019/Conference/-/Paper25/Official_Comment	['AKBC.ws/2019/Conference/Paper25/Reviewers/Unsubmitted']	2		['everyone']	SJxnuy1TZV	['AKBC.ws/2019/Conference/Paper25/Authors']	1549016732657		1549016732657	['AKBC.ws/2019/Conference/Paper25/Authors', 'AKBC.ws/2019/Conference']
3	1549013917038	{'title': 'Details on AMIE+ explained', 'comment': 'We would like to thank the reviewer for the comments.\n\n1) The proposed approach marginally performs better than deterministic rule learning.\n\nThis is true only for PR curves but not for cross entropy. In the section of Parameter Learning, we explain how maximizing expected log likelihood is equivalent to minimizing cross entropy (Equation 3). As our proposed approach optimizes on cross entropy, it induces an average reduction in the cross entropy of 82% and 85% as compared to ProbFOIL+ and AMIE+ respectively. The insignificant differences in precision-recall curves only suggest that obtaining a ranking of tuples based on predicted probabilities (but not the actual probabilities) can be quite reliably done already by models that are not very precise when it comes to predicting the actual probabilities.\n\n\n2) The approach is straightforward and depends heavily on the candidate rules produced by AMIE+.\n\nOur approach is straightforward on purpose as we want to see how much the proper treatment of probabilities in the KB completion task using a rule-based approach helps. At first glance, SafeLearner does seem to be heavily dependent on the rules generated by AMIE+. But AMIE+ is not a black-box as we exactly know the kind of rules we require as candidates. Had AMIE+ not been developed, we could have coded the function ourselves to generate candidate rules. Since AMIE+ is a multi-threaded package in Java, it does the job well by scaling well to large KBs. Furthermore, as SafeLearner is not specific to any particular candidate generation method, it can be used with any other relational rule learner instead of AMIE+.\n\n\n3) The paper does not provide insights into the drawbacks of using AMIE+, the kind of rules that will be hard for AMIE+ to produce.\n\nAs mentioned in the cited paper ‘Fast rule mining in ontological knowledge bases with AMIE+’ (Galarraga et al.,2015), AMIE+ uses 3 types of language biases to restrict the size of the search space:\n\t1) The rules learned by AMIE+ omit reflexive atoms of the form ‘x(A, A)’.\n\t2) The rules are connected, i.e., every atom shares at least one variable transitively to every other atom of the rule. This omits vague rules of the form ‘x(A, B) :- y(C, D)’.\n\t3) The rules are closed, i.e., all the variables in a rule appear at least twice within itself. This omits open rules of the form ‘x(A, B) :- b(A, C)’ which would hold for any substitution of B and C.\n\nMoreover, AMIE+ only works with binary relations in a KB and does not have negations in its rules. For instance, the types of non-recursive rules of length <= 3, that can be generated by AMIE+ within SafeLearner are:\n\t  1) x(A, B) :- y(A, B).\n    \t  2) x(A, B) :- y(B, A).\n\n    \t  3) x(A, B) :- y(A, C), y(C, B).\n    \t  4) x(A, B) :- y(A, C), y(B, C).\n    \t  5) x(A, B) :- y(C, A), y(C, B).\n    \t  6) x(A, B) :- y(C, A), y(B, C).\n\n    \t  7) x(A, B) :- y(A, C), z(C, B).\n    \t  8) x(A, B) :- y(A, C), z(B, C).\n    \t  9) x(A, B) :- y(C, A), z(C, B).\n     \t10) x(A, B) :- y(C, A), z(B, C).\nIn the context of doing Probabilistic Rule Learning for KB Completion, these are precisely all the forms of rules which we require as we can not practically predict missing tuples using reflexive, disconnected or open rules. SafeLearner is capable of learning rules of any length by specifying the maximum rule length as an input parameter. \n\n\n4) How can the proposed method be improved to learn rules beyond the candidate rules?\n\nThe method can be used with any other method that learns deterministic rules to make them probabilistic. We do not claim that using AIME+ is the best. Our main interest is in seeing if/how the proper treatment of probabilities helps in the KB completion tasks using a rule-based approach.'}		HkyI-5667	rJgHsQcWNN	AKBC.ws/2019/Conference/-/Paper54/Official_Comment	['AKBC.ws/2019/Conference/Paper54/Reviewers/Unsubmitted']	5		['everyone']	HyxYWizrfV	['AKBC.ws/2019/Conference/Paper54/Authors']	1549013917038		1549013917038	['AKBC.ws/2019/Conference/Paper54/Authors', 'AKBC.ws/2019/Conference']
4	1549013795105	"{'title': 'Comparisons to Neural Theorem Provers explained', 'comment': ""5) ‘End-to-end differentiable proving’ from NeurIPS (NIPS) 2017 also tackles the same problem and it would be nice to see a comparison to that work. \n\nWe have qualitatively drawn parallels between our Statistical Relational Learning (SRL) based approach and Knowledge Graph Embedding (KGE) based approaches for the problem of KB completion in Appendix E that we added to the revision. The SRL based approach is much more interpretable and explainable as compared to the black-box KGE based approaches but also, in our opinion, as compared to Neural Theorem Provers (NTPs). KGE based approaches, including NTPs, also need the test data to get the embedding which is not required by any SRL based approaches. SRL based approaches can reason with unseen constants in the data as they learn first-order rules. KGE based approaches would require re-training in order to incorporate a lot of new constants which is not the case with our SRL based approach. Please refer to Appendix E for further details.\n\nTo compare the scalability of SafeLearner with NTPs, ‘End-to-end differentiable proving’ uses 4 deterministic KBs that are not at a large scale (Countries KB has 1158 facts, Kinship KB has 10686 facts, Nations KB has 2565 facts and UMLS KB has 6529 facts). Recently, they submitted a follow-up paper, ’Towards Neural Theorem Proving at Scale’, where they claim to have made their technique more scalable. The follow-up paper uses the following 3 deterministic KBs: 1) WordNet18 KB with 151,442 facts, 2) WordNet18RR with 93,003 facts, and 3) Freebase FB15k-237 KB with 14,951 facts. On the other hand, SafeLearner has demonstrated that it scales to YAGO 2.4 KB of 948,000 probabilistic tuples. Although previous SRL techniques were not as scalable, SafeLearner is as scalable as the latest version of NTPs because it is the first SRL technique to use lifted inference.\n\nMoreover, in our opinion, NTPs do not actually 'learn' rules. They enumerate all possible rules up to a defined length and learn how to activate them.  Essentially, NTPs optimize theorem-proving procedure given the rules. In their follow-up paper, they focus on finding just one proof efficiently (instead of all proofs, as in the initial version) and this brings them scalability.""}"		HkyI-5667	BkgjQm5Z44	AKBC.ws/2019/Conference/-/Paper54/Official_Comment	['AKBC.ws/2019/Conference/Paper54/Reviewers/Unsubmitted']	4		['everyone']	HyxYWizrfV	['AKBC.ws/2019/Conference/Paper54/Authors']	1549013795105		1549013795105	['AKBC.ws/2019/Conference/Paper54/Authors', 'AKBC.ws/2019/Conference']
5	1549013409404	{'title': 'Kindly refer to the Appendix', 'comment': 'We would like to thank the reviewer for the comments.\n\n1) The paper identifies sparsity of Knowledge bases as a challenge but does not deal with it. How does sparsity affect the algorithm design?\n\nSince SafeLearner is a Statistical Relational Learning (SRL) approach that would learn first-order rules to predict tuples, it can even reason about new constants being included in the KB without re-training. On the other hand, since knowledge graph embedding (KGE) based approaches implicitly consider all tuples, these may discard the existence of a tuple with a new constant as the embedding for the new constant does not exist. Thus, it is easier for SRL based approaches to handle highly sparse KBs as they can handle the high number of constants since they only learn first-order rules. \n\n\n2) I am not convinced by the potential application of the methods. After generating the probabilistic rules, how can I apply them? It will be appreciated if the authors can present some examples of the use of the probabilistic rules. If it is mainly to complete probabilistic KBs, how does this probabilistic logics based approach compare against embedding based approach?\n\nWe have answered this question in Appendix E. Please have a look.  '}		HkyI-5667	HJeYib5WEE	AKBC.ws/2019/Conference/-/Paper54/Official_Comment	['AKBC.ws/2019/Conference/Paper54/Reviewers/Unsubmitted']	2		['everyone']	HJxMMAfMGV	['AKBC.ws/2019/Conference/Paper54/Authors']	1549013409404		1549013409404	['AKBC.ws/2019/Conference/Paper54/Authors', 'AKBC.ws/2019/Conference']
6	1549011885311	"{'title': 'Clarifications addressed', 'comment': 'We would like to thank the reviewer for the comments and would like to clarify further.\n\n1) Section 5.1.1.1 : ""(ii) tuples contained in the answer of Q\' where Q\' is the same as Q but without the rule with an empty body, but not in the training set"" is unclear.\n\nWe have elaborated more on the 3 categories of target tuples in the revised paper. We hope it is clearer now.\n\n\n2) In the algorithm, line 22, aren\'t rules removed until H leads to a ""safe"" UCQ?\n\nWe have elaborated further on our algorithm in the paper. Checking for a safe UCQ is performed in Line 8 of QueryConstructor function. The only way to check for a safe query is by trying to construct a query plan, which is exactly what happens inside SlimShot. So if SlimShot fails to construct a query plan, the UCQ is considered to be unsafe.\n\n\n3) Section 6.1: ""In line 7 we formulate a UCQ Q from all the candidate rules in H (explained in 5.2 with an example)"". I was unable to find the example in section 5.2.\n\nThank you for pointing it out. It has now been rectified in the revision.\n\n\n4) It would be interesting to have an idea of the maximum scale that ProbFoil+ can handle since it seems to be the only competitor to the suggested method.\n\nWe have conducted a small experiment to demonstrate that ProbFOIL+ struggles to scale upto large KBs with a number of target tuples > 5000. On the other hand, such large KBs are handled reasonably faster by SafeLearner.  For instance, for a simple probabilistic KB with 20000 target tuples and 20000 non-target tuples, ProbFOIL+ took 15 hours and 54 minutes and SafeLearner took just 30 minutes. The detailed procedure and results of the experiment could be found in Appendix C.\n\n\n5) In section 7.2 does the ""learning time"" include the call to AMIE+?\n\nYes, the learning time includes both structure learning (including AMIE+) and parameter learning components.\n\n\n6) I found that it was sometimes hard to understand what was a part of the proposed system, and what was done in AMIE+ or Slimshot.\n\nOnly line 3 in Algorithm 1 uses AMIE+. On the other hand, SlimShot is used in lines 7 (QueryConstructor function) and 10 (ProbabilityPredictor function). Every other line of Algorithm 1 is part of SafeLearner.\n\n\n7) Explain the breaking down of queries into independent sub-queries in the algorithm.\n\nIf a query can be written as a union of independent sub-queries then its probability can be expressed as a unification of the probability of its sub-queries. We have explained this further in Appendix D.\n\n\n8) Caching is not mentioned anywhere in the algorithm.\n\nWe use caching/memoization before calling SlimShot in SafeLearner.  This is primarily done to speed up SafeLearner by storing the results of the expensive function call to SlimShot and returning the cached result when the structure of the query occurs again in the input. Since SlimShot produces a query plan, we exploit the fact that isomorphic queries naturally have the same query plans. We have explained this further in Appendix D.\n\n\n9) Some of the components which are sold as important for the speed-up in the conclusion aren\'t clear enough in the main text. Some numbers to experimentally back-up how important these additions are to the algorithms would be welcome.\n\nWe performed an experiment where we compared SafeLearner with and without the 2 speed-up techniques, memoization, and query disintegration. Our results on NELL (850th iteration) and YAGO show that memoization and query disintegration give an average speed-up of 50% and 7% respectively. It is important to understand that query disintegration would only give a speed-up when SafeLearner learns a high number of rules with a lot of rules being independent to one another. The detailed procedure and results of the experiment could be found in Appendix D.\n'}"		HkyI-5667	SkgHhsFZNE	AKBC.ws/2019/Conference/-/Paper54/Official_Comment	['AKBC.ws/2019/Conference/Paper54/Reviewers/Unsubmitted']	1		['everyone']	r1eeNpZbzV	['AKBC.ws/2019/Conference/Paper54/Authors']	1549011885311		1549011906890	['AKBC.ws/2019/Conference/Paper54/Authors', 'AKBC.ws/2019/Conference']
7	1542459718516	{'title': 'Scalable Rule Learning in Probabilistic Knowledge Bases', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper54/Authors'], 'keywords': [], 'TL;DR': 'Probabilistic Rule Learning system using Lifted Inference', 'abstract': 'Knowledge Bases (KBs) are becoming increasingly large, sparse and probabilistic. These KBs are typically used to perform query inferences and rule mining. But their efficacy is only as high as their completeness. Efficiently utilizing incomplete KBs remains a major challenge as the current KB completion techniques either do not take into account the inherent uncertainty associated with each KB tuple or do not scale to large KBs.\n\nProbabilistic rule learning not only considers the probability of every KB tuple but also tackles the problem of KB completion in an explainable way. For any given probabilistic KB, it learns probabilistic first-order rules from its relations to identify interesting patterns. But, the current probabilistic rule learning techniques perform grounding to do probabilistic inference for evaluation of candidate rules. It does not scale well to large KBs as the time complexity of inference using grounding is exponential over the size of the KB. In this paper, we present SafeLearner -- a scalable solution to probabilistic KB completion that performs probabilistic rule learning using lifted probabilistic inference -- as faster approach instead of grounding. \n\nWe compared SafeLearner to the state-of-the-art probabilistic rule learner ProbFOIL+ and to its deterministic contemporary AMIE+ on standard probabilistic KBs of NELL (Never-Ending Language Learner) and Yago. Our results demonstrate that SafeLearner scales as good as AMIE+ when learning simple rules and is also significantly faster than ProbFOIL+. ', 'pdf': '/pdf/11cb7fc5c86ee50e0ee55d3edb7c763ab4def9eb.pdf', 'archival status': 'Archival', 'subject areas': ['Machine Learning', 'Databases'], 'paperhash': 'anonymous|scalable_rule_learning_in_probabilistic_knowledge_bases', '_bibtex': '@inproceedings{    \nanonymous2019scalable,    \ntitle={Scalable Rule Learning in Probabilistic Knowledge Bases},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=HkyI-5667},    \nnote={under review}    \n}'}		HkyI-5667	HkyI-5667	AKBC.ws/2019/Conference/-/Blind_Submission	[]	54	rkev_rSTp7	['everyone']		['AKBC.ws/2019/Conference']	1542459718516		1549011474270	['AKBC.ws/2019/Conference']
8	1538087971209	{'title': 'Competitive experience replay', 'abstract': 'Deep learning has achieved remarkable successes in solving challenging reinforcement learning (RL) problems. However, it still often suffers from the need to engineer a reward function that not only reflects the task but is also carefully shaped. This limits the applicability of RL in the real world. It is therefore of great practical importance to develop algorithms which can learn from unshaped, sparse reward signals, e.g. a binary signal indicating successful task completion. We propose a novel method called competitive experience replay, which efficiently supplements a sparse reward by placing learning in the context of an exploration competition between a pair of agents. Our method complements the recently proposed hindsight experience replay (HER) by inducing an automatic exploratory curriculum. We evaluate our approach on the tasks of reaching various goal locations in an ant maze and manipulating objects with a robotic arm. Each task provides only binary rewards indicating whether or not the goal is completed. Our method asymmetrically augments these sparse rewards for a pair of agents each learning the same task, creating a competitive game designed to drive exploration. Extensive experiments demonstrate that this method leads to faster converge and improved task performance.', 'keywords': ['reinforcement learning', 'sparse reward', 'goal-based learning'], 'authorids': ['lhao499@gmail.com', 'atrott@salesforce.com', 'rsocher@salesforce.com', 'cxiong@salesforce.com'], 'authors': ['Hao Liu', 'Alexander Trott', 'Richard Socher', 'Caiming Xiong'], 'TL;DR': 'a novel method to learn with sparse reward using adversarial reward re-labeling', 'pdf': '/pdf/e731df56fa1904cfcf16ead38abb459990904385.pdf', 'paperhash': 'liu|competitive_experience_replay', '_bibtex': '@inproceedings{\nliu2018competitive,\ntitle={Competitive experience replay},\nauthor={Hao Liu and Alexander Trott and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Sklsm20ctX},\n}'}		Sklsm20ctX	Sklsm20ctX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1391	S1l_Fc65KQ	['everyone']		['ICLR.cc/2019/Conference']	1538087971209		1549010262887	['ICLR.cc/2019/Conference']
9	1549005112455	{'title': 'Summary of changes in the new version of the paper.', 'comment': 'We thank all the reviewers for their valuable comments and constructive suggestions. The main concerns highlighted from the reviewers are about the evaluation part, mainly about missing an empirical comparison with some recent relevant work. We have now updated the original paper with:\n(1) More related work and discussion about their relevance to the proposed method\n(2) An empirical comparison with the suggested relevant work in all the evaluation tasks\n(3) A new evaluation task (section 4.4) on the graded lexical entailment.'}		S1xf-W5paX	H1leBb_WEV	AKBC.ws/2019/Conference/-/Paper26/Official_Comment	['AKBC.ws/2019/Conference/Paper26/Reviewers/Unsubmitted']	5		['everyone']	S1xf-W5paX	['AKBC.ws/2019/Conference/Paper26/Authors']	1549005112455		1549005112455	['AKBC.ws/2019/Conference/Paper26/Authors', 'AKBC.ws/2019/Conference']
10	1548986878652	"{'title': 'Response to reviewer 1', 'comment': 'Thank you for taking time to review the paper and providing positive feedback.\nPlease find the clarification of each of the concerns below.\n\n* Clarification 1 (what is the input to the system? and What is the input to the classifier?): \n\nThe framework (Figure 2) has two stages: (i) relevant sentence retrieval, and (ii) classification. \n\nThe input to the system is a set of triple. Each triple could also contain zero or more supporting provenance sentences/pieces of text. If provenance is provided we consider the provenance provided by the IE system and do not attempt to find additional provenance (only stage 2 is executed).\n\nIf provenance is missing, then provenance is retrieved from the text corpus, i.e., both stages are executed. \n\nWe convert either the retrieved sentences (or the provided provenance) and the input triple into a feature vector for the classifier. We encode the sentences by taking the average of the word embeddings, and then concatenate that average embedding with binary features (see Section 4.1 last paragraph) and the result of the outer product of one-hot relation vector and the EncodedSentence vector.\n\nThe input ""EncodedSentence"" in Figure 4 represents the feature vector provided to the classifier for all retrieved sentences. (We have renamed ""Encoded Sentence"" to ""Encoded Input"" in resubmitted manuscript)\n\nWe have added the above clarification  to the manuscript to highlight these details; see Page 2, second last paragraph.\n\n* Clarification 2 (Details about the Gumbel-Softmax):\n\nWe have added the following clarification to the revised paper; see Section 4.2 KG-Cleaner with re-parameterization.\n\nWe use the Gumbel-Softmax parametrization to allow the neural network to make a discrete prediction and use that prediction itself (not just the probability of that predicted value) in future computation in the same network.\n\nAs shown by Maddison et.al. 2017, the Gumbel-Softmax helps to reduce gradient variance; this has been shown to allow to better generalization. \n\nReference: Chris J. Maddison, Andriy Mnih, & Yee Whye Teh. “The Concrete Distribution: A Continuous relaxation of discrete random Variables”. International Conference on Learning Representations (ICLR) 2017.\n\n* Clarification 3 (manual annotation and feature extraction)\n\nAs mentioned in Section 2 First paragraph we followed Ellis(2015a,b). We followed the definition provided on Page 4 (for Correct, Incorrect relation, Subject missing, Misc, and Object missing).\n\nDuring annotation we sequence the error categories as follows: “Subject Missing”, “Object Missing”, “Incorrect relation”, “Misc”, and “Correct”. We check for an error category on a rolling basis: if the fact is found to contain an error in, e.g., “Object Missing” we don’t consider it for categories later in the sequence.\n\n* Feature extraction: We followed the procedure as described in Section 4.1 . \n\n* Baseline comparison: Yes, competing systems were given the same information as ours for fair comparison. \n\n* Changes in the revised manuscript\nBeside clarifying the input to the system and the classifier in Introduction Page 2 second last paragraph, we have also updated Figure 4 and better contextualized the our classifier models in Section 4.1 and 4.2 .\n\nWe have added all the details about Gumbel-Softmax and how and why its used in our problem in Section 4.2 . We added clarification on manual annotation in Section 2.\n'}"		ryleB-56pQ	SkxvWcQZV4	AKBC.ws/2019/Conference/-/Paper49/Official_Comment	['AKBC.ws/2019/Conference/Paper49/Reviewers/Unsubmitted']	2		['everyone']	BJghiSFyGE	['AKBC.ws/2019/Conference/Paper49/Authors']	1548986878652		1549000975265	['AKBC.ws/2019/Conference/Paper49/Authors', 'AKBC.ws/2019/Conference']
11	1548987562617	"{'title': 'Response to reviewer 2  (2 of 2) ', 'comment': '* Clarification 6 (Explanation of why Gumbel-Softmax works so well):\n\nThe Gumbel-softmax reparametrization has been shown to lead to a reduction in gradient variance [Maddison et al., 2017]. We believe this is one reason for the improved performance. (Section 4.2 in updated manuscript) .\n\nReference: Chris J. Maddison, Andriy Mnih, & Yee Whye Teh. “The Concrete Distribution: A Continuous relaxation of discrete random Variables”. International Conference on Learning Representations (ICLR) 2017.\n\n* Clarification 7 (Retrieval statistics):\n\nWe found that on an average 7.18 sentences are retrieved per training fact and 7.88 sentences are retrieved per test fact. Considering Figure 5, our approaches work well with fewer sentences are available and as highlighted by the reviewer the performance drops with more number of sentences. This is likely due to additional noise while considering large number of sentences. \n\n* Clarification 8 (model with Outer product under “minor things”):\n\nAs mentioned above (in Clarification 5b) the network with ""outer product"" makes the neural network aware of the extracted relation and achieves better performance compared to other models.\n\n* Changes in the latest manuscript\nWe have corrected typos and removed the duplicate paragraph. We have updated the latest manuscript with\n\nClarification feature extraction process (binary features, average of word embedding, and combining retrieved sentences) in Section 4.1\nExplanation on effectiveness of Gumbel-Softmax works so well Section 4.2\nClarification on our closest and strong baselines (Section 5 first two paragraphs and Section 5.2 baselines)\nClarification on choice of negative sampling (Section 4.3) \nClarification on “outer product” (Section 5.2 Baseline last bullet point) '}"		ryleB-56pQ	rJlQ32QbE4	AKBC.ws/2019/Conference/-/Paper49/Official_Comment	['AKBC.ws/2019/Conference/Paper49/Reviewers/Unsubmitted']	4		['everyone']	SJgldh7WNE	['AKBC.ws/2019/Conference/Paper49/Authors']	1548987562617		1549000905544	['AKBC.ws/2019/Conference/Paper49/Authors', 'AKBC.ws/2019/Conference']
12	1548987768600	"{'title': 'Response to reviewer 3', 'comment': ""Thank you for reviewing the paper and providing valuable insights. We are happy that you found our approach valuable and enjoyed reading our work and found our modeling choices and evaluation metric to be sound. While we were running low in the number of pages available, we attempted to fit in the most important details. \n\nWe answer each of the questions below.\n\n* Clarification 1 (Explanation of why the approach works so well):\n\nThe Gumbel-softmax reparametrization has been shown to lead to a reduction in gradient variance [Maddison et al., 2017]. We believe this is one reason for the improved performance.\n\nReference: Chris J. Maddison, Andriy Mnih, & Yee Whye Teh. “The Concrete Distribution: A Continuous relaxation of discrete random Variables”. International Conference on Learning Representations (ICLR) 2017.\n\n* Clarification 2 (positioning the model w.r.t the problem):\n\nFrom Tables 3 and 4, comparing the last row (with 'Shared Gumbel noise' and 'Outer product') with other models, we found making the neural model aware of the extraction relation and contextualizing with the retrieved sentence or provided provenance helps improve performance. Moreover, using the Gumbel-Softmax reparameterization helps to reduce variance in the gradient helping to better generalize across multiple dataset. \n\n* Clarification 3 (adding few details to describe our novel models architecture):\n\nWe have added a more details in Section 4.2 along with the reparameterization trick and equations of Gumbel-Softmax distribution. \n\n* Clarification 4 (why average word embeddings?):\n\nAs pointed out correctly, average word embeddings does not help to generalize better across datasets (row 6 Table 3, and row 4 Table 4). However, we found average word embedding along with the model aware of the extraction relation with the outer-product of one-hot relation vector with average word embedding provided better context to classify and yielded reasonable results to focus on the core tasks of our effort---joint credibility and fact repair adjudication. As stated in Section 5.4 last paragraph we also tried using CNN and LSTM for sentence representation and found it to perform poorly with same training time and higher computation cost. \n\n* Changes in the latest manuscript: We have made changes in introduction section to better frame the model, updated Section 4.1 to discuss why we used average of word embedding, Section 4.2 and Figure 4 to justify why Gumbel-Softmax performs well and better contextualized proposed models. ""}"		ryleB-56pQ	BkebYTXZV4	AKBC.ws/2019/Conference/-/Paper49/Official_Comment	['AKBC.ws/2019/Conference/Paper49/Reviewers/Unsubmitted']	5		['everyone']	Bkgi6WVSzE	['AKBC.ws/2019/Conference/Paper49/Authors']	1548987768600		1549000716230	['AKBC.ws/2019/Conference/Paper49/Authors', 'AKBC.ws/2019/Conference']
13	1548986462705	{'title': 'Summary of changes in resubmitted manuscript.', 'comment': 'We thank the reviewers for their time and detailed, insightful feedback. We have updated our paper accordingly as follows:\n\n* Better framing of the model: We have significantly improved the writing in the introduction to better frame our model and highlight the contributions. We have updated Figure 4 to provide more clarity on the input of the classifier. We have changed “Encoded Sentence” to “Encoded Input” in Figure 4. \n\n* Annotation Procedure: We have updated the writing to include the criteria we used to manually annotate a fact into one of the error category (Section 2).\n\n* Better model description and justification of why Gumbel based models performed better: We better describe each of our model. We have added equations to describe the Gumbel-Softmax layer and provide justification of why and how we used it. We also described why adding Gumbel-Softmax layer helps improve the performance of the system and provide a more expressive model (Section 4.2).\n\n* Comment on other negative sampling: We have added a comment based on our observation on other kinds of negative sampling (Section 4.3).\n\n* Clarification on feature extraction (Section 4.1).\n\n* Justification on choice of baseline: We clarified our choice of using well-studied graph based embedding models as our baseline (Section 5 first two paragraph, and 5.2).\n\n* Fixed typos.\n\nThank you reviewing. We’ve included individual responses and detailed clarifications to each of your questions.\n'}		ryleB-56pQ	ryePP_m-E4	AKBC.ws/2019/Conference/-/Paper49/Official_Comment	['AKBC.ws/2019/Conference/Paper49/Reviewers/Unsubmitted']	1		['everyone']	ryleB-56pQ	['AKBC.ws/2019/Conference/Paper49/Authors']	1548986462705		1549000599772	['AKBC.ws/2019/Conference/Paper49/Authors', 'AKBC.ws/2019/Conference']
14	1548996164688	{'title': 'Citation is already present', 'comment': 'Citation to this paper was added in our final submission.'}		ryf7ioRqFX	BkgprCHZEV	ICLR.cc/2019/Conference/-/Paper609/Official_Comment	['ICLR.cc/2019/Conference/Paper609/Reviewers/Unsubmitted']	11		['everyone']	BygPNUrZ4V	['ICLR.cc/2019/Conference/Paper609/Authors']	1548996164688		1548996164688	['ICLR.cc/2019/Conference/Paper609/Authors', 'ICLR.cc/2019/Conference']
15	1538087888410	{'title': 'LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING', 'abstract': 'The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ', 'keywords': ['few-shot learning', 'meta-learning', 'label propagation', 'manifold learning'], 'authorids': ['csyanbin@gmail.com', 'juho.lee@stats.ox.ac.uk', 'mike_seop@aitrics.com', 'shkim@aitrics.com', 'eunhoy@kaist.ac.kr', 'sjhwang82@kaist.ac.kr', 'yi.yang@uts.edu.au'], 'authors': ['Yanbin Liu', 'Juho Lee', 'Minseop Park', 'Saehoon Kim', 'Eunho Yang', 'Sung Ju Hwang', 'Yi Yang'], 'TL;DR': 'We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.', 'pdf': '/pdf/1dd32c1e72af34575dbe33dabd0ef648fd7cf1af.pdf', 'paperhash': 'liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning', '_bibtex': '@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}'}		SyVuRiC5K7	SyVuRiC5K7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	911	r1gNKqKqK7	['everyone']		['ICLR.cc/2019/Conference']	1538087888410		1548994579684	['ICLR.cc/2019/Conference']
16	1548994095129	{'comment': 'In addition to the uRNN series of works, the recent IndRNN (Independently recurrent neural network) also addresses the gradient exploding and vanishing problems. Experiments have also shown its effective performance in solving problems concerning long-term dependency (even up to 5000 timesteps). Also it shows a great advantage in constructing deep RNN networks (easily over 20 layers).\n[1] S. Li, W. Li, C. Cook, C. Zhu, Y. Gao, “Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN,” IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, Jun. 18-22, 2018.\n\n\nThanks.', 'title': 'related work on solving gradient exploding and vanishing problems'}		ryf7ioRqFX	BygPNUrZ4V	ICLR.cc/2019/Conference/-/Paper609/Public_Comment	[]	3		['everyone']	B1xmq2H5bV	['~Shuai_Li5']	1548994095129		1548994095129	['~Shuai_Li5', 'ICLR.cc/2019/Conference']
17	1542459715488	"{'title': 'Capturing Food Knowledge with Semantics and Embeddings', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper53/Authors'], 'keywords': ['data acquisiton', 'semantic extract transform load', 'concept similarity', 'entity resolution', 'relationship discovery', 'nutrition'], 'abstract': ""Poor quality eating patterns contribute significantly to the incidence of preventable chronic diseases. The proliferation of recipes and other food information sources on the Web presents an opportunity for discovering and organizing diet related knowledge into a knowledge graph, which can in turn be used to generate food recommendations tailored to an individual's dietary habits and preferences. In this paper, we present our work on building a food knowledge graph using semantics oriented knowledge ingestion. We further augment the knowledge graph by incorporating inferences we have made on the similarity between food ingredients. These similarities are derived from embeddings generated from online recipe data. In the true spirit of linked data, we have linked to many of the existing concepts in other related ontologies, as well as community maintained resources such as DBpedia. The resulting knowledge graph is capable of answering questions related to the composition of dishes, nutritional content of food items and potential substitutions."", 'pdf': '/pdf/f11673c2480aacb87f75800f26881c72c5450da5.pdf', 'archival status': 'Archival', 'subject areas': ['Natural Language Processing', 'Information Extraction', 'Information Integration', 'Knowledge Representation', 'Semantic Web'], 'paperhash': 'anonymous|capturing_food_knowledge_with_semantics_and_embeddings', 'TL;DR': 'Knowledge graph for food recommendation', '_bibtex': '@inproceedings{    \nanonymous2019capturing,    \ntitle={Capturing Food Knowledge with Semantics and Embeddings},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=B1gjSZ9TpQ},    \nnote={under review}    \n}'}"		B1gjSZ9TpQ	B1gjSZ9TpQ	AKBC.ws/2019/Conference/-/Blind_Submission	[]	53	ryenFHBppQ	['everyone']		['AKBC.ws/2019/Conference']	1542459715488		1548992231133	['AKBC.ws/2019/Conference']
18	1548989427041	{'title': 'Thank you for the feedback', 'comment': 'Thank you for the review and constructive feedback. We address the raised concerns below:\n\n(1) Thank you for the comment. In the updated submission we revised the related work section to better represent existing work on coherence, including the list of works mentioned above. \n\n(2) We agree that macro-F1 could give valuable insights into the performance of the normalization algorithms on rarely seen entities. The reason we reported micro-F1 numbers was mainly to keep in line with the recent publications in the area on the particular datasets that focus on micro-F1 performance. However, we still evaluated the macro-F1 performance of our model on NCBI and found out that it outperformed DNorm with 0.856/0.823/0.833 P/R/F1 compared to 0.828/0.809/0.819 P/R/F1 for DNorm (TaggerOne does not report macro-F1). We added a brief discussion of this to Section 6.3.1. '}		BJerQWcp6Q	rJeogNVb44	AKBC.ws/2019/Conference/-/Paper39/Official_Comment	['AKBC.ws/2019/Conference/Paper39/Reviewers/Unsubmitted']	3		['everyone']	H1g1fkDsWE	['AKBC.ws/2019/Conference/Paper39/Authors']	1548989427041		1548989427041	['AKBC.ws/2019/Conference/Paper39/Authors', 'AKBC.ws/2019/Conference']
19	1548989069897	{'title': 'Thank you for the feedback', 'comment': 'Thank you for the review and insightful questions. We address them below:\n\n(1) In general, based on anecdotal evidence it seems that the relative predictive performance of GRU and LSTM cells depends on the particular task at hand. In our case, we selected GRU cells based on experiments we performed with both LSTM and GRU cells during the model design, which showed that GRU cells led to better results. Another potential benefit of this choice is increased training performance, as GRU cells are less complex and less computation-intensive than LSTM cells. We revised Section 4.3 to explain the reasoning behind our choice of GRU cells. \n\n(2) We are referring to the complexity of modeling and performing inference from the joint probability of the entire set of tags, which is an NP-hard problem. To avoid the exponential blowup, existing techniques employ different types of approximation algorithms (e.g., Ganea and Hoffman (2017) present an N^2 approximation algorithm using a fully-connected pairwise conditional random field, which requires loopy belief propagation to train). Our proposal is to model the problem as a tag sequence using a recurrent net to avoid combinatorial explosion, though other solutions could also be proposed to reduce the complexity (i.e. model it as a tag sequence and use a conditional random field or a hidden Markov model). We cleaned up the language surrounding this point both in the abstract and in Section 4.3. \n\n(3) We intend to make the code of the best models for each dataset available upon acceptance and will be providing a link to it in the paper. \n\n(4) We attempted to obtain significance results during the author feedback period by performing 10-fold cross-validation on the NCBI disease corpus both for the best NormCo model and the best baseline model (which is TaggerOne). While we were able to obtain the evaluation metrics for NormCo, we ran into several issues while retraining TaggerOne on new splits, including (a) TaggerOne’s code breaking (i.e., throwing null pointer exceptions) and (b) TaggerOne’s internal F-score evaluation failing for concepts that have multiple labels (such as “inherited neuromuscular disease”, which is mapped to both MESH:D009468 “Neuromuscular disease” and MESH:D030342 “Genetic diseases, inborn”), which were not present in the original test set. Ultimately these issues, coupled with TaggerOne’s long training times documented in Section 6.4.5, did not allow us to obtain significance results for TaggerOne. However, we were able to perform cross-validation of the best NormCo model (i.e., MC-synthetic), which resulted in an average accuracy of 0.853 with a low standard deviation of 0.013. '}		BJerQWcp6Q	BJeUcfVZ4N	AKBC.ws/2019/Conference/-/Paper39/Official_Comment	['AKBC.ws/2019/Conference/Paper39/Reviewers/Unsubmitted']	1		['everyone']	Skg8RfIzfN	['AKBC.ws/2019/Conference/Paper39/Authors']	1548989069897		1548989273158	['AKBC.ws/2019/Conference/Paper39/Authors', 'AKBC.ws/2019/Conference']
20	1548989234006	{'title': 'Thank you for the feedback', 'comment': 'Thank you for the review! We address some of the issues raised below:\n\n(1) The reason that results on BC5 are mixed is that our model is more conservative, favoring high precision over recall (see Table 3). Since the BC5CDR dataset has a greater diversity of concepts that the NCBI dataset (1082 concepts in BC5CDR compared to 753 in NCBI), the lower recall becomes more important, leading to a slightly lower accuracy than the baseline models. However, note that even in this case the NormCo model still outperforms the baselines on the average LCA distance performance metric, which, as explained in the paper, takes into account not only the overall accuracy but also the severity of the errors. We added an explanation of this to Section 6.4.1. \n\n(2) The AwA models were applied only at test time and the models were not re-trained. We have added language to make this clearer in Section 6.3.1. The purpose of this experiment was to observe how abbreviation resolution affects the performance of the trained models. '}		BJerQWcp6Q	BklcN7Nb4E	AKBC.ws/2019/Conference/-/Paper39/Official_Comment	['AKBC.ws/2019/Conference/Paper39/Reviewers/Unsubmitted']	2		['everyone']	BJlXzXCyfN	['AKBC.ws/2019/Conference/Paper39/Authors']	1548989234006		1548989234006	['AKBC.ws/2019/Conference/Paper39/Authors', 'AKBC.ws/2019/Conference']
21	1542459676751	{'title': 'NormCo: Deep Disease Normalization for Biomedical Knowledge Base Construction', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper39/Authors'], 'keywords': ['Entity Normalization', 'Biomedical Knowledge Base Construction'], 'TL;DR': 'We present NormCo, a deep coherence model which considers the semantics of an entity mention, as well as the topical coherence of the mentions within a single document to perform disease entity normalization.', 'abstract': 'Biomedical knowledge bases are crucial in modern data-driven biomedical sciences, but auto-mated biomedical knowledge base construction remains challenging. In this paper, we consider the problem of disease entity normalization, an essential task in constructing a biomedical knowledge base.  We present NormCo, a deep coherence model which considers the semantics of an entity mention, as well as the topical coherence of the mentions within a single document. NormCo mod-els entity mentions using a simple semantic model which composes phrase representations from word embeddings, and treats coherence as a disease concept co-mention sequence using an RNN rather than modeling the joint probability of all concepts in a document, which requires NP-hard inference.  To overcome the issue of data sparsity, we used distantly supervised data and synthetic data generated from priors derived from the BioASQ dataset.  Our experimental results show thatNormCo outperforms state-of-the-art baseline methods on two disease normalization corpora in terms of (1) prediction quality and (2) efficiency, and is at least as performant in terms of accuracy and F1 score on tagged documents.', 'pdf': '/pdf/2954e09ccbbfbda453f4508cc72fd05efabe5a6d.pdf', 'archival status': 'Archival', 'subject areas': ['Machine Learning', 'Natural Language Processing'], 'paperhash': 'anonymous|normco_deep_disease_normalization_for_biomedical_knowledge_base_construction', '_bibtex': '@inproceedings{    \nanonymous2019normco:,    \ntitle={NormCo: Deep Disease Normalization for Biomedical Knowledge Base Construction},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=BJerQWcp6Q},    \nnote={under review}    \n}'}		BJerQWcp6Q	BJerQWcp6Q	AKBC.ws/2019/Conference/-/Blind_Submission	[]	39	BJlrm0N66X	['everyone']		['AKBC.ws/2019/Conference']	1542459676751		1548988927797	['AKBC.ws/2019/Conference']
22	1544743760580	{'title': 'The system is a combination of many existing techniques, and is outperformed by several works.', 'review': 'This paper introduces an end-to-end system to answer science exam questions for the ARC challenge. The system is a combination of several existing techniques, including (i) query rewriting based on seq2seq or NCRF++, (ii) answer retriever, (iii) entailment model based on match-LSTM, and (iv) knowledge graph embeddings. The description of the system is clear, and there is abundant ablation study. However, I have following concerns about this paper:\n\n1. There seems to be no new techniques proposed in the system. Hence, the novelty of this work is questioned.\n2. I do not understand why the authors use TransH, which is a KG embedding model that differentiates one entity into different relation-specific representations.\n3. The system is significantly outperformed by Sun et al. 2018 and Ni et al. 2018.', 'rating': '6: Marginally above acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		HJxYZ-5paX	HJlYIoPlgV	AKBC.ws/2019/Conference/-/Paper29/Official_Review	['AKBC.ws/2019/Conference/Paper29/Reviewers/Unsubmitted']	1		['everyone']	HJxYZ-5paX	['AKBC.ws/2019/Conference/Paper29/AnonReviewer1']	1544743760580		1548988556070	['AKBC.ws/2019/Conference']
23	1548987620118	{'title': 'Comparisons to the mentioned related work', 'comment': 'Thank you very much for the positive overall remarks, and for pointing us to\nmore related work. Please find below our responses to the two remaining\nconcerns.\n \n> The proposed method is not compared with better existing works, such as RGCN,\n> ConvE and SimplE.\n\nWe present comparisons of our experimental results to the strongest baselines\nthat we could find in the literature. In particular, the “ComplEx” baseline to\nwhich we compare our results outperforms results reported for all “RGCN”,\n“ConvE”, and “SimplE”. Our model is competitive with this strongest baseline and\noutperforms it in many cases.\n\nThere has been an explosion of proposals for competing knowledge graph embedding\nmodels in the recent literature. Our method is orthogonal to those proposals\nsince we focus instead on the problem of hyperparameter tuning. By applying our\nmethod to two different models we demonstrate that our method is agnostic to the\ndetails of the model, as long as the model admits a probabilistic\ninterpretation.\n\n\n> The authors might also consider discussing the differences of their work and\n> [1] in the related work section.\n> [1] Chen et al. Embedding Uncertain Knowledge Graphs. AAAI-19.\n\nThank you for pointing us to this paper, we will mention it in the Related Works\nsection. Chen et al. discuss a different kind of uncertainty than we: they\naddress uncertainty in the training data. For example, they consider a knowledge\nbase where each data point (fact) is labeled with a confidence score. By\ncontrast, our approach addresses uncertainty in the latent space. Even if the\ndata set contains only certain facts, conclusions that we draw from these facts\nstill come with some uncertainty because the rules that we infer from patterns\nin the data are probabilistic.'}		rylPm-5a67	Skg21pQZNE	AKBC.ws/2019/Conference/-/Paper40/Official_Comment	['AKBC.ws/2019/Conference/Paper40/Reviewers/Unsubmitted']	4		['everyone']	BklM0iqh1E	['AKBC.ws/2019/Conference/Paper40/Authors']	1548987620118		1548987620118	['AKBC.ws/2019/Conference/Paper40/Authors', 'AKBC.ws/2019/Conference']
24	1548987495743	"{'title': 'Response to reviewer 2  (1 of 2) ', 'comment': 'Thank you for taking time and providing insightful reviews. Please find the clarification of each of the concerns below.\n\n* Clarification 1 (strong baselines mentioned under Pros):\n\nTo best of our knowledge we are first to propose a standalone framework to jointly determine credibility and propose repair. As noted in ""Related Work"" other systems determine credibility but cannot be applied to our problem since (i) they depend on search engine results which are opaque to internal processing), and (ii) they use ensemble of output from multiple IE systems on same text corpus. The closest to our approach which are also standalone were DistMult and ComplEx which are inferring on graph structure (treating an IE system as a black box) as described in Section 5.2.\n\n* Clarification 2 (Retrieving sentences and passing it to IE system mentioned under Pros):\n\nWe treat the internal workings of an IE system as a black-box. Our system can be used as a post diagnosis approach to tackle errors from an IE system. This is different from running an IE system on retrieved sentences. \n\n* Clarification 2a (independent performance same as joint performance):\n\nAs noted by Kaiser et.al 2017 when the dataset is large the performance difference between training a model independently or jointly is small. Our results support this observation: our WikiData dataset is large, which we believe to be the reason for the close performance between our independently trained and jointly trained models.\n\nReference: Lukasz Kaiser, Aidan N. Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, Jakob Uszkoreit. “One Model To Learn Them All”. arxiv.org/abs/1706.05137 (2017)\n\nWe have added the clarification in last paragraph of Section 5.3.\n\n* Clarification 3 (approach to combine (or represent) retrieved sentences):\n\nWe combined all the retrieved sentenced and agree that averaging word embeddings is a straightforward approach for getting a sentence embedding; nevertheless, we found it yielded reasonable results and focused on the core tasks of our effort---joint credibility and fact repair adjudication. As stated in Section 5.4 last paragraph we also tried using a CNN and an LSTM for sentence representation and found it to perform poorly with same training time and higher computation cost.\n\n* Clarification 4 (negative sampling):\n\nWe tried using other negative sampling strategies, where we changed all combinations of predicate, object and provenance but found each of the techniques to perform on par with, or marginally lower than, our final sampling strategy. We found that the negative sampling described in Section 4.3 to perform best (with F1 score between 87 to 89) on a balanced dataset containing 50% positive and 50% negative test instances.  We use both F1-Macro and F1-Micro, which internally uses Precision, and Recall, to avoid considering performance gain only on negative classes. Hence Negative Sampling helps in learning without inflating score of negative examples. Moreover, we also consider using MRR to understand ranking of the repair relations.\n\n* Clarification 5a (combining binary features):\n\nBinary features are extracted from each of the retrieved sentences and combined using bitwise OR-ing. We found that concatenating all retrieved sentences with average of word embedding gave reasonable results. \n\n* Clarification 5b (explicit input of extracted relation):\n\nWe study the effect of making the neural network model aware of the extracted relation in the ablation study of ""OuterProduct,"" where we compute the outer product of the relation one-hot vector with concatenated average of the sentence word embedding and binary features. The effect of making the model implicitly aware of the extracted relation can be seen by comparing the “outer product” rows with the rest in Tables 3 and 4. Performance gain is high across multiple datasets when “OuterProduct” is used compared to the network which is not aware of the extracted relation like in KG-Cleaner Basic.\n\n'}"		ryleB-56pQ	SJgldh7WNE	AKBC.ws/2019/Conference/-/Paper49/Official_Comment	['AKBC.ws/2019/Conference/Paper49/Reviewers/Unsubmitted']	3		['everyone']	ryxmHcy4GV	['AKBC.ws/2019/Conference/Paper49/Authors']	1548987495743		1548987526333	['AKBC.ws/2019/Conference/Paper49/Authors', 'AKBC.ws/2019/Conference']
25	1548987199238	{'title': 'Empirical results and clarification of the role of uncertainty', 'comment': 'Thank you very much for the kind overall assessment of our paper, and for the\ninsightful questions. Please find responses to each of your comments below.\n\n\n> The main downside are the empirical results. Or to be more precise, the\n> presentation of the empirical results.\n\nWhile the comparison is arguably close for the ComplEx model, improvements on\nthe DistMult model are much more pronounced. Also, please note that our proposed\nhyperparameter optimization method is much more efficient than the traditional\ngrid search approach. We obtained our results from a single run of our method,\nwhereas the baseline relied on the extensive hyperparameter search reported in\n[Lacroix et al. (2018)]. We hope to be able to inspire future knowledge base\nembedding approaches by providing a fast way to tune hyperparameters in new\nmodels.\n\n\n> Empirical results on the running time should be added. The benefit of avoiding\n> to spend resources on hyperparameter tuning is not illustrated.\n\nThank you for this suggestion! We will add a paragraph that compares the runtime\nof our method to an estimate for the runtime of the baseline method.\nUnfortunately, the difference in computational cost is so large that we can only\nprovide theoretical estimates here. We internally estimated the computing\nresources that would be required to reproduce the hyperparameter search leading\nto our main baseline [Lacroix et al. (2018)] (Facebook). We concluded that\nrepeating the baseline hyperparameter search would by far exceed our hardware\nresources, which is why we ended up using the hyperparameters reported in the\npaper for our baseline. By contrast, our proposed method is fast enough to be\nrun routinely when debugging a new model implementation.\n\n\n> the paper should discuss in more details the parameter uncertainty argument\n> [...] While [...] the Bayesian treatment of the embeddings is required to\n> guarantee convergence, the conclusions also argue that variational inference\n> many not be the right thing to do for high-dimensional embeddings. This leaves\n> the reader somewhat alone. Should I use the approach or not?\n\nWe think there is a misunderstanding about our comment in the conclusions. We\nwill clarify that the comment on the role of dimensionality does not question\nthe proposed approach. Instead, the comment in the conclusions discusses an\nalternative approach that would go beyond the proposed method. In this\nalternative approach, one would take uncertainty into account not only to tune\nthe hyperparameters, but also to do link prediction (by sampling from the\napproximate posterior).\n\nOur empirical observation is that this alternative approach is even better than\nthe proposed method if the embedding dimension is small, but that in high\nembedding dimensions, the proposed method works better. In either case (both low\nand high embedding dimensions), the proposed method works better than the\nbaseline method, which does not take uncertainty into account at all.'}		rylPm-5a67	ryxwromb4E	AKBC.ws/2019/Conference/-/Paper40/Official_Comment	['AKBC.ws/2019/Conference/Paper40/Reviewers/Unsubmitted']	3		['everyone']	BJewUs0-fV	['AKBC.ws/2019/Conference/Paper40/Authors']	1548987199238		1548987199238	['AKBC.ws/2019/Conference/Paper40/Authors', 'AKBC.ws/2019/Conference']
26	1548986836371	{'title': 'Details of the optimization and the role of uncertainty', 'comment': 'Thank you again for the feedback. We addressed the questions about related\nwork and variational EM in our first answer below. This answer addresses the\nquestions about details of the optimization and about the role of uncertainty\nin our method.\n\n> It is not clear how the hyperparameters are set in the first MAP iteration.\n> [...] How sensitive the model was to the initialization?\n\nOur approach amounts to different training and initialization procedures. The\nfirst training phase (the “pre-training”) is used to find good initializations\nfor the embedding vectors of the second phase (the variational EM). For this\npre-training, we ran experiments both with uniform initial $\\lambda$ and with\ninitial $\\lambda$ proportional to the frequency. We obtained indistinguishable\nperformance at the end, meaning that initialization here is not important. This\nis plausible and just shows that different initializations of the pre-training\nphase result in similar embeddings vectors.\n\n\n> Given that Bayesian modelling is not used, I do not understand why the authors\n> claim that they “keep track of parameter uncertainty”. In the end their\n> approach is not Bayesian.\n\nWe will clarify the role of uncertainty in the proposed hyperparameter\noptimization strategy. The final prediction does indeed not take uncertainty\ninto account, as we discuss in the conclusions. The proposed hyperparameter\noptimization algorithm, however, only works because of a Bayesian treatment of\nthe latent embedding vectors. As we mention in the last paragraph on page 6, a\ngradient based hyperparameter optimization that ignores posterior uncertainty\nwould lead to divergent solutions. Specifically, minimizing the loss function\n$L$ simultaneously over model parameters and hyperparameters would send the\nhyperparameters to infinity. This minimizes the loss if the model parameters\n(=embedding vectors) are strictly zero, which is possible in a point estimated\nmodel.\n\nTaking posterior uncertainty into account solves the issue of divergent\nsolutions. Since we attribute a nonzero uncertainty to every embedding vector,\nembedding vectors can no longer be deterministically zero, and the divergent\nsolutions are no longer possible. Technically, the optimization cannot diverge\nbecause the ELBO is bounded, see discussion in the last paragraph on page 7.\n\n\n> What are termination conditions? How is T chosen? Has the method actually\n> converged?\n\nWe used a standard stopping condition in optimization, i.e., we terminate the\noptimization when the hyperparameters do not change much.'}		rylPm-5a67	r1xn0K7-VN	AKBC.ws/2019/Conference/-/Paper40/Official_Comment	['AKBC.ws/2019/Conference/Paper40/Reviewers/Unsubmitted']	2		['everyone']	Byl-M0GfzV	['AKBC.ws/2019/Conference/Paper40/Authors']	1548986836371		1548986836371	['AKBC.ws/2019/Conference/Paper40/Authors', 'AKBC.ws/2019/Conference']
27	1548986641731	{'title': 'Detailed response on related work and variational EM', 'comment': 'Thank you very much for the detailed comments! We address the the questions\nabout related work and variational EM below, and we address the remaining\nquestions in a separate response.\n\n> The general idea is not as new and the discussion of related work does not\n> seem to be quite adequate.\n> The authors should contrast their approach with previous work [...]\n\nThank you for pointing us to these related works. We will mention them in the\nRelated Works section of our paper, and we will highlight the novelty of our\npaper as follows:\n\nFirst, rather than proposing a model following traditions in the probabilistic\nmodels literature, we propose a Bayesian version of the best performing\nknowledge graph embedding model that we could find in the literature [Lacroix et\nal. (2018)]. This is how we were able to beat the state of the art on standard\nbenchmark tasks: we build on the experience that went into the design of the\nembedding model, and we add to it the ability to do fast model selection using\nvariational EM.\n\nSecond, we focus specifically on the task of hyperparameter optimization. As\npointed out in [Kadlec et al. (2017)], the predictive performance of knowledge\ngraph embedding models is very sensitive to the choice of hyperparameters\n(mainly the strength of the regularizer). Unfortunately, hyperparameter tuning\nwith traditional grid search becomes very expensive due to the large number of\nhyperparameters. We point out that the observed sensitivity to the strength of\nthe regularizer can be explained by the fact that knowledge graphs operate in\nthe low-data regime (see Figure 1). We therefore propose to use a Bayesian\nmethod for hyperparameter tuning.\n\nTo the best of our knowledge, our paper is the first publication that proposes\nBayesian methods for hyperparameter tuning in knowledge graph embedding models,\nand that reaches state of the art on standard benchmarks while dramatically\nreducing the computational cost for hyperparameter tuning.\n\n\n> The paper and the approach is in many respects counter-intuitive: E.g., an\n> embedding for each entity is drawn from its own prior [...]\n\nWe think that there may be a misunderstanding that we will resolve with a more\nexplicit notation for the model and hyperparameters. We will make it more\nexplicit that the prior has only a single scalar tunable parameter per entity\nand relation, while the approximate posterior has 8,000 free parameters per\nentity and relation. This is because the prior is not only centered around zero\nas the reviewer correctly points out. Importantly, the prior is also isotropic\nin the $K=2,000$ dimensional embedding space. By contrast, the approximate\nposterior has four tunable parameters *for each dimension* (real and imaginary\npart of both mean and standard deviation).\n\n\n> Using EM with single-sample-per-paramterized-prior is an unorthodox choice [...] \n\nWe would like to clarify that we fit a prior for each embedding vector and not\nfor each data point. The embedding vector for a given entity is shared across\nall data points that contain this entity. Therefore, our method fits each prior\ndistribution to several data points.\n\n> [...] which would generally result in degenerate solutions (the prior will\n> simply match the posterior).\n\nWe agree that EM would fit priors that simply match the posterior if one chose a\nvery flexible family of prior distributions. This is a general property of EM\nand not specific to our model. In practice, one avoids this degenerate limit of\nEM either by imposing an additional “hyper-prior”, or by making the family of\nprior distributions much more restrictive than the variational family of\napproximate posterior distributions. We opted for the latter solution for\nsimplicity. As mentioned above, our prior family has 8,000 times fewer free\nparameters than the family of approximate posterior distributions, thus making\nit much more restrictive.'}		rylPm-5a67	HJg5MtQWNN	AKBC.ws/2019/Conference/-/Paper40/Official_Comment	['AKBC.ws/2019/Conference/Paper40/Reviewers/Unsubmitted']	1		['everyone']	Byl-M0GfzV	['AKBC.ws/2019/Conference/Paper40/Authors']	1548986641731		1548986690527	['AKBC.ws/2019/Conference/Paper40/Authors', 'AKBC.ws/2019/Conference']
28	1548437927916	"{'title': 'RE- Interesting insights into crowd-provided relation annotations', 'comment': '\n> Reviewer 2: Overall this paper generates a lot of interesting insights, but does not close the loop by feeding these\n> back into the Mark2Cure platform and improving annotation quality. So while there is potential for impact, a lot of\n> this potential has not yet been actualized. \n\n--We wholeheartedly agree with this statement as much of this work establishes the potential benefit of Citizen Science to biocuration. Although preliminary, we believe there is sufficient value in this work to share these current findings, and have added a sentence to the text to clarify this point.\n\n> Reviewer 2: Based on Figure 1C, the system maxes out at around 73%, even with an ensemble of workers. It is not \n> clear to me how satisfactory this level of accuracy is. For example, would this be good enough to train a relation\n> extraction model? \n\n--We believe that our current 73% maximum accuracy could be improved with further refinement of our platform based on the findings reported in this paper.  In general, our intuition is that Citizen Science alone would be sufficiently accurate for assistive or statistical analyses, but not of comparable accuracy to expert curation. We are exploring more quantitative methods of assessing the accuracy needed for various biomedical applications.  In addition, we have added a short explanation on the differences in our relationship extraction effort from other efforts which make drawing direct comparison difficult.\n\n> Reviewer 2: Some design choices seem odd and perhaps merit some explanation. For example, why not use\n> Mark2Cure on relations between entities of the same type? It was noted that SemMedDB has such relations, and in\n> general there seem to be many valid cases of these (drug-drug interactions, gene regulation, etc.). \n\n--In our limited experience and from what we’ve observed of SemMedDB, the vast majority of relationships between entities of the same type tend to be hypernymic propositions (“is a”). SemMedDB is actually very good with hypernymic propositions (https://www.sciencedirect.com/science/article/pii/S1532046403001175?via%3Dihub), so we limited the annotations to be done by citizen scientists to those of different entity types. Clarification has been added to the methods section.\n\n> Reviewer 2: I don\'t understand the grey dots in Figure 4. What does it mean that no identifier was available? \n\n--Not every annotation from Pubtator is linked to an identifier. Annotations lacking identifiers may or may not be synonymous with other annotations lacking identifiers within an abstract so it becomes difficult to calculate the minimum distance within the text between two concepts if there are multiple annotations lacking identifiers. We thank the reviewer for pointing out this oversight and have made the necessary clarifications in the text.\n\n> Reviewer 2: Some minor formatting things: M2C was used as an acronym but not defined. Use ""(i.e., xyz)"" instead of\n> ""(ie- xyz)"", and same for ""e.g."".\n\n--Acronym added to text and formatting fixed.\n'}"		r1loaec6pm	H1ge3Kpd7V	AKBC.ws/2019/Conference/-/Paper5/Official_Comment	['AKBC.ws/2019/Conference/Paper5/Reviewers/Unsubmitted']	1		['everyone']	SkxedGg0eN	['AKBC.ws/2019/Conference/Paper5/Authors']	1548437927916		1548985466613	['AKBC.ws/2019/Conference/Paper5/Authors', 'AKBC.ws/2019/Conference']
29	1548437976755	{'title': 'RE- promising initial results, but missing comparison with different relation extraction methods', 'comment': '\n> Reviewer 1: To make it more readable, I suggest adding an explanation of SemMedDB in the introduction, as well as\n> explaining the meaning of UMLS and the UMLS CUI in the section they are mentioned in.\n\n--We have incorporated these suggestions in the revised text.\n\n> Reviewer 1: What is missing is a more thorough positioning within the related work on relation extraction. In\n> particular, it would be useful to evaluate this application in comparison with with well-known general-purpose\n> automated relation extraction models [1], and models that are tailored for medical relation extraction [2]. Another\n> interesting comparison to make is with the different active [3] and semi-supervised learning methods [4], that also\n> have experimented with different ways of aggregating crowd data.\n\n--We thank the reviewer for these excellent references and have added them to the text as exciting avenues for exploration. Investigations with active learning are ongoing, but are not yet complete. We look forward to performing the other comparisons to existing methods in future work, and look forward to meeting potential collaborators at AKBC who might be interested in exploring this direction of work with us.\n'}		r1loaec6pm	r1xW156O7N	AKBC.ws/2019/Conference/-/Paper5/Official_Comment	['AKBC.ws/2019/Conference/Paper5/Reviewers/Unsubmitted']	2		['everyone']	rJloLmskGE	['AKBC.ws/2019/Conference/Paper5/Authors']	1548437976755		1548985456728	['AKBC.ws/2019/Conference/Paper5/Authors', 'AKBC.ws/2019/Conference']
30	1548439388242	{'title': 'RE- Interesting but missing solid takeaway messages', 'comment': '\n> Reviewer 3: While the authors report several findings on the specific selected dataset, as a reader I struggle to easily \n> identify: \n> - the novelty: most of the findings are consistent with previous literature, both on the viability of performing medical\n> relation extraction via non-expert crowd (Aroyo and Welty, 2013) and and the fact that NER performance affects RE\n> performance\n\n---We thank the reviewer for taking the time to read our paper and offer constructive criticism. We agree that non-expert crowdsourcing (especially via paid micro-task platforms) has been demonstrated, and feel that applying citizen science is a logical next step. We hope to further address the citizen science aspects of this work in the future.\n\n> - clear takeaway guidelines and messages: the discussion is somewhat limited to specific issue of the one analyzed\n> datasets, but fail to provide reusable insights\n\n---We thank the reviewer for confirming the concerns of the other two reviewers on potential impact being unrealized, and the suggestions for improving the paper with additional comparative work. We hope that we have sufficiently addressed those concerns with the current revision and in the other reviewer responses.'}		r1loaec6pm	rylEPyCOX4	AKBC.ws/2019/Conference/-/Paper5/Official_Comment	['AKBC.ws/2019/Conference/Paper5/Reviewers/Unsubmitted']	3		['everyone']	ryewOgvTMV	['AKBC.ws/2019/Conference/Paper5/Authors']	1548439388242		1548985444707	['AKBC.ws/2019/Conference/Paper5/Authors', 'AKBC.ws/2019/Conference']
31	1546492655566	"{'title': 'Good motivation and system description, but the novelty and accuracy are unclear.', 'review': '\n== Summary ==\nThe paper describes the construction of a knowledge base for food knowledge. The key components of the KB are (1) food nutrition data, (2) recipes and ingredients, and (3) links to DBpedia concepts. The data was imported from various structured data sources. The paper also explains how to embed ingredients based on their co-occurrences.\n\n== Pros ==\n- The main motivation for building a KB for food is for dietary recommendation, which is a laudable goal.\n- The KB schema is sound and the description of the system is pretty clear.\n\n== Cons ==\n- The novelty of the work is unclear.\n\nIf my understanding is correct, the data sources are all structured data with known schema, and the task boils down to (1) importing the data and (2) linking entities between data sources. Part (1) is an engineering task that all KB developers already have to do, so the main part that could contain novelty would be entity linking.\n\nHowever, the methods described in the paper are either a heuristic (e.g., string manipulation ""Cheese, Blue"" --> dbr:blue_cheese), an external API (e.g., DBpedia search), or a well-known simple technique (e.g., cosine similarity between bag-of-word vectors). The paper describes several challenges in Section 3.3 (e.g., ambiguous names, synonyms, spelling errors), but the proposed methods do not seem to address these challenges. The proposed ingredient embedding method in Section 5.2 also greatly resembles food2vec mentioned in the related work section. \n\nAs such, it seems that the only novel contribution of this paper is the KB itself, which leads to the second point:\n\n- It is unclear if the KB is accurate enough for actual use.\n\nApart from Section 5.1 on entity resolution, there is no formal evaluation of the methods. As the described methods do not address the challenges in Section 3.3, it is less likely that the identified links are accurate enough for actual use in health-sensitive use cases.\n\nThe evaluation in Section 5.1 also raises a question regarding how the disagreements (~50% of the cases) between the proposed entity linking method and DBpedia search are reconciled.', 'rating': '6: Marginally above acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		B1gjSZ9TpQ	SJg_eiGjWE	AKBC.ws/2019/Conference/-/Paper53/Official_Review	['AKBC.ws/2019/Conference/Paper53/Reviewers/Unsubmitted']	1		['everyone']	B1gjSZ9TpQ	['AKBC.ws/2019/Conference/Paper53/AnonReviewer1']	1546492655566		1548982764680	['AKBC.ws/2019/Conference']
32	1548981811774	"{'title': 'Very incisive review', 'comment': ""Thank you for the careful review of our paper.\n\nFor (1) we must somewhat disagree -- since submission indeed there have been several new papers posted to arXiv which include rewriter modules for the ARC dataset inspired by the work of Boratko et al. (https://arxiv.org/pdf/1806.00358.pdf) but as of submission time the notion of combining a rewriter with the entailment module for ARC is novel.  We also point to Reviewer2’s comment, “Query reformulation methods have been used on several QA tasks (like Buck et al 2018 above), and incorporating background knowledge has been used before too (as described in the paper), but I think it’s fairly original to do both in the same time.”\n\nWith respect to (2) -- we used TransH to have a common and easily trainable baseline for comparison. We do not have a priori beliefs about which aspects of an entity’s relations are useful for answering a particular science question, but given that many science questions pertain to different functional properties of matter it does not seem unreasonable to use the vector embedding provided by TransH to do so.\n\nWith respect to (3) -- we have not yet incorporated transformer networks/BERT as done by Sun 2018 and Ni 2018 and some other groups have done (https://leaderboard.allenai.org/arc/submissions/public).  However, these papers have been posted between the time of submission to AKBC and the response period so while we are working to integrate these aspects we feel that it is unfair to punish us on this point. There's no question that massively pre-trained models like BERT represent a substantial shift in the state-of-the-art of NLP, and we expect that all future work in the near future should use them to represent text. However, we feel that the contributions of our paper are mainly focused around the end-to-end pipeline and providing techniques for leveraging a corpus of background knowledge that improve on the assumptions of prior work from e.g. AI2.\n""}"		HJxYZ-5paX	B1e248zWN4	AKBC.ws/2019/Conference/-/Paper29/Official_Comment	['AKBC.ws/2019/Conference/Paper29/Reviewers/Unsubmitted']	3		['everyone']	HJlYIoPlgV	['AKBC.ws/2019/Conference/Paper29/Authors']	1548981811774		1548981811774	['AKBC.ws/2019/Conference/Paper29/Authors', 'AKBC.ws/2019/Conference']
33	1542459704304	{'title': 'KG-Cleaner: Jointly Learning to Identify and Correct Errors Produced by Information Extraction Systems', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper49/Authors'], 'keywords': ['knowledge graph', 'semantics-aware'], 'TL;DR': 'An approach to clean noisy extraction considering an IE system as a blackbox', 'abstract': 'KG-Cleaner is a semantics-aware framework combining two different approaches for knowledge correction: text-based classiﬁcation to identify, and schema backed classiﬁcation to correct, errors in data produced by information extraction systems. The approach is novel in being an independent, standalone system addressing both tasks in a uniﬁed, joint manner. We evaluate KG-Cleaner and other models on two collections: a Wikidata corpus of 700K facts and 5M fact-relevant sentences and a collection of 30K facts extracted by systems participating in the 2015 TAC Knowledge Base Population task. We ﬁnd that simple parameter-efﬁcient shallow neural networks, combined with a continuous relaxation of a discrete predicted latent variable, provide a good common representation for the two tasks, achieving absolute performance gains of 30-35 F1 points on the evaluation datasets for both credibility prediction and relation repair.', 'pdf': '/pdf/e305ade35191900fe922cdbd0ef9661ba8d905d6.pdf', 'archival status': 'Archival', 'subject areas': ['Information Extraction'], 'paperhash': 'anonymous|kgcleaner_jointly_learning_to_identify_and_correct_errors_produced_by_information_extraction_systems', '_bibtex': '@inproceedings{    \nanonymous2019kg-cleaner:,    \ntitle={KG-Cleaner: Jointly Learning to Identify and Correct Errors Produced by Information Extraction Systems},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=ryleB-56pQ},    \nnote={under review}    \n}'}		ryleB-56pQ	ryleB-56pQ	AKBC.ws/2019/Conference/-/Blind_Submission	[]	49	BkldsdBTam	['everyone']		['AKBC.ws/2019/Conference']	1542459704304		1548981558903	['AKBC.ws/2019/Conference']
34	1548981434929	{'title': 'Added example, significance tests, additional references & discussion', 'comment': 'We very much appreciate the thoughtful suggestions of your review, and have added the citations for the paraphrase-based approaches to query reformulation in open QA to the related work section in our revision.\n\nThe comments about pointer networks vs. our seq2seq formulation as well as measuring the effect of the CRF layer are well taken; we will attempt to include those results in the paper and update our revision, time permitting.\n\nExperiments comparing the performance of the entailment module on the somewhat larger SciTail dataset are conducted in https://arxiv.org/pdf/1809.05724.pdf . We will update the discussion to reflect this result in the science domain, and try to provide references for its performance on other large-scale NLI corpora.\n\nIn the original ARC paper (https://arxiv.org/pdf/1803.05457.pdf), they evaluate their baselines (DGEM, BiDAF, and DecompAttn) using a significance threshold of ±2.5% compare to random chance:\n“The most striking  observation is that none of the algorithms score significantly higher than the random baseline on the Challenge set, where the 95% confidence interval is ±2.5%. In contrast, their performance on the Easy set is generally between 55% and 65%. This highlights the different nature and difficulty of the Challenge set.”\nBy both that measure and by using a one-sample t-test, we found that our models outperformed the reported accuracy of those three baselines at the 99% confidence interval on the test set; however, our methods are not significantly better than KG2 at the 95% level (p=0.652 for NCRF++). Comparing to the reported accuracy of the BiLSTM Max-Out model of Mihaylov et al., our regular NCRF++ models (as well as the ones augmented with TransH and PPMI embeddings) are not significantly worse at 95% confidence.\n\nOn the other hand, paired t-tests comparing the AI2 rule to the Top-2 rule (without splitting) favored the Top-2 rule with at least 99.9% confidence on the test set. While we observe that splitting is harmful for the test set, the results are only significant at the 95% level for PPMI and Original Question.'}		HJxYZ-5paX	SylQ6EMbVE	AKBC.ws/2019/Conference/-/Paper29/Official_Comment	['AKBC.ws/2019/Conference/Paper29/Reviewers/Unsubmitted']	2		['everyone']	H1eRU4D1GN	['AKBC.ws/2019/Conference/Paper29/Authors']	1548981434929		1548981434929	['AKBC.ws/2019/Conference/Paper29/Authors', 'AKBC.ws/2019/Conference']
35	1548981328716	{'title': 'Added discussion of AQA, example, and clarifications on training -- many good ideas for future work!', 'comment': 'Thank you for the very considerate review!\n\nWe will include a discussion of our paper in the context of the AQA work, which tackles the very interesting problem of open-vocabulary questions reformulation. Their paper illustrates the difficulty of the task, noting that “99.8% of [AQA-QR rewrites] start with the prefix What is [name]...”.  They speculate that this “might be related to the fact that virtually all answers involve names, of named entities (Micronesia) or generic concepts (pizza).” We hope to expand our work to include question and topic expansion in the near future, as we agree that making progress on open-vocabulary query reformulation in the context of non-factoid question answering is an exciting effort.\n\nWe have space in the paper and have included examples of the rewritten queries in our revision.  This also goes along with your Clarity Con point 2 -- the rewritten query is a concatenation of the selected terms.\n\nWe will try to work in an example of the decision rule in the appendix.  Since the decision rule uses many query results it takes up significant space.  Informally, the decision rule looks at the average or max entailment for each answer for the various query results.  This is tuned on the ARC-Challenge-Train dataset.\n\nIndeed, as you say the rewriter is trained only on the Essential Terms dataset and the entailment is only trained on the SciTail dataset.  We are in the process of transforming some of the ARC-Challenge questions to fine tune the entailment model but this work is not completed yet.  The Decision Rules are trained on 1/5th of the ARC-Training set which we can use as we have not used the ARC-Train for any other portion of the model.  \n\nWe agree that leveraging background knowledge is an important line of research, and that the work is this paper represents very early steps towards fully utilizing it in an open-domain QA pipeline. Future work indeed involves jointly training the modules together -- we are in the process of integrating background knowledge into the entailment model, both by a similar approach to https://arxiv.org/pdf/1809.05724.pdf and by using more sophisticated graph embeddings.\n'}		HJxYZ-5paX	B1xFLEzWV4	AKBC.ws/2019/Conference/-/Paper29/Official_Comment	['AKBC.ws/2019/Conference/Paper29/Reviewers/Unsubmitted']	1		['everyone']	S1gYa35MzN	['AKBC.ws/2019/Conference/Paper29/Authors']	1548981328716		1548981411103	['AKBC.ws/2019/Conference/Paper29/Authors', 'AKBC.ws/2019/Conference']
36	1542459649404	{'title': 'Answering Science Exam Questions Using Query Reformulation with Background Knowledge', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper29/Authors'], 'keywords': ['open-domain question answering', 'science question answering', 'multiple-choice question answering', 'passage retrieval', 'query reformulation'], 'TL;DR': 'We explore how using background knowledge with query reformulation can help retrieve better supporting evidence when answering multiple-choice science questions.', 'abstract': 'Open-domain question answering (QA) is an important problem in AI and NLP that is emerging as a bellwether for progress on the generalizability of AI methods and techniques. Much of the progress in open-domain QA systems has been realized through advances in information retrieval methods and corpus construction. In this paper, we focus on the recently introduced ARC Challenge dataset, which contains 2,590 multiple choice questions authored for grade-school science exams. These questions are selected to be the most challenging for current QA systems, and current state of the art performance is only slightly better than random chance. We present a system that reformulates a given question into queries that are used to retrieve supporting text from a large corpus of science-related text. Our rewriter is able to incorporate background knowledge from ConceptNet and -- in tandem with a generic textual entailment system trained on SciTail that identifies support in the retrieved results -- outperforms several strong baselines on the end-to-end QA task despite only being trained to identify essential terms in the original source question. We use a generalizable decision methodology over the retrieved evidence and answer candidates to select the best answer. By combining query reformulation, background knowledge, and textual entailment our system is able to outperform several strong baselines on the ARC dataset. ', 'pdf': '/pdf/23491961b2a9ecb9cbff3780af262d45b9d22a48.pdf', 'archival status': 'Archival', 'subject areas': ['Natural Language Processing', 'Question Answering'], 'paperhash': 'anonymous|answering_science_exam_questions_using_query_reformulation_with_background_knowledge', '_bibtex': '@inproceedings{    \nanonymous2019answering,    \ntitle={Answering Science Exam Questions Using Query Reformulation with Background Knowledge},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=HJxYZ-5paX},    \nnote={under review}    \n}'}		HJxYZ-5paX	HJxYZ-5paX	AKBC.ws/2019/Conference/-/Blind_Submission	[]	29	ByeKhWjn67	['everyone']		['AKBC.ws/2019/Conference']	1542459649404		1548981207952	['AKBC.ws/2019/Conference']
37	1548979497430	"{'title': 'Author response', 'comment': ""We than the AC for his/her comments. Below please find our response.\n\nWe'd like to clarify that while we focus on presenting the test negative log-likelihoods/ELBOs in the tables, we have provided the trace plots of training and validation negative ELBOs in Figures 2, 6, 7. \n\nThe toy experiment of maximizing E_z [(z-p_0)^2] indeed is very easy for the proposed ARM estimator, but becomes very challenging for REBAR and RELAX when p_0 approaches 0.5.\n\nWe use $\\phi$ for the parameters to avoid confusion for our experiments in discrete variational autoencoders, where $\\phi$ is commonly used to denote the encoder parameter, while $\\theta$ is commonly used to denote the decoder parameter. While the derivation may still appear complicated and heavy in notation, the actual implementation is actually rather straightforward.\n\nAntithetic sampling for variance reduction is indeed old. However, antithetic sampling only becomes useful after performing variable augmentation and REINFORCE in the augmented space; without the Augment and REINFORCE steps, it is unclear how antithetic sampling can be applied to binary variables. \n\nCategorical extension of the binary ARM estimator involves much more sophisticated variable-swap and merge operations (much more notation heavy). We had a preliminary solution, which can be found in https://arxiv.org/abs/1807.11143 , and we have recently discovered another significantly improved solution. We plan to update that ArXiv submission in the near future. ""}"		S1lg0jAcYm	HkgbEpWZ44	ICLR.cc/2019/Conference/-/Paper862/Official_Comment	['ICLR.cc/2019/Conference/Paper862/Reviewers/Unsubmitted']	15		['everyone']	BJli5EK3y4	['ICLR.cc/2019/Conference/Paper862/Authors']	1548979497430		1548979623679	['ICLR.cc/2019/Conference/Paper862/Authors', 'ICLR.cc/2019/Conference']
38	1548978365992	{'title': 'New version', 'comment': 'We thank all reviewers for the remarkable comments. A new version of the paper has been uploaded by following their suggestions.'}		SkxE1b56TQ	SklUaO--4E	AKBC.ws/2019/Conference/-/Paper14/Official_Comment	['AKBC.ws/2019/Conference/Paper14/Reviewers/Unsubmitted']	7		['everyone']	SkxE1b56TQ	['AKBC.ws/2019/Conference/Paper14/Authors']	1548978365992		1548978365992	['AKBC.ws/2019/Conference/Paper14/Authors', 'AKBC.ws/2019/Conference']
39	1548977880679	{'title': 'RE: Response to rebuttal', 'comment': 'We agree with your decision. Thank you again for your valuable feedback. We uploaded a new version by including your suggestion. Anyway, we want to point out that, currently, we cannot perform the analysis of the possible overlaps on the transfer learning experiment just because we did not log the entity pairs (randomly) selected during the training phase. We will re-train the model by considering your concern.'}		SkxE1b56TQ	rylZkDZZ4N	AKBC.ws/2019/Conference/-/Paper14/Official_Comment	['AKBC.ws/2019/Conference/Paper14/Reviewers/Unsubmitted']	6		['everyone']	H1x31SIyNN	['AKBC.ws/2019/Conference/Paper14/Authors']	1548977880679		1548977949936	['AKBC.ws/2019/Conference/Paper14/Authors', 'AKBC.ws/2019/Conference']
40	1538087879855	"{'title': 'ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks', 'abstract': 'To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low variance, and has low computational complexity. Exploiting variable augmentation, REINFORCE, and reparameterization, the ARM estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers. The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric ""self-control"" baseline function together with the REINFORCE estimator in that augmented space. Experimental results show the ARM estimator provides state-of-the-art performance in auto-encoding variational inference and maximum likelihood estimation, for discrete latent variable models with one or multiple stochastic binary layers. Python code for reproducible research is publicly available.', 'keywords': ['Antithetic sampling', 'variable augmentation', 'deep discrete latent variable models', 'variance reduction', 'variational auto-encoder'], 'authorids': ['mzyin@utexas.edu', 'mingyuan.zhou@mccombs.utexas.edu'], 'authors': ['Mingzhang Yin', 'Mingyuan Zhou'], 'pdf': '/pdf/5e88b16c21ed525f101915dc79067b994bb8f958.pdf', 'paperhash': 'yin|arm_augmentreinforcemerge_gradient_for_stochastic_binary_networks', 'TL;DR': 'An unbiased and low-variance gradient estimator for discrete latent variable models', '_bibtex': '@inproceedings{\nyin2018arm,\ntitle={{ARM}: Augment-{REINFORCE}-Merge Gradient for Stochastic Binary Networks},\nauthor={Mingzhang Yin and Mingyuan Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lg0jAcYm},\n}'}"		S1lg0jAcYm	S1lg0jAcYm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	862	H1xy9x29F7	['everyone']		['ICLR.cc/2019/Conference']	1538087879855		1548977552045	['ICLR.cc/2019/Conference']
41	1542459612213	{'title': 'Learning Relational Representations by Analogy using Hierarchical Siamese Networks', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper14/Authors'], 'keywords': ['relation extraction', 'textual representation', 'siamese network', 'one-shot learning', 'transfer learning'], 'TL;DR': 'We propose an approach to learn representations of relations in text using hierarchical siamese networks.', 'abstract': 'We address relation extraction as an analogy problem by proposing a novel approach to learn representations of relations expressed by their textual mentions. In our assumption, if two pairs of entities belong to the same relation, then those two pairs are analogous. Following this idea, we collect a large set of analogous pairs by matching triples in knowledge bases with web-scale corpora through distant supervision. We leverage this dataset to train a hierarchical siamese network in order to learn entity-entity embeddings which encode relational information through the different linguistic paraphrasing expressing the same relation. We evaluate our model in a one-shot learning task by showing a promising generalization capability in order to classify unseen relation types, which makes this approach suitable to perform automatic knowledge base population with minimal supervision. Moreover, the model can be used to generate pre-trained embeddings which provide a valuable signal when integrated into an existing neural-based model by outperforming the state-of-the-art methods on a downstream relation extraction task.', 'pdf': '/pdf/af8c37ea31d739c4ecf2d7dd2e130fb4e987df78.pdf', 'archival status': 'Archival', 'subject areas': ['Machine Learning', 'Natural Language Processing', 'Information Extraction', 'Semantic Web'], 'paperhash': 'anonymous|learning_relational_representations_by_analogy_using_hierarchical_siamese_networks', '_bibtex': '@inproceedings{    \nanonymous2019learning,    \ntitle={Learning Relational Representations by Analogy using Hierarchical Siamese Networks},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=SkxE1b56TQ},    \nnote={under review}    \n}'}		SkxE1b56TQ	SkxE1b56TQ	AKBC.ws/2019/Conference/-/Blind_Submission	[]	14	Hylvp5VnpX	['everyone']		['AKBC.ws/2019/Conference']	1542459612213		1548976965662	['AKBC.ws/2019/Conference']
42	1542459631317	{'title': 'Semi-supervised Ensemble Learning with Weak Supervision for Biomedical Relationship Extraction', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper22/Authors'], 'keywords': ['weak supervision', 'meta-learning', 'biomedical relationship extraction', 'semi-supervised learning', 'ensemble learning'], 'TL;DR': 'We propose and apply a meta-learning methodology based on Weak Supervision, for combining Semi-Supervised and Ensemble Learning on the task of Biomedical Relationship Extraction.', 'abstract': 'Natural language understanding research has recently shifted towards complex Machine Learning and Deep Learning algorithms. Such models often outperform significantly their simpler counterparts. However, their performance relies on the availability of large amounts of labeled data, which are rarely available. To tackle this problem, we propose a methodology for extending training datasets to arbitrarily big sizes and training complex, data-hungry models using weak supervision. We apply this methodology on biomedical relation extraction, a task where training datasets are excessively time-consuming and expensive to create, yet has a major impact on downstream applications such as drug discovery. We demonstrate in two small-scale controlled experiments that our method consistently enhances the performance of an LSTM network, with performance improvements comparable to hand-labeled training data. Finally, we discuss the optimal setting for applying weak supervision using this methodology.', 'archival status': 'Archival', 'subject areas': ['Machine Learning', 'Natural Language Processing', 'Information Extraction', 'Applications: Biomedicine'], 'pdf': '/pdf/14a8e3161b7a6a84bec22c7ee442effb090564a8.pdf', 'paperhash': 'anonymous|semisupervised_ensemble_learning_with_weak_supervision_for_biomedical_relationship_extraction', '_bibtex': '@inproceedings{    \nanonymous2019semi-supervised,    \ntitle={Semi-supervised Ensemble Learning with Weak Supervision for Biomedical Relationship Extraction},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=rygDeZqap7},    \nnote={under review}    \n}'}		rygDeZqap7	rygDeZqap7	AKBC.ws/2019/Conference/-/Blind_Submission	[]	22	HJxQu2_267	['everyone']		['AKBC.ws/2019/Conference']	1542459631317		1548974574494	['AKBC.ws/2019/Conference']
43	1538087960022	"{'title': 'An Empirical Study of Example Forgetting during Deep Neural Network Learning', 'abstract': ""Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a ``forgetting event'' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set's (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance."", 'keywords': ['catastrophic forgetting', 'sample weighting', 'deep generalization'], 'authorids': ['mariya.k.toneva@gmail.com', 'alsordon@microsoft.com', 'retachet@microsoft.com', 'adtrisch@microsoft.com', 'yoshua.bengio@mila.quebec', 'geoff.gordon@microsoft.com'], 'authors': ['Mariya Toneva*', 'Alessandro Sordoni*', 'Remi Tachet des Combes*', 'Adam Trischler', 'Yoshua Bengio', 'Geoffrey J. Gordon'], 'TL;DR': 'We show that catastrophic forgetting occurs within what is considered to be a single task and find that examples that are not prone to forgetting can be removed from the training set without loss of generalization.', 'pdf': '/pdf/e6f24f0a844c3f9322d34994c197b5051f9603ad.pdf', 'paperhash': 'toneva|an_empirical_study_of_example_forgetting_during_deep_neural_network_learning', '_bibtex': '@inproceedings{\ntoneva2018an,\ntitle={An Empirical Study of Example Forgetting during Deep Neural Network Learning},\nauthor={Mariya Toneva and Alessandro Sordoni and Remi Tachet des Combes and Adam Trischler and Yoshua Bengio and Geoffrey J. Gordon},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlxm30cKm},\n}'}"		BJlxm30cKm	BJlxm30cKm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1326	SJelTc6cFm	['everyone']		['ICLR.cc/2019/Conference']	1538087960022		1548969929511	['ICLR.cc/2019/Conference']
44	1548965548150	{'title': 'Thanks a lot for sharing!', 'comment': 'Very happy to hear! \n\nIf you get new empirical insights on what is best to use, or applications where you found it useful, please feel free to post it on this thread :)\n\nWill keep an eye on your work!\n'}		r1eiqi09K7	HygEnIClNE	ICLR.cc/2019/Conference/-/Paper561/Official_Comment	['ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted']	10		['everyone']	r1xCwSC7-V	['ICLR.cc/2019/Conference/Paper561/Authors']	1548965548150		1548965548150	['ICLR.cc/2019/Conference/Paper561/Authors', 'ICLR.cc/2019/Conference']
45	1548964499332	"{'title': 'Clarifications', 'comment': '""I worry that OPIEC may be too skewed towards the predictions of a specific OIE system""\n\n-> Yes, OPIEC is based on a modified variant of MinIE, which allows us to provide syntactic annotations, semantic annotations, and confidence scores. The pipeline used to create OPIEC can be applied to other datasets and (perhaps with minor modifications) with other OIE systems as well. We plan to publish the source code of this pipeline along with the corpus and data access tools along with OPIEC.\n\n\n""the work presented here consists mainly of running off-the-shelf can be extended to contain more novel substance, such as a new Open IE system and its evaluation against this corpus"". \n\n-> We use an improved version of MinIE, which adds space-time awareness + confidence score. MinIE, and in particular the OPIEC corpus, is indeed being used in other research projects already. The goal of this paper is to introduce the dataset to other researchers and provide insight into its properties.\n\n\n""The crux of the matter here I think, is the accuracy of the dataset, reported tersely in Section 5.3, in which a manual analysis (who annotated? what were their guidelines? what was their agreement?) finds that the dataset is estimated to have 60% correct tuples. Can this be improved? Somehow automatically verified?""\n\n-> We added subsection 4.6 to provide more information about the confidence scores that OPIEC provides. These scores allow filtering the corpus for only high-confidence triples, for example. \n\n\n""I think that the paper should make it clear in the title or at least in the abstract that the corpus is created automatically by running an OIE system on a large scale. From current title and abstract I was wrongfully expecting a gold human-annotated dataset.""\n""I don\'t think I agree with the claim in Section 4.3 that ""it is the largest corpus with golden annotations to date"". As far as I understand, the presented corpus is created in a completely automated manner and bound to contain prediction errors.""\n\n-> We carefully revisited the paper to be more precise. For example, we now always refer to “golden annotations for arguments”.\n\n\n""Following on previous points, I think the paper misses a discussion on gold vs. predicted datasets for OIE, and their different uses. Some missing gold OIE references: Wu and Weld (2010),  Akbik and Loser (2012), Stanovsky and Dagan (2016).""\n\n-> To the best of our knowledge, no data was released from Wu and Weld (2010) and Akbik and Loser (2012). We now mention the benchmark dataset of Stanovsky and Dagan (2016) in the related work section. The focus of this work is on large OIE resources, however. \n\n\n- ""I think that some of the implementation decisions seem sometimes a little arbitrary. For instance, for the post-processing example which modifies (Peter Brooke; was a member of; Parliament) to (Peter Brooke; was ; a member of Parliament), I think I would\'ve preferred the original relation, imagining a scenario where you look for all members of parliament (X; was a member of; Parliament), or all of the things Peter Brooke was a member of (Peter Brooke; was a member of; Y) seems more convenient to me.""\n\n-> We agree with the reviewer in this particular example. However, “Member of Parliament” is a concept in Wikipedia. OPIEC avoids to split concepts or named entities across arguments/relations; such an approach is often erroneous.\n\n\n“I assume that in Table 1, unique relations and arguments are also in millions? I think this could be clearer, if that\'s the indeed the case.”\n\n-> Fixed\n\n- I think it\'d be nice to add dataset sizes to each of the OPIEC variants in Fig 1.\n-> Fixed\n\n- Typos -> fixed\n'}"		HJxeGb5pTm	ryxiqG0gV4	AKBC.ws/2019/Conference/-/Paper32/Official_Comment	['AKBC.ws/2019/Conference/Paper32/Reviewers/Unsubmitted']	3		['everyone']	rylqOh2HgE	['AKBC.ws/2019/Conference/Paper32/Authors']	1548964499332		1548964635527	['AKBC.ws/2019/Conference/Paper32/Authors', 'AKBC.ws/2019/Conference']
46	1548964318999	"{'title': 'Clarifications', 'comment': '""1) It uses the NLP pipeline and the MinIE-SpaTe system. When you get the results, do you evaluate to what extent that the results are correct?""\n""2) In Section 3.4, the author mentioned the correctness is around 65%, what do you do for those incorrect tuples?""\n\n-> We report on precision and confidence scores in the new subsection 4.6. \n\n\n""3) Have you tried any task-based evaluation on your dataset?""\n\nThe OPIEC corpus is being used in other research projects. The goal of this paper is to introduce the dataset to other researchers and provide insight into its properties.\n\n'}"		HJxeGb5pTm	S1lPJM0xVN	AKBC.ws/2019/Conference/-/Paper32/Official_Comment	['AKBC.ws/2019/Conference/Paper32/Reviewers/Unsubmitted']	2		['everyone']	B1eTN16XfN	['AKBC.ws/2019/Conference/Paper32/Authors']	1548964318999		1548964318999	['AKBC.ws/2019/Conference/Paper32/Authors', 'AKBC.ws/2019/Conference']
47	1548964124855	{'title': 'New subsection added addressing common reviewers comments', 'comment': 'Thank you very much for your helpful and insightful comments! All of you asked for more information about precision; we added Section 4.6 “Precision and Confidence Score” to provide more details and statistics about precision as well as the confidence scores provided with the dataset.'}		HJxeGb5pTm	B1lrmbClEE	AKBC.ws/2019/Conference/-/Paper32/Official_Comment	['AKBC.ws/2019/Conference/Paper32/Reviewers/Unsubmitted']	1		['everyone']	HJxeGb5pTm	['AKBC.ws/2019/Conference/Paper32/Authors']	1548964124855		1548964124855	['AKBC.ws/2019/Conference/Paper32/Authors', 'AKBC.ws/2019/Conference']
48	1542459656480	{'title': 'OPIEC: An Open Information Extraction Corpus', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper32/Authors'], 'keywords': ['open information extraction', 'text analytics'], 'TL;DR': 'An Open Information Extraction Corpus and its in-depth analysis', 'abstract': 'Open information extraction (OIE) systems extract relations and their\n  arguments from natural language text in an unsupervised manner. The resulting\n  extractions are a valuable resource for downstream tasks such as knowledge\n  base construction, open question answering, or event schema induction. In this\n  paper, we release an OIE corpus called OPIEC extracted from the text of\n  English Wikipedia and analyze its contents. OPIEC complements the available\n  OIE resources: It is the largest OIE corpus publicly available to date (over\n  340M triples) and contains valuable metadata such as provenance information,\n  confidence scores, linguistic annotations, and semantic annotations including\n  spatial and temporal information. We analyze the OPIEC corpus by comparing its\n  content with knowledge bases such as DBpedia or YAGO, which are also based on\n  Wikipedia. We found that most of the facts between entities present in OPIEC\n  cannot be found in DBpedia and/or YAGO, that OIE facts \n  often differ in the level of specificity compared to\n  knowledge base facts, and that OIE open relations are generally highly\n  polysemous. We believe that the OPIEC corpus is a valuable resource for future\n  research on automated knowledge base construction.', 'archival status': 'Archival', 'subject areas': ['Information Extraction', 'Applications: Other'], 'pdf': '/pdf/21aa4cf3d4757825d6bcf943f6cfd748e798ff5f.pdf', 'paperhash': 'anonymous|opiec_an_open_information_extraction_corpus', '_bibtex': '@inproceedings{    \nanonymous2019opiec:,    \ntitle={OPIEC: An Open Information Extraction Corpus},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=HJxeGb5pTm},    \nnote={under review}    \n}'}		HJxeGb5pTm	HJxeGb5pTm	AKBC.ws/2019/Conference/-/Blind_Submission	[]	32	B1xK6jJ66Q	['everyone']		['AKBC.ws/2019/Conference']	1542459656480		1548964021769	['AKBC.ws/2019/Conference']
49	1548954133305	{'title': 'More evaluation task has been added.', 'comment': '\n-->Q1: While the method seems to learn some amount of semantic properties, most of the baselines reported seem either outdated or ill fitted to the task and do not serve well to evaluate the value of the proposed method for the given task.\n\nAns: We have now updated the paper with two more recent relative work, Poincare[1] and LEAR[2], and empirically compare the proposed method against them in all the three evaluation tasks.\n\n[1] Poincare Embeddings [Nikel and Kiela] - NIPS 2017\n[2] Specialising Word Vectors for Lexical Entailment [Vulic and Mrksic] - NAACL-HLT 2018\n\n-->Q2: For example the JointRep baseline is based on a semantic similarity task which primarily learns word embeddings based on synonym relations and seems to not be an appropriate baseline to compare the current approach to.\n\nAns: The JointReps method used different semantic relations (synonyms, hypernyms, hyponyms… etc), however, here we used the hypernym relation when we train the JointReps to compare the proposed method with it in all the evaluation tasks.\n\n-->Q3: Further, there are two primary methods of incorporating semantic knowledge into word embeddings - by incorporating them during the training procedure or by post processing the vectors to include this knowledge. While I understand that this method falls into the first category, it is still important and essential to compare to both types of strategies of word vector specialization.\n\nAns: The evaluation proposed in the paper does, in fact, compare against both type of categories. JointReps and HyperVec fall into the first category, whereas the Retrofit method and the newly added (in the updated version of the paper) LEAR method falls into the second. In all of these methods, we used the hypernym relations to incorporate the semantic knowledge into the learnt embeddings.\n\n-->Q4: In this regard [3] has been shown to beat HyperVec and other methods on hypernym detection and directionality benchmarks and should be included in the results. \n\nAns: Thank you for the suggestion. We have now added LEAR to the updated version of the paper and empirically compare the proposed method against in all the three evaluation tasks.\nThe proposed method reports better or comparable results to LEAR in two of the main evaluation tasks (hypernym detection and hierarchical completion).\n\n-->Q5: It would be also interesting to see how the current approach fares on graded hypernym benchmarks such as Hyperlex.\n\nAns: Thank you for the suggestion. We have added a new sub-section (section 4.4) in the updated version of the paper with a new evaluation task on the graded lexical entailment prediction using HyperLex.\n\n-->Q6: Section 4.2 there is a word extending out of the column boundaries. \n\nAns: Thank you. This has been modified in the updated version of the paper.'}		S1xf-W5paX	H1g6G5sxN4	AKBC.ws/2019/Conference/-/Paper26/Official_Comment	['AKBC.ws/2019/Conference/Paper26/Reviewers/Unsubmitted']	4		['everyone']	B1eYBtGmG4	['AKBC.ws/2019/Conference/Paper26/Authors']	1548954133305		1548954133305	['AKBC.ws/2019/Conference/Paper26/Authors', 'AKBC.ws/2019/Conference']
50	1548953738863	"{'title': 'Concerns clarification', 'comment': ""-->Q1: One major question I have is for the taxonomy evaluation part, I think there are works trying to do taxonomy evaluation by using node-level and edge-level evaluation. 'A Short Survey on Taxonomy Learning from Text Corpora:\nIssues, Resources, and Recent Advances' from NAACL 2017 did a nice summarization for this. Is there any reason why this evaluation is not applicable here?\n\nAns: The above-mentioned paper is mainly about the recent work on taxonomy construction from free texts, which is different from what we are proposing in this paper. Our goal is not to create taxonomies but to learn word embeddings that preserve taxonomic information as vector representations. As we do not create taxonomies, we cannot evaluate the word embeddings using taxonomy evaluation methods.\n\n-->Q2: At the end of section 4.2, the author mentioned Retrofit, JointReps and HyperVec are using the original author prepared wordnet data. Then the supervised training data is different for different methods? Is there a more controlled experiment where all experiments are using the same training data?\n\nAns: The models Retrofit, JointReps, and HyperVec (and the newly added two recent relevant work (Poincare and LEAR)) work with pairwise relation data. However, the proposed HWE works on a full hierarchical hypernym path. Therefore, the models require slightly different data.\n\n-->Q3: In section 4.4, there are three prediction methods are introduced including ADD, SUM, and DH. The score is calculated using cosine similarity. But the loss function used in the model is by minimizing the L2 distance between word embeddings? Is there any reason why not use L2 but cosine similarity in this setting?\n\nAns: We have empirically tested both the L2 and cosine and found that the cosine to work better in the given experiment.\n\n-->Q4: Also, I'm assuming SUM and DH are using cosine similarity as well? It might be useful to add that bit of information. \n\nAns: Yes. This has been added in the updated version of the paper.\n\n-->Q5: The motivation for this paper is to using taxonomy instead of just hypernym pairs?\n\nAns: Yes\n\n-->Q6: Another line of research trying to encode the taxonomy structure into the geometry space such that the taxonomy will be automatically captured due to the self-organized geometry space. Some papers including but not restricted 'Order-Embeddings of Images and Language', 'Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures'. Probably this line of work is not directly comparable, but it might be useful to add to the related work session.\n\nAns: Thank you for the suggestion. We have now updated the paper with two more related works, Poincare[1] and LEAR[2] and empirically compare the proposed method against them.\nPlease note that Probabilistic and Box Embeddings are relatively less related to the proposed method as they working on phrase embeddings and used pre-trained word embeddings to feed an LSTM for learning phrase embeddings.\n\n[1] Poincare Embeddings [Nikel and Kiela] - NIPS 2017\n[2] Specialising Word Vectors for Lexical Entailment [Vulic and Mrksic] - NAACL-HLT 2018\n\n-->Q7: In equation four of section 3, t_max appears for the first time. This equation maybe part of the GLOVE objective, but a one-sentence explanation of t_max might be needed here.\n\nAns: Yes, it is the weighting function of GloVe so that it becomes relatively small for words of large frequency and set to 100  as stated in section (4.1).\n\n-->Q8: at the end of section 3, the calculation of gradients for different parameters are given, but the optimization is actually performed by AdaGrad. Maybe it would be good to move these equations to the appendix.\n\nAns: The gradients equations have been moved to the appendix in the updated version of the paper.\n\n-->Q9: In section 4.1 experiment set up, the wordnet training data is generated by performing transitive closure I assume? How does the wordnet synsets get mapped to its surface form in order to do further training and evaluation?\n\nAns: We lemmatise the corpus and use the form given in the WordNet as the surface form.""}"		S1xf-W5paX	Sklm9dog44	AKBC.ws/2019/Conference/-/Paper26/Official_Comment	['AKBC.ws/2019/Conference/Paper26/Reviewers/Unsubmitted']	3		['everyone']	ryg8XPy4fE	['AKBC.ws/2019/Conference/Paper26/Authors']	1548953738863		1548953738863	['AKBC.ws/2019/Conference/Paper26/Authors', 'AKBC.ws/2019/Conference']
51	1548953125709	{'title': 'More recent relevant work have been added', 'comment': '-->Q1: There has been significant interest in recent work on representations aiming for exactly this goal, including:\nPoincare Embeddings [Nikel and Kiela], \nOrder Embeddings [Vendrov et al], \nProbabilistic Order Embeddings [Lai and Hockenmaier], \nBox embeddings [Vilnis et al]. \nIt seems that there should be empirical comparisons to these methods. \n\nAns: Thank you for the suggestion. Poincare seems to be an excellent fit to be empirically compared against the proposed method, as they both share a similar spirit to explicitly learn hierarchical word embeddings rather than hypernymy-specific embeddings\nWe have now updated the paper with two more related works, Poincare[1] and LEAR[2] and empirically compare the proposed method against them.\nWe have also updated the paper with a new evaluation task (section 4.4) to test the proposed method on a graded lexical entailment as suggested by reviewer3.\nThe proposed method reports an improvement over most of the prior works, including Poincare, in the three tasks (hypernym detection, graded lexical entailment, and the hierarchical path completion), except for the LEAR in two datasets.\nMore interestingly, Poincare seems to perform well in the proposed hierarchical path completion task in contrast to the other methods apart from HWE. The fact that Poincare embeddings, a hierarchical word embeddings learn method, reports good performance on this hierarchical path completion task, suggests that this is an appropriate task for evaluating hierarchies and embeddings. \n\nPlease note that Probabilistic and Box Embeddings are relatively less related to the proposed method as they work on phrase embeddings and use pre-trained word embeddings to feed an LSTM for learning phrase embeddings.\n\n[1] Poincare Embeddings [Nikel and Kiela] - NIPS 2017\n[2] Specialising Word Vectors for Lexical Entailment [Vulic and Mrksic] - NAACL-HLT 2018\n\n\n-->Q2: I found the order of presentation awkward, and sometimes hard to follow. For example, I would have liked to see a clear explanation of test-time inference before the learning objective was presented, and I’m still left wondering why there is not a closer correspondence between the multiple inference methods described (in Table 3) and the learning objective.\n\nAns: We use the hierarchical word embeddings produced by the proposed method (HWE) in three tasks: hypernym detection (section 4.3), graded lexical entailment (section 4.4) and the hierarchical path completion (section 4.5).\nEach task has different inference methods, and that is the reason why we describe the inference methods under each section separately and not in the method for learning hierarchical word embeddings section.\nFor example, for the first task (section 4.3) we used the concatenation approach as stated in the section.\nSimilarly, for the graded lexical entailment task, we used the inference method described in section 4.4 (Eq. (6)).\nThe inference methods described in Table 3 are specific to the hierarchical path completion task.\nAmong the different inference methods compared in Table 3, ADD corresponds closely to the training objective used by the HWE learning method we propose (see Eq. (2)). This might explain why ADD turns out to be the best inference methods in Table 3.\n\n\n-->Q3: I would also have liked to see a clear motivation for why the GloVE embedding is compatible with and beneficial for the hypernym task. “Relatedness” is different than “hypernymy.”\n\nAns: All the datasets in the hypernym identification task are pairwise relation data, and it could be the case that it easier for such distributional methods to pick the hypernymy, where hypernymy tend to occur in similar context.'}		S1xf-W5paX	ByeAXLjlEV	AKBC.ws/2019/Conference/-/Paper26/Official_Comment	['AKBC.ws/2019/Conference/Paper26/Reviewers/Unsubmitted']	2		['everyone']	ByxfoMkHM4	['AKBC.ws/2019/Conference/Paper26/Authors']	1548953125709		1548953125709	['AKBC.ws/2019/Conference/Paper26/Authors', 'AKBC.ws/2019/Conference']
52	1542459642242	{'title': 'Joint Learning of Hierarchical Word Embeddings from a Corpus and a Taxonomy', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper26/Authors'], 'keywords': ['Hierarchical Embeddings', 'Word Embeddings', 'Taxonomy'], 'TL;DR': 'We presented a method to jointly learn a Hierarchical Word Embedding (HWE) using a corpus and a taxonomy for identifying the hypernymy relations between words.', 'abstract': 'Identifying the hypernym relations that hold between words is a fundamental task in NLP. Word embedding methods have recently shown some capability to encode hypernymy. However, such methods tend not to explicitly encode the hypernym hierarchy that exists between words. In this paper, we propose a method to learn a hierarchical word embedding in a speciﬁc order to capture the hypernymy. To learn the word embeddings, the proposed method considers not only the hypernym relations that exists between words on a taxonomy, but also their contextual information in a large text corpus. The experimental results on a supervised hypernymy detection and a newly-proposed hierarchical path completion tasks show the ability of the proposed method to encode the hierarchy. Moreover, the proposed method outperforms previously proposed methods for learning word and hypernym-speciﬁc word embeddings on multiple benchmarks.', 'pdf': '/pdf/1bc7582c4036fe878cc9b187ea4c7f3e95007c28.pdf', 'archival status': 'Non-Archival', 'subject areas': ['Machine Learning', 'Natural Language Processing', 'Knowledge Representation'], 'paperhash': 'anonymous|joint_learning_of_hierarchical_word_embeddings_from_a_corpus_and_a_taxonomy', '_bibtex': '@inproceedings{    \nanonymous2019joint,    \ntitle={Joint Learning of Hierarchical Word Embeddings from a Corpus and a Taxonomy},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=S1xf-W5paX},    \nnote={under review}    \n}'}		S1xf-W5paX	S1xf-W5paX	AKBC.ws/2019/Conference/-/Blind_Submission	[]	26	H1lSaLU2am	['everyone']		['AKBC.ws/2019/Conference']	1542459642242		1548952818736	['AKBC.ws/2019/Conference']
53	1548949438962	{'comment': 'Hi, I have a follow-up question regarding condition Batchnorm. What are the values of the embedding_dimension used for G with different resolutions? I could not find this information in the paper. Thanks.', 'title': 'embedding_dimension'}		B1xsqj09Fm	HklDpwqgVV	ICLR.cc/2019/Conference/-/Paper563/Public_Comment	[]	11		['everyone']	rJlFIzT0YX	['(anonymous)']	1548949438962		1548949438962	['(anonymous)', 'ICLR.cc/2019/Conference']
54	1548947633960	{'comment': 'Hi, thanks for this interesting work. Do you have public available codes to reproduce the results?', 'title': 'Avaliable Code'}		HkxLXnAcFQ	Bye5ng5xEV	ICLR.cc/2019/Conference/-/Paper1361/Public_Comment	[]	4		['everyone']	HkloHv_hxV	['(anonymous)']	1548947633960		1548947633960	['(anonymous)', 'ICLR.cc/2019/Conference']
55	1548947574833	"{'pros': 'This paper proposes and demonstrates a deep learning method for\nreconstruction and segmentation of tomographic ultrasound images that\ncapture tissue properties. The authors focus on breast tumor\nsegmentation as the application. The initialize the reconstruction by\napplying the commonly used optimization-based method, followed by a\nneural network to refine the reconstructed image and segment the\ntumor. The authors demonstrate reasonable accuracy in synthetically\ngenerated computational breast tumor phantoms.\n\nThe paper tackles an important problem, and the authors take a\nreasonable approach. Unfortunately, the experimental demonstration\nfocuses entirely on synthetic images (both training and testing) of\nthree tumor phantoms and fails to convince that the proposes approach\nwill generalize to real images. The paper could use a thorough editing\nto remove tutorial text and add specific details on what the authors\ndid. Overall, the paper falls short and I recommend rejection.', 'cons': 'The paper\'s innovation is in employing neural networks to improve\nreconstruction, rather than on the methodology of the neural networks\nthemselves. This suggests two changes to make this a stronger\ncontribution. First, the paper should be rewritten to substantially\nshorten the exposition on the tomographic reconstruction in the\nintroduction of the paper, and to add missing details as discussed\nbelow. Second, the experimental evaluation must be strengthened to\ninclude real images. At this preliminary stage, it would be fine to\nuse (physical) phantoms. Without such a demonstration, it is hard to\nbelieve that the methods trained on digital phantoms with limited\nanatomical variability and well controlled noise would generalize to\nimages acquired by a real imaging system.\n\nI was confused by the experimental setup where the authors train on\n""unrotated"" images, and test on images that were rotated. Since the\nauthors provide no details on data augmentation, it is hard to tell\nhow rotation at test time would interact with augmentation during\ntraining. The table indicates that the performance drops when the test\nimages are rotated, which suggests the authors need to augment more\naggressively.\n\nThe paper is very unclear about how training and testing data sets are\ngenerated. In particular, the authors should clarify that no phantom\ncontributed to both data sets (even not different images of the same\nphantom). Sharing examples (e.g., different images of the same\nphantom) between training and testing image sets would explain very\nhigh accuracy, which I would expect to drop of the two data sets are\ntruly disjoint.\n\nIn conclusion, the paper would be strengthened by careful editing to\ninclude relevant details and by including evaluation on real\nimages.', 'rating': '1: strong reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		H1lkgwfeg4	rylyYe9l4N	MIDL.io/2019/Conference/-/Paper78/Official_Review	[]	3		['everyone']	H1lkgwfeg4	['MIDL.io/2019/Conference/Paper78/AnonReviewer2']	1548947574833		1548947574833	['MIDL.io/2019/Conference/Paper78/AnonReviewer2']
56	1548947482628	{'pros': 'This paper proposes a novel data augmentation approach based on the\nsuperpixel representation of the image. In particular, the authors\ngenerate superpixel parcelation of the training images and add a term\nto the cost function that penalizes classifier (segmentor) errors when\napplied to the superpixelized image. The authors evaluate their\napproach on several biomedical image data sets and demonstrate robust\nimprovement in the segmentation accuracy. The paper offers an\ninteresting idea that others in the community might find useful to\nimprove the robustness of their models.\n', 'cons': 'The innovation is relatively minor.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		H1gLLOgxeE	Bke77eclVN	MIDL.io/2019/Conference/-/Paper55/Official_Review	[]	3		['everyone']	H1gLLOgxeE	['MIDL.io/2019/Conference/Paper55/AnonReviewer3']	1548947482628		1548947482628	['MIDL.io/2019/Conference/Paper55/AnonReviewer3']
57	1538087764182	{'title': 'Distribution-Interpolation Trade off in Generative Models', 'abstract': 'We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models. Our work revolves around the phenomena arising while decoding linear interpolations between two random latent vectors -- regions of latent space in close proximity to the origin of the space are oversampled, which restricts the usability of linear interpolations as a tool to analyse the latent space. We show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations. We prove that there is a trade off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean. We use the multidimensional Cauchy distribution as an example of the prior distribution, and also provide a general method of creating non-linear interpolations, that is easily applicable to a large family of commonly used latent distributions.', 'keywords': ['generative models', 'latent distribution', 'Cauchy distribution', 'interpolations'], 'authorids': ['damian.lesniak@doctoral.uj.edu.pl', 'igor.sieradzki@doctoral.uj.edu.pl', 'igor.podolak@uj.edu.pl'], 'authors': ['Damian Leśniak', 'Igor Sieradzki', 'Igor Podolak'], 'pdf': '/pdf/c8cf3f89c46e12f811b33ef4b0dc06b18fa6e477.pdf', 'paperhash': 'leniak|distributioninterpolation_trade_off_in_generative_models', 'TL;DR': 'We theoretically prove that linear interpolations are unsuitable for analysis of trained implicit generative models. ', '_bibtex': '@inproceedings{\nleśniak2018distributioninterpolation,\ntitle={Distribution-Interpolation Trade off in Generative Models},\nauthor={Damian Leśniak and Igor Sieradzki and Igor Podolak},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyMhLo0qKQ},\n}'}		SyMhLo0qKQ	SyMhLo0qKQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	211	r1gNI7nDtQ	['everyone']		['ICLR.cc/2019/Conference']	1538087764182		1548942047024	['ICLR.cc/2019/Conference']
58	1548268985956	{'pros': 'This paper proposed an end-to-end framework that reconstructs B-mode US images directly from the acquired\nraw US signals.\n\nThe technical contribution seems sound.\n', 'cons': 'The motivation for this architecture is unclear.\nThe paper need to be compared with state-of-the-art methods.\nSome technical details are missing.\nIt is not clear to me why the proposed DeepFormer architecture should be better than traditional Deep Learning.\n- Sec. 3. The authors write that a comprising of data from 6 healthy participants and 2541 total frames. Can you describe how, or provide a reference?\n- There are 8 classes in total for the classification task and Table2 shows a single accuracy for all classes. You must mention the accuracy for each class. \n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		SygEcVLHl4	BklfaHV8QV	MIDL.io/2019/Conference/-/Paper158/Official_Review	[]	2		['everyone']	SygEcVLHl4	['MIDL.io/2019/Conference/Paper158/AnonReviewer2']	1548268985956		1548929948969	['MIDL.io/2019/Conference/Paper158/AnonReviewer2']
59	1538087762337	"{'title': 'Practical lossless compression with latent variables using bits back coding', 'abstract': ""Deep latent variable models have seen recent success in many data domains. Lossless compression is an application of these models which, despite having the potential to be highly useful, has yet to be implemented in a practical manner. We present '`Bits Back with ANS' (BB-ANS), a scheme to perform lossless compression with latent variable models at a near optimal rate. We demonstrate this scheme by using it to compress the MNIST dataset with a variational auto-encoder model (VAE), achieving compression rates superior to standard methods with only a simple VAE. Given that the scheme is highly amenable to parallelization, we conclude that with a sufficiently high quality generative model this scheme could be used to achieve substantial improvements in compression rate with acceptable running time. We make our implementation available open source at https://github.com/bits-back/bits-back ."", 'keywords': ['compression', 'variational auto-encoders', 'deep latent gaussian models', 'lossless compression', 'latent variables', 'approximate inference', 'variational inference'], 'authorids': ['james.townsend@cs.ucl.ac.uk', 'thomas.bird@cs.ucl.ac.uk', 'david.barber@ucl.ac.uk'], 'authors': ['James Townsend', 'Thomas Bird', 'David Barber'], 'TL;DR': 'We do lossless compression of large image datasets using a VAE, beat existing compression algorithms.', 'pdf': '/pdf/08cc19e14ef95359074bad044b4451196660e84c.pdf', 'paperhash': 'townsend|practical_lossless_compression_with_latent_variables_using_bits_back_coding', '_bibtex': '@inproceedings{\ntownsend2018practical,\ntitle={Practical lossless compression with latent variables using bits back coding},\nauthor={James Townsend and Thomas Bird and David Barber},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryE98iR5tm},\n}'}"		ryE98iR5tm	ryE98iR5tm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	201	H1eapj0dKQ	['everyone']		['ICLR.cc/2019/Conference']	1538087762337		1548928956905	['ICLR.cc/2019/Conference']
60	1548310544672	{'title': 'Thanks for the critical review. We were intrigued by your comments. Our response is as follows:  (1/2)', 'comment': '\nComment 1:\nIncomplete evaluation and mention detection not improving on FIGER as the results reported for the baseline used in our work is different for the results reported in the original FIGER paper (Table 2 of http://xiaoling.github.io/pubs/ling-aaai12.pdf)\n\nResponse to comment 1:\nThis is indeed a very interesting observation. We would like to clarify here that the results reported in the FIGER paper and in the baseline of the proposed work are not comparable due to the following reasons:\n1. The datasets used are different as the FIGER paper does not use the FIGER dataset for a part of their experiments. This might seems surprising and can be easily missed as it is mentioned in the footnote 4 of the FIGER paper, decoupled with the result table.\n2. The learning models are different.\n\nWe followed a simple and consistent evaluation strategy for Table 5, which we summarize below:\n1. Entity Recognition can be decomposed into two sub-problems: Entity Detection and Entity Classification\n2. Use current state-of-the-art learning model for both the sub-problems in a pipelined manner and report the result for the entity recognition task. \n3. The dataset used for the sub-problems is the same, as mentioned in Table 5. There is other interpretation of results as that of the results reported in the FIGER paper.\n\nWe apologize for not bringing the correct interpretation of the results in the FIGER paper beforehand, which could have avoided this confusion.\n\nWe would like to add here that mention detection is improving on both the datasets: FIGER and 1k-WFB-g. Please refer to Table 4. The resulting analysis of entity detection is available in Section 5.2.4. Also, there is an analysis of entity detection in Section 3. The improvement for mention detection is 0.08% F1 on FIGER dataset and 15.21% F1 on 1k-WFB-g. Note that although the improvement is small on FIGER, this improvement is over a model trained on a manually annotated (complete noise-free) dataset which matches well with FIGER dataset as both have predominant mentions of person, location and organization entity types. When we compare with a distantly supervised dataset, we have an improvement of 37.11% on FIGER dataset.\n\nWe hope that this clarification will resolve the concerns about incomplete evaluation. We have updated Section 3 (last paragraph) and Section 5.2.2 (last paragraph) to clearly state that the results in FIGER paper are not directly comparable. \n\nWe will be glad to answer any other questions related to the evaluation. \n\n\nComment 2:\nResemblance to http://www.aclweb.org/anthology/P18-2015\n\nResponse to comment 2:\nWe agree that at an abstract level, the pruning of noisy dataset idea is the same. However, there are fundamental differences between these works:\n1. The tasks are different, the referred paper by the reviewer is removing the false positives for relation extraction task and our work is removing false positives and false negatives for the entity detection task.\n2. The fundamental approach for these two different tasks is also different, where the referred work is a modeling approach and our being a heuristic approach.\n\nIt is possible to add a learning model to make certain decisions instead of a pure heuristic approach used in our framework. We consider this as a future work. This is also suggested by Reviewer 1.\n\nWe would like to mention here that the main contributions of this work are:\n1. An analysis that the existing coarsely annotated datasets such as CoNLL are not suitable to detect entity mentions in the context of FIGER and TypeNET type hierarchies. (Section 3)\n2. A framework to automatically construct quality datasets suitable for fine entity detection and typing. (Section 4)\n3. Establish the state-of-the-art baselines on FIGER and new manually annotated corpus which is much richer than FIGER evaluation corpus. (Section 5)\n\nAdditionally, in the final version, we will update the related work section incorporating a reference to the work suggested by the reviewer along with other works which are also similar at an abstract level. \n\n\nComment 3:\nDescription of the new datasets is missing\n\nResponse to comment 3:\nWe would like to specify that the generated datasets description is mentioned in Section 5, overview paragraph. We also report various statistics of the datasets in Table 2. We have updated the Section 5.2.2 to include a description of evaluation datasets.\n\nWe hope that our response will clarify your concerns. '}		HylHE-9p6m	S1lKf_A8mE	AKBC.ws/2019/Conference/-/Paper45/Official_Comment	['AKBC.ws/2019/Conference/Paper45/Reviewers/Unsubmitted']	2		['everyone']	H1enOAVBGE	['AKBC.ws/2019/Conference/Paper45/Authors']	1548310544672		1548915578754	['AKBC.ws/2019/Conference/Paper45/Authors', 'AKBC.ws/2019/Conference']
61	1538087776118	{'title': 'Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization', 'abstract': 'Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling.  \nIn our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy.  Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.', 'keywords': ['Hierarchical reinforcement learning', 'Representation learning', 'Continuous control'], 'authorids': ['osa@mfg.t.u-tokyo.ac.jp', 'voot.tangkaratt@riken.jp', 'sugi@k.u-tokyo.ac.jp'], 'authors': ['Takayuki Osa', 'Voot Tangkaratt', 'Masashi Sugiyama'], 'TL;DR': 'This paper presents a hierarchical reinforcement learning framework based on deterministic option policies and mutual information maximization. ', 'pdf': '/pdf/203bab11dc8eb66646694b8d563258a05059a345.pdf', 'paperhash': 'osa|hierarchical_reinforcement_learning_via_advantageweighted_information_maximization', '_bibtex': '@inproceedings{\nosa2018hierarchical,\ntitle={Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization},\nauthor={Takayuki Osa and Voot Tangkaratt and Masashi Sugiyama},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyl_vjC5KQ},\n}'}		Hyl_vjC5KQ	Hyl_vjC5KQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	278	S1gPflx5KQ	['everyone']		['ICLR.cc/2019/Conference']	1538087776118		1548910905737	['ICLR.cc/2019/Conference']
62	1548310672963	{'title': 'Thanks for the critical review. We were intrigued by your comments. Our response is as follows: (2/2)', 'comment': '\nComment 4:\nSupervision being used in existing entity typing systems: (TypeNet and Open-type)\n\nResponse to comment 4:\nWe would like to mention here that we proposed an approach to generate a training dataset for Fine-grained Entity Recognition (FgER) task. FgER task can be decomposed into two subproblems: Fine entity detection and fine entity typing. TypeNET and Open-type fall into the latter category. Our work has shown that as the number of types increase, entity detection task becomes the bottleneck for FgER task. Existing works such as FIGER, TypeNet and open-type do not address this issue. We established this point in Section 3 and proposed a framework for generating datasets to remove the bottleneck for FIGER and TypeNET hierarchy. Our benchmarking experiments were focused on providing quality estimates for the generated datasets. Thus we kept the learning model fixed and varied the dataset to demonstrate that the generated dataset help learning model to generalized better.\n\nAdditionally, in the case of open-type, it is an elusive problem of detecting 10k+ types of entities as many of these types go beyond knowledge bases.\n\nWe hope that our response will clarify your concerns. \n'}		HylHE-9p6m	BkgFcO0Um4	AKBC.ws/2019/Conference/-/Paper45/Official_Comment	['AKBC.ws/2019/Conference/Paper45/Reviewers/Unsubmitted']	3		['everyone']	H1enOAVBGE	['AKBC.ws/2019/Conference/Paper45/Authors']	1548310672963		1548908555244	['AKBC.ws/2019/Conference/Paper45/Authors', 'AKBC.ws/2019/Conference']
63	1548310833143	{'title': 'Thanks for the review. Indeed the issues highlighted are important and we hope that our response will address these issues. ', 'comment': '\nComment 1:\nThe approach is tailored towards Wikipedia text.\n\nResponse to comment 1: \nWe agree that all our experiments use Wikipedia as a text source. In fact, to the best of our knowledge, only Wikipedia text exists in public domain that is highly interlinked to knowledge bases such as Freebase, DBpedia and WikiData. Our approach fundamentally requires a linked text corpus, and the current options are limited to Wikipedia only.\n\nAlso, we would like to mention here is that FIGER evaluation dataset consists of sentences sampled from news and specialized magazines (photography and veterinary domain). Thus, it has a different writing style as compared with Wikipedia text. In Table 4, we can observe that the entity detection model trained on our dataset outperforms models trained on other existing datasets. Hence, the learning model is able to generalize beyond the Wikipedia style text.\n\nFurther, for several other domains, such as scientific articles, conversation text, where directly utilizing the generated datasets might not yield good performance, we consider our datasets as a source dataset for transfer learning. The generated dataset is richer in entity types (1k+ from several domains) and is very large, which makes it a good candidate to use as a source dataset for transfer learning setting when the task is to do NER for a specific domain text.\n\nWe have updated the discussion section (last paragraph) and 5.2.4 section (3rd paragraph, last sentence) to reflect this view.\n\n\nComment/Question 2:\nAre discarded and retained sentences fundamentally same or different? \n\nResponse to comment/question 2:\nThis is an interesting question. To answer the question we analyzed the discarded and retained sentences on the following parameters:\n1. Are the discarded sentences longer on average?\n2. Does the entity mention in discarded sentences longer on average?\n3. Is there a fundamental change in the token and entity mention distribution of discarded and retained sentences?\n\nThis analysis is added as Appendix B to the paper. The summary of the analysis is that, other than the sentence length distribution, there is not a significant change in other distributions. The mean sentence length in the retained corpus is around 22, whereas in the discarded corpus is around 27. \n\nNote that, this result has a subtle interpretation. We observe that in the discarded sentences, there are more than 100k sentences with the length greater than 100 tokens. In fact, a corpus constituting of only these longer sentences is larger than several news-domain NER datasets. We observe that the majority of these sentences are caused due to incorrect sentence segmentations or they follow a list like patterns such as:\n1. PER, PER, PER, PER, PER …\n2. PER - PER - PER - PER - PER …\n3. Director (Movie), Director (Movie), Director (Movie), ....\n4. Project (year), project (year), project (year), …\n5. NUMBER NUMBER NUMBER NUMBER … \n6. Movie (year), movie (year), movie (year), …\n7. PER | PER | PER | PER | PER ...\n\nThe largest sentence length in discarded sentences is 6564 tokens!! \n\nOur dataset also captures these long sentences but the number is far less: 7664 sentences with the length greater than 100. The largest sentence length in the retained sentences is 624 tokens. \n\nAlthough, being a basic analysis, the analysis conveys that these longer sentences might not be suitable for applications where NER systems are used. To support this claim, we plotted the sentence length distribution of five NER datasets from different domains in Figure 7 and 8. The result conveys that, sentences longer than 100 words occur rarely in these domains and the sentence length distribution in the retained sentences is closer to the sentence length distribution in these domains when compared with the discarded sentences.\n\n\nComment 3: \nscales to thousands of types is a bit overstatement\n\nResponse to comment 3:\nWe agree with the reviewer that the current experiments demonstrate the efficacy of the framework for up to 1.1k types. We would like to add on this that the Freebase (which has 1.1k entity types) can be replaced with WikiData (which as 15k entity types) in the HAnDS framework to generate a training dataset constituting of 15k entity types. However, since we don’t have any empirical results on this, we will modify the scalability claim in the paper.\n\n\nWe hope that our response will provide a better insight into the proposed work and will clarify your concerns.\n'}		HylHE-9p6m	BylF4KALQ4	AKBC.ws/2019/Conference/-/Paper45/Official_Comment	['AKBC.ws/2019/Conference/Paper45/Reviewers/Unsubmitted']	4		['everyone']	HyelSNY5gE	['AKBC.ws/2019/Conference/Paper45/Authors']	1548310833143		1548907162146	['AKBC.ws/2019/Conference/Paper45/Authors', 'AKBC.ws/2019/Conference']
64	1548310059651	{'title': 'Thanks for an encouraging review! Here is our response to your questions:', 'comment': '\nQuestion 1: \nIn Table 3, can you also report the differences in entities?\n\nResponse to question 1:\nWe have updated the Table 3 and section 5.1 to report differences in entities. The summary of the table is that our corpus has around 1.5 times more unique entities when compared with the NDS approach to annotate the same sentences. Also, there are slight changes due to a mistake in row 1 and 3 of WikiFbT numbers. The entity mentions annotated are now even more!  (42 million -> 45 million and 24 million -> 27 million).\n\nQuestion 2:\nAre you planning to release the two training datasets (and if possible the code ?)\n\nResponse to question 2:\nAll experiments reported in this work will be easily reproducible. A note for the same is included in the paper.\n\nWe are planning to publicly release the following things:\n1. The preprocessed Wikipedia text along with the preprocessed Freebase to facilitate the dataset construction research.\n2. The code used to generate the two training datasets (WikiFbF and WikiFbT) along with the generated datasets.\n3. The code used to train learning models and the evaluation corpus.\n\nResponse to suggestion:\nThanks for the great suggestion. Indeed there is a possibility of further improvement by replacing some of the heuristics with learning models. We consider this as a future work.\n'}		HylHE-9p6m	SygEEL087N	AKBC.ws/2019/Conference/-/Paper45/Official_Comment	['AKBC.ws/2019/Conference/Paper45/Reviewers/Unsubmitted']	1		['everyone']	Hkl7S1hHfN	['AKBC.ws/2019/Conference/Paper45/Authors']	1548310059651		1548906927683	['AKBC.ws/2019/Conference/Paper45/Authors', 'AKBC.ws/2019/Conference']
65	1542459671876	{'title': 'Overcoming Annotation Scarcity for Shallow Semantic Parsing in Scientific Procedural Text', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper37/Authors'], 'keywords': ['shallow semantic parsing', 'event extraction', 'procedural text', 'weakly supervised', 'n-ary relations'], 'TL;DR': 'Weakly supervised method for unlabelled shallow semantic structures and an associated dataset of materials science procedural text.', 'abstract': 'Materials science literature contains millions of synthesis routes described in unstructured natural language text. The large scale mining of these synthesis procedures promises to allow a deeper scientific understanding of materials synthesis and the automated planning of synthesis procedures. This however requires the construction of knowledge bases of synthesis procedures from natural language text. A major bottleneck in extraction of these structured synthesis representations from text is the lack of labeled data on which to train or evaluate extraction models. To address this bottleneck, we introduce a dataset of 230 synthesis procedures annotated with the labeled graph structures which express the semantics of the synthesis sentences. The nodes are operations and arguments in the synthesis, while labeled edges specify relations between the nodes. Next, we describe a novel weakly supervised approach to the extraction of unlabeled graph structures from synthesis sentences. The proposed model is framed as a matrix completion model parameterized by a DeepSet neural network \\cite{DeepSets2017}. The proposed model outperforms a strong heuristic baseline by 4 points precision and 2 points F1.\n', 'pdf': '/pdf/7ba9192c9231638144e1e10fa0dae1cdd009ddb6.pdf', 'archival status': 'Archival', 'subject areas': ['Machine Learning', 'Natural Language Processing', 'Information Extraction', 'Knowledge Representation', 'Applications: Science'], 'paperhash': 'anonymous|overcoming_annotation_scarcity_for_shallow_semantic_parsing_in_scientific_procedural_text', '_bibtex': '@inproceedings{    \nanonymous2019overcoming,    \ntitle={Overcoming Annotation Scarcity for Shallow Semantic Parsing in Scientific Procedural Text},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=S1eg7-9pp7},    \nnote={under review}    \n}'}		S1eg7-9pp7	S1eg7-9pp7	AKBC.ws/2019/Conference/-/Blind_Submission	[]	37	r1l1kEEa6m	['everyone']		['AKBC.ws/2019/Conference']	1542459671876		1548904383592	['AKBC.ws/2019/Conference']
66	1542459654103	{'title': 'Time-resolved compound repositioning predictions on a texted-mined knowledge network', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper31/Authors'], 'keywords': ['Compound Repositioning', 'Model Evaluation', 'Machine Learning', 'Text-mined Databases', 'SemMedDB', 'UMLS. DrugCentral', 'PMIDs'], 'TL;DR': 'We partitioned a text-mined database by time, and performed compound repositioning by training on contemporary data and testing on future indications.', 'abstract': 'Computational compound repositioning has the potential for identifying new uses for existing drugs. New algorithms and data source aggregation strategies provide ever-improving results via in silico metrics. However, the number of compounds successfully repositioned via computational screening remains low. Using a text-mined database, we applied a previously described network-based computational repositioning algorithm, yielding strong results via cross-validation, averaging 0.95 AUROC on test-set indications. The text-mined data was then used to build networks corresponding to different time-points in biomedical knowledge. Training the algorithm on contemporary and indications and testing on future showed a marked reduction in performance, peaking in performance metrics with the 1985 network at an AUROC of .797. Examining performance reductions due to removal of specific types of relationships highlighted the importance of drug-drug and disease-disease similarity metrics. Using data from future timepoints, we demonstrate that further acquisition of these kinds of data may help improve computational results. We also suggest focusing efforts on improving algorithmic performance in a time-resolved paradigm may further improve computational repositioning predictions.', 'archival status': 'Non-Archival', 'subject areas': ['Machine Learning', 'Applications: Biomedicine'], 'pdf': '/pdf/9dcd13acd620c80d7e886c832268e995299e5492.pdf', 'paperhash': 'anonymous|timeresolved_compound_repositioning_predictions_on_a_textedmined_knowledge_network', '_bibtex': '@inproceedings{    \nanonymous2019time-resolved,    \ntitle={Time-resolved compound repositioning predictions on a texted-mined knowledge network},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=BkeCW-q6aQ},    \nnote={under review}    \n}'}		BkeCW-q6aQ	BkeCW-q6aQ	AKBC.ws/2019/Conference/-/Blind_Submission	[]	31	HJeTWVAhT7	['everyone']		['AKBC.ws/2019/Conference']	1542459654103		1548900047013	['AKBC.ws/2019/Conference']
67	1548899589787	{'pros': 'This study evaluates the performance of U-Net based CNNs  for segmenting mLNs on I-131 RxWBS. Different sampling strategies (i.e. class equivalence balance batch sampling, hard sampling) and loss functions have been investigated for the un-balanced segmentation.  The study was evaluated  on a test of 95 I-131 RxWBS and demonstrated that sampling based UNet-H outperformed other learning strategies.\n\nThis is an important and relevant topic. The study was useful for clinical practice. In general, the  paper is well written and easy to understand.\n', 'cons': 'The technical novelty of this work is limited. There are many studies in the literature addressing the unbalanced dataset segmentation problem. For instance,  \n1. A Skeletal Similarity Metric for Quality Evaluation of Retinal Vessel Segmentation, TMI, 2018. \n2. The Impact of Imbalanced Training Data for CNN, 2015.\nThe authors should include a more comprehensive survey for methods targeting this issue.\n\nBoth weighted loss function and dice loss have been proposed in the past, used and validated in many existing studies. The authors should provide references to the existing works.\n\nWhat are the characteristics of the training and testing  datasets. How many mLN and remnant are included and what are average sizes and ratios?\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		S1evJSAxgN	SJg0WHRk4N	MIDL.io/2019/Conference/-/Paper136/Official_Review	[]	3		['everyone']	S1evJSAxgN	['MIDL.io/2019/Conference/Paper136/AnonReviewer2']	1548899589787		1548899589787	['MIDL.io/2019/Conference/Paper136/AnonReviewer2']
68	1542459690173	{'title': 'MERMAID: Production-level Knowledge Base Construction System', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper44/Authors'], 'keywords': ['Knowledge base', 'information extraction', 'entity match', 'knowledge base refinement'], 'abstract': 'Knowledge bases play crucial roles in a wide variety of information systems, such as search engines and intelligent personal assistants. For responding constantly fluctuating user information demands, we aim to construct a large-scale and well-structured global knowledge base from the world’s evolving data. In this paper, we discuss enterprise-specific issues with knowledge base construction and present how to deal with these issues in our construction system called “MERMAID.” To maintain the quality of our knowledge base at the production-level, MERMAID is carefully designed to incorporate various automatic and manual validation methods. We partly leverage manual validation methods to deal with business requirements and user feedbacks quickly since it is difficult to filter out all incorrect facts automatically in practice. Moreover, we propose a novel information extraction method that obtains reliable factual information from Web-crawled data on the basis of distant supervision. Our constructed knowledge base is already utilized in real-world Japanese Web services, and the number of entities in it keeps growing steadily.', 'pdf': '/pdf/2a84263819598ce1b740d43a98bc3ef10b946240.pdf', 'archival status': 'Non-Archival', 'subject areas': ['Information Extraction', 'Databases', 'Knowledge Representation', 'Semantic Web'], 'paperhash': 'anonymous|mermaid_productionlevel_knowledge_base_construction_system', '_bibtex': '@inproceedings{    \nanonymous2019mermaid:,    \ntitle={MERMAID: Production-level Knowledge Base Construction System},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=SJxfV-q6Tm},    \nnote={under review}    \n}'}		SJxfV-q6Tm	SJxfV-q6Tm	AKBC.ws/2019/Conference/-/Blind_Submission	[]	44	SJlXskAqam	['everyone']		['AKBC.ws/2019/Conference']	1542459690173		1548896870341	['AKBC.ws/2019/Conference']
69	1548886496097	{'pros': '- The paper addresses an important problem, namely the defense of CNNs against adversarial attacks.\n\n- In this context, the proposed architecture and strategy seem to be novel.\n\n- A thorough series of experiments is proposed, in particular in terms of internal baselines which include ablation studies and experiments studying the effect of the hyperparameters balancing the introduced losses.', 'cons': '- Two important references are missing from my point of view:\n\n(1) Vincent et al [a] proposed a very similar training of denoising auto-encoders which also optimizes the quality of the reconstruction of synthetically deformed samples. The obtained feature representations were demonstrated to improve the performance of classifiers subsequently using them.\n\n(2) In Ghifary et al [b], a similar architecture was proposed where an auto-encoder and a CNN sharing its encoding layers are jointly trained. The context in [b] is however different (unsupervised domain adaptation).\n\nTaking these works into account, I believe that the novelty of the proposed mechanism is a bit lower than claimed. It is not necessarily a problem since the current paper addresses the different scenario of adversarial attacks, but the paper still has to be revised accordingly.\n\n- Most importantly, I am concerned about some parts of the evaluation and the reported results. Indeed, for the huge majority of proposed defenses, it seems that the adversarial examples from the C&W attack based on (Carlini & Wagner, 2017) are surprisingly weak: in fact, the accuracy of the targetted network on these adversarial examples is even higher than on the original unmodified examples, i.e. before running the attack. I might be misunderstanding something but I would at the very least expect an adversarial deformation of an example to do as good as the example it is based on, i.e. as good as doing nothing. This sounds like a quite odd result and should probably be double-checked – again, unless I missed something here.\n\n- I found that the paper has in general a few issues in terms of clarity, some of which are reflected in the points below. The English should also probably be revised before publication, but I did not feel that the language itself was an issue to understand the paper.\n\n- The paper mentions at several places (abstract, Section 3.1 and Table 1) that, in addition to the two medical datasets, the evaluation was performed on a dataset of natural images (CIFAR10). However I could not find any results on this dataset. I believe the corresponding reference “(cif,2009)” also has an issue, and the two first references in the list of references are missing their authors or title.\n\n- This point is probably only due to a slight lack of clarity, but I find a bit misleading to say that the proposed approach is “independent of model structures” (in the abstract). Since the encoding layers of the auto-encoder are shared with the CNN, a given CNN architecture imposes the encoding layers, and it still must decided at which layer of the CNN one should create the branch towards the decoding layers. In this sense, the performance certainly depends on this choice and on the architecture of the original CNN. It is perfectly acceptable and I agree that the idea can be adapted to many architectures, but this still contrasts with (Liao et al. 2017) on Figure 2 where the denoising network and the CNN are completely separated, allowing to design fully independently the two architectures.\n\n- In the tables used for results, the numbers in bold do not always reflect the best performance for each column, which can be misleading.\n\n- The defense mechanism relies on synthetic deformations of training images, where a parameter \\sigma encodes the magnitude of the added deformations. I could not find how this parameter was set. Was it adjusted to the expected magnitude of each attack, or was it set once for all experiments? \n\n\n[a] Vincent et al, Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion, JMLR 2010\n[b] Ghifary et al, Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation, ECCV 2016\n\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		rygMhHISx4	H1gOkfj1NV	MIDL.io/2019/Conference/-/Paper159/Official_Review	[]	3		['everyone']	rygMhHISx4	['MIDL.io/2019/Conference/Paper159/AnonReviewer1']	1548886496097		1548886496097	['MIDL.io/2019/Conference/Paper159/AnonReviewer1']
70	1548589739343	{'pros': 'A 2D U-Net approach is presented for the complex task of segmenting in total 16 anatomical structures in MRI of the wrist, added with  distance map and angular feature mapps to add sufficient context information compensating for the relative small patches being used. Method has been validated on 13 patients from a cohort with good dice scores. The strongest point of this work is that they have tackled a complex segmentation task with limited number of training data obtaining good results for which previously no methods were available (to my knowledge). This is a solid step towards wrist tendon segmentation in MRI using 2D U-Nets.', 'cons': '- I am surprised that the system was designed to work on such small patches instead of the full 2D slice of 512x512 thereby circumventing the need to explicitly add a distance and angular maps to encode for context information. Also, a patch size of 85x85 seems impractical in 2D U-Nets. Memory consumption can be decreased by using smaller batch sizes. More memory efficient libraries can be considered such as pytorch. And more modern video cards provide 12G of memory and higher, doubling the available memory they currently have available.\n- Related work is missing. Adding spatial information or geometric cues to 2D U-Nets has been done before.\n- Reference to Aizenberg is missing (on their use of an atlas to generate ground truth). \n- I suggest to report the performance of the method on a full independent test set.\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		r1lVVk3RyE	Hkgm3czj7E	MIDL.io/2019/Conference/-/Paper22/Official_Review	[]	1		['everyone']	r1lVVk3RyE	['MIDL.io/2019/Conference/Paper22/AnonReviewer2']	1548589739343		1548886141873	['MIDL.io/2019/Conference/Paper22/AnonReviewer2']
71	1543290400155	{'title': 'Convergence, parameters and hyper-parameters, and experimental setting', 'comment': 'Thank you for your comments, please see our responses below.\n\n“Convergence issues; the algorithm may go wrong; bifurcation rate can be too slow/fast:”\nIndeed, a limited number of layers might give an exact solution to the quadratic criterion in Eq. (2). However, our results imply that using few iterations with learned weights outperforms a converged solution using heuristic weights.\nBecause our model optimizes accuracy, each point point in Fig, 4 corresponds not only to a different T but also to different learned weights, making it difficult to compare convergence across points.\nWhile bifurcation can potentially change the rate of convergence, it does not have to. Since the bifurcation parameters (\\theta^\\tau) are learned, and since \\theta^\\tau = 0 implies no change in rates, bifurcation will be used (by learning that \\theta^\\tau != 0) only if it results in better performance.\n\n“Is introducing entropy always helpful?”:\nSince the entropy parameters (\\theta^e) are learned, and because \\theta^e = 0 implies uniform weights (like in LP), the model will learn to use entropy only if it results in improved performance. The same applies to KL divergence.\n\n“Difference in experimental setting and results from GCN:”\nThe main differences between our setup and GCN are the number of labeled nodes and how they can be used. In GCN labeled nodes are partitioned into training (20 nodes per class) and validation (500 additional nodes). While training sets are kept small (3.6% for Citeseer, 5.2% for CoRA, and 0.3% for pubmed), the total number of labeled nodes (training+validation) is rather large (18.6%, 23.6%, and 2.8% of all nodes, respectively), and a huge portion of labeled nodes is pre-allocated for validation (81%, 78%, and 89% of labeled data, respectively) and so can only be used for tuning, not training. This puts methods that require little or no tuning (such as LP, over which our method is built) at an immediate disadvantage. \nIn our view, methods should be free to choose how to best use the available labeled data, be it for training, tuning, or other. We have experimented in the GCN setting, allowing our model to use all labeled data for training. While our model outperforms GCN (83.4 vs. 79.6 on CoRA and 69.8 vs. 67.5 on Citeseer, averaged over 10 random splits), this may also seem unfair, since other baselines might also benefit from a allocation of the labeled budget. There also several other issues with the “standard” setting used in GCN - see the recent paper by Shchur et al. (2018) [1] for details.\nDue to the above, our solution was to revert to the classic SSL experimental setting used in numerous papers, where a fixed percentage of labeled nodes are drawn uniformly at random. We used 1% as it is a reasonable number in the range of the GCN setting, and allowed us to fully train all baselines for all settings and datasets over 10 random splits in a reasonable amount of time.\n\n“Number of GCN layers:”\nWe use the published GCN code which has one graph-convolution layer and is used in their paper.\n\n“Too many hyperparameters to tune”:\nPlease note that we have only *one* network-related hyper-parameter that requires tuning - the number of layers (T). As Fig. 4 shows, choosing T can be made robust by using bifurcation. All other model parameters (denoted by \\theta) are learned. The regularization coefficient \\lambda is chosen by standard cross validation.\n\n“Minor points”:\nThank you for these, we will fix them.\n\n[1] Shchur, O., Mumme, M., Bojchevski, A., & Günnemann, S. (2018). Pitfalls of Graph Neural Network Evaluation. arXiv preprint arXiv:1811.05868.'}		r1g7y2RqYX	BJeOQ0NqCm	ICLR.cc/2019/Conference/-/Paper972/Official_Comment	['ICLR.cc/2019/Conference/Paper972/Reviewers/Unsubmitted']	2		['everyone']	SklckBGU3X	['ICLR.cc/2019/Conference/Paper972/Authors']	1543290400155		1548885348710	['ICLR.cc/2019/Conference/Paper972/Authors', 'ICLR.cc/2019/Conference']
72	1543291412735	"{'title': 'Clarifications, justifications, and updates', 'comment': ""Thank you for your comments. Below please find details describing our modeling and experimental choices. The updated paper includes an enriched related materials section and GAT as a baseline. Fig. 4 quantifies the added value of the bifurcation component.\n\n“Total number of parameters, and compared to other methods?”\nThe total number of parameters is between 12 and 44, depending on dataset and experimental setting. Based on their published codes, GCN has 23,040 and GAT has 92,391 for CoRA. We use 38. These include:\n- Weights: 30 edge features, some are per-class (Appendix B)\n- Attention: 2 parameters per class, one for entropy and one for divergence (Sec. 3.1)\n- Bifurcation: 2 parameters (Eq. (13))\n\n“Relation to papers by Saha et al. and Cilberto et al.”:\nThe above papers propose methods for multi-task learning (Saha et. al for online, Ciliberto et al. for batch), and consider relations between tasks, which are fixed and given as input. Our paper focuses on semi-supervised learning, and considers weighted relations between examples, which are learned.\n\n“Discern contribution of learning the weights vs propagating labels instead of embeddings”:\nThe LP baseline, which we generalize, propagates labels with fixed weights. Fig. 4 shows the how adding bifurcation (LPN_bif) compares to only learning weights (LPN_nobif).\n\n“Please specify that \\theta are learned”:\nWe will clarify this.\n\n“Entropy and divergence - inversely proportional?”:\nWe use *negative* entropy and *negative* divergence (see Eq. (11) and above). This aligns with your intuition.\n\n“Use threshold for rounding instead of bifurcation”:\n“Hard” rounding is non-differentiable, and cannot be used efficiently with back-propagation. Bifurcation is differentiable, and much more expressive than simple rounding. It can interpolate between “rounding up” (large \\tau) and “rounding down” to uniform (\\tau-->zero), or result in no rounding (\\tau=1). \n\n“Are a, b tuned using cross-validation? Can't we learn them?”:\nBoth a and b (=\\theta^\\tau) are learned, not tuned.\n\n“Can’t the loss in Eq. (14) be replaced by the standard empirical loss?”:\nUnfortunately, no. With the standard empirical loss, Eq. (14) becomes degenerate. This is because the it compares the true and predicted labels of the labeled nodes. As in LP, predicted labels of labeled nodes are set to their true labels, so the loss is always 0. This is also noted in Zhang & Lee (2007).\n\n“If available, how are node features used?”:\nNode features are used to parameterize edge weights (Eq. (7) & appendix B). Sec. 2.2 now includes more details.\n\n“Why is k chosen to be equal to 1%?”:\nWe chose 1% as it is a reasonable number in the range of those used in GCN and others (3.6%, 5.2%, and 0.3%). Note that we re-train all baselines on all datasets over 10 random splits in two experimental settings, which requires considerable computational resources.\n\n“Reduce features to avoid overfitting”:\nThe overall number of parameters we use is very small, especially compared to other methods. We observed that reducing the number of features only degrades performance.\n\n“Differences from GCN”:\nThere are several notable differences, most of which become apparent when comparing the form of classifiers proposed by each method. The classifier of GCN is f(x, W; \\theta), while ours is f(y; W(x; \\theta)). This means that:\n- GCN operates on features, while we propagate labels. The benefit of propagating labels is that labeled information is used not only to penalize wrong predictions (in the loss), but also to *generate* predictions. This is the hallmark of the LP algorithm, which we adopt.\n- GCN assumes edge weights W are given as input. These are typically set heuristically. In contrast, our method learns weights by optimizing predictive accuracy.\n- GCN uses node features x to generate embeddings, and hence does not apply to tasks where node features are not available. For our method, when features are available, we use them to parametrize W. When they are not available, we use the information-gated attention mechanism.\n\n“No comparison to GraphSage:”\nGraphSage applies to an inductive learning setting. Our method is designed for a transductive learning setting.\n\n“Explain ‘LPN_nobif degrades with large T’”:\nFig. 4 shows that without bifurcation, accuracy can be sensitive to T. Adding bifurcation provides robustness. This is true even when the effects of bifurcation are subtle (\\tau~=1).\n\n“Trend in Table 1; why is performance poor for Flickr?”:\nFlickr is dense compared to others (edge/node ratio of 60:1, vs. between 1.5:1 and 5.3:1). To reduce the computational load, we sparsify the graph, which may explain the low accuracy.\n\n“Uncertainty estimates”:\nNow added.""}"		r1g7y2RqYX	S1l6GGSqR7	ICLR.cc/2019/Conference/-/Paper972/Official_Comment	['ICLR.cc/2019/Conference/Paper972/Reviewers/Unsubmitted']	3		['everyone']	HJxISgAn2m	['ICLR.cc/2019/Conference/Paper972/Authors']	1543291412735		1548885340496	['ICLR.cc/2019/Conference/Paper972/Authors', 'ICLR.cc/2019/Conference']
73	1543291506442	{'title': 'Comments', 'comment': 'Thank you for your comments, please see our responses below.\n\n“Not sure I understand dynamic weights”:\nYes, this is correct. The attention mechanism turns incoming soft labels (h^t) into edge weights (a^{t+1}). For given parameters \\theta^\\alpha, the attention function \\alpha is indeed fixed, but since soft labels change as they pass through the layers, so do the edge weights. When viewed as a label-propagation mechanism, weights can be thought of as changing over time.\n\n“What is \\theta^\\tau in Eq. (13):\n\\theta^\\tau is defined just above Eq. (13) - it is simply the concatenation of a and b. The left hand side can be written as \\tau(t;a,b). In general, we use \\theta to denote parameters, and the corresponding superscript to denote what they parameterize. We will make this clearer.\n\n“The term ‘time’ is misleading”:\nWe apologize for this inclarity. Indeed, we use time, iterations, and number of layers (or depth) interchangeably. This is because the network’s layers simulate the iterations of LP, which we think of as applied over time. We will clarify this.\n\n“How are raw features incorporated in the loss?”:\nWe use “raw” features to parameterize edge weights. This means that the weight w_{ij} of an edge (i,j) is a function of various edge measures \\phi_{ij}, some of which are derived from raw node features (see appendix B). The function is parameterized by \\theta^\\phi - the exact form is given in Eq. (7). Edge weights then determine the predicted labels (through the label propagation mechanism), and predictions are plugged into the loss function in Eq. (14), where they are evaluated against the ground-truth labels.\n'}		r1g7y2RqYX	rkg5ufBcAQ	ICLR.cc/2019/Conference/-/Paper972/Official_Comment	['ICLR.cc/2019/Conference/Paper972/Reviewers/Unsubmitted']	4		['everyone']	rJxDdu4A37	['ICLR.cc/2019/Conference/Paper972/Authors']	1543291506442		1548885328945	['ICLR.cc/2019/Conference/Paper972/Authors', 'ICLR.cc/2019/Conference']
74	1548884593762	{'title': 'Response ', 'comment': 'We would like to thank the reviewer for the useful comments. We will take your suggesstions about organizing the paper in the final version. '}		r1e3WW5aTX	Bkl9d5ck44	AKBC.ws/2019/Conference/-/Paper30/Official_Comment	['AKBC.ws/2019/Conference/Paper30/Reviewers/Unsubmitted']	3		['everyone']	rklXmgmXMV	['AKBC.ws/2019/Conference/Paper30/Authors']	1548884593762		1548884593762	['AKBC.ws/2019/Conference/Paper30/Authors', 'AKBC.ws/2019/Conference']
75	1548876189371	"{'pros': 'The paper presents a novel methodology to reconstruct ultrasound images from raw echo data. In particular, a neural network model is trained to learn optimal transmitter (Tx) and receiver (Rx) beamforming patterns for fast (high-resolution) ultrasound  image acquisition. \n\nI think the idea of designing an end-to-end learning system from Tx signal generation to Rx image reconstruction is an interesting approach, and the authors formulated this in a very nice way. \n\nAdditionally, the paper is very well written. Especially, the fundamental concepts of ultrasound imaging is presented in a clear way as such wider audience can easily follow the content of the paper. \n\n', 'cons': 'Some minor points \n--\n\n1) I and Q components should be explicitly specified:  in-phase (I) and quadrature (Q) components of the echo signal\n\n2) Page 3, please specify parameter t.\n\n3) Page 5 - Tx BF formulation. There is no dependency on j index on the right hand side. Please correct the formula. \n\n4) Page 6: What is the momentum optimiser ? e.g. Adam optimiser can have a momentum term in order to provide smoother parameter convergence (less oscilations in the search space). Is it what is meant? \n\n5) Typo - Page 9 intitlization \n\nMajor comments \n--\n\n1) The presented solution is based on a neural network architecture. However, the paper does not specify in anywhere how this model and results can be reproduced, and what the building blocks of this architecture are? \n(convolution parameterisation? recurrent models?)\n\n2) Please specify that the proposed BFtransform does not have any trainable parameter, but it is introduced to allow gradient flow from Rx BF to Tx BF. \n\n3) Figure 2 - Please specify why the Tx BF layer parameters are shared across both I and Q components? Transducer or  hardware limitations? \n\n4) Figure 2 - What are the parameters of the reconstruction network (\\theta) ? \nIt is described as an autoencoder but how is it formulated? \n\n5)  The presented approach is evaluated on only a single clinical data. Why not use leave-one-out cross validation?\n\n6) I think the title (""Learning Beamforming in ultrasound imaging"") is very assertive given that the approach is evaluated on a single cardiac ultrasound scan. I would recommend that the authors reconsider updating the title of the paper to better reflect the content. \n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		Skxq9YqJg4	SyeSiFuJVN	MIDL.io/2019/Conference/-/Paper36/Official_Review	[]	3		['everyone']	Skxq9YqJg4	['MIDL.io/2019/Conference/Paper36/AnonReviewer2']	1548876189371		1548882375329	['MIDL.io/2019/Conference/Paper36/AnonReviewer2']
76	1546045566716	{'comment': 'Can you elaborate more on applying this method with A3C. Especially the modification of the Advantage function?  When calculating the advantage function in A3C , scalar rewards and state-value function is used.   With this method are you proposing to modify only the value function computation in the Advantage function with USF?', 'title': 'Modification of Advantage function and Critic in A3C'}		ByxHb3R5tX	H1lwK_BNWV	ICLR.cc/2019/Conference/-/Paper1169/Public_Comment	[]	3		['everyone']	Skl17Kw7g4	['~Shane_Gayal1']	1546045566716		1548880889881	['~Shane_Gayal1', 'ICLR.cc/2019/Conference']
77	1545175812825	"{'comment': ""I have noted some interesting facts about the proposed USF architectures by the Authors.  I will point it down since I also wants to know weather I am correct. \n\n1. Previous methods of SF-RL mainly based on DQN architectures and they not highlight any zero shot transfer learning ability. \n\n2. Previous networks have used an autoencoder loss to learn the state representation(Kulkarni et al., Zhan et al.) while training on the DQN task. This can add some instability to the system when the state representation is complex. Barreto et al. (2018) have replaced state representation vectors by assuming the current state as a linear combination of scalar rewards to set of base tasks. With Barreto et al. there should be some base tasks.  Ma et al. have introduced USF idea first where the state representation vector is extracted from an Auto-encoder.  When working with visual states, this can be troublesome. Contrasting to previous work on SF-RL and USF-RL  Authors have used a more straightforward way to predict the state representation with USF which is more scalable. \n\n3. In SF-RL previous work the reward prediction vector also trained by regressing scalar rewards while training on the DQN baseline. In the USF-RL (Ma et al) introduced a goal-oriented reward vector is produced by a neural network that takes the goal as the input. However, still its hard to train the goal-oriented reward vector prediction network due to the sparsity of the reward structure. Because in an example task of navigation, once the training starts agent will see many negative rewards for a long time and decidedly less positive rewards (If its A3C the network will get updated by many agents ).  This makes the training weights of the network unstable. In this paper, authors proposed to use scalar rewards as it is and trained the reward vector prediction network with Q loss.\n\n4.  This combined the general value function approximates(GVFA) with  USF which is useful for the large-scale deep reinforcement learning frameworks like A3C. Because in A3C we can train different agents on different goals. Let's say in a navigation task with different targets, and we can train this whole architecture while learning general patterns.  \n\n\nI also have few questions regarding to train this with complex representations like images for a task of navigation in robotics. \n\n\n1. Let's say we have a fixed number of targets where each target is sentimentally different from each other, and both the states and targets are represented in images.  So mainly in the architecture proposed by authors, we need to have CNNs for goal embedding, state embedding and reward vector prediction network. How to proceed with this kind of situation ? is it scalable? \n\n2. Do we maintain two CNNs for state embedding network and goal embedding network ?  Cant we use same networks which is more like a Siamese network?  I think this can be a problem when training since practically we only train with given number of goals. \n\n3. Do we maintain two CNN for state embedding network and goal embedding network?  Can't, we use the same networks which is more like a Siamese network?  I think this can be a problem when training since practically we only train with a given number of goals. \n\n4. What is the best way to calculate Advantage in the A3C setting?  To calculate the advantage in A3C, we need a scalar reward at each time steps. If we replace scalar reward as a linear combination of state representations the A3C agent can get unstable because we use the advantage to update the policy directly. So can we use scalar reward as it is to calculate advantage while the USF replaces the value function? "", 'title': 'Interesting idea which enables to use USF with large scale RL methods like A3C , IMPALA '}"		ByxHb3R5tX	SJxab7-DlN	ICLR.cc/2019/Conference/-/Paper1169/Public_Comment	[]	2		['everyone']	ByxHb3R5tX	['~Shane_Gayal1']	1545175812825		1548880884097	['~Shane_Gayal1', 'ICLR.cc/2019/Conference']
78	1538087983886	{'title': 'NOODL: Provable Online Dictionary Learning and Sparse Coding', 'abstract': 'We consider the dictionary learning problem, where the aim is to model the given data as a linear combination of a few columns of a matrix known as a dictionary, where the sparse weights forming the linear combination are known as coefficients. Since the dictionary and coefficients, parameterizing the linear model are unknown, the corresponding optimization is inherently non-convex. This was a major challenge until recently, when provable algorithms for dictionary learning were proposed. Yet, these provide guarantees only on the recovery of the dictionary, without explicit recovery guarantees on the coefficients. Moreover, any estimation error in the dictionary adversely impacts the ability to successfully localize and estimate the coefficients. This potentially limits the utility of existing provable dictionary learning methods in applications where coefficient recovery is of interest. To this end, we develop NOODL: a simple Neurally plausible alternating Optimization-based Online Dictionary Learning algorithm, which recovers both the dictionary and coefficients exactly at a geometric rate, when initialized appropriately. Our algorithm, NOODL, is also scalable and amenable for large scale distributed implementations in neural architectures, by which we mean that it only involves simple linear and non-linear operations. Finally, we corroborate these theoretical results via experimental evaluation of the proposed algorithm with the current state-of-the-art techniques.', 'keywords': ['provable dictionary learning', 'sparse coding', 'support recovery', 'iterative hard thresholding', 'matrix factorization', 'neural architectures', 'noodl'], 'authorids': ['rambh002@umn.edu', 'lixx1661@umn.edu', 'jdhaupt@umn.edu'], 'authors': ['Sirisha Rambhatla', 'Xingguo Li', 'Jarvis Haupt'], 'TL;DR': 'We present a provable algorithm for exactly recovering both factors of the dictionary learning model. ', 'pdf': '/pdf/4f07ed8c7c2696730f4c48b368aad6f7f245f28e.pdf', 'paperhash': 'rambhatla|noodl_provable_online_dictionary_learning_and_sparse_coding', '_bibtex': '@inproceedings{\nrambhatla2019NOODL,\ntitle={NOODL: Provable Online Dictionary Learning and Sparse Coding},\nauthor={Sirisha Rambhatla and Xingguo Li and Jarvis Haupt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeu43ActQ},\n}'}		HJeu43ActQ	HJeu43ActQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1464	r1guzLFcK7	['everyone']		['ICLR.cc/2019/Conference']	1538087983886		1548880528691	['ICLR.cc/2019/Conference']
79	1548879251179	{'pros': '- This work proposes a new experimental study on the effect of architecture choices and cross-dataset transfer of learned representations for the detection of microcalficiation in mammography.\n\n- The paper is clearly written and is very thorough in terms of implementation details and pre-processing of data, which facilitates the reproducibility of the results.\n\n- The conducted analysis seems sound.', 'cons': '- The paper is an experimental summary of the performance of three architectures and the effect of fine-tuning / transfer of features between datasets, and therefore does not contain any methodological novelty.  Although this is not necessarily a problem at all, I do not find that the experimental study is sufficiently insightful beyond the reported results on the investigated datasets:\n\n(1) the compared methodologies are mainly standard architectures and procedures in deep learning. The benefit of fine-tuning and cross-dataset transfer of features are well-known, including for standard classifiers as investigated here (SVM/ random forests). A study on the latter can be for example found in [a], among others. Although additional evidence is shown here, including on the effect of the architecture choices, I find difficult to interpret these experimental findings and connect them to a certain property of the learning task or of the medical application addressed here. \n\n(2) the work is not positioned experimentally with respect to existing approaches for this clinical problem, so that a general benefit in terms of application - such as for example a new state-of-the-art in terms of accuracy or computational time for microcalcification detection - is not demonstrated either.\n\nTherefore, seen as a research paper, I have difficulties to identify here a take-home message which would go beyond the respective accuracies of the attempted approaches on these mammography datasets. Although these results can of course be of interest to researchers working specifically on this clinical problem, I am afraid this remains a too limited audience.\n\n\n [a] Razavian et al, CNN Features Off-the-Shelf: An Astounding Baseline for Recognition, CVPR Workshop 2014\n\n\n- In “5. Discussion”, the focal loss is claimed to be novel, which is misleading since it was introduced in (Lin et al. 2017). I also believe that the notations of the equation of the focal loss in 3.2.1 must be clarified: they are clear in the context of (Lin et al. 2017), but should be ideally quickly reminded here for the paper to be self-contained.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		B1luMUUxgN	Hkxs5SKyN4	MIDL.io/2019/Conference/-/Paper107/Official_Review	[]	3		['everyone']	B1luMUUxgN	['MIDL.io/2019/Conference/Paper107/AnonReviewer3']	1548879251179		1548879251179	['MIDL.io/2019/Conference/Paper107/AnonReviewer3']
80	1548873304714	{'title': 'Response to Reviewer 3', 'comment': 'We appreciate your effort in reviewing our work. \n== Cons ==\n1-- Thanks for your comment. We will add couple of sentences in the abstract and the introduction to emphasize the generalization ability of the proposed model. \n\n2-- The supervised baselines with different inputs (PairDiff, Concat and BiLin) that are described in Section 4.4 are all trained using the same loss: ranking loss defined in equation 5.\xa0\n-\tWe perform a direct comparison between two ways to derive relation representations: the penultimate layer of a neural network trained on relation prediction task using the softmax cross-entropy loss among relations vs direct relation representation learning models using analogous and non-analogous pairs of word-pairs considering the ranking loss. In this case, the two models are exactly same including the number of hidden layers, non-linear activation etc, with only a change in the loss. Empirically the proposed MnnPL (softmax cross entropy) outperforms the direct relation representation trained on the pairs of word-pairs relational data. \n-\tIn fact, there are many objective functions that can be used to train a supervised model for relation representations which we can evaluate them extensively in our future work.\xa0\n\n3-- In Table 2, MnnPL outperforms the unsupervised baselines for BATS but the gap is smaller than those in DiffVec. In BATS, we got two semantic relation types as shown in Table 3. Breaking down the performance of encyclopaedic and lexicographic indicates that MnnPL performs significantly better the PairDiff for lexicographic relations and slightly better for encyclopaedic ones. Discussion about the two relation types with some justification are illustrated in Page 12 (Figure 2). \xa0\n-\tAs shown in Appendix A (table 5), all the semantic relations in DiffVec are lexicographic type of relations. Even though the comparison made for BATS dataset show the difficulty of capturing lexicographic relations compared to encyclopaedic one, it is shown that MnnPL reports 48.6% accuracy with CBOW embeddings (statistically significant that the random baseline).\xa0\n\n4-- If the test word-pair is (a,b), then if there is a train word-pair (a,c), (b,c), (c,a) or (c,b) then we say there is a lexical-overlap between the test and the train datasets. For example, (‘animal, cat) and (animal, dog) has lexical-overlap because animal is a common word in the two pairs. We will add this clarification for lexical-overlap in the revised version. \n\n\n\n '}		r1e3WW5aTX	BygZw0Pk44	AKBC.ws/2019/Conference/-/Paper30/Official_Comment	['AKBC.ws/2019/Conference/Paper30/Reviewers/Unsubmitted']	2		['everyone']	HJl-uAtpZ4	['AKBC.ws/2019/Conference/Paper30/Authors']	1548873304714		1548873304714	['AKBC.ws/2019/Conference/Paper30/Authors', 'AKBC.ws/2019/Conference']
81	1548872514171	{'pros': '- This paper proposes the first end-to-end solution to the problem of semantic segmentation of teeth from intra-oral 3D scans. This is an interesting application of deep learning to 3D point clouds, as opposed to the more traditionally encountered image-based segmentation.\n\n- The paper is clearly written. In particular I found the work to be strongly motivated in the introduction by a clear presentation of the specific properties and challenges of the addressed application.\n\n- The proposed method and evaluation seem sound.\n\n- The two contributions introduced in the paper are well validated, both against external baselines and individually through an ablation study. The evaluation confirms the positive impact of each contribution on the segmentation performance.', 'cons': '- One of the two contributions, namely the addition of a discriminative network during training to structure the prediction, is insufficiently discussed in comparison to the previous works. The idea of using an adversarial training which differentiates realistic from unrealistic label configurations was already introduced in several works, starting (I believe) with Luc et al [a]. Multiple additional references for medical applications are for example available in the introduction and related work sections of the paper (Ghafoorian et al, 2018). However, although (Ghafoorian et al, 2018) is mentioned in the Methods section, none of these works are mentioned in the introduction or the related work, which I found to be misleading regarding the novelty of this contribution.\n\n- The adversarial network is based on simple features extracted from the predicted labels (mean and variance of 3D voxel positions for each class). While this is technically indeed a novelty, I find this aspect to go a bit against the end-to-end claim, since it amounts to handcrafting features in the label space before applying a multilayer perceptron. This contrasts for example with [a] where a network is trained end-to-end to learn how a realistic label prediction should look like, possibly discovering high-level criteria related for example to object shapes, etc. Even if using these features might be perfectly sound for this application, I find that this is not motivated enough in comparison to the existing end-to-end approaches (which also goes back to the previous point), and rather in contradiction with the narrative of the paper.  The limitations of handcrafted features are indeed regularly pointed out in “Related work”.\n\n- The lack of universal coordinate system is mentioned as a challenge for this clinical application, for example in “Discussion and conclusion”. However, the features in the label space used to discriminate realistic from unrealistic labels include the means of 3D coordinates. Does not this possibly break the invariance with respect to the choice of coordinate system? I wonder whether using pairwise distances between classes, i.e. pairwise differences of means instead of the means directly, would be more suitable to guarantee an invariance to the choice of coordinates directly in the feature representation.\n\n\n[a]  Luc et al, Semantic Segmentation using Adversarial Network, NIPS Workshop 2016\n\n\nMinor comments:\n\n- If the extracted statistical features are sufficient, it might be worth training a simpler and more interpretable adversarial classifier than a multilayer perceptron – maybe simply a logistic regression? This would give a better understanding on how the notion of realistic segmentation output is encoded.\n\n- I wonder if the discriminator could not be used at prediction time as well, in addition to structuring the training. Could it for example provide a confidence measure on the output?\n\n- In general, I believe it is better to avoid using an ArXiV reference when the work was published and peer-reviewed, as is for example the case with the PointCNN paper (Li et al 2018).', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		ByxLSoblgV	HkgcroPJNE	MIDL.io/2019/Conference/-/Paper65/Official_Review	[]	3		['everyone']	ByxLSoblgV	['MIDL.io/2019/Conference/Paper65/AnonReviewer2']	1548872514171		1548872514171	['MIDL.io/2019/Conference/Paper65/AnonReviewer2']
82	1548872002267	{'pros': 'This paper presents an interesting work on pathological image synthesis by developing adversarial learning models. The paper is well-written and organized for readers to follow. Experimental results show that the proposed model performs better than other baseline methods: conditional GAN and CycleGAN. ', 'cons': 'While the authors show that the proposed method has better synthesized image quality than other baseline algorithms, the synthesized image quality is substantially worse than the original image (as shown in figure 4). Many of the ‘healthy’ parts of the image either introduce artifacts, or with over-smoothed structures. In particular, artifacts that seem like ‘check-board pattern’ appear in the pathological areas. \n        \nBased on the experimental results, I am wondering whether this problem could be solved just by segmentation itself since reconstruction on pathological areas is not considered. It is trivial to adjust the contrast of the segmented pathology to better match the intensity distribution of healthiness. In this case, you shall perfectly preserve the image information on healthy parts. ', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		H1gMYCQxg4	S1ecHtPkVE	MIDL.io/2019/Conference/-/Paper91/Official_Review	[]	3		['everyone']	H1gMYCQxg4	['MIDL.io/2019/Conference/Paper91/AnonReviewer3']	1548872002267		1548872002267	['MIDL.io/2019/Conference/Paper91/AnonReviewer3']
83	1548868228545	{'title': 'Response to rebuttal', 'comment': 'Thanks for your rebuttal. I agree that building a practical system which works is of great value, both academic and otherwise. But as a researcher, it is hard for me to decide which algorithms introduced in the paper contribute to the working system and which do not. Hence, it is difficult to decide what to take away from the paper.\n\nI am not sure I agree that is is impossible to evaluate the heuristics. For example, you have a set of 2421 extracted triples which the doctors have verified. How many of these would fail to be extracted if copula verbs are not eliminated, or if noun phrase relations are not included? Can that give us an idea about the contribution of these rules to the recall? \n\nI completely agree that 48% is indeed a high score for a real-world system like this. This is what I intended to point out in the original review as well. Overall despite the above limitation, I think its an interesting paper, and hence will stick by my original decision of marginally above acceptance.'}		HJxmxbq66Q	ryxpFqIyN4	AKBC.ws/2019/Conference/-/Paper20/Official_Comment	['AKBC.ws/2019/Conference/Paper20/Reviewers/Unsubmitted']	7		['everyone']	BygmpQLL7E	['AKBC.ws/2019/Conference/Paper20/AnonReviewer1']	1548868228545		1548868228545	['AKBC.ws/2019/Conference/Paper20/AnonReviewer1', 'AKBC.ws/2019/Conference']
84	1545166835283	{'title': 'Interesting task formulation, but a possible methodological flaw.', 'review': 'The paper presents a framework to evaluate the recall of information extraction systems. \nThe problem is phrased as a binary decision (extractions are complete or incomplete) with regards to *the real world*. For example, the extraction (John, FatherOf, Sally) may be incomplete given the corpus “John took his Daughter Sally to school” (as he may have more children), but complete with regards to the corpus “John has a single child, named Sally”.\nFirst it outlines possible uses for such a metric (inform a user that they should look for more information, decide where to invest search resources, etc.), and then describes an evaluation framework of Open IE4 using WikiData. \nFollowing, the metric is assessed both automatically and manually, and an empirical use-case of the metric is presented (knowledge base completion)\n\nOverall, I enjoyed reading the paper, I find the task is very interesting and well presented, and I can see the uses of such a metric.\n\nMy main objection is in the way the actual metric is compute:\n“A text  snippet is considered to be complete, and labelled as such, if we can match alias names for all objects found in Wiki-data. It is labelled as incomplete otherwise.” (Section 4, p. 8)\n\nDoesn’t this defeat the purpose of evaluating against the “real world”?\nInstead, I think that the described process conflates evaluation of two resources: while aiming to evaluate only an Open IE system, it also evaluates the completeness of the knowledge base & paraphrase dictionary, which may be incomplete. This is also acknowledged in the paper (“KBs such as WikiData have many gaps where they are incomplete”) p. 13) . For example, if WikiData doesn’t have all children of Barack Obama as facts, it may label a text saying “Barack Obama took his daughter Malia to school” as complete. \n\nInstead, i expected from the (excellent) intro to have a linguistic indication that such sentences are in fact incomplete, and hinting that there may be additional information somewhere.\n\nIn addition If the process of getting the distant supervision is fully automatic, why limit it to only 5 relations?\n\nIf this is indeed the case, I think that this should be made much clearer that the current work doesn’t actually address the real-world coverage of IE systems, but rather matches them against a KB.\n\nI’d be happy to raise my score if I got something wrong with respect the use of WikiData for labelling completeness of the extracted tuples.\n\n', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		HkgAyb5aaQ	rkgsleJDxE	AKBC.ws/2019/Conference/-/Paper18/Official_Review	['AKBC.ws/2019/Conference/Paper18/Reviewers/Unsubmitted']	1		['everyone']	HkgAyb5aaQ	['AKBC.ws/2019/Conference/Paper18/AnonReviewer1']	1545166835283		1548868073201	['AKBC.ws/2019/Conference']
85	1548866787692	{'title': 'Response to rebuttal', 'comment': 'Thanks for the detailed response! I think it will be good to add to section 5.3 that there is no overlap between the 3 test sets and the training set. The reasons pointed out for the transfer experiment all make sense, but without an exact analysis of whether there is an overlap, and what effect it might have, I will stick to my original score.'}		SkxE1b56TQ	H1x31SIyNN	AKBC.ws/2019/Conference/-/Paper14/Official_Comment	['AKBC.ws/2019/Conference/Paper14/Reviewers/Unsubmitted']	5		['everyone']	Hkey-SjC7V	['AKBC.ws/2019/Conference/Paper14/AnonReviewer1']	1548866787692		1548866787692	['AKBC.ws/2019/Conference/Paper14/AnonReviewer1', 'AKBC.ws/2019/Conference']
86	1548865353307	"{'title': 'Response to Reviewer 1', 'comment': 'We want to thank the reviewer for the time and review.\xa0\nQuestions for rebuttal: \n1-- CONCAT is a general operator that maps a pair to a higher dimensional space that considers the features of the two words in the word-pairs. We empirically observe that CONCAT operator for relations is significantly affected by the pairwise similarities of the corresponding words a and c, b and d for (a, b) and (c, d) word-pairs [1]. For relation prediction task in our work, we alleviate the effect of getting high relational similarity scores due to attributional similarities by excluding the 25% nearest pairs for each stem using cross-pair attributional similarity scores given by Equation 6. However, in measuring the degree of relational similarities and compare them with the human scores we did not consider such pre-processing step because of the nature of this evaluation is different than the relation prediction. In Chen dataset, there are many word-pairs that are assigned higher relational scores (human scores) than others because of the similarities between the corresponding entities in pairs. For example, (animal, pig) and (insect, ant) got 6.33 compared to (animal, pig) and (song, opera) that scored 4.3. It was also consistent with a closely related task measuring degrees of prototypicality in a recent work by Jameel, et al (2018) wherein PairDiff and Concat report 0.173 and 0.167 Spearman correlations, respectively [2]. \xa0\n\n2--\tFirst we want to note that this result is consistent with a recent comparative study by Hakami and Bollegala (2017). In [3] it has been shown that CBOW embeddings with unsupervised PairDiff operator perform the best for relation representations compared to various embedding types among many relational datasets. Similarly, Levy et al. (2015) show that CBOW (and SG) are better than GloVe in most cases for Google and MSR relational dataset using 3CosAdd and 3CosMult pair-based operators for answering analogical questions [4]. \xa0\n-\tThe GloVe objective for learning word embeddings aims to reduce the difference between the dot product of the vectors of the two words and the log of their co-occurrence. Thus, GloVe model is trained on the word-context co-occurrence matrix instead of the text corpus. According to this, GloVe is close to traditional word representation methods such as LSA that factorizing a word-context co-occurrence matrix. On the other hand, the CBOW objective seeks to predict a target word from its surrounding context.\xa0This difference in the objective between GloVe and CBOW might make sense for the superiority of CBOW prediction word embedding to capture informative information to induce relations between words. \n-\tThis work was not aimed to compare the embedding types, instead, we wanted to show that the proposed MnnPL method for representing relations is performing well across various type of word embeddings. Despite this, we train all the models on the same corpus to make a fair comparison to some extent. It is also known that hyperparameters tuning and pre-processing (or post-processing) are affecting the performance of such word embedding models. We also carry out our experiments using the 300 dimensional pre-trained publicly available GloVe and CBOW embeddings and it turns out that again CBOW outperforms GloVe for our tasks.\xa0\n\n3--\tIn BATS dataset, the semantic relations are classified to encyclopaedic and lexicographic where we show the performance on each relation type. On the other hand, as shown in Appendix A (table 5), all the semantic relations in DiffVec are lexicographic.\xa0\n\nReferences:\n[1] Turney, Peter D. ""Domain and function: A dual-space model of semantic relations and compositions."" Journal of Artificial Intelligence Research 44 (2012): 533-585.\n[2] Jameel, Shoaib, Zied Bouraoui, and Steven Schockaert. ""Unsupervised Learning of Distributional Relation Vectors."" Proceedings of ACL, Melbourne, Australia (2018).\n[3] Hakami, Huda, and Danushka Bollegala. ""Compositional approaches for representing relations between words: A comparative study."" Knowledge-Based Systems 136 (2017): 172-182.\n[4] Levy, Omer, Yoav Goldberg, and Ido Dagan. ""Improving distributional similarity with lessons learned from word embeddings."" Transactions of the Association for Computational Linguistics 3 (2015): 211-225.\n\n'}"		r1e3WW5aTX	r1x-L1LkVN	AKBC.ws/2019/Conference/-/Paper30/Official_Comment	['AKBC.ws/2019/Conference/Paper30/Reviewers/Unsubmitted']	1		['everyone']	S1l3DPGSfN	['AKBC.ws/2019/Conference/Paper30/Authors']	1548865353307		1548865353307	['AKBC.ws/2019/Conference/Paper30/Authors', 'AKBC.ws/2019/Conference']
87	1548824965250	{'title': 'Response to Reviewer 2', 'comment': 'We thank the reviewer for the comments. We have added an experimental analysis as suggested by the reviewer (in section 6.2).\n\nFor the other questions,\n\n(1) For the first point, we used the average hidden vectors as the sentence embedding because it shortens the gradient backpropagation path. For the second point, we split the GCN vectors (i.e. we used different vectors as input for the forward and backward LSTMs) because we do not want to restrict the two directions of the LSTM to the same initial vector. The idea is to give the model the capacity to learn what initialization is the best for each of them. We have explored alternative model designs and made decision based on empirical performance. We will add justifications to the design choices. Discussion about some choices were omitted when they do not significantly affect the performance.\n\n(2) The AECR dataset includes reports from different institutions. Some institutions use specific form templates to write their reports. Figure 2 in the appendix shows two examples. '}		Sye7fZcTTm	Hkl6KbnCmV	AKBC.ws/2019/Conference/-/Paper33/Official_Comment	['AKBC.ws/2019/Conference/Paper33/Reviewers/Unsubmitted']	3		['everyone']	rkg3R7XOe4	['AKBC.ws/2019/Conference/Paper33/Authors']	1548824965250		1548863046849	['AKBC.ws/2019/Conference/Paper33/Authors', 'AKBC.ws/2019/Conference']
88	1548824703528	"{'title': 'Response to Reviewer 1', 'comment': 'We thank the reviewer for the comments.\n\nFor the reviewer’s questions,\n\n(1) We use the ""exponential linear unit"" (ELU). We have clarified it in the revised version. The performance of our model is not sensitive to the choice of non-linearity.\n\n(2) We have tried using attention/gating mechanisms in our graph neural network, but didn’t observe significant improvements. We hypothesize the reason is that in most of our datasets, the meaningful patterns induced by the graph are relatively simple, thus it does not require a complex model with more parameters. We agree however that it is an interesting direction for future investigation. \n\n(3) We are exploring using self-attention in our later work, which is essentially learning the useful structure from a fully-connected graph. It is an important and challenging problem to identify the important edges automatically. In this paper we mostly meant to create the infrastructure (with pre-defined graphs) for a future step (i.e. learning the graph), which is currently being investigated.\n\nWe have expanded the related work as suggested by the reviewer.'}"		Sye7fZcTTm	SylOtxh0m4	AKBC.ws/2019/Conference/-/Paper33/Official_Comment	['AKBC.ws/2019/Conference/Paper33/Reviewers/Unsubmitted']	2		['everyone']	HJlIqRffz4	['AKBC.ws/2019/Conference/Paper33/Authors']	1548824703528		1548863008089	['AKBC.ws/2019/Conference/Paper33/Authors', 'AKBC.ws/2019/Conference']
89	1548862805327	{'comment': 'I saw the code release. Linked it in the readme of the repository.', 'title': 'Thank you for the code release.'}		H1ewdiR5tQ	rkx68SByEV	ICLR.cc/2019/Conference/-/Paper363/Public_Comment	[]	7		['everyone']	B1lazXHDXV	['~Benedek_Rozemberczki1']	1548862805327		1548862805327	['~Benedek_Rozemberczki1', 'ICLR.cc/2019/Conference']
90	1548860786007	"{'pros': 'The authors propose a discrete optimization framework for registration of medical images, using deep learning coupled with a graph-based mechanism for regularization of (soft) displacement labels.\n\n| Pros:\n* The ideas have potential, the work should be interesting to the MIDL audience.\n\n| Cons:\n* The paper is difficult to follow. Methodological elements are presented (e.g. min-convolutions), but not truly exploited later on. \nThe link between these theoretical ideas and what is concretely implemented is often not convincingly conveyed. Without it the general formulation does not have as sound a theoretical basis. \nThe practical algorithm boils down to fairly standard operations (e.g. pooling, averaging, graph regularization). The algorithm could have been presented from the very start by skipping message passing and min convolutions, since there is no further attempt to elaborate on the underlying approximations, or to strengthen the connection empirically; nor a reference to relevant work.\n* The validation is somewhat shallow. The evaluation is performed at the sparse keypoints rather than densely. In general, it seems natural for a method that optimizes dice to reach higher dice scores. Other metrics should be evaluated (including the smoothness of the resulting dense deformation). The ablation study is a good idea.\nAre training images reused for testing? (provided that they do not belong to a training pair)\n* A few statements are unconvincing or should be clarified (see below). (+ a few typos or sentences to rephrase)\n\nI am leaning towards acceptance because despite the shortcomings, some of the ideas should be interesting to discuss at the conference.\n\n| Miscellaneous:\n* ""In contrast to previous work, our model [...] enables a scale-agnostic inference. Meaning that we can directly transfer a model trained using a coarse B-spline transformation grid to be used for affine alignment or finer-grained local alignment respectively"" The paper should elaborate on this claim or it should be removed.\n* ""During test we increase the number of points fourfold and respectively reduce λ = 0.5 (since neighbours are spatially closer)."" How was this value of λ chosen?\n* ""Table 1 shows the average Dice scores across 66 registrations of our three-fold cross validation, where no pairs of two scans that were both seen during training are considered."" This suggests that the training images themselves can be reused within a yet unseen pair. It would be more convincing to have a separate test dataset.', 'cons': '.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		SygkpKmglN	B1e9_aVyEV	MIDL.io/2019/Conference/-/Paper89/Official_Review	[]	3		['everyone']	SygkpKmglN	['MIDL.io/2019/Conference/Paper89/AnonReviewer4']	1548860786007		1548860824531	['MIDL.io/2019/Conference/Paper89/AnonReviewer4']
91	1548859990669	"{'comment': 'Thanks for a very well done work. With Bayes by backprop we have indeed experienced that it was necessary to tune priors, to adjust the learning rate to compensate for the increased stochasticity (less robust training) and even to weight the complexity term (KL to the prior) in the case of non-iid data (dependent pixels, augmentations, etc). I would appreciate very much if the authors could comment on the following questions.\n\n(1) The heteroskedastic model predicting mean and variance of a Gaussian distribution is a bit unclear to me. In the Bayesian predictive distribution there is variance Sigma_{mm} coming from the uncertainty of parameters and the variance e^{l} learned as a part of the model. In case of a large training sample, Sigma_{mm} approaches zero and the predictive distribution (9) to the ML plug-in. However, we are interested to see the effect of the Bayesian learning, the quality of estimating Sigma_{mm}, i.e. in the case of a small training sample. I am not sure this is the case in Fig 3. It looks like ML would give a similar predictive distribution and the Bayesian Sigma_{mm} and especially Sigma_{ll} are small corrections on top of that. Does it really show the utility of the method? \nWhat is further confusing, is that the whole difference between Sigma_{mm} and e^{l} is that the former is not fitting the predictive model to the data whereas the later is, otherwise technically they are replaceable, at least Sigma_{mm} can implement e^{l} with some extra neurons and noisy weights.\n\n(2) The paper mentions that ""strong test performance of DVI is largely retained by dDVI"" and ""dDVI surprisingly retains much of the performance"". However, the only experiment relevant to this is the model with one hidden layer (Table 2). The only non-zero covariance term in this case is Sigma_{m,l} for the output unit, which by the way is only appears in the expected likelihood (8) but not in the predictive probability (9). So the two methods may be expected to be nearly identical. Accounting for correlations is one main technical contribution of the work. It would be really interested to know whether such accounting for correlations is important in deeper models. And, more broadly, whether restricting the space of models to such where activations given the input are uncorrelated and learning a model in this space is in any way inferior.\n\n(3) A follow up on the question from the referees ""who cares about test likelihood"". Could you suggest to researchers interested in BNNs what would be a more practically relevant and accessible to non-experts set of experiments to test the quality of the learned probabilistic model for both regression and classification? And also, related to (1), to see the value of the Bayesian part?\n\nWith kind regards,', 'title': 'Discussion'}"		B1l08oAct7	H1gkP9VJ44	ICLR.cc/2019/Conference/-/Paper220/Public_Comment	[]	1		['everyone']	B1l08oAct7	['~Alexander_Shekhovtsov1']	1548859990669		1548859990669	['~Alexander_Shekhovtsov1', 'ICLR.cc/2019/Conference']
92	1548858298663	"{'title': 'Response to Reviewer 3', 'comment': 'Thanks a lot for the thoughtful comments. In the following, we explain how we addressed them and accordingly revised the paper. In the revised version of the paper, all modified text parts are in red color.\n\nComment 1) Automatic labelling via KB: \nIndeed the most reliable labelling would be by manual annotation. We performed a limited manual evaluation (Table 5), whereas for the larger evaluation, for scalability, we used weak supervision.\nWe solely use the most popular entities per relation (top 7%/25%/98%/8%/11% of subjects per relation - see Table 1), based on the observation that Wikidata is much more complete for popular entities than for long-tail entities.\nWhile this is only a heuristic and does not guarantee that the weak labels are free of error, we hope the results are interesting enough and motivate further studies. We clarified this potential limitation in the paper.\n\nComment 2) Limit to 5 relations: \nThe data sources used, Wikidata and Wikipedia, indeed have limitations. An ideal test setting would have relations where 1) Wikidata has enough subjects with complete information, and 2) Wikipedia contains enough text segments mentioning the objects. We investigated a few other relations such as ""administrative subdivision"", ""sport team membership"" and ""works at"", but for each of these cases, either Wikidata is too incomplete, or Wikipedia does not have enough text segments.'}"		HkgAyb5aaQ	ryxmaQEJEN	AKBC.ws/2019/Conference/-/Paper18/Official_Comment	['AKBC.ws/2019/Conference/Paper18/Reviewers/Unsubmitted']	3		['everyone']	rkgsleJDxE	['AKBC.ws/2019/Conference/Paper18/Authors']	1548858298663		1548858298663	['AKBC.ws/2019/Conference/Paper18/Authors', 'AKBC.ws/2019/Conference']
93	1548858222936	{'title': 'Response to Reviewer 2', 'comment': 'Thanks a lot for the thoughtful comments. In the following, we explain how we addressed them and accordingly revised the paper. In the revised version of the paper, all modified text parts are in red color.\n\nComment 1) Problem formulation: \nThanks for pointing out this important issue! Not considering the IE method and its extracted facts in the problem formulation is a crucial point of our approach, but this was obviously not well expressed in the paper.\nIn short, we do not utilize the actually extracted facts because our goal is to investigate what a perfect method could principally extract from a piece of text, and we want to clearly separate this fundamental problem from the issue of experimentally evaluating the recall of a specific IE method/tool. We have revised the problem statement in the paper and added this clarification to the surrounding text.\n\nComment 2) Imbalanced training data: \nWe have tested balancing the training data by adjusting the positive/negative sampling ratio. This does indeed lead to improved performance, but does not fully overcome the inherent label bias. We added a comment on this issue to the experiment section.\n\nComment 3) Evaluation on existing IE evaluation datasets: \nThanks for the suggestion, we plan using TACRED in future work.\nThe reason why we are not including it in the revised version of the paper is the following technical challenge. Our model does not work out of the box, as the style of TACRED (News) is very different from Wikipedia, and the relevant TACRED relations are not large enough to allow train/test splits. So we again need to devise another form distant supervision.\n'}		HkgAyb5aaQ	r1xPuQE1E4	AKBC.ws/2019/Conference/-/Paper18/Official_Comment	['AKBC.ws/2019/Conference/Paper18/Reviewers/Unsubmitted']	2		['everyone']	SkxczSYEf4	['AKBC.ws/2019/Conference/Paper18/Authors']	1548858222936		1548858222936	['AKBC.ws/2019/Conference/Paper18/Authors', 'AKBC.ws/2019/Conference']
94	1548858111514	"{'title': 'Response to Reviewer 3', 'comment': 'Thanks a lot to for the thoughtful comments. In the following, we explain how we addressed them and accordingly revised the paper. In the revised version of the paper, all modified text parts are in red color.\n\nComment 1a) Predicates of interest: \nThanks for pointing out this aspect - recall is indeed not of interest for properties that are both mandatory and functional, such as dateOfBirth. It becomes a question when properties are either not mandatory (e.g., dateOfDeath), or are not single-valued (e.g., hasChild). We have added this clarification at the begin of Section 2.3.\n\nComment 1b) Example ""first adopted son"": \nWe have now clarified that in the example, the predicate of interest is “child”. Thus, the additional qualifiers are not of relevance and are dropped.\nIn general it is common that textual occurrences of facts come with more specific qualifiers (“first adopted son”, ""second daughter"", ""eldest son"", ""third child"") than the predicate of interest, and reasoning over such qualifiers would be an interesting direction for future work.\n\nComment 2) Problem statement:\nThanks for pointing out this important issue! Not considering the IE method and its extracted facts in the problem formulation is a crucial point of our approach, but this was obviously not well expressed in the paper.\nIn short, we do not utilize the actually extracted facts because our goal is to investigate what a perfect method could principally extract from a piece of text, and we want to clearly separate this fundamental problem from the issue of experimentally evaluating the recall of a specific IE method/tool. We have revised the problem statement in the paper and added this clarification to the surrounding text.\n'}"		HkgAyb5aaQ	SklwW7NkV4	AKBC.ws/2019/Conference/-/Paper18/Official_Comment	['AKBC.ws/2019/Conference/Paper18/Reviewers/Unsubmitted']	1		['everyone']	SJeja3GrME	['AKBC.ws/2019/Conference/Paper18/Authors']	1548858111514		1548858174155	['AKBC.ws/2019/Conference/Paper18/Authors', 'AKBC.ws/2019/Conference']
95	1542459621860	{'title': 'RecallIE: Making Information Extraction Recall-aware', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper18/Authors'], 'keywords': ['Information extraction', 'recall', 'completeness'], 'TL;DR': 'Information extraction usually estimates precision but does not know about recall; we propose to fix this.', 'abstract': 'Information extraction from text, IE for short, is the backbone of automated knowledge base construction. IE usually comes with precision estimates; however, it lacks awareness of recall. This paper introduces and discusses the issue of IE recall estimation and its practical importance. We present RecallIE, a methodology for estimating the possible recall from a given text segment. RecallIE uses distant supervision to estimate from language features whether a passage contains all objects for a given subject-predicate pair. We evaluate RecallIE across various granularities of text, and across various predicates. Our preliminary results indicate that estimating recall is a promising direction and technically feasible.', 'pdf': '/pdf/2939f37f33ede2fcc8ec3a09bc0459b42d8ba98f.pdf', 'archival status': 'Archival', 'subject areas': ['Natural Language Processing', 'Information Extraction', 'Knowledge Representation'], 'paperhash': 'anonymous|recallie_making_information_extraction_recallaware', '_bibtex': '@inproceedings{    \nanonymous2019recallie:,    \ntitle={RecallIE: Making Information Extraction Recall-aware},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=HkgAyb5aaQ},    \nnote={under review}    \n}'}		HkgAyb5aaQ	HkgAyb5aaQ	AKBC.ws/2019/Conference/-/Blind_Submission	[]	18	r1er6kHOpQ	['everyone']		['AKBC.ws/2019/Conference']	1542459621860		1548857978191	['AKBC.ws/2019/Conference']
96	1548685947004	"{'pros': 'This work deals with the problem of label contradiction in joint learning from multiple datasets with different labelsets on the same anatomy. The proposed solution is an adaptation of the cross-entropy loss, where the voxels with contradictory labels in different datasets are treated differently than usual. Overall, the paper is well-written, the application is well-motivated and the contribution is novel to the best of my knowledge.', 'cons': ""In my opinion, there are no major flaws in the paper. Having said this, here are a few things that I think could further improve it:\n\n1. What happens if the 'voxels that are not lesion background' are not penalized at all? I think it is important to compare this form of 'naive' adaptive loss to the proposed method.\n\n2. The quantitative results for the brain tissue + WMH experiment with the naive dice loss seem to be at par with the multi-unet and the ACE, but the qualitative results are considerably worse. Is the case shown in the qualitative results an outlier for this setup?\n\n3. After first reading the problem statement (Sec. 2), I was a little unclear as to what is exactly meant by union of all labelsets. Perhaps a sentence to clarify this (saying that the union refers to a set where the background label is over-written if it is foreground in the other dataset) might be helpful.\n\n4. In Sec. 2.1, it is said that the mini-batches are sampled with equal probability from all datasets and all classes. As voxel-wise labels are predicted, how is it ensured that the mini-batches contain, on average, an equal number of voxels from each class?\n\n5. In the text following Eq. 4, consider using 'labelled as anything but lesion background' instead of 'non-lesion background'. In my opinion, this would be clearer.\n\n6. Depicting the quantitative results in a table instead of the box plot might be better to appreciate the differences between the various methods.\n\n7. If possible, consider moving the qualitative results into the main text instead of the appendix.\n\n8. Finally, I am not sure if it is necessary to give the multi-unet benchmark the advantage of additional modalities as described in Appendix C, although this only strengthens the benchmark. Providing all experiments with the same inputs would be a cleaner setup and would help focus entirely on the label contradiction issue.\n\n9. practise --> practice."", 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		Syest0rxlN	Bke7FMqhX4	MIDL.io/2019/Conference/-/Paper103/Official_Review	[]	2		['everyone']	Syest0rxlN	['MIDL.io/2019/Conference/Paper103/AnonReviewer1']	1548685947004		1548856758241	['MIDL.io/2019/Conference/Paper103/AnonReviewer1']
97	1548686540203	"{'pros': ""1. The paper attempts to address the classification problem in skin lesions and pneumonia in chest x-rays with a focus on 'paying attention' to the the ROI. It claims that focussing on the ROI of the minor class improves performance in cases of high data imbalance.\n\n2. The idea of forcing the Grad-CAM output to be inline with the bounding boxes is interesting. So is the idea of the 'inner' and 'outer' losses. \n\n3. The variety of experiments performed is extensive, and the improvement in results make a favourable case for the proposed method.   "", 'cons': ""1. Authors' claim of improved performance in imbalanced data when attending to ROIs is backed solely by empirical evidence. At the outset, the improved performance can be attributed to higher loss values for the minority class, induced by the additional supervision in the form of bounding boxes. (since L_a = 0 for majority class which doesn't have bounding boxes, eq. 1?). A more rigorous backing in this regard would be of interest. Otherwise, the novelty of the work is limited.\n\n2. Attention is fully supervised in this case,  and hence it should be made explicit that the term 'attention' here is not equivalent to its traditional counterparts in literature [1] [2]. \n\n3. The text seems to under-estimate the effort of requiring an additional annotations, even for the minority class. A dataset with 1 million examples with 10000 minority examples is still an imbalanced dataset. Also, it would be interesting to see if this ratio of this imbalance is crucial.\n\n4. Minor: L_g in text above eq. 1 is undefined. Few errors in text. \n\n\n[1] Oktay et al. 'Attention U-Net: Learning Where to Look for the Pancreas'. In: MIDL 2018.\n[2] Jetley et al. 'Learn to pay attention'. In: ICLR 2018"", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BJl2cMBHlN	ryx4RNc37V	MIDL.io/2019/Conference/-/Paper153/Official_Review	[]	2		['everyone']	BJl2cMBHlN	['MIDL.io/2019/Conference/Paper153/AnonReviewer1']	1548686540203		1548856757197	['MIDL.io/2019/Conference/Paper153/AnonReviewer1']
98	1548683742467	"{'pros': 'This paper proposed an interesting encoder-decoder network to solve the problem of label ambiguity in multi-assessor circumstance. \n\nThe solution of providing multiple segmentation with different certainty is interesting.\n\nAdding a parameter to control the compactness of the segmentation is interesting.', 'cons': 'Theoretical support of the algorithm is not solid. Could the authors provide a more detailed and theoretical explanation on the effectiveness of the HyperSphere Loss?\n\nWhat does the ""Transposed convolution 16*16"" do in the network in Figure 4? What is the size of the input feature map of that layer?\n\nThe description of the vanishing shapes data set is not clear. I cannot find clear correspondence between the label and the input image.\n\nI am wondering will false positive also increase when the hyper-parameter $\\rho$ goes up. ', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		SJl27gYRkN	rJeIy9K2QN	MIDL.io/2019/Conference/-/Paper14/Official_Review	[]	3		['everyone']	SJl27gYRkN	['MIDL.io/2019/Conference/Paper14/AnonReviewer1']	1548683742467		1548856756984	['MIDL.io/2019/Conference/Paper14/AnonReviewer1']
99	1548429706754	"{'pros': 'The paper describes a very nice approach for dealing with rotation invariant texture/feature detection in convolutional neural networks. From a classical point of view rotation invariant convolution layers can be obtained with filters that are isotropic. This is a rather limiting viewpoint and the current submission nicely extends the class of invariant CNN layers by defining a locally rotation invariant filter by a roto-translation lifting convolution (see work on group convolutions networks) directly followed by a max-pooling over rotations. Such layers are based on non-isotropic filters and can be efficiently implemented by defining convolution kernels in polar coordinates, relying on spherical harmonics.\n\nThe theory is well explained and contains interesting details that are also valuable from a practical point of view. Although not validated on very large datasets (and with minimal architecture design), the experiments are carefully setup and convincingly demonstrate the potential of the proposed way of dealing with rotationally invariant feature/texture descriptors.\n\nMy recommendation is to accept the submission.\n', 'cons': 'Overall I really enjoyed reading the paper. In this section I provide some minor comments and suggestions. \n\nSmall fixes:\n\nTypo on page 2 “first convolution layer, what exploits” -> “first convolution layer, that/which exploits”\n\nI found the introduction of 2.4 confusing in the transition from h_i being a 1D function (from \\mathbb{R} to \\mathbb{R}) to its voxelized version (from \\mathbb{R}^3 \\rightarrow \\mathbb{R}) with the mention of isotropy constraint. I don’t see this as a constrained as by definition \\rho = \\lVert \\mathbf{x} \\rVert is isotropic. What I think is important here is that the radial profile extends all the way to the corners of  3D kernel, and this then sets the number of trainable parameters in h_i (with unit voxel distance spacing). Perhaps this can be clarified?\n\nFigure 2 (which nicely illustrates the above) raises the following question: the last weights in the h_i vector only affect the corners of the 3D kernel and therefore some “voxel features” only appear at diagonal rotations, doesn’t this affect the invariance you aim for? The shape of the kernel is not isotropic and therefore it is not truly rotation invariant. True rotation invariance could be achieved by limiting \\rho leq (c-1)/2 (so the corners are always zero).\n\nI really appreciated the short comment on the angular Nyquist frequency, these things are good to realize when working with the actual code.\n\nPage 6 regarding padding, this is zero padding?\n\nMinor comments:\n\n[Just a remark for a possible interesting extension (intro of section 2)] Regarding the action of SO(3) on R^3 and S^2:, for an example of efficient spherical harmonics implementations of SO(3) acting on axially symmetric functions on S^2 also see citation [2] below.\n\nAfter Eq.(1), you could possibly identify “I*f(R\\cdot)” as a lifting group-convolution (see e.g. Cohen et al. or [5] below), followed by sub-group pooling (max of rotations).\n\nPage 5 after Eq. 6. A nice property of expanding each filter in the same basis is that you can pre-filter the input whit your set of basis-functions separately and then combine the results with the corresponding coefficients to create all feature maps. \nThe second to last paragraph of section 4 sounds contradicting and could be rewritten: “the improvement… of SH convolution is limited … yet a significant increase in accuracy”.\n\nFinally I would like to mention some very related work both on steerable filters and local rotation invariance which could be addressed in the submission.\n\nFor work on (optimal) 3D steerable filters in texture analysis: See e.g. [1] (and references therein) for a recent overview and toolkit for 3D steerable image filtering. See e.g. [2] for (optimal) steerable filter construction/fitting for axially symmetric texture detection in 3D medical image data. In [2] additional axial symmetry is exploited to further reduce the number of SH coefficients and it makes rotation over your \\gamma redundant. This essentially boils down to relying on Fourier transforms/irreducible representations on the sphere S^2 (<> quotient group SO(3)/SO(2)), see e.g. the book by Chirikjian and Kyatkin [3] and [4].\n\nFollowing up on your discussion paragraph on page 8 regarding LRI in relation to the work in the group-CNN context by Cohen et al.: In addition to the already cited work by Weiler et al. 2017 the works described in [5] and [6] are very related to the current proposal in two ways. (1) In these papers the construction of local invariance is also studied by following “lifting convolutions” (creating feature maps in a higher-dimensional position-rotation space) by max-pooling over rotations. In these works this has been done with additional group convolution layers in between the lifting and rotation-pooling layers (creating local rotation invariance over the net receptive field size). (2) In [5] and Weiler et al. 2017 the authors describe a similar behavior as observed in Table 1: a higher angular resolution improves performance. A main advantage of your method is the very high efficiency in dealing with trainable parameters, however, it should be noted that neither in [5] nor the method of Weiler et al. the number of trainable parameters increases with angular resolution when only a lifting layer directly followed by a rotation pooling is considered.\n\nThe work by Mallat et al is very much concerned with both local and global rotation invariances in image data (see e.g. the Ph.D. thesis by L. Sifre [7]).\n\n[1] Skibbe, H, and Reisert, M.. ""Spherical tensor algebra: a toolkit for 3d image processing."" Journal of Mathematical Imaging and Vision 58.3 (2017): 349-381.\n[2] Janssen, M, et al. ""Design and processing of invertible orientation scores of 3D images."" Journal of Mathematical Imaging and Vision 60.9 (2018): 1427-1458.\n[3] Kyatkin, A, and Chirikjian, G. Engineering applications of noncommutative harmonic analysis: with emphasis on rotation and motion groups. CRC press, 2000.\n[4] Duits, R, et al. ""Fourier Transform on the Homogeneous Space of 3D Positions and Orientations for Exact Solutions to Linear Parabolic and (Hypo-) Elliptic PDEs."" arXiv preprint arXiv:1811.00363 (2018).\n[5] Bekkers and Lafarge et al. “Roto-Translation Covariant Convolutional Networks for Medical Image Analysis”. In: MICCAI 2018\n[6] Zhou, Yanzhao, et al. ""Oriented response networks."" Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017.\n[7] Sifre, Laurent, and Stéphane Mallat. Rigid-motion scattering for image classification. Diss. PhD thesis, Ph. D. thesis, 2014.\n', 'rating': '4: strong accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'special_issue': ['Special Issue Recommendation'], 'oral_presentation': ['Consider for oral presentation']}"		H1gXZLzxeE	Byx75tjuQN	MIDL.io/2019/Conference/-/Paper77/Official_Review	[]	1		['everyone']	H1gXZLzxeE	['MIDL.io/2019/Conference/Paper77/AnonReviewer3']	1548429706754		1548856756765	['MIDL.io/2019/Conference/Paper77/AnonReviewer3']
100	1548435308456	{'pros': 'The submission addresses very relevant problems in CNN based medical image analysis. Computation time and memory constraint are important issues. The proposed framework deals with computation time and memory constraints via sparse image analysis and it furthermore enables to incorporate more context (another important issue in image analysis) in the network via graph CNNs.\n\nThe results are impressive with quite a big improvement over U-nets, while the proposed architecture has much less parameters. I do have some concerns however regarding the experiment, which may bias the experiment in favor of the proposed approach (see comment below). Nevertheless, I very much appreciate the creativity of this paper and the ambition to solve the above mentioned challenges in medical image analysis.\n\nI recommend accept conditional to minor changes/clarifications.', 'cons': 'I put all my minor and major comments and suggestions in this section.\n\nThe paper puts a lot of emphasis on context aggregation, however, to me it is not completely clear how this is achieved. The authors mention that they rely on graph CNNs on which they perform pooling (via graph diffusions), but there is no explanation of how this contributes to a higher contextual “understanding” of the data. I would appreciate it if this were somewhere in the document explained (preferably in the introduction).\n\nA related issue is the following. In the conclusion the following is mentioned: “we showed that GCNNs can successfully mimic … UNet-like encoder-decoder architectures of pooling global context information”. I think that this sentence in a way down-plays your own work. I don’t think it was the goal to mimic UNets, but I also don’t directly see how it mimics the context aggregation features of UNet type architectures other than that there is some global information analysis going on. The UNets are hierarchical in nature (it is in this sense not unique, there are many other multi-scale analysis approaches in MedIA) and I don’t think this hierarchical nature is apparent in this paper. As far as I can tell, the method works “only” on two levels (though quite effectively): at pixel level (structure head) and global level (graph based semantic head).\n\nFinally, regarding context aggregation. Could you explain in what way context is exploited. I have a feeling that the semantic head sort of has a function of telling the structure head “hey, I am pretty confident that you can correctly predict this class in this region, but I am not going to let you contribute to the segmentation in this region”, but I’m not fully sure that this is the idea behind this splitting. In a naïve way, you could also just let the semantic head spit out confidence scores of 1 for each location and each class (it is then a standard Hough voting). It would be nice if the motivation for the design was better explained in the paper.\n\nIn the related work section of the introduction the transition to work on graphs comes a bit out of the blue. Up to that point there is no mention that the proposed work relies on graph CNNs. Perhaps the introduction could be improved by mentioning up-front the general idea of the paper?\n\nDue to some missing details in the paper on graph CNNs I went to study the references of this paper and found that the embedding of some these references in the introduction is not fully correct. It is suggested that the works of Henaff et al. [H] and Kipf and Welling [KW] aim for local support of spectral filters in response to the work by Bruna et al. [B], which supposedly doesn’t have this property. In [B] the filters are indeed localized (localization is obtained through smoothness in the spectral domain) and the main contribution of [H] is not to enable local support (they do rely on results of [B]) but rather to describe theory for constructing graphs when they are not yet defined a priori, and they additionally nicely present/summarize the framework for graph-CNNs. A large part in [KW] is indeed concerned with locality of the spectral filters. Instead of relying on splines (as is done in [B] and [H]), [KW] rely truncated Chebyshev polynomials based on the work of Hammond et al. (2011), and describe clear properties of this approach regarding the support size of the graph filters.\n\nThere is a serious typo in the first equation. In the definition of the adjacency matrix the 2 \\sigma^2 should be within the exponential. If it is outside, as it is now, the sigma does not have any effect on the graph other than scaling all weights (this scaling is for example undone in the Laplace operator D^{-1}A and is also undone by simple scaling of the graph convolution kernels). \n\nSmall suggestion: In the second equation either the \\alpha or \\beta can be omitted (only one parameter is sufficient to balance between the two terms). \n\nStart of section 2.2.: Could you explain why you designed the network in such a way (parallel structure and semantic head). In principle you don’t need the semantic head for Hough voting, but it does seem to improve the results. Some intuition would be appreciated.\n\nOn page 5 you describe the pooling of features on a graph. At first it was not apparent to my how this is done, but I believe the approach is a sort of equivalent of average pooling in classical CNNs (except for the down-sampling part, which is not done in this work). Perhaps this link, or some intuition, could be provided in the paper.\n\nRegarding the diffusion process I would personally find an intuitive explanation more important that trying to describe the mathematics of it, especially because I have the feeling that the provided matrix L is incorrect (see also your own work Hansen et al. 2018 for definitions of L). The part about the “diffusion matrix” on page 5 is unclear: You provide a matrix which I believe is usually referred to as “transition matrix” in Brownian motion processes. This matrix could be used to define a Laplacian operator L=I – D^{-1}A, which in turn can be used to describe a diffusion process (p\uf0dfp+L.p). I have several problems with this paragraph: 1. There is probably a typo (a missing identity matrix) and 2. The section does not describe how this matrix is used, 3. nor does it provide intuition (e.g. you pool features by means of graph smoothing, similar to average pooling in standard CNNs). I had to dive into spectral theory on graphs to understand what you meant in this section.\n\nIn the experimental setup (on page 6) you describe that class weighting was not applied. I think this choice could have a quite severe effect on you experiments. In neither of the networks in this paper you deal with the disbalance of labels. The fact that the U-net underperforms could be due to the lack of balancing the data/losses. See also figure 3, where the small bladder is completely ignored by the U-net. Of course it could also be that your method is more robust against this disbalance (possibly due to the sampling strategy). This would indeed be a good thing, but it is not addressed in the paper. For me this leaves the impression that I cannot really tell if your method is intrinsically better, or that it is just less sensitive to unbalanced data.\n\nThe sigma parameters is set very small (0.1). This would mean that the Gaussians decay within a pixel distance. Is there any connectivity left then? Or do you use normalized coordinates?\n\nSmall typo in results section: “yields a higher score as all UNet” , “as”->”than”.\n\nFinally some additional questions:\n\nIs the computation time indeed reduced compared to e.g. the U-Nets?\n\nWhat kind of graph-CNN is used (you mentioned in the introduction some variations but not which one you actually use, I suppose the same “convolution” type as the one in Kipf and Welling is used)?\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		SkxccsWxxE	S1l4u1a_m4	MIDL.io/2019/Conference/-/Paper66/Official_Review	[]	1		['everyone']	SkxccsWxxE	['MIDL.io/2019/Conference/Paper66/AnonReviewer3']	1548435308456		1548856756420	['MIDL.io/2019/Conference/Paper66/AnonReviewer3']
101	1548683451884	"{'pros': 'The paper identifies import problems in AI based (medical image) analysis, and presents novel idea to deal with the notion of rotation invariance. The idea is inspired by the success of ORB (Oriented fast and Rotational Brief), and provides an interesting approach to equip CNNs with a method to deal with rotation invariance/equivariance. The experimental results are in favor of the proposed CNNs over classical CNNs.', 'cons': 'This section contains all minor and major comments and suggestions.\n\nOverall, I do not recommend to accept this submission based on the following: \n– citations are sometimes incorrect or missing and I find some statements in the manuscript to be disrespectful \n– the authors makes claims which are not supported by the paper, nor by citations \n– the core method itself is not clearly described (I’m left with a lot of unanswered questions).\n\n[Introduction]\n\nThe introduction starts good, it is quite ambitions and identifies some core challenges in AI based image analysis: “… there is still a lack of knowledge on the emergence of concrete interactions within the network”, “..it might be advisable to optimize the network’s choice of transformation itself..”. However, I do not see why some of the challenges are mentioned, or how these are solved. In fact, it left me a bit confused and it gave me the impression that it was somewhat contradictive. A lot of it regards invariance to unknown transformations and it is suggested that you should not assume too many invariances a priori. It is suggested that in this paper these problems are addressed on a generic level, however, I found the paper to be highly specific to rotation invariance only (an a priori choice), which contradicts the grant view posed in the paragraphs before.\n\nThe paragraph on encoding rotation invariance into NNs is a bit weak. It misses some key publications. For a very recent overview see e.g. [1]. Related work on spatial transformer networks (see e.g. [2]) and group theoretical approaches are missing. For theory see e.g. [1,3,4]. For successful applications (and theory) in medical imaging see e.g. [5,6], in addition to the ones you already have. Furthermore, when citing work on steerable filters in CNNs I think [7] deserve an explicit mention as well. \n\nI also don’t think it is correct to say that Weiler et al. 2017 is based on the work of Jacob and Unser (they are not even cited in Weiler et al. 2017). Jacob and Unser have made great impact in the computer vision field with steerable filters, and I think they do deserve a cite, but when doing so, I think it is fair to also acknowledge the ones that actually came up with the notion of steerable filters in computer vision in 1991: Freeman and Adelson [8]. \n\nI also found the statement in the last paragraph on page 2 dubious: “While a) *unintentionally* reduces the effective network capacity…”. What is meant here? As far as I understand from the above mentioned methods is that the transformation invariances/equivariances are explicitly made part of the network architecture such that the networks do not need to learn for geometric relations. E.g., the network does not need to learn rotated copies and network capacity becomes available for learning tasks specific representations, thereby increasing performance. I think this effect is neither “unintentional” nor does it reduce network capacity, it in fact increases it.\n\n\n[2. Materials and methods]\n\n\n(punctuation is missing, in particular (3) and (4) should end with a “.”)\n\nTypo on page 3 “the belief that data amount might”, “data amount”->”the amount of data”?\n\nThe last sentence of the intro of 2: “With our approach … a matter of the network optimization process”? I don’t understand this statement. From my point of view it is more a matter of network design than a network optimization process.\n\nThe sentence before (3) is logically not correct. The moments do not allow for rotation (you can rotate patches without any need for moments). A representative orientation can be derived using the moments and this orientation can be used to rotate the patches.\n\nEq. (3) describes a way of estimating a global patch orientation. I expect however that this angle is highly sensitive to global (low-freq) intensity variations. The given angle is essentially the angle that the center of mass makes w.r.t. the origin. If this center and the origin coincide this angles does not even exist, and if they are close I expect a high uncertainty on the angle. \n\n“generalization to the n-dimensional case is straightforward”, such statements should not be made lightly unless you have a good citation to support this. Already in the case n=3 you’d have to make decisions in how to couple orientations (in S^2, which has 2 parameters) with rotations (in SO(3) which has three parameters).\n\n\n“Further approaches could include a learned transformation”. Also here I think such extensions are not trivial at all and do not follow directly from the work presented here. I have several problems with this: \n1.\tYou do not provide any details to support this statement (except for a reference to section 4 which basically repeats this statement)\n2.\tYou could of course try to learn this transformation matrix (which is essentially happening in spatial transformer networks (STNs) [2]), but I understood that the whole point of using matrix (4) was that it is parameterized by a rotation which you “know” (or at least are able to estimate), you cannot estimate more general parameters in the same way you estimate orientation.\n\n\nEq. (5) gives a description of the rotationally invariant layer, which includes a convolution of weight matrix W with an image patch. To me this equation is confusing since the difference in notation between weights (capitalized) and patches (lowercase) suggest that they are different data types. You could clarify (5) by also mentioning the size of W (which is k x k?). Am I right that (5) specifies a fully connected layer? (the patches have the same size as W)\n\n\nLet’s assume p is larger than the convolution kernels. Then I have a problem with interpreting the framework: The rotation invariance is global (first the entire patch is rotated and then a standard conv is applied). In the next layer, the full input is rotated and then again a conv is applied. How does this work on full image level, e.g. in the U-nets? If now again the full image is rotated then you completely destroy locality property and structure of the convolutions. E.g. a rotation of 180 degree’s moves a pixel all the way to the other side of the image, and if the feature maps are rotated independently of each other you completely miss spatial correspondences. I am probably missing something here, but this is as much I can make from the presented methodology. A clearer, less ambiguous, explanation of the methodology would have been helpful.\n\n\nThis brings me to my next question. You choose to go for a hybrid approach. What happens if you go fully rotation invariant? (non-hybrid)\n\n\nWhat happens if you do not additionally provide the values sin \\theta(x,y), cos\\theta(x,y) as extra feature maps? Why are the orientations position dependent? Eq. (3) describes a global orientation estimate (independent of x and y). Perhaps the orientations are indeed determined at each pixel location, but then how would you define the convolution of (5); maybe rotate each kernel locally? This would make the conv layer highly non-linear and computationally very expensive. Either way, essential details are missing…\n\n\nSection 2.3.1 on page 6, “for three iterations with varying”. Perhaps you can rewrite this sentence since “iteration” typically refers to one optimization step. Perhaps you can say, we repeated each experiment 3 times with different initializations (at least if this is what is meant here, otherwise I don’t understand the sentence).\n\n\n[4. Discussion]\n\n“Though there generally is some understanding on how rotational invariance can be realized…”. This sentence undermines the work by many others in this direction. The last couple of years has seen quite some contributions to rotationally invariant and equivariant networks (see cites below). The general theory is well understood from a group theoretical point of view [1,2,3] and it has seen great success in medical imaging (see the cites in your manuscript and e.g. [4,5]) in terms of performance, network capacity/complexity and use of limited training samples. To say that there is “some understanding” is in my opinion a severe understatement.\n\n\n[references]\n[1] Cohen, Taco, Mario Geiger, and Maurice Weiler. ""A General Theory of Equivariant CNNs on Homogeneous Spaces."" arXiv preprint arXiv:1811.02017 (2018)\n[2] Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. ""Spatial transformer networks."" Advances in neural information processing systems. 2015.\n[3 ]Cohen, Taco, and Max Welling. ""Group equivariant convolutional networks."" International conference on machine learning. 2016.\n[4] Kondor, Risi, and Shubhendu Trivedi. ""On the generalization of equivariance and convolution in neural networks to the action of compact groups."" arXiv preprint arXiv:1802.03690 (2018).\n[5] Bekkers and Lafarge et al. “Roto-Translation Covariant Convolutional Networks for Medical Image Analysis”. In: MICCAI 2018\n[6] Veeling, Bastiaan S., et al. ""Rotation Equivariant CNNs for Digital Pathology.” In: MICCAI 2018\n[7] D. Worrall, S. Garbin, D. Turmukhambetov, and G. Brostow, “Harmonic networks: Deep translation and rotation equivariance,” Preprint arXiv:1612.04642, 2016. 6, 8\n[8] W. Freeman and E. Adelson, “The design and use of steerable filters,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 13, no. 9, pp. 891–906, 1991\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		BJlVNY8llV	rkxN6dt2X4	MIDL.io/2019/Conference/-/Paper111/Official_Review	[]	3		['everyone']	BJlVNY8llV	['MIDL.io/2019/Conference/Paper111/AnonReviewer2']	1548683451884		1548856756129	['MIDL.io/2019/Conference/Paper111/AnonReviewer2']
102	1548682846119	"{'pros': 'The manuscript is mostly well written.\n\nIt is original in the sense of showing how far one can get with an established cycle-GAN approach and model-based training data to segment simple shaped objects (glomeruli) which differ in number of objects, size and shape from other repeated simple objects (nuclei, tubuli). \n', 'cons': 'The claim that the method is very robust to the object parameters and that ""the shapes of the simulated objects does not have a major impact on the final segmentation performance"" is not supported by sufficient evidence. The authors should include the statistics of the ground truth annotations and the predicted segmentations when assuming circles and when assuming ellipsoids. Only then readers might be convinced that these claims hold. Doubts stem from the observation that cycle-GAN will synthesize any differences in the distributions to please the discriminator.\n\nThe F1 scores of the shown examples in Fig.4 should be stated, such that readers can judge if these are representative examples or not. Results from ME and MC for the same image and for different stains should be shown to be able to appreciate their performance differences. Contour overlays of the ground truth and predicted glomeruli segmentation on the image will save space and enable readers to better judge segmentation accuracy.\n\nThe mean F1 scores for the supervised method should be included in the text. For claiming ""better performance"" a statistical significance test should be performed.\n\nGadermayr 2017 was used as baseline supervised method. Gadermayr 2017) achieved very good results (F1 0.91 for CN2 method) when trained only on PAS stain on 18 WSIs. How was this method trained for the different stains for this dataset (3 stains, 6 images each) when using 1, 2, 4, 8 WSIs (Fig. 3)? It seems Gadermayr 2018a should be used as baseline supervised method, as it can cope with different stains (Dice 0.81-0.86)?\n\nThe claim that the method can easily be adapted to other applications by changing the model should be tuned down. Most annotation problems would require quite complex size and shape models and might also need an appearance model if similarly sized and shaped objects are present.\n\nMinor:\n\nPlease clarify what is changing in the repeated experiments, e.g. new simulated annotations or different random selection of the same dataset?\n\n...where nuclei cannot be clearly detected (Fig. 4, third column)... Should this refer to second column?\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		B1gT9_B81E	S1gIPIF2XE	MIDL.io/2019/Conference/-/Paper2/Official_Review	[]	3		['everyone']	B1gT9_B81E	['MIDL.io/2019/Conference/Paper2/AnonReviewer3']	1548682846119		1548856755833	['MIDL.io/2019/Conference/Paper2/AnonReviewer3']
103	1548682338111	{'pros': 'The submission proposes to combine a simple generative model of segmentation mask with GAN-based image to image translation in order to learn how to segment glomeruli in digital pathology slides. Using information about the distribution of shape, size and number of objects in a segmentation mask, the method can be trained without supervision to achieve performance comparable to a fully supervised method.\n\nAlthough prior knowledge is needed about the distribution of objects, it seems to be enough with rough estimates based on visual inspection. \n\nOverall, the presentation is clear and the experiments nicely illustrate the potential, as well as some limits, of the suggested approach. If the approach generalizes well, this could become a valuable tool in many applications due to the ease of generating synthetic segmentation masks.', 'cons': 'The method is validated on a relatively small dataset (9 images in total?). It is not clear from this scale that performance estimates are reliable. This should be considered in future work. I would also like to see if there is a difference between dyes.\n\nA problem for GANs is differences in label distribution. If the generated segmentation masks contains significantly less/more glomeruli than the images, it is likely that performance will degrade. The experiment with circle/elipse shape indicates that capturing the exact shape is not very important.  I would like to have seen an experiment investigating the importance of estimating the other parameters (shape, number).\nIt is not clear if the visual assessment of parameters is only based on training images or if test images are also included. \nI am missing some information about the workload of the visual assessment. F.ex is it much less than providing rough segmentations by clicking the center of glomeruli?\n\nThe plots on the left in Figure 2 are difficult to view. I suggest you try a different color, linestyle, thickness, ...\n\n', 'rating': '4: strong accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		B1gT9_B81E	B1l5vNF2QV	MIDL.io/2019/Conference/-/Paper2/Official_Review	[]	2		['everyone']	B1gT9_B81E	['MIDL.io/2019/Conference/Paper2/AnonReviewer1']	1548682338111		1548856755617	['MIDL.io/2019/Conference/Paper2/AnonReviewer1']
104	1548682294503	{'pros': 'The authors propose a method to correct k-space of cardiac MR images affected by motion artefacts. The method employs a previously proposed deep learning method (Automap) and extend it with a GAN. The Automap is trained to map K-space images to an output image. By introducing synthetically created motion artefacts by modifying K-space, Automap is trained to generate original images from corrupted K-space images. Evaluation is performed by automatic segmentation of images after artefact correction.\n\nThe paper is clearly written.\n\nThe authors provide extensive experiments, comparing their method with other artefact correction methods.\n', 'cons': 'Figure 3 does not provide clear evidence that the proposed method corrects motion artefacts in k-space images “in the wild”. Both segmentation results are very similar and have a major error in left ventricle myocardium segmentation. It seems that the proposed method only seems to correct synthetically created motion artefacts.\n\nIt is unclear if the method would reconstruct K-space images correctly if motion artefacts are absent. What is the influence on segmentation performance applying artefact correction on k-space images without motion artefacts?\n\nOnly average scores are provided. It would be interesting to see boxplots (including visualization of outliers).\n\nHow does the Dice of 0.91 on original images provide a clarification for higher scores in Table 1?\n\nIt is unclear how activity regularization was performed\n\nFigure 2 is hard to assess because input images are absent.\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		BkgjbQ30yN	HkxCNVFh7N	MIDL.io/2019/Conference/-/Paper24/Official_Review	[]	3		['everyone']	BkgjbQ30yN	['MIDL.io/2019/Conference/Paper24/AnonReviewer2']	1548682294503		1548856755398	['MIDL.io/2019/Conference/Paper24/AnonReviewer2']
105	1548681455971	{'pros': '-  This paper solves a tracing problem of thin structures by foregoing segmentation.\n-  It gives more insight into the applications of Deep Reinforcement Learning (DRL) in bio-medical imaging and its related applications.\n-  Results show comparable performance with existing standard software (Vaa3D).\n- Method uses a stochastic policy to measure the trackers uncertainty given its entropy, compared to traditional trackers that do not include this measure. \n', 'cons': '- Authors evaluated the tracker on synthetic and microscopy dataset (Bass et al, 2017); synthetic data is generated by simulation of single axons fitted by polynomial splines to random walks in 2D space with Gaussian noise. How can one evaluate the quality of the synthetic data? We recommend authors to include a brief discussion on quantitative evaluation of synthetic images to real images.  How does one account for bias?\n- DRL trackers are trained on synthetic (32,000) and validated on 1,000 samples. Hyper-parameters tuning set is quite small. What is the impact of using varying sizes for training and validation on the 20 2D held out test set?  Unless there is some reasoning behind using a very set for validation. (50 -50), (70-30) splits performance would be of value. This may answer the question what is considered a reasonable amount of data to use in such settings. \n- Lastly, in the second stage of experiments; authors mention the use of microscopy data (Bass et al) for testing. With reference to the work of Bass et al, there are 20 test and 80 training samples i.e. 100 tiff files. Is there a reason authors did not fine tune the tracker on the train-set (80) but rather used a k-fold method on the same test set with (15 – 4 splits). Kindly clarify this issue, it would valuable to assess the performance of the tracker after finetuning on the set train-set.   \n', 'rating': '4: strong accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'special_issue': ['Special Issue Recommendation'], 'oral_presentation': ['Consider for oral presentation']}		HJxrNvv0JN	H1lugWKnQE	MIDL.io/2019/Conference/-/Paper13/Official_Review	[]	3		['everyone']	HJxrNvv0JN	['MIDL.io/2019/Conference/Paper13/AnonReviewer1']	1548681455971		1548856755177	['MIDL.io/2019/Conference/Paper13/AnonReviewer1']
106	1548681414461	"{'pros': 'The authors present an unsupervised approach that combines variational auto-encoders (VAE) to get a representation of the input image, with a convolutional neural network discriminator that evaluates obtained representations. Through this approach, the authors aim to face the following points:\n- The obtaining of an accurate latent space of all the images (embeddings) that allows for detecting the mechanisms-of-action (MOA) of the chemical used to treat cells.\n- A model that provides an accurate reconstruction of each image.\n\nThe main difference between the work of Larsen et al 2016 and this one, is the definition of loss functions: Instead of integrating the loss function of a GAN into the VAE loss function, as done by Larsen et al, 2016, the loss of the VAE and the loss of the discriminator are combined in a way that they complement each other. \n\nWith the proposed approach, the authors have obtained a good balance between both tasks: accurate detection of MOA (even if it does not outperform the results of Ando et al, 2017) and realistic reconstruction of images which are more accurate than the ones obtained when using GANs.\n\nBesides, the manuscript provides a precise and very updated review of state-of-the-art methods, which are taken into account in the proposed methodology. Most of the text is written in a clear way: the problem to solve is well illustrated, each of the procedures followed in this work is either described in detail or cited properly, and the results are exposed concisely.\n', 'cons': 'While both LVAE and LDi are defined, I miss a final complete expression in which it can be seen how both functions are combined.\n\n""We conjecture that the reconstruction term in LVAE should not be discarded and that the additional losses LDi can be all used to compensate the limited reconstruction ability induced by LVAE, as opposed to the formulation of Larsen et al."" The method of Larsen et al. 2016 was evaluated in a different field (reconstruction of human faces) so, to prove the statement done by the authors, in future work, I would recommend them to compare both methods \n\nSection 3.4. It is not clear the number of convolutional layers used in each part of the network. Please, specify it either in the text or in Figure 1 so the method can be reproducible. For instance, it is written ""All three CNNs have four convolution layers"", did you mean ""All the CNNs""? or on the contrary, are you referring to the encoder, decoder and the discriminator? How many filters of size 5x5 do you have in each convolutional layer? Do you use zero padding? \n\n""Images were randomly shuffled and presented to experts to assess whether each cell was real or synthetic."" Are the biologists told whether the cells are treated or not, and which is the treatment in each case? Would this affect their classification?\n\nReferences. Please, review all the references and make sure that all of them are correctly written:\n- Claire McQuin, Allen Goodman, Vasiliy Chernyshev, Lee Kamentsky, Beth A Cimini, Kyle W Karhohs, Minh Doan, Liya Ding, Susanne M Rafelski, Derek Thirstrup, and Others. --> ... et al. instead of and Others,\n- arXiv and bioRXiv references: specify the version you are referring to and indicate always the name of the journal or site (in most cases it is not said)\n\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'oral_presentation': ['Consider for oral presentation']}"		HyxX96_xeN	HJxRTet37E	MIDL.io/2019/Conference/-/Paper121/Official_Review	[]	3		['everyone']	HyxX96_xeN	['MIDL.io/2019/Conference/Paper121/AnonReviewer3']	1548681414461		1548856754966	['MIDL.io/2019/Conference/Paper121/AnonReviewer3']
107	1548680069342	"{'pros': 'Authors propose a deep learning -based method for  challenging problem of cardiac 4D-flow MRI.\n\n1) Authors recognise problem of SNR vs resolution tradeoff during imaging\n2) Proposed method outperforms cubic spline interpolation as shown on validation part of in vivo data and CFD simulations\n3) Invariance of the method due to data domain shift is shown via comparison of accuracy on in vivo data and CFD simulations, which is very reassuring.\n4) Introduced ""mutually projected l1 error"" shows improved results compared to l1 error.', 'cons': 'Problems of super resolution in MRI and optical systems are quite different, due to the fact that MRI samples k-space directly (see discussion Uecker, M. , Sumpf, T. J. and Frahm, J. (2011), Reply to: MRI resolution enhancement: How useful are shifted images obtained by changing the demodulation frequency?. Magn. Reson. Med., 66: 1511-1512. doi:10.1002/mrm.22989)\n\nTherefore, it would be interesting to have more understanding weather author aim to filter out acquisition artefacts, or extract prior knowledge about flow fields in aorta.\n\n1) Authors don\'t discuss sensitivity of their method to varying noise levels.\n2) Even fully sampled 4D flow data can be rather noisy and have partial voluming effects. Applicability of using in vivo data as ground truth should be discussed.\n3) It is not clear what kind of reconstruction algorithm was used during imaging process.\n4) I expect that major improvement over cubic interpolation is due to the properties of fluid flow fields learned by the network. Therefore, analysing divergence of the super-resolved flow field would be a great addition to method evaluation.\n\nSome minor comments:\n* Title should be capitalised \n* pg 5. notation ""v="" is misleading\n* Capitalisation of bibliographic entries is incorrect. E.g. mri, laplacian.', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		S1g0Vms-lE	HyeTFjOhQ4	MIDL.io/2019/Conference/-/Paper140/Official_Review	[]	3		['everyone']	S1g0Vms-lE	['MIDL.io/2019/Conference/Paper140/AnonReviewer3']	1548680069342		1548856754714	['MIDL.io/2019/Conference/Paper140/AnonReviewer3']
108	1548679977424	"{'pros': 'The authors present a temporal convolutional neural network for the segmentation of surgical tasks and validate it against the JIGSAWS dataset. The problem they present is of relevance and of current active research. For instance, several challenges have been held in the past years to assess the quality of state of the art methods addressing the same problem.\n\nBeing an active area of research, the field is quite rich in literature. For a conference paper, it is quite difficult to cover all the existing works. The authors have made a good effort to present a concise summary of relevant related work. \n\nThe obtained results present an accuracy higher than other methods from the state of the art. ', 'cons': '- Clarity: \n** The authors do not deliver very well the message. After reading the paper, I find it difficult to establish what exactly is their contribution. From my understanding, they have used previously proposed network architectures for the task they aim to solve being their main contribution to add skip connection to an encoder/decoder architecture previously proposed by Lea et al, 2016. The authors should try to make this quite clear in the paper. \n** The authors claim that a second contribution is to add a parallel convolution layer (Fig 3) to the modified ED-TCN network (Figure 2). However, I have the impression that these two networks are nearly equivalent. The sole difference is an extra parallel convolution layer that starts at layer 2 and connects at the output of the encoder. \n**  Could you please explain why you chose the filter sizes as such? It seems that you have replicated the architecture proposed by Lea et al. While this is a valid choice, it is important to justify why the very same network architecture works well for your problem.\n** In general, the methods should be better explained as to try to justify the different methodological choices (e.g. why you need to add skip layers, why the ED-TCN was not changed at all or if experiments proof it works well as such, how is the kinematic data combined with the video data frames?). This would make the paper more clear.\n** The paper has numerous errors in the use of the English language. I recommend to have a careful review of it and/or have it proof read by a native speaker. Some common mistakes I have found:\n    1) Using ""an"" before a word starting with h. It should be ""a"".\n    2) Wrong use of verb tenses. Many times the authors use the tense for the third singular person when the noun is plural or the opposite. Examples: ""which would allows"", ""Measurements from the dataset includes"", and ""but it also increase"" (second case). \n    3) Using the singular form of a noun when the plural should be used. Examples: ""surgical task"", ""autonomous vehicle"", ""field"", ""block"", ""layer"", among others.\n   4) There multiple cases where the wrong indefinite or define article is used or it is missing.\n   5) results are not as high -> Good or accurate should be preferred.\n\n** The images from Figure 4 have been taken from the paper of Ahmidi et al, TBME 2017. Please give the right credits.\n\n- Quality of the evaluation:\n** Given that the benchmark from Ahmidi et al uses the very same dataset, one would expect that the authors use the same setup and metrics used there. Is there any particular reason why the authors decided to exclude some of the metrics and experimental setup (leave-one-super-trial-out)?\n** Table 3: The work from Lea et al 2016 (ED-TCN) has no reported accuracies. Is this an error? \n\n-Originality:\nAs previously mentioned, it is difficult to establish the original contributions of this paper. Currently, I consider its contributions are mainly incremental as it reuses state of the art work. I encourage the authors to re-structure their paper so that one can easily assess their unique contributions.\n\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		B1g5dgfee4	HJeZEs_h7E	MIDL.io/2019/Conference/-/Paper69/Official_Review	[]	3		['everyone']	B1g5dgfee4	['MIDL.io/2019/Conference/Paper69/AnonReviewer3']	1548679977424		1548856754495	['MIDL.io/2019/Conference/Paper69/AnonReviewer3']
109	1548677074735	{'pros': '* This paper is clearly written about purpose, methodology, and results.\n* This paper presents a novel method using the CNN for extracting sampling locations and the patch-based network with GCNN semantic head and CNN structure head (i.e., the proposed sparse structured prediction net), to solve the challenging problem of edge detection of multiple organs from medical images.\n* From the experiments, the authors showed that the proposed method, which utilizes the proposed network with the sampling points, outperformed conventional FCN (U-net)-based approaches.', 'cons': '* In terms of parameters for network training in validation experiments, there are some parameters in which the reasons why the authors set the values are not explained sufficiently.\n\nComments\n- In terms of the losses for network training, the authors set the control parameters of BCE and DICE losses to 0.001 and 1, respectively. Please describe why the values was set to decrease the effect of BCE on the loss computation. Also, the authors should describe why class weighting was not applied to the class-specific loss.\n- In Figs. 3 and 4, the qualitative results and the quantitative results should be divided into different figure and table, respectively. Also, I suggest to add the legend of anatomical structures in the figures instead of describing them in the figure titles.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		SkxccsWxxE	rkxjAyO3QE	MIDL.io/2019/Conference/-/Paper66/Official_Review	[]	3		['everyone']	SkxccsWxxE	['MIDL.io/2019/Conference/Paper66/AnonReviewer2']	1548677074735		1548856754247	['MIDL.io/2019/Conference/Paper66/AnonReviewer2']
110	1548675777121	{'pros': '-\tGreat database. \n-\tThe group attention module is an interesting idea\n', 'cons': '-\tGeneral poor presentation. The paper is very hard to follow. \n-\tLack of comparison to algorithms specifically designed for lung cancer detection, such as Setio16 or Wang 18. \n-\tNo indication model complexity between the proposed method and the alternatives.\n-\t“all these algorithms neither making use of the spatial relations between slices” – false. Setio16 does, since does planar-reformatting. Wang18 does, since they use 3D volumes.\n-\tThe reviewer wonders how other better performing detection networks compare to the proposed method, such as YOLO9000, which outperforms candidate-selection-and-classification-networks being a unified framework.\n-\t“The proposed method showed superior sensitivity and fewer false positives compared to previous frameworks”. That statement does not hold from Table 3. The sensitivity is only superior for ggn nodules. There is no overall sensitivity column in that table. This reviewer believes that the method is non-superior to the state-of-the art, it is less sensitive and has less false positives, this means that it is operating at another point of the ROC. \n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		H1x-pmF0JE	rygt69P37V	MIDL.io/2019/Conference/-/Paper15/Official_Review	[]	2		['everyone']	H1x-pmF0JE	['MIDL.io/2019/Conference/Paper15/AnonReviewer3']	1548675777121		1548856754031	['MIDL.io/2019/Conference/Paper15/AnonReviewer3']
111	1548672947207	"{'pros': 'The submission suggests an approach for better utilization of the large amount of unlabeled data when applying deep learning methods to digital pathology slides. The suggested approach has two steps (1) train a CNN to segment glomeruli using bounding box segmentations as labels (2) predict segmentations on separate data and use these as labels for training another CNN to segment glomeruli.\nThe problem is highly relevant and I like the the idea of using the distribution of glomeruli shape and size as criteria for filtering segmentation suggestions in Stage 2.', 'cons': 'I have several issues with the submissions. Overall, I find the presentation confusing and unclear and it is possible that most of the issues arise from the lack of clarity.\nThe first paragraph of section 2.1 is a good example of what I find confusing and unclear. What characteristics are you exploiting? What is your approach very similar to? What are BBs? (Keeping track of custom abbreviations is tricky. I had to go back in the text, because I forgot what it referred to. )\n\nAnother example is Figure 5  where missing captions for the subplots makes it impossible to understand without reading the text simultaneously. Additionally, all four plots seem to have their own scaling and extent of the y-axis, making comparisons tricky.\n\nI also find it problematic that 4 out of 9 references are to your own work. Are you the only ones working on segmentation in digital pathology?\n\n\nThe main result is comparing a CNN trained on 8 images with bounding boxes (IS1) to a pair of CNNs trained on the same 8 images with bounding boxes + 9 images without bounding boxes (IS2). \nAs I understand it, Figure 5 (a) shows F-score for different combinations of number of images and number of annotations per image using images from IS1. The F-score is then calculated on five test images (IS3) with the best results being almost 0.90. You then use this result to select the number of images and number of annotations and retrain the model, but this time you get less 0.80 in F-score. Where does this difference come from?\nMore importantly, if my understanding is correct you have used performance on the test set (IS3) to select a model and then you report performance of this model on IS3. This is methodologically wrong, likely overestimates performance and invalidates your comparison.\n\nIf we ignore the above problems for a second, we are left with the conclusion that you get almost 0.90 F-score in Stage 0 where you train on IS1, and slightly lower F-score when you train on IS1 + IS2. So it seems best to just train on the bounding boxes. I think you might have used contour segmentations in Stage 0 (otherwise your conclusion does not make sense), but it is not clear to me that this is the case.\n\n\nIt is not clear what the contributions are in this submission. In the abstract you promise\n""a method for optimizing the overall trade-off between (low) annotation effort and (high) segmentation accuracy"".\nI do not believe you deliver on this promise. You do not present a method that optimizes this trade-off. You conclude that combining bounding box segmentations with unlabeled data works (almost) as well as using contour segmentations. This just shows that we can reduce annotation effort ""for free"", but does not provide a method for optimizing the trade-off.\nI also find it unclear exactly how the proposed method is different from the referenced related work. In the introduction of the methods section you state that you adapt the method from Gademayr et al (2019), and promise details in section 3. I see no mention of this in section 3 and it is not clear to me what you have done. I have a similar problem in section 2.1 where you adapt the method in Khoreva et al. (2017), but you do not clearly state what is adapted.\nIt seems to me, that the actual main contribution is the constraints applied in stage 2 (Cues 2), size and shape of glomeruli, yet these are not clearly described. \n\n\nFinally, I do not agree with your statement in the conclusion that you ""work with noisy easy-to-collect labels"". As I understand it, you derive bounding boxes from contour segmentations by fitting the smallest rectangle that contains all of the contour segmentation. This implies that you have weak labels without noise and perfect accuracy and precision (assuming your ground truth segmentations are 100%, which they most certainly are not). What you lack is detail.\nI suggest you investigate how important the quality of bounding box segmentations is, by either using segmentations from multiple annotators or by adding random shifts and scaling.', 'rating': '1: strong reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		SklToCZ2J4	BJxo3JPh7N	MIDL.io/2019/Conference/-/Paper5/Official_Review	[]	3		['everyone']	SklToCZ2J4	['MIDL.io/2019/Conference/Paper5/AnonReviewer2']	1548672947207		1548856753817	['MIDL.io/2019/Conference/Paper5/AnonReviewer2']
112	1548672709403	{'pros': '- The reconstruction of the permittivity is an important problem and a potentially good fit for a deep learning approach. \n- The paper is well written.\n', 'cons': '- The novelty is only in the application, yet the design and experimental setup are a bit weak for the following reasons.\n- The description of MWI and CSI is very long (although good), whereas more in-depth discussion about the method, choice of architecture, dataset, experiments etc. would be required.\n- No real data was used.\n- I think that complex convolutions (with complex kernels and appropriate activation functions) should be used and compared against as it would maintain the phase information. A motivation and discussion should be provided if not using a complex network.\n- Regarding the experiments: Leave one model out would have been better than only testing on model II.\n- Is it relevant to evaluate on flipped and rotated images? Is the orientation not always controlled in the acquisition? Anyway if it is relevant, why not augmenting the training data as it is generally done for deep learning to learn the rotation invariance? There have been many papers on generalization to rotated and flipped images of CNNs. This work is an application to a specific task and if rotation invariance is necessary here, we want to see how it can best perform when it learns this rotation invariance.\n- The reported performance should ideally reflect both the real and imaginary parts.\n\nOther comments:\n- Explanation in 2.2 about splitting the real and imaginary parts is not very good. From what I get, you simply treat the real and imaginary as 2 input channels. Since you don’t have complex filters, it may be better to define a mapping function R^(2xMxN)-> R^(2xMxN). However, I think complex filters and convolutions would be more appropriate.\n- The time consumption of current reconstruction methods is stressed in the introduction, yet you use it in the proposed method\n- CSI discards information and creates artifacts. Why not using also the problem domain as input to the network? Something similar to what was done in “Highly Scalable Image Reconstruction using DeepNeural Networks with Bandpass Filtering”, Cheng et al. 2018 \n- In 1. Introduction: “Neural networks have recently been combined with...” The reference to Rahama2018 with the deep learning introduction is not appropriate. It is not a deep learning approach.\nThe input and output of the network are not clear in the abstract.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		H1lkgwfeg4	Bke66RI2mV	MIDL.io/2019/Conference/-/Paper78/Official_Review	[]	1		['everyone']	H1lkgwfeg4	['MIDL.io/2019/Conference/Paper78/AnonReviewer3']	1548672709403		1548856753560	['MIDL.io/2019/Conference/Paper78/AnonReviewer3']
113	1548672595034	{'pros': 'The paper presents a deep learning based approach to mitigate the problem of weakly labelled data in fundus dataset. The authors combine labels from different datasets and perform segmentation which are further discriminated between manually labeled vs automatic segmentation. In addition, they propose to add another discriminator which will provides the score for presence of different class in the datasets. \n\n1) The paper gives a strong motivation towards bridging the gap between sparse availability of different class type annotations and requirement of full annotation of different class or at least the classes that are adherently present in that image/dataset\n2) The paper proposes a novel way of guarenteeing semantic segmentation and learning from multiple datasets\n3) Interesting approach\n4) Well written paper with few typos to fix ', 'cons': '1)\tReference to abstract: The authors write that no semantic segmentation is used. However, I can see that they have done semantic segmentation and discriminators are used only to identify between truthfulness of the available manual vs segmented maps. Thus, in abstract, “we use an adversarial learning approach over a semantic segmentation” need to be justified. Does it mean that you are using a generative adversarial network where your generator is a semantic segmentation module which is rectified via discriminators’ decision? If so please include it to clarify more.\n2)\tPage 2, second paragraph, “One of the major…”: segmenation of all parts in fundus image may not be feasible especially when they have proliferative exudates present in them. So, please correct it with “different classes that are adherently available..”\n3)\tPlease remove words that define functions, e.g., ChannelShuffler(), LeakyRelu() … check for all those in the entire paper\n4)\tIn Table 2, what might be the region of hard exudates not performing well especially when we see at the results for other class types. Is it due to lack of ground truth labels in your data? There are other available datasets for these may be including those can improve this result?\n', 'rating': '4: strong accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'special_issue': ['Special Issue Recommendation'], 'oral_presentation': ['Consider for oral presentation']}		HJe6f0BexN	B1ej80L2QN	MIDL.io/2019/Conference/-/Paper102/Official_Review	[]	3		['everyone']	HJe6f0BexN	['MIDL.io/2019/Conference/Paper102/AnonReviewer3']	1548672595034		1548856753342	['MIDL.io/2019/Conference/Paper102/AnonReviewer3']
114	1548672397657	"{'pros': 'The paper aims at identifying mislabeled data utilizing sequences of loss functions in the deep learning networks. \n\n1) The paper focuses on problem interesting to both medical and non-medical datasets\n2) The paper is written well', 'cons': '1)\tConsidering each sample loss using a pretrained model on completely different dataset can mean that you have not generalized to the problem. So, I am very sceptic that this will really work to identify mislabels\n2)\tI believe, doing k-means clustering with the features that the authors mention to extract is an iterative process. What is the stopping criteria? \n3)\tDid the authors try to first train their network on correctly labeled datasets and then use these to identify the mislabeled images? This could be more straightforward solution?\n4)\tWhat does the synthetic labels look like and how these were generated? \n', 'rating': '2: reject', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		rJxCyxj1xN	HyeL5TU3mE	MIDL.io/2019/Conference/-/Paper39/Official_Review	[]	3		['everyone']	rJxCyxj1xN	['MIDL.io/2019/Conference/Paper39/AnonReviewer1']	1548672397657		1548856753131	['MIDL.io/2019/Conference/Paper39/AnonReviewer1']
115	1548670769478	{'pros': '* The authors are working on the challenging task of segmenting the microlesions from high-resolution colored fundus images.\n* This paper proposes novel patch-based segmentation approach to utilize not only the multi-scale fully convolutional networks (i.e., HGNs) but also the classification network (i.e., PRN) with the triplet loss to improve the segmentation accuracy.', 'cons': '* Overall, I find the paper hard to read and follow. Especially, the authors should more clearly describe the methodology and the experiments.\n* In the results of Table 2, it seems inadequate to validate the effect of using the multiple scales for HGNs and using the PRN on the segmentation performance.\n\nComments\n- I suggest to clearly describe the detail of each network, such as the architecture, the input/output, and the loss function, with some figures, because I fail to understand how the segmentation maps with less false positives can be finally obtained by inputting the patches extracted from the output of HGNs to the Resnet-based classification network.\n- Please clearly describe what the anchor patches are in the section 2.2.\n- Please clarify what the difference between PRN and Triplet loss geometric/arithmetic in Table 2 is. Don’t the authors define the classification network with the triplet loss as the PRN?\n- Please add the description of the vanilla fully convolutional neural network in the section 3.3 and its result in Table 2. I can’t find the result of the vanilla fully convolutional neural network in Table 2 although it says “Using triplet loss in a multi-scale approach with geometric averaging has an overall 19.59% PR AUC improvement over the vanilla fully convolutional neural network.”\n- The authors should add the results of HGN 0.5x and HGN arithmetic in Table 2, considering the manuscript in the section 3.2.\n- Please add the description or the citation from the IDRiD iFLYTEK-MIG to IDRiD SDNU in Table 2.\n- Please more clearly describe what the development set and the local test set are in the section 3.1.\n- Please correct typos because there seem to be many typos in the manuscript.\n- In Fig. 3, either of the color for true positives and false negatives should be changed because green and cyan are difficult to distinguish in the figure.\n- Please add the title of Fig. 4.\n- Please clearly decide whether the authors use some abbreviations (e.g., DR) or not in the manuscript.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		BJlt-XMlgN	rJeYVD82XV	MIDL.io/2019/Conference/-/Paper73/Official_Review	[]	2		['everyone']	BJlt-XMlgN	['MIDL.io/2019/Conference/Paper73/AnonReviewer3']	1548670769478		1548856752917	['MIDL.io/2019/Conference/Paper73/AnonReviewer3']
116	1548670882724	"{'pros': 'This work presents the U-NetPlus architecture which utilizes pre-trained encoder, data augmentation and replaces up-convolution with Nearest Neighbor Interpolation to achieve State of the Art results on MICCAI 2017 Endoscopic Vision Challenge. \n\n- The paper is overall quite lucid and comprehensible and the evaluation criterions are also described clearly in detail. \n- The method achieves better performance that other competing procedure although only by a slight margin (~0.2%). \n- The proposed motivation for suggested improvements are convincing although other alternatives exist in literature (see below)\n- The results are reported correctly -- with both the central tendency measurements as well as the Standard deviation on those measurements. \n', 'cons': '- Both data augmentation and initializing the encoder are methods already proposed in the related literature for improving the U-Net performance, as the authors themselves argue. The novelty perhaps only lies in this particular way of using the elements together and not on the elements themselves.  \n- Recently, other methods were proposed to remove the artifacts from upconvolution like [1]\n- The margin of improvement over competing methods (~0.2%) is too small compared to the typical SD in the measurement (>=15%) to be sure of an actual improvement. \n\nOther minor comments:\n\n- In the Related works section (""The classic U-net model (Dong et al., 2017) relies on .. convolutional layers."") the Dong et al. 2017 work is perhaps cited wrong. \n- Typos: ""up-sampled"" inAbstract, ""servoing"" in Introduction. \n\nOverall, the paper misses the acceptance threshold but proper improvement on above suggested directions would be key for improving its quality. \n\n[1]: Odena, et al., ""Deconvolution and Checkerboard Artifacts"", Distill, 2016. http://doi.org/10.23915/distill.00003', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		SkxQ6fjlx4	HkxiiwIh74	MIDL.io/2019/Conference/-/Paper128/Official_Review	[]	2		['everyone']	SkxQ6fjlx4	['MIDL.io/2019/Conference/Paper128/AnonReviewer3']	1548670882724		1548856752702	['MIDL.io/2019/Conference/Paper128/AnonReviewer3']
117	1548670035758	{'pros': '-\tAddress an important topic – could we train the networks by learning the data augmentation policy itself? Can we generate rotationally invariant networks?\n-\tAttempt to generate rotationally invariant networks through a orientation-normalization patch-based layer.\n-\tOutperforms detection on CIFAR-100 and STL-10 \n-\tBetter AUC when predicting tumor growth\n-\tSimilar results when doing segmentation\n', 'cons': '-\tThe presented method is a combination of standard convolutions and rotationally invariant layers. It would be interesting to have a unified framework.\n-\tAs the authors acknowledge, their method applies a per-patch rotation correction, while there is a lot of rotation covariance in natural images, thus the need of a hybrid network.\n-\tUnclear if the authors used data augmentation when training the standard networks. One of the key motivations in the work is that using the proposed method one could avoid costly data augmentation techniques. However, such statement is not demonstrated empirically.\n-\tDiscussion incoherent with results – in the presented results the method does not always outperform classical convolutions. \no\tFor CIFAR-10, results are the same. \no\tFor tumor growth prediction, while having a higher AUC, the accuracy is much lower (how was the threshold selected?). It would be of interest to see the ROCs, since sometimes AUCs can be misleading due to early mistakes. \no\tDICE results on liver lesion segmentation are very similar and likely will not pass any statistical test.\n-\tPresentation: the second paragraph of the introduction seems out of place. Houndred -> hundred\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		BJlVNY8llV	H1x3LN82QN	MIDL.io/2019/Conference/-/Paper111/Official_Review	[]	2		['everyone']	BJlVNY8llV	['MIDL.io/2019/Conference/Paper111/AnonReviewer3']	1548670035758		1548856752485	['MIDL.io/2019/Conference/Paper111/AnonReviewer3']
118	1548669479985	{'pros': 'This paper presents a stain-transforming cycle-consistent GAN for improving image recognition in histopathology.\nThere are several contributions from this paper:\n1. Presenting the GAN method for stain transformation, which is straightforward for GAN methods. \n2. Introducing a modified overlapping strategy to remove tiling artifacts occured in patch sliding window approaches.\n3. Cross center tissue segmentation has been validated using stain transformation. \nOverall, this paper is well written and experimental results are extensively validated. \n', 'cons': 'For the evaluation metrics on stain transformation, it was not elaborately explained, including SSIM and Wasserstein distance. Please cite related references and provide detailed explaination;\nIn Table 1, the depth number affected the performance significantly, whether this is the same case for cycleGAN-baseline? \nThe results show that with stain transformation, the augmentation did not improve the segmentation performance. This is quite interesting. From my perspective, augmentation and stain transformation are kind of complementary. Please show more cross-validation or cross-center results to draw the conclusion. ', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'oral_presentation': ['Consider for oral presentation']}		BkxJkgSlx4	BylgEMLnX4	MIDL.io/2019/Conference/-/Paper99/Official_Review	[]	2		['everyone']	BkxJkgSlx4	['MIDL.io/2019/Conference/Paper99/AnonReviewer2']	1548669479985		1548856752269	['MIDL.io/2019/Conference/Paper99/AnonReviewer2']
119	1548669226393	{'pros': 'The authors propose a deep neural network composed of a UNet and an autoencoder. The UNet is trained for segmentation and the autoencoder is trained with proposed geodesic maps. The geodesic maps are generated by a fast marching distance transform.', 'cons': '- The authors use ACDC data, but fail to cite Bernard et al. (doi: 10.1109/TMI.2018.2837502)\n\n- Although there are many citations to other work, key-references seem to be missing.\n\n- The authors propose a weakly supervised method employing noisy labels. The proposed noisy (i.e. synthesized non-expert) labels shown in Figure 3 do not look plausible. It is reasonable to assume that non-experts would make annotations that are smooth and continuous, i.e. annotations without holes. It is unclear how the method would deal with images made by non-experts.\n\n- An ablation study was performed to investigate the proposed method. Based on this ablation study, the authors concluded that the proposed method outperforms state-of-the-art CMR segmentation methods. However, performance differences are small. The authors should perform tests for significance. Furthermore, performance in terms of Dice and HD is low for all experiments, even in the the experiments using expert labels. Since the ACDC dataset is used, I refer the authors to Bernard et al. (doi: 10.1109/TMI.2018.2837502) for state-of-the-art results, which are far better than the results of the proposed method. Furthermore, A fairly complicated method is used in an attempt to increase performance using the geodesic priors. It would be interesting to investigate how a simpler architecture would fare with these priors. \n', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		r1lkO0u1lV	rkeGEbI3m4	MIDL.io/2019/Conference/-/Paper33/Official_Review	[]	2		['everyone']	r1lkO0u1lV	['MIDL.io/2019/Conference/Paper33/AnonReviewer2']	1548669226393		1548856752059	['MIDL.io/2019/Conference/Paper33/AnonReviewer2']
120	1548667473135	"{'pros': 'This is an interesting paper presenting the discovery about physical attacks in dermoscopy.\nRobustness is very important in deep learning based methods. This paper studied the robustness and susceptibility of various deep learning architectures under physical attach. \n', 'cons': ""The experimental dataset is relatively small, which may be subject to vulnerability;\nAlthough the discovery is interesting, I would suggest if authors can propose some methods for increasing the robustness of deep learning methods, this is more insightful.\nIn table 1, the authors listed several physical attach types for dermoscopy applications. Is it comprehensive or how much it's related to the clinical setting?\n"", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		Byl6W7WeeN	H1eK8cSnmN	MIDL.io/2019/Conference/-/Paper60/Official_Review	[]	3		['everyone']	Byl6W7WeeN	['MIDL.io/2019/Conference/Paper60/AnonReviewer3']	1548667473135		1548856751847	['MIDL.io/2019/Conference/Paper60/AnonReviewer3']
121	1548665716339	"{'pros': ""This paper addresses the problem of histological image segmentation. As annotations in histological image are costly to obtain, the authors consider weakly supervised as well as unsupervised learning. Their approach is based on Khoreva's approach that leverages bounding boxes instead of precise pixelwise segmentations to feed a segmentation CNN. \n\nTheir proposal is a cascade of two segmentation models, that make use of 'cues', which are statistical rules applied on the area of the segmentation results. \nThe first model is trained with BB and iteratively improved thanks to the cues.\nThe second model is trained with the results of the first model and iteratively improved thanks to some other cues.\nExperiments include accuracy results depending on the number of iterations (for both stages), and comparison to a fully supervised network.\n\nThe paper is well written. It presents an interesting contribution to the weakly supervised segmentation of histological images. \n- Image patch size is set to 492. How was this value set?\n- Are the image processed in the RGB space?\n- Results are given on a patch basis, would it be possible to give some accuracy image-wise or glomeruli-wise ?\n\nMinor comments:\n- please specify what training set is used to train the fully supervised CNN, IS1 and IS2?\n- captions of Fig 5 could be improved (eg (a) stage 0, (b) stage 1 (c) and (d) stage 2). The general caption could also reflect better the figure content. Although not a big deal, this might help when skimming through the paper.\n- in Fig 4, one can see the nomber of FP decreasing, however one cannot distinguish the evolution of green/blue areas (too small).\n- typo in conclusion: automation (...) demandS"", 'cons': ""The 'cues' are specific to the application at stake and rely on some hard-constrained statistics, which are assessed form the data. They seem be too constraining especially in Stage 2, as acknowledged by the authors.\n"", 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		SklToCZ2J4	SyghOQH2mN	MIDL.io/2019/Conference/-/Paper5/Official_Review	[]	2		['everyone']	SklToCZ2J4	['MIDL.io/2019/Conference/Paper5/AnonReviewer3']	1548665716339		1548856751635	['MIDL.io/2019/Conference/Paper5/AnonReviewer3']
122	1548665571136	"{'pros': 'In this paper, the authors use the well-known cycle GAN model to segment WSI (whole slide images) in an unsupervised fashion. They design several annotation models, that mimic the real label images. The framework is evaluated on  WSI from renal pathology and against a fully supervised scenario. This paper shows a valuable application of cycle GAN to histological image segmentation.\n\nThe authors have published a paper at MICCAI\'18 on the same subject, what is the added value in this submission? To better assess the contribution, it would be also interesting to specify if the cycle GAN has been used in a segmentation setting in other medical imaging cases.\n\nSome questions:\n- is there any preprocessing on the image?\n- ""For each stain individually..."": it is not clear what the authors meant in this sentence.\n- rotation parameter alpha is drawn in [0, 2pi]. Given the symmetry of the elliptic shape, an interval of [0, pi] should be sufficient, or am I missing something?\n- does the color difference between first row of images a,b and c,d account for anything?\n\ntypos: with the a, where used\nFigure 4: caption ""ettings"" and also 2nd row of images: twice (c)', 'cons': 'The annotation models contain many parameters, which are tuned ""visually"". Influence of these parameters could be assessed. How interesting would it be to implement an iterative process that would alternate between segmentation and parameter update?\n\nI could not find the exact number of patches on which the framework is assessed. In order to fairly compare the proposed approach to fully supervised FCN, is the test set the same in both cases?', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		B1gT9_B81E	BkeiJXrnmV	MIDL.io/2019/Conference/-/Paper2/Official_Review	[]	1		['everyone']	B1gT9_B81E	['MIDL.io/2019/Conference/Paper2/AnonReviewer2']	1548665571136		1548856751418	['MIDL.io/2019/Conference/Paper2/AnonReviewer2']
123	1548340246424	"{'pros': 'This work aims to tackle continual learning (specifically, catastrophic forgetting) in the context of anatomy classification from ultrasound images. Further, the new classes that are introduced after the base training have a relatively small number of training samples (thus, few-shot). The work attempts an important and relatively novel problem setup and is well-motivated.', 'cons': ""* The manuscript is riddled with unclear and inconsistent use of terminology - 'base knowledge network', 'base feature generator', 'few-shot feature and weight learner', 'few-shot feature generator', 'representation - confidence score layer weight vectors', 'classification weight vectors', 'novel weight classification vectors', 'new feature generator', 'characteristic set of weights'. I suggest the authors formalize the problem early on in the methods section and then stick to the introduced notation.\n\n* '... the weight vectors are optimized for the latter using a cosine similarity function.' which weight vectors? cosine similarity between what and what? The cosine similarity idea is mentioned already at the beginning of page 4, but is only explained towards the end of page 5. My suggestion would be to divide the methods section into sub-sections and to introduce concepts in a structured manner.\n\n* what is meant by '... heavily imbalanced arrivals of datasets...'?\n\n* The writing is extremely dense, redundant in several places and lacks clarity. There are easily half a dozen sentences than span over 4 lines of text. The entire discussion section, which happens to be more than a page long, has been presented in one single paragraph. There are at least a dozen typos and grammatical errors. In my opinion, the writing alone renders the paper unsuitable for acceptance.\n\n* How exactly is continual learning achieved - how is forgetting of earlier learning prevented? Is the 'new feature generator' initialized with the 'base feature generator' and then optimized during the learning of the new classes? Does that mean that there is a 'new feature generator' for each new class?\n\n* How exactly is few-shot learning achieved - how is over-fitting to the small number of examples of new classes prevented?\n\n* What is the 'Few-shot weight generator' module depicted in Figure 2? It is never mentioned in the text. What do the bi-directional arrows between the 'Few-shot weight generator' and the 'Feature representations' indicate?\n\n* The authors say that they 'split the videos into frames and apply augmentation.' What type of augmentation is applied?\n\n* The cropping out of the 'anatomy clips' from the video frames seems to severely limit the receptive field. Would a larger receptive field be useful for accurate anatomy classification?\n\n* in the related works section, references [14] and [15] are placed in the generative replay category. This is incorrect. 'Generative' replay methods involve a generative model of the data of previous tasks. [14] uses exemplars - real samples of the previous tasks. The relevant paper that introduced generative replay (Shin, Hanul, 2017) is not cited. \n\n* citations [3] and [4] are the same."", 'rating': '1: strong reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BkeM-yMgeE	BJg0z2BvXN	MIDL.io/2019/Conference/-/Paper68/Official_Review	[]	2		['everyone']	BkeM-yMgeE	['MIDL.io/2019/Conference/Paper68/AnonReviewer1']	1548340246424		1548856751201	['MIDL.io/2019/Conference/Paper68/AnonReviewer1']
124	1548663576300	{'pros': '-\tClassification of neurodegenerative diseases based on MRI is a relevant field. The authors perform a comparison study of different classification methods: DNN, SVM, GP and RF. Such a  comparison is relevant and has not been done before to the best of my knowledge.\n-\tThe dataset is highly interesting because of it large size and wide range of neurodegenerative diseases.\n-\tThe validation process is thorough and well explained.\n', 'cons': '-\tThe problem is not directly clinically interesting. It is a highly artificial problem to distinguish those 11 classes purely based on imaging data.\n-\tAll methods used region wise volumes as features, which result in a limited comparison. Although still relevant, this is suboptimal because of 2 reasons: 1) DNN are especially knows to be good feature extractors. Applying DNN to precomputed volumes does not fully show the potential of these methods. 2) The literature shows that region-wise volumes do not result in state-of-the-art performance. The authors should use multiple types of features (including shape, texture, etc) or full images.\n-\tThe network architecture description is not specific enough. What type of ‘feed forward DNN’ is used?\n-\tImportant evaluation metrics that would be useful to compare with the literature are missing, e.g. multiclass AUC, balanced accuracy. \n-\tIt would be very insightful to see a confusion matrix to better interpret this 11-class classification problem. \n-\tTable 3 is hard to interpret, a visual graph would be better. \nThe work is relevant, but needs further improvement. No novel methodology is presented, but validation is thorough. \n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		BJlZBmBrl4	Skgx7sN3mE	MIDL.io/2019/Conference/-/Paper154/Official_Review	[]	3		['everyone']	BJlZBmBrl4	['MIDL.io/2019/Conference/Paper154/AnonReviewer3']	1548663576300		1548856750946	['MIDL.io/2019/Conference/Paper154/AnonReviewer3']
125	1548662790132	"{'pros': 'The paper presents the finetuning of ResNet34 to classify Pap smear images. Some visualizations have been added to the analysis to get insights about the model features. The paper quality is good on the overall. However, some major drawbacks do not let me suggest the acceptance of the paper. \nFirst above all, the claim of the paper is not clearly stated. If the claim is the direct classification of WSIs, the finetuning of ResNet is too little novelty, and dataset choice for the experiments is not clearly justified. If the claim is the interpretation of the proposed model, too little insights and discussion have been given to fully understand the methodologies and results. \n\n- The paper has a clear structure and is well organized.  The informative content in the paper could be improved by further summarizing the description of previous works and introducing more insights and discussion on the proposed methods.  \n- The PCA analysis and visualization of the features gives useful and quite novel insights', 'cons': '- The written English could be improved, as some passages in the paper are not extremely clear.\n- There sentence ""In spite of achieving high accuracy on the classification task using our proposed methodology, we acknowledge that to be deployed in clinics, the results of the model have to be transparent and interpretable."" is a bit incoherent with the claim of the paper of outperforming the state of the art performances.\n- Figure 1 only refers to the application to the first dataset. The clarity could be improved and redundant information removed. \n- The data preprocessing technique should be further clarified. At what scale are the WSIs zoomed in ? Is the same preprocessing applied to all datasets?\n- The choice of the datasets should be justified. If the claim of the paper is to classify WSIs, why single cells images are used? How many WSIs have been used in total?\n- The PCA feature extraction is really interesting and quite novel. However little detail has been given about the analysis and methods.\n- A bit of discussion of the results would have substantially improved the quality of the paper.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		S1lrzR0gg4	H1g0WdV274	MIDL.io/2019/Conference/-/Paper137/Official_Review	[]	2		['everyone']	S1lrzR0gg4	['MIDL.io/2019/Conference/Paper137/AnonReviewer2']	1548662790132		1548856750730	['MIDL.io/2019/Conference/Paper137/AnonReviewer2']
126	1548576293664	"{'pros': '- The authors tackle the challenge of catastrophic forgetting in the setting of continual learning on a stream of new data and classes, and introduce several new concepts. Catastrophic forgetting in the context of this work is where a CNN is trained on some initial set of data with an associated set of classes, and then is subsequently trained on a new set of data with new classes, where without appropriate retention of instances from the first set of data, the network can begin to forget what it had learned about the initial classes. The introduced concepts to guide retention during continual learning in such scenarios would be of interest to the medical imaging community.\n\n- The authors propose the use of novel saliency quality metrics to identify which instances from the initial data set to retain for continual learning. The authors introduce Overall Saliency Quality (OSQ) to determine the quality of saliency of a given instance with respect to a class at a given training iteration, and a KL divergence for assessing pixel-level probability shifts for a given instance at a given training iteration with respect to the previous iteration. In order to identify and retain the most representative instances for given classes in data used for training on previous tasks, metrics are introduced including Average Representative Distance Selection (ARDS) as well as Distributed Exemplar Distance Selection (DEDS). These are used in a saliency-driven curriculum learning approach for exemplar instance retention, where identified instances are incorporated into the continual training of a network on new data with new classes (after training on some initial classes and data).\n\n- The proposed methods are compared with previous approaches for continual learning (iCaRL, LwF.MC, E2EL, FDR), demonstrating that it outperforms these methods on retaining validation classification accuracy on classes contained in previously trained on data when introducing new data with additional classes to a CNN classifier. ', 'cons': ""\n1) While the authors have provided plenty of detail of their proposed approach, the paper is formatted such a way that its readability is hindered, and it significantly exceeds the suggested limit of 8 pages. The manuscript is 10 pages long with references and does not have a standalone Results section. There are an additional 3 pages for an Appendix containing necessary details about the dataset and model training, as well as a larger selection of relevant figures and results. The main text does not include any space for numbered equations making it difficult to refer back to equation definitions, and it contains 2 figures which are barely legible without zooming in. The purpose of the suggested 8-page limit is to encourage concise information, relevant for the conference format. Removing material from the paper which is not directly pertinent to the message and reformatting for improved readability would greatly benefit readers.\n\n2) While the general paradigm of continual learning is of particular significance in scenarios where very large datasets with continually updating data and classes are being tackled, the vast majority of current medical imaging tasks (including those presented) are much more narrowly focused. `While the authors illustrate the improvements that their proposed method has over other continual learning methods, it would be helpful to the medical imaging community to compare results to a more standard baseline: i.e. simply a CNN trained on all available data with all classes simultaneously, without a continual learning approach. After all, retention of exemplars in the setting provided requires access to the training data used for all tasks, so the value of such a retention scheme over simply retraining the network on all data and all classes (once they became available) should be demonstrated. While it seems plausible that continual learning would have meaningful use-cases in medical imaging, it would be helpful if the authors could point to any such use-cases currently in the literature. Otherwise, demonstrating the value of continual learning on current public challenge data for example (or simply against a baseline model and training approach as described above above) would be illustrative of the proposed framework's` practical value for current applications. \n\n3) It seems that ARDS favours instances where scale and position of the salient features for a given class happen to coincide with the average saliency map. How does this affect the selection of exemplar instances for classes which have large variation in scale and position?\n\n4) Increasing data augmentation (via translation, scaling, noise addition, rotation, affine transformations, elastic deformations, etc.) is well known to improve performance of CNNs in medical imaging tasks. The authors only use a very minor rotation to augment their data. Could the authors comment on how applying richer data augmentation (listed above) would affect the retention scheme? Would exemplars be chosen from augmented images? If so, it seems this would  change the average saliency map and ARDS. Tying in with the previous point, if richer data augmentation cannot effectively be used in conjunction with the proposed retention schemes, it would again be valuable to the medical imaging community to see a performance comparison with a baseline model (as described in point (2) above) that incorporates richer data augmentation.\n\n\nMinor\n- Several minor spelling and grammatical errors throughout which should be addressed along with formatting issues. \n- Aorta repeated in list of cardiac structures.\n- figure 3 should be labelled as a table, not a figure. Acronym is 'ADRS' in table instead of 'ARDS'."", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		S1gyUgN7eE	HJlAXIJjXE	MIDL.io/2019/Conference/-/Paper142/Official_Review	[]	2		['everyone']	S1gyUgN7eE	['MIDL.io/2019/Conference/Paper142/AnonReviewer2']	1548576293664		1548856750514	['MIDL.io/2019/Conference/Paper142/AnonReviewer2']
127	1548661852309	{'pros': '-\tThe authors address the highly complex but highly relevant problem of detection and classification of perivascular spaces and lacunes. Although the problem is very difficult, due to rater uncertainty and high class imbalance, the authors achieve reasonable sensitivity.\n-\tThe authors present interesting improvements, both in network architecture and in training approach, that could be beneficial to other applications. Especially the use of the multiple raters is original. In addition, the sampling based on distance maps is interesting.\n-\tFor interpretation of the results, some baseline models are missing. I appreciate that the authors made an effort to train such models. Unfortunately, these models gave no results.\n', 'cons': '-\tThe problem is complex, but the authors made it even more complex than needed by combining two tasks (EPVS and lacunes) and by incorporating multirater information. It would be useful to see the model’s (and reference models’) performance on the separate problems of EPVS and lacunes.\n-       The authors did not assess the added value of the multirater encoding. It will be insightful to include for example a baseline method trained on a single rater. \n-       The authors did not assess the added value of their dedicated sampling strategy.\n-\tThe test set is very small, consisting of only 2 subjects. The validation is therefore quite limited.\n-\tFigure 7 is hard to interpret. What is the meaning of all the blue (=uncertain) ‘Nothing’ boxes? Why displaying the blue boxes if they should be disregarded?\n\n\nAlthough validation can be improved, this work is highly relevant and gives many interesting leads for discussion at the MIDL conference.\n', 'rating': '4: strong accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'oral_presentation': ['Consider for oral presentation']}		BJlXpDh1gV	BJeVv4V3QV	MIDL.io/2019/Conference/-/Paper44/Official_Review	[]	2		['everyone']	BJlXpDh1gV	['MIDL.io/2019/Conference/Paper44/AnonReviewer1']	1548661852309		1548856750258	['MIDL.io/2019/Conference/Paper44/AnonReviewer1']
128	1548318655731	"{'pros': '- The authors present a novel multi-task architecture that allows for the simultaneous prediction of multiple continuous variables (number of nodules in different segments of the lungs) as well as the prediction of multiple binary variables (different symptoms that can manifest in each segment of the lungs) from 3D CT lung images of subjects with Tuberculosis. This appears to be a novel application, as detection of these two sets of variables has not been simultaneously performed in a deep learning framework before. \n\n- The authors also incorporate homoscedastic uncertainty to allow the network to learn appropriate weights during training for each task, and additionally utilize a self-normalizing network framework.  \n\n- While the proposed architecture is designed for this particular application, such a network could be used for other applications that require simultaneous prediction of continuous and binary variables. ', 'cons': ""Main concerns\n\n-The motivation for using self-normalizing neural networks (SNNs) needs to be made clearer, and unfortunately its incorporation into the presented model makes no significant improvement over a baseline batch normalisation. The author first describes the term 'residual connections' as connections in the original V-net which forward encoder features to the decoder, which they state alleviates vanishing/exploding gradients, and which cannot be used in the presented network since no decoder is present. The authors of the V-net paper, (Milletari et al., 2016) explain that the forwarding (or 'skip') connections in V-Net are primarily there to preserve fine-grained features which would otherwise be lost during the full encoding-decoding pathway; these would also help tackle exploding/vanishing gradients. However, it seems the authors have also chosen to remove the residual units seen in the original V-net encoder (i.e. where the input of a block of filters is added to its output), which speeds up training considerably and also helps tackle exploding/vanishing gradients. There is no reason why these residual units could not be used in the encoder block of the presented CNN. The authors use the reason of not being able to use 'residual connections' to motivate the use of SNNs, to tackle vanishing/exploding gradients. Authors of the SNN paper (Klambauer et al., 2017) specify that CNNs do not suffer from exploding/vanishing gradients nearly as much as feed-forward neural networks (FNNs) (hence why the SNN paper focuses on FNNs and not CNNs). (Klambauer et al., 2017) explain that since there is weight-sharing with convolutional filters and already effective normalization approaches for CNNs (e.g. batch normalization, layer normalization, or weight normalization), SNNs are less likely to improve performance of CNNs. The motivation in Section 2.3 should accurately take into account the above factors.\n\n- The evaluation of the proposed approach would benefit from additional and more thorough experiments. The results section shows only a comparison of using a SNN framework to using batch normalization in the CNN, which they show produces no significant difference. It's hard to tell what the significance of the reported metric of RMSE is for nodule count as well, since the number of nodules typically present in a given lung lobe is not given in the first place (e.g. if mean nodule count is 1, and RMSE is 1, that does not indicate very good performance). The manuscript would benefit from additional results demonstrating if perhaps the inclusion of homoscedastic uncertainty improved performance, or whether there is a drop in performance if fewer tasks are included (e.g. only lung nodule quantification and not binary class prediction). Statistics on the labeled data (e.g. distribution of nodule counts and frequency of binary classes in the data) would also be helpful.\n\n- The evaluation is limited somewhat due to the small sample size (56 image volumes from 14 macaques). This dataset also has a lot of correlation across the 4 images per subject acquired at different stages of disease progression. Lung nodule detection also suffers from severe class imbalance, for which various strategies are typically employed but are not used or discussed in this paper.\n\nMinor comments\n- Why were ReLUs used for the output of FCNr? A more common choice for regression tasks is a linear activation on the output; was a comparison made with this? \n\n- There is inconsistency in the terms above the product symbol in Equation (5) and the summation symbol in Equation (6), where B is switched to R for the binary loss terms. This is also the case in Figure 1 in the 'Multi-task loss' box.\n\n- The acronym FCN is defined by the author as 'fully convolutional network'. However, the description in the text of Section 2.2 and Figure 1 suggests that the FCNs are actually fully connected networks, i.e. without the use of convolutional filters. This should be clarified/corrected. \n\n- There are some semantic errors throughout which could be resolved for a final version. For example, the use of the word 'sensibility', and 'exploiting gradients' should be 'exploding gradients'.\n"", 'rating': '1: strong reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		ryxPCJNexV	rkgO6DePQ4	MIDL.io/2019/Conference/-/Paper92/Official_Review	[]	2		['everyone']	ryxPCJNexV	['MIDL.io/2019/Conference/Paper92/AnonReviewer3']	1548318655731		1548856750042	['MIDL.io/2019/Conference/Paper92/AnonReviewer3']
129	1548660202967	"{'pros': '- the paper is well written and tackles an important topic of feature interpretability for the purpose of image registration\n- the proposed SUITS approach builds on previous methods  and is novel enough to excite the attention of the community', 'cons': '- the description of the method should be replaced by a algorithm environment detailing the different steps. The way it is presented in the paper makes it a bit difficult to follow\n- as an additional comparison the authors should consider presenting the results of a CNN based method\n- although not directly related to feature interpretability, the work ""Deformable medical image registration using generative adversarial networks"" , Mahapatra et. al. , ISBI 2018 could be relevant as it uses GANs for multimodal image registration', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'oral_presentation': ['Consider for oral presentation']}"		SkeI0-QelE	ByeXeCm2mN	MIDL.io/2019/Conference/-/Paper84/Official_Review	[]	1		['everyone']	SkeI0-QelE	['MIDL.io/2019/Conference/Paper84/AnonReviewer1']	1548660202967		1548856749794	['MIDL.io/2019/Conference/Paper84/AnonReviewer1']
130	1547673378307	{'pros': 'The author presented a successful application of a previously published method combining 3-plane 2D-Unet to the task of hippocampus segmentation.\n\nThe article is well structured and written clearly. Experiment parameters were carefully studies.', 'cons': '-\tThe idea of using (a) adjacent slices and (b) tri-plane segmentation for 2D CNN are not new, which may question of the novelty of this paper.\no\t(a1) Sun et al. 2017 Automatic segmentation of liver tumors from multiphase contrast-enhanced ct images based on fcns \no\t(a2) Ben-Cohen et al. 2016 Fully convolutional network for liver segmentation and lesions detection\no\t(a3) Cai et al. 2017 Improving deep pancreas segmentation in ct and mri images via recurrent neural contextual learning and direct loss function\no\t(b1) Prasoon et al. 2013 Deep feature learning for knee cartilage segmentation using a triplanar convolutional neural network\no\t(b2) The paper that the author also mentioned (Lucena et al. 2018)\n-\tIn the introduction, the author mentioned about the time consuming of the traditional image registration and label fusion technique.\no\tHowever, the author is trained on an MNI-registered T1W MRI, which they considered as part of the pre-processing. The involvement of registration (even if it’s only affine registration) might potentially undermine their statement of one of their “main contribution” fast processing with short execution time, since the actual time-consuming part is registration, the label fusion (which normally takes less than 1 minutes).\n\uf0a7\tIn 4.3 Post-processing and evaluation, the author mentioned: “Careful attention was given to the registration process to the ﬁnal volume to avoid errors”. So did the author actually doing registration as a “postprocessing” step, and why? What kind of registration (affine/non-rigid)? And would that increase processing time?\n-\tFor the 3-plane activation map fusion postprocessing, the author claimed the improvement of to (Lucena et al. 2018) is: instead of using a DNN to combine the 3 segmentations, the activation map of 3 planes were summed up with equal weight followed with thresholding. \no\tThe author is simply using majority voting (as equal weight is given = 1/3), and claim their method improved the performance significantly over the original study they referred to (Lucena et al. 2018) in which a DNN is used to combine the 3-plane segmentations), and raise the performance of their best model by 10% (section 4.3). If the author is treating the problem as label fusion, it would make more sense if the author uses the state-of-the-art label fusion methods (such as joint label fusion), (even though they only have three candidates to fuse).  \no\tFrom Figure 4 (b), the 3-plane consensus seems to give better mean dice, but actually with larger standard deviation compared to the coronal, which might indicate that adding the worst segmented layer (the axial) may have the adverse effect to the consensus result with simple fusion method. The author should perform statistical comparisons between the 3-plane consensus results and all the three single-plane segmentation results first just to show that their proposed approach actually improves the segmentation accuracy compared to the two in-plane segmentations (sagittal and coronal)\n\uf0a7\tWould it be solved by just introducing atlas selection (i.e. 2 out of 3), which might in effect discard the axial segmentation for most cases as it’s the least accurate segmentation?\n\uf0a7\tThe author used Dice as loss function. However, Dice is known to be sensitive to structure volume. And since hippocampus is relatively small region in the image, it’s better to use weighted Dice to avoid highly imbalanced label segmentation, such as the Generalised Dice overlap (GDL) (by Sudra et al. 2017).\nMethods comparison\n-\tIn “5. Results and discussion”, the author compared their method with hippodeep, which use consensus for label fusion. They didn’t report the actual dice, but instead reported the improvement of the dice when comparison with the hippodeep method, along with the visual results in Figure 5.\n\uf0a7\tIf the authors consider the in Hippodeep mask as the ground truth, it would be good to show the actual dice score similar to Table 1.\n\uf0a7\tOtherwise, if the author doesn’t consider the Hippodeep mask as ground truth, then the increase in dice doesn’t necessarity reflect the improvement in segmentation accuracy, but rather, more similarity to the Hippodeep mask.\n-\tThe author only compared their method with the traditional (hippodeep Thyreau et al. 2018). How about other methods they mentioned in the 2 Related work, \no\tThere are other 3D U-net methods, (e.g. the 3D U-Net by cicek et al. 2016). It would be more fair to compare with these methods and see how much performance the proposed 2D network can hold.\n\nMinor:\n-  the a-axis label of Figure 4(B) is truncated: Sagittal -> agital, Coronal: -> oronal, and Consensus -> onsensus\n- Page 7 last word: “then” should be “than”', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		rkg6MIRa1V	Bygq7ymaf4	MIDL.io/2019/Conference/-/Paper11/Official_Review	[]	1		['everyone']	rkg6MIRa1V	['MIDL.io/2019/Conference/Paper11/AnonReviewer2']	1547673378307		1548856749579	['MIDL.io/2019/Conference/Paper11/AnonReviewer2']
131	1548653502586	{'pros': '- Use of cycle GAN to ensure that images from different cameras are compatible with the trained model.\n- combining the YOLO type architecture and cycle GAN for localization and segmentation of the optic disc', 'cons': '- there is not much theoretical novelty\n- application is not really novel\n- experimental results only show the basic outcomes. There is no attempt to do a through analysis of different components of the system which could have strengthened the paper', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		B1gV2yZ4g4	BJev6mznmN	MIDL.io/2019/Conference/-/Paper143/Official_Review	[]	2		['everyone']	B1gV2yZ4g4	['MIDL.io/2019/Conference/Paper143/AnonReviewer1']	1548653502586		1548856749324	['MIDL.io/2019/Conference/Paper143/AnonReviewer1']
132	1548652999756	{'pros': 'The idea is simple, but interesting. The proposed loss function has the potential to train models across multiple datasets that complement to each other. The loss function addresses correctly the problem when multiple labels are distributed across several datasets and they need each other to produce a more complete output.\n\nA single model that can perform multiple task is preferable because it is more robust to image variations and has a lower computational load.\n\nThe problem statement is clear and the process to solve it through the experiments is also clear.\n', 'cons': 'For the regions such as WMH, EDEMA, and tumor, the proposed method achieved worse results than MultiUNet. it would be good to add an analysis of this part.\n\nThe proposed method requires the same data modality availability across several datasets which reduces the model input images to just one or two. This may represent a major drawback if the datasets depend on different types of image modalities. However, this problem may be outside the scope of this work.\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'special_issue': ['Special Issue Recommendation']}		Syest0rxlN	H1exCZfnXN	MIDL.io/2019/Conference/-/Paper103/Official_Review	[]	1		['everyone']	Syest0rxlN	['MIDL.io/2019/Conference/Paper103/AnonReviewer2']	1548652999756		1548856749109	['MIDL.io/2019/Conference/Paper103/AnonReviewer2']
133	1548652686427	"{'pros': '0) Summary\n  The manuscript proposes a methodology to render mammogram classification with neural networks (NN) explainable. After normal NN training, medical experts provide semantic labels for the most class-specific internal neurons, which is then used as an explanation for the prediction on unseen test data. The approach adds a human expert into the learning loop in order to augment the NN prediction with an justification in terms of human domain vocabulary.\n\n1) Quality\n  The paper is sound.\n\n2) Clarity\n  The paper is well written and the technical content is well described.\n\n3) Originality\n  The use of heatmaps as a posthoc explanation in the context of human-in-the-loop-learning has not been explored so far. While the SPIE paper [1] uses network dissection by Bau et al. showing via human annotations that activations of intermediate neurons correlate well with human-interpretable findings, the manuscript under review tries to make use of the discriminative correlating internal activations by visualizing them for the user.\n\n4) Significance\n  In the light of limited training data, having human annotators in the learning loop is a valuable direction to explore. Also, interpretable predictions hold the potential to increase confidence.\n\n5) Reproducibility\n  The basic pretrained network ResNet-152 is a public standard tool and the DDSM dataset along with the labels is available online. This makes the work reproducible in principle.\n\n[1] Wu et al., Expert identification of visual primitives used by CNNs during mammogram classification, https://arxiv.org/abs/1803.04858, SPIE, 2018.', 'cons': '1) Quality\n  The difference to prior work [1] is not fully clear from the paragraph in the Introduction.\n\n2) Clarity\n  There are some typos:\n   - Intro: ""Qualitatively tell where is important in an image but fails to""\n   - Bibliography: Please check capitalization e.g. ""cnn"", ""x-rays"".\n  With some reformulations, the text can meet the (soft) limit of 8 pages.\n\n3) Originality\n  It seems that the SPIE paper [1] uses the very same experimental setup and the very same data; this could have been made a little more explicit.\n\n4) Significance\n  While having an explainable prediction, the analysis in the paper falls short in exploring the trade-off between interpretability and predictive performance. An experiment quantifying how well the network performs, if ONLY the explainable parts are used, is missing. Without such an analysis (e.g. in the spirit of ANOVA in statistics) trust in the explanation is hard to establish.\n\n5) Reproducibility\n  The expert annotations of the internal units and the DeepMiner framework itself is not available which makes the results a little hard to reproduce in practice.\n\n\n[1] Wu et al., Expert identification of visual primitives used by CNNs during mammogram classification, https://arxiv.org/abs/1803.04858, SPIE, 2018.', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		Bkxms_x1lN	rygIqeG27V	MIDL.io/2019/Conference/-/Paper28/Official_Review	[]	3		['everyone']	Bkxms_x1lN	['MIDL.io/2019/Conference/Paper28/AnonReviewer2']	1548652686427		1548856748891	['MIDL.io/2019/Conference/Paper28/AnonReviewer2']
134	1548649093821	{'pros': '1. The method introduces the idea of funneling sub networks which is a novel way to deal with the dense segmentation problem\n2. Experimental results are well validated', 'cons': '1. The description of subnetworks could have been more elaborate as I did not fully grasp its advantage and architecture nuances', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'oral_presentation': ['Consider for oral presentation']}		Hkx5C9QeeN	BylRFzWnQV	MIDL.io/2019/Conference/-/Paper90/Official_Review	[]	3		['everyone']	Hkx5C9QeeN	['MIDL.io/2019/Conference/Paper90/AnonReviewer3']	1548649093821		1548856748673	['MIDL.io/2019/Conference/Paper90/AnonReviewer3']
135	1548025269545	"{'pros': 'The authors present a method for constructing LDDMM atlases by combining convolutional neural networks and LDDMM using the momentum parametrisation. Whilst previous works have shown how to combine LDDMM registration and deep neural networks (Yang et al 2017, ""Quicksilver""), to my knowledge this is the first paper to demonstrate using this method to construct an atlas. The solution presented is elegant and has nice theoretical guarantees on the smoothness of the registration as a result of using the LDDMM framework. This is therefore a very important contribution to the literature, and may be of great practical importance. Overall I think the ideas in this paper give it the potential to be amongst the most interesting at the conference.\n\nIn general the paper is very well written and clear. It has well-chosen comparisons with alternative methods that demonstrate convincingly that the proposed method achieves results that are as least as good as the conventional LDDMM method (with the important caveat highlighted below), whilst requiring just a single forward pass of the model at test time.\n\nI am also very pleased that the authors have made their code publicly available, and that they have separated this into a re-usable library and code to reproduce the specific experiments detailed in the paper.\n\nOverall I feel this is a novel and important methodological contribution but I have some serious concerns that would need to be addressed before the paper can be accepted. I think it is plausible that this could be achieved in the brief rebuttal period, and if so I would be happy to recommend this paper for acceptance.', 'cons': '\nI have two very serious concerns about this paper and a number of other suggestions for improvement. \n\nMajor Concern 1: Lack of Results Demonstrating Generalisation to Novel Images\n----------------------------------------------------------------------------------------------------------------\n\nIn the discussion of the dataset, no mention is made of a held-out test set used to evaluate the model. The only numerical results in the paper are in Figure 2, which relate to the value of the objective function *during training*. I am therefore lead to conclude that the images presented in Figures 3 and 4 may well also come from the set of 25 images used for training, as there is no indication otherwise (there is a possibility that this is a misunderstanding). It is unreasonable to evaluate the performance of the model on the dataset that it was trained on, as the neural network may have overfit to the very small number of cases in the training set and the momentum fields it produces for novel may not be meaningful. The entire point of the diffeomorphic autoencoder model is that it can rapidly register novel images to the learnt atlas.  Therefore it is of utmost importance that the paper include numerical results demonstrating the performance of the model on unseen images to show that the model has not overfit, and that all figures showing registrations are clearly from images that were not used to train the model so that they reflect the expected quality of the registrations on novel images. I believe that the paper cannot be published without this, however it also seems likely that the authors should be able to rectify this issue quite quickly, especially as there is plenty of unseen data available in the OASIS dataset. For this reason I am advising ""reject"" at this point in time, however I hope that I will be able to change this in the future if satisfactory changes are made.\n\nMajor Concern 2: Insufficient Discussion Of Relationship to Previous Work and Omitted Citations\n-------------------------------------------------------------------------------------------------------------------------------------\n\nThe second serious concern I have is that whilst I believe there is considerable novelty in the proposed method the authors need to take far greater care to highlight their contributions relative to existing work, including some that is not cited in the submitted manuscript.\n\nFirstly, the authors should discuss the relationship between their paper and Yang et al. 2017, (""Quicksilver..."") in greater depth. There are important similarities between that paper and the submitted manuscript that are not discussed. Yang et al also use a neural network to predict the momentum parametrisation of a geodesic shooting LDDMM method directly from the input image, but this is not sufficiently acknowledged in the literature review, where this is presented as being entirely novel. From my understanding the key differences between Yang et al. 2017 are that \n\na) Yang et al. rely on precomputed momentum estimates to supervise training of their CNN model but in the submitted manuscript they actually train their model by backpropagating through the differentiable EPDiff method and directly optimise the registration loss function\nb) The submitted manuscript also learns an atlas along with the momentum encoder whereas Yang et al. are only concerned with registering a single moving image to a single, known target image.\n\nThese are both very important contributions but I would very much like to see the differences more clearly delineated in the manuscript.\n\nFurthermore, and more importantly, the authors do not discuss the relationship to the following paper:\n\nUnsupervised Learning for Fast Probabilistic Diffeomorphic Registration\nAdrian V. Dalca, Guha Balakrishnan, John Guttag, and Mert R. Sabuncu\nMICCAI 2018, pages 729-738\nhttps://link.springer.com/chapter/10.1007%2F978-3-030-00928-1_82\nor\nhttps://arxiv.org/pdf/1805.04605.pdf\n\nThis work is very close to the current paper in that both use neural networks to speed up diffeomorphic registration, though Dalca et al.  do not include atlas learning as part of their framework. Furthermore there appear to be differences in the way that the two papers parametrise the diffeomorphic deformation and implement the solution of the resulting differential equation but unfortunately my knowledge of the underlying mathematics here is not sufficient to comment on this with authority in limited time. What is clear however is that the current paper needs to discuss its relationship to Dalca et al. in some technical detail.\n\nAssorted Minor Comments and Suggestions:\n--------------------------------------------------------------\n\nNow on to some other suggestions that would improve the paper but should not prevent acceptance.\n\nTo start with, a very easy fix - the Figure references have somehow become mixed up. The text frequently refers to Figure 3.2 and 3.3 but there are no such figures!\n\nIn my opinion there is room for improvement in the schematic diagram in Figure 1. It would be clearer if the encoder network were explicitly drawn and the process by which the atlas is involved in the learning process made clearer. It is important that this figure is clear to give readers the best chance of understanding the novel and complex method quickly.\n\nAnother concern is that the authors used only a very very small subset of the available OASIS dataset -- 25 out of nearly 2000 volumes -- to construct their atlas, but no justification for this is offered. It seems like their proposed method should be trivially scalable to any number of volumes, so why not use them? It occurs to me that this may be because the authors wanted a fair comparison with the standard LDDMM technique, and it would take a long time to use the standard LDDMM method on nearly 2000 images. This would be a good justification but the authors should state it explicitly.\n\nThe authors claim that they ""did not observe need for [...] heuristics such as batch normalization"" (section 3.2, first paragraph). However using batch normalisation (or occasionally other sorts of normalisation) has become a de facto standard in neural networks because it is consistently observed to speed up training considerably and also improve generalisation performance. I would suggest the authors try using batch norm in future experiments, unless they are using very small batch sizes.\n\nThe authors claim that one potential reason that the proposed neural network method may reach a lower value of the loss function on the training data than the standard LDDMM method is that mini-batch updates can be used in the optimisation process (section 3.2, second parapgraph). It is difficult to assess this however, as the authors do not state what size of minibatch they used in their experiments.\n\nThe primary justification for using the Diffeomorphic Autoencoder to predict the momentum initialisation for EPDiff is that it should be considerably faster than having to perform an iterative optimisation method to register a new image to the atlas. This is alluded to in section 2.2, but there are no results that actually investigate this. The paper would be far more compelling if results were given for how much faster a novel image can be registered to the atlas using the diffeomorphic auto-encoder versus using the standard auto-encoder. To be clear: I expect that their method is much faster, it has just not been demonstrated. Without this, the authors have not really demonstrated that their method has any advantage over the standard method.\n\nIn the explanation of the momentum encoder model, it is quite unclear how the 64D latent code is transformed into the momentum vector field. The paper simply states the latent code is ""followed by the output vector field of the network of size 3 × 83 × 118 × 110"". More detail is needed here. Are there some upsampling or transposed convolutional layers here or just a fully connected layer followed by a reshaping? I briefly looked at the source code but was not immediately able to figure it out. The reader should be able to understand this without looking at the source code.\n\nThis leads me to a comment on the encoder model. It seems to me that the purpose of including the bottleneck (the 64D layer) in the model is dubious. On the one hand, it allows you do some nice interpolations in the latent space as shown in Figure 4, but is this really practically very useful? Maybe it is if you are looking to do certain types of shape modelling, but this is only briefly alluded to in the conclusion and not in much detail. It also provides a degree of regularisation, but it\'s not clear that this is necessary. On the other hand, it will likely reduce the ability of the model to match fine details of the two images. Why not instead use a U-Net-like model (or any image-to-image model without a bottleneck) to output the momentum vector field directly? This would enable the network to consider both global and local information in the input image when creating the momentum image, and would therefore probably be more able to match fine details and give a better registration. Dalca et al (see above) use this approach. The paper would benefit from some justification of the authors\' choice here, and the authors may like to consider this carefully in future work.\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		Hkg0j9sA1V	SyxA2TOzXN	MIDL.io/2019/Conference/-/Paper19/Official_Review	[]	2		['everyone']	Hkg0j9sA1V	['MIDL.io/2019/Conference/Paper19/AnonReviewer1']	1548025269545		1548856748453	['MIDL.io/2019/Conference/Paper19/AnonReviewer1']
136	1548640025236	"{'pros': 'Summary:\nIn this work the authors have described a deep network (Automap-GAN)-based k-space artifact correction algorithm that improves image quality, which leads to improved segmentation accuracy. \n\n* Demonstrates useful results of motion artifact correction for improving segmentation of synthetic and real data. \n* Interesting network design (though there appear to be significant errors in the description) \n', 'cons': '* The network architecture is rather hastily described and not clear to me.\nThe discriminator architecture description lacks details such as number of filters, filter-size, final layer activation function etc. A picture will help.\n\n* The network architecture has been called an Automap-GAN but there is no mention of a discriminator loss in the eventual loss function. The training of GANs is tricky to control depending on how many iterations are the generator and the discriminator trained before updates, the details of which are missing. \n\n* What if the adversarial loss (if it exists) is removed? \n\n* What was the k-space dimension size $n$ that was eventually used?\n\n* Not sure if the novelty claim of using SSIM and MSE as loss functions is true. These are available in tensorflow and are used frequently for image synthesis tasks.\n\n* The details of the activity regularizer are not mentioned. \n\n* I assume the training and test subject sets were non-overlapping? The description in terms of the 2D images does not make this clear.\n\n* Standard deviations for the metrics in Table 1 and Table 2 will give a better idea of the improvement in performance.\n\n\n\nMinor:\nTypo in ""prerequisite"" caption of Fig 1. ', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BkgjbQ30yN	H1gZQyynmV	MIDL.io/2019/Conference/-/Paper24/Official_Review	[]	1		['everyone']	BkgjbQ30yN	['MIDL.io/2019/Conference/Paper24/AnonReviewer3']	1548640025236		1548856748155	['MIDL.io/2019/Conference/Paper24/AnonReviewer3']
137	1548477110802	{'pros': 'The authors have proposed a SCNN-based brain tissue classification method using DMRI data. The idea of applying SCNN on fODF is straightforward and looks interesting. The paper is well written and it is easy to follow. ', 'cons': 'But, I have concerns on the evaluation of the proposed method.\n\n-\tThere is no comparison to other methods. To convince applying SCNN is promising, a comparison, as least, to a method with CNN on fODF directly could be helpful. There is one dice score from an existing study reported. Did this study analyze the same data?\n-\tI do not see why it is a good idea to train on a single subject. Including more subjects for training to account for individual variability should be a much better idea.\n-\tThe performance on CSF is very low (around 60 of DC). Discussion about the performance on this could be helpful.\n-\tThe HCP data provides tissue segmentation label map in diffusion space. What is the reason to perform reference labeling again? ', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		BJgm2DkJgN	r1gkazwFQ4	MIDL.io/2019/Conference/-/Paper27/Official_Review	[]	2		['everyone']	BJgm2DkJgN	['MIDL.io/2019/Conference/Paper27/AnonReviewer1']	1548477110802		1548856747943	['MIDL.io/2019/Conference/Paper27/AnonReviewer1']
138	1548634990721	"{'pros': 'Summary:\nThe authors propose an active learning strategy for the skin lesion classification task with three aspects 1) choosing informative samples via classification probability  2) choosing representative samples via PCA+LSH binning, and 3) introducing augmented examples that are constructed by tiling 4 labeled examples to form an image the same size as the labeled examples.\nThey show that their active learning approach achieves close to state-of-the-art performance using only 50% of the data and is better than two other active learning approaches. \n\n* Interesting (possibly novel) aggregative supervision step that seems to provide a performance boost. \n* Paper is clearly written\n* Validation is extensive\n', 'cons': '* It is not clear to my why the authors chose to use PCA and LSH when there are arguably far better methods to build a representative space (variational autoencoders, for instance), any clustering approach. It is not motivated why PCA+LSH is the best choice. \n* The aggregative supervision seems to me completely non-intuitive. Why does tiling 4 images of the same class to create an augmented sample with the same class label make sense? If the authors can provide the intuition behind their choice it would be helpful. Empirically it seems to help (which I don\'t understand why), but to me it\'s a strange choice. Why 2x2, why not any other grid-size such as 4x4? The authors have some explanation on Pg 5 (first and second paragraph), but to me it would appear that they are introducing images with 4 diverse lesions instead of one--and thus would end up neither in the benign nor malignant class distributions. \n* In general it is not clear to me which of the authors add-ons to the training are truly novel. Maybe the novelty lies in that they have all been used together in this fashion.\n* I could not find the information on the ISIC website but what is the distribution of benign vs malignant class samples in the test and the validation datasets? The sensitivity of all algorithms seems pretty low, which means the True malignant/ All Malignant ratio is low? So given a malignant lesion the algorithms are not doing a good job identifying it as a malignant lesion.\n* Why is SA (green curve in Fig 3 (a)) worse for 40% than for 20% or 30%?\n* Are these methods statistically significantly better? It would help to see the standard deviations of these metrics. \n* Results are good (86% accuracy) for training with 50% of the data, but random sampling (82.5%) is not far behind--approximately 12 more misclassifications in a test dataset of 600. Maybe if it is applied to a harder task, it may bring out a larger improvement.\n\nMinor points:\n* Grammar in the abstract ""while, can still achieve"" -> ""while still achieving""\n* In the first paragraph of Method, the authors mention they use randomly annotated 10% data to start with. I think they mean randomly chose 10% of the annotated data.\n* The writing can be more clear, for instance the choosing of low certainty (high informativeness) samples is described in Eq (1). It is simply the N_I samples with the least prediction probability by the current model. \n* Fig. 2 caption says the ""the model is non-trained with these data.."" --> ""..not trained..""?\n* The state of the art method seems to be worse than random (Fig. 3 (a))\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		rygLeM3C1N	Bygw_sajXV	MIDL.io/2019/Conference/-/Paper23/Official_Review	[]	2		['everyone']	rygLeM3C1N	['MIDL.io/2019/Conference/Paper23/AnonReviewer3']	1548634990721		1548856747727	['MIDL.io/2019/Conference/Paper23/AnonReviewer3']
139	1548632129194	"{'pros': '-well written and clear introduction\n-the authors proposal of using CAAE makes sense, especially the combination of local and global information.\n-The authors are tackling a very important problem, of harvesting annotations to reduce annotation labour, and their proposed system seems to be a well thought out and logical approach to the problem. \n-Moreover, I think that there is enough technical novelty and application to for MIDL, which I think is unfortunately weakened by important clarity and evaluation issues. ', 'cons': '\n\nClarity\n\n-Authors say that ground truth labels are only given to ROIs with consensus from two radiologists. When there was no consensus, were these ROIs removed from analysis, or were they categorized as ""normal"".  Ideally it\'s the latter, as these hard cases should be included in the system testing. \n-In general, I found parts the method/experiments unclear. When training the CAAE, how did you pick the stopping criteria? Did you separate any of the training into a validation set for gauging when to stop training, i.e., when LOSS2 or LOSS1 began to increase?\n-Most importantly, what\'s the ultimate goal of the system? Is the idea that you can reduce labour by only labelling clusters? If deployed on another dataset, how would these clusters be labelled? By picking an exemplar and propagating its label to the rest of the instances in the cluster? How would such an exemplar be chosen? If deployed, would you still need radiologists to label slices, or would that not be needed? \n-Text and images of figure 2 are way too small\n\n\nExperiments\n\n \n-It\'s not clear to me at all how the precision/recall was measured. As I understand it, the authors cluster the ROIs into 64 clusters. How was the label assigned to each cluster determined? Is the idea that you can reduce labour by just classifying each cluster? If so, that\'s certainly valuable, but details need to be provided on how the cluster labels are assigned. \n-Also, should you not be measuring cluster purity as well? What about the degree of labour reduction? \n-Sec 3.2 shows that CAAE with ROI+slice outperforms CAAE with only one of ROI or one of slice. Seems like the authors should also show the performance of CAE with ROI+slice as well, to quantify how much the adverserial component adds to discrimination when using both ROIs and slices. \n-Apart from major clarity issues above, a fundamental weakness of the evaluation is that I don\'t see that a held-out test set was used to measure precision/recall performance of the clusters. If the authors iterated the design of their system based on the precision/recall scores, then it\'s definitely possible that their performance has overfitted to these metrics. As I understand it, the idea is that their system can be deployed and generalized to much larger datasets and/or environments to harvest ROIs. But such datasets will not have any ground truth available in order to tune hyperparamters properly, so the question on whether a held-out test set was used or not in their own evaluation of precision/recall becomes an important one. \n\nMinor\n\n-Fuzzy logic -> fuzzy logic (capitalization)', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		HylqNO4RkV	rJxtHx6iXV	MIDL.io/2019/Conference/-/Paper12/Official_Review	[]	2		['everyone']	HylqNO4RkV	['MIDL.io/2019/Conference/Paper12/AnonReviewer3']	1548632129194		1548856747475	['MIDL.io/2019/Conference/Paper12/AnonReviewer3']
140	1548631976092	"{'pros': 'The paper introduces a novel unsupervised method for lesion detection based on Normative prior. The paper is well-written, and the method is validated on a publicly available database showing an improvement over the state-of-the-art. \n', 'cons': ""While the proposed approach is quite interesting, the experiments do not rather validate the novel contributions. For instance, I was expecting to see the following experiments: \n\n1. A comparison with spatial VAE (Baur et al. 2018) which is similar to the proposed method, however, with a single multivariate Gaussian mixture --> To validate the need of modeling the latent code as a mixture of Gaussians.\n2. A comparison of  GMVAE (w/o Image restoration) vs. GMVAE(TV) --> To validate the need for Image Restoration. For instance, n = 0 vs. n = 500 steps as reported in the paper. \n\nFurther, I was expecting a section on the sensitivity analysis showing the following: \n\n1. the influence of the number of mixtures \n2. the influence of the number of steps in the image restoration (accuracy vs. time complexity)  \n\n\nApart from that, here are some questions/comments:  \n1. The network p(c|z,w) wasn't reported in Appendix A, so I was wondering whether it was implemented or not. Any observations, regarding the last term in Eq.2, similar to what reported in Dilokthanakul et al. 2016? \n2. if Eq.2 is converged, then can't we detect outliers from p(c|z,w)? For instance, outlier pixels (regions) would have lower probabilities in all mixtures and should be easily detected. \n3. Can't we use the MR distribution, i.e., WM, GM, CSF, and background as p(c)? \n4. M in Eq.7 is not defined. "", 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		S1xg4W-leV	BJxlnypimV	MIDL.io/2019/Conference/-/Paper57/Official_Review	[]	3		['everyone']	S1xg4W-leV	['MIDL.io/2019/Conference/Paper57/AnonReviewer2']	1548631976092		1548856747221	['MIDL.io/2019/Conference/Paper57/AnonReviewer2']
141	1548630934410	{'pros': 'The paper proposed a deep-learning-based pacemaker artifact removal method. Different from standard MAR procedures, the authors segmented pacemaker leads in the projection domain, to address the perturbation on metal shadow caused by cardiac motion.\n\nThe method showed good performance on dynamic pacemaker removal with solid validation. \n- The design of dataset-splitting 6:4:4 in supervised learning (sec 3.2) is reasonable, keeping datasets independent. \n- It is also thoughtful to consider the foreground-background class imbalance with patch sampling.\n- The implementation of a single decoder structure with skip connection of only the center slice information reduces parameters.\n\nThe paper targets an interesting and useful research problem. It is of interest to see more details and discussions of this metal artifact removal pipeline.\n', 'cons': 'The paper is well written, except that some parts need a lecture back-and-forth. For example, an ensemble of 5 CNNs is detailed in Sec 4, helpful to understand the pipeline (Sec 3.3 (a)); Test data with pacemakers (Sec 2) can be confused with testing datasets with synthetic metal leads  (might specify real test data, to distinguish from synthetic test data). \n\nFor evaluation on real data, comparison with standard MAR approach is lacking (Fig 6).\n\nTo evaluate newly introduced artifacts form false positives (Sec 4.1 end), real data without pacemaker or other metals would be a good scenario to test the model’s reliability and to quantify false positives. \n\nIt is not clear if the projection data of size 128*672 are resampled to 20*20 as the input of U-Net for prediction. How does the resampling affect the accuracy? \n\n', 'rating': '4: strong accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'special_issue': ['Special Issue Recommendation'], 'oral_presentation': ['Consider for oral presentation']}		rkx5InjA1N	r1x05ohiXN	MIDL.io/2019/Conference/-/Paper20/Official_Review	[]	3		['everyone']	rkx5InjA1N	['MIDL.io/2019/Conference/Paper20/AnonReviewer3']	1548630934410		1548856746980	['MIDL.io/2019/Conference/Paper20/AnonReviewer3']
142	1548629133898	{'pros': '* Deep model to generate  high-resolution (140^3) mandible images from the set of surface landmarks (29 landmarks)', 'cons': '* It is not clear why f(V) network (auxiliary network) is required in this image generation task. For example, because the input Z is the coordinates of the surface landmarks, the f and g models can be formed as a cycle model for cycle consistency. \n* More training samples are desirable. Currently the number of training samples is just 87. \n* It is desirable to show the input landmarks together in Figure 4. It may be better to understand the mapping between the landmarks and output images (mandible shapes) by the g model.\n* Comparison with the segmentation methods is very confusing. It is recommended to measure the surface distances between the generated model and the surface mesh where the input landmarks are extracted from for the accuracy evaluation in the surface generation.  \n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		BkltUK71xV	Bkg8cN2i7V	MIDL.io/2019/Conference/-/Paper31/Official_Review	[]	3		['everyone']	BkltUK71xV	['MIDL.io/2019/Conference/Paper31/AnonReviewer2']	1548629133898		1548856746763	['MIDL.io/2019/Conference/Paper31/AnonReviewer2']
143	1548627444066	"{'pros': 'The authors present an experimental setup to compare different strategies to refine weak annotations that can later be used as training data for a U-Net. The main contribution of the paper is the experimental setup and comparison of four different strategies over a large set of data.\n\nThe authors present the work in the context of bone marrow microscope images and their final goal is to segment the white blood cells present in the images. \n\nThe idea is in itself interesting and original although one could argue that some of the methods used for refinement represent a huge overhead for the whole process.\n\nI find the paper in general well written and easy to follow, although there are some specific sections that might require further clarification (see cons).\n\n', 'cons': '- Clarity: There is one key point that is not sufficiently clear from the paper regarding the segmentation task the authors aim at. In section 3.4, they talk about instance segmentation which means they are interested in being able to segment each of the cells present in the image (i.e. each cell has one label as opposed to having only two labels: a label for the cells and another for the background). However, in section 2 they refer to the segmentation task of separating cells from background which is not instance segmentation. This raises a couple of questions:\n1) Which task are you aiming at in the end?\n2) If the goal is to do instance segmentation, you need to better explain how the circular approximations are obtained from the ground truth. Figure 1a displays the ground truth and the cells are not individually identified. They are grouped under a single label. How do you extract the different cells?\n3) If the goal was to do a semantic segmentation, why there was a need to modify the U-Net described in point 3.4?\n\n- Quality of the experimental setup: I consider that the main weakness of this experiment is the way in which the "" weak annotations"" are obtained. These come from a perturbation done to the ground truth through a circular approximation followed by noise adding (some further steps are done for the U-Net). How realistic is to represent weak annotations through an alteration of the ground truth? Is it possible to derive conclusions on how these methods would improve some hypothetical  weak annotations provided by a user based on these experiments? I think your idea is interesting but you should either: 1) define an experimental setup where the weak annotations are closer to what a user would provide or 2) provide a reasonable justification of your current choice.\n** As previously stated, your idea is interesting. However, is it feasible to think that one would use two chained U-Net architectures to cope for weak annotations. I find this to be an unrealistic scenario.\nMoreover, you should consider that the training set of the first network consists of an already good guess of the real data. How would this work when the quality of the training data is poor at first instance?\n** A previous work from Rajchl et al (arXiv 2017), explores the effect of weak annotations on medical image segmentation. The  authors could consider a similar setup to define initial weak annotations and then use those in combination with simple methods (not deep learning, see for example the section Merging pre-segmentations from Rajchl et al) that refine the weak annotations and lead to a better final result. \n\n- Significance: The authors do not compare to other works addressing weak annotations missing to demonstrate the significance of their work. They should compare their work to recent works showing good segmentation results directly from weak annotations such as those cited in the introduction or the work of Wang et al, MedIA 2016. \nIt would be interesting to see if by using simple methods, as the ones they evaluate, it is possible to achieve higher performance that when using more complex methods.', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		BklpVE1eg4	Hklhx0ii7V	MIDL.io/2019/Conference/-/Paper49/Official_Review	[]	2		['everyone']	BklpVE1eg4	['MIDL.io/2019/Conference/Paper49/AnonReviewer1']	1548627444066		1548856746545	['MIDL.io/2019/Conference/Paper49/AnonReviewer1']
144	1548625918520	{'pros': '* Bone lesion image generation using VAE based generator trained by adversarial learning with cycle consistency.\n* Promising results by the data augmentation using the proposed method in lesion classification\n* Reasonable approach using transfer learning for femur and tibia cases. ', 'cons': '* Expert assessment or any quantitative analysis is required to evaluate the visual quality of the synthetic images, generated by the proposed method.\n* Lack of description of the generator model structure. \n* Lack of comparison with recent approaches in medical image generation \n* It is not clear how to choose the parameters, n, i and j, for blending. \n* It is not clear that the generator can synthesize various patterns of bone lesion including structural changes. Residual connection between encoder and decoder may help to generate more photo-realistic images, but the lesion patterns can be less diverse.  \n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		rkxOKaj0yV	H1lvb_soQE	MIDL.io/2019/Conference/-/Paper21/Official_Review	[]	2		['everyone']	rkxOKaj0yV	['MIDL.io/2019/Conference/Paper21/AnonReviewer1']	1548625918520		1548856746293	['MIDL.io/2019/Conference/Paper21/AnonReviewer1']
145	1548625835038	"{'pros': 'The authors propose a deep learning method for super-resolving 3D vector field data, for 4D flow MRI, by adding a squeeze and excitation(SE) block to a widely activated residual 3D (WDSR) block - which is the baseline model. The extra SE block exploits cross channel information. This, in conjunction with an l1 loss converges faster than the original WDSR architecture.\n\nThey also introduce a mutually projected l1 loss function which is direction sensitive. The modifications to the original loss function with the L1 loss(WDSESR-1), mutually L1 loss(WDSESR-2), and combination of the L1 and mpL1 loss(WDSESR-3), converge to a better validation PSNR on their datasets as compared to the baseline WDSR.\n\nThe results are validated on CFD simulations and in-vivo flow MRI data.', 'cons': ""Major Concerns :\n\n1. The results from Table 1 show very minor improvements in terms of the masked and full PSNR, and SSIM. For the  3x case, WDSESR-1 and WDSESR-3 are reported as having the same SSIM value, but WDSESR-3 is highlighted as having the best performance. The closeness of the reported performance metrics is not address or explained conclusively. \n\n2. The qualitative comparisons in Figure 5 do not indicate the performances of WDSESR-1 and WDSESR-2. Given this figure, it isn't entirely clear why a combination of the two loss functions is preferred over the other two cases.\n\n3. Table 2 and Figure 7 also fail to include WDSESR-1 and WDSESR-2. \n\n4. Specifically, the merit of adding mp L1 to L1, or even mp L1 instead of L1 is not clear from the comparisons referred to in the above two points, or from the tables\n\nMinor Concerns:\n\n1. Appendix A: The learning rate for ADAM has been reported to be 10^3. Either a typo or needs explaining for this application.\n"", 'rating': '2: reject', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		S1g0Vms-lE	BJlQhvsjQE	MIDL.io/2019/Conference/-/Paper140/Official_Review	[]	2		['everyone']	S1g0Vms-lE	['MIDL.io/2019/Conference/Paper140/AnonReviewer2']	1548625835038		1548856746081	['MIDL.io/2019/Conference/Paper140/AnonReviewer2']
146	1548625144211	"{'pros': 'The method presents a a method for cell detection in H&E stained histopathology images based on convolutional networks.\nThe model predicts three maps, which are then combined and post-processed to get the predicted location of cells.\n\nThe paper is well written and authors show that their method outperforms state of the art approaches on a public dataset of manually annotated cells in colon cancer, and claim that their method is faster than other methods that address the same task.\n\nIn my opinion, the main contribution over previously presented methods is the introduction of the wt_map, because something equivalent to a combination of conf_map and loc_map was already present in other works, such as Sirinukunwattana et al. (2016). Additionally, the formulation of the problem as a multi-task approach is novel in this context, to the best of my knowledge.', 'cons': 'The proposed method shows improvements over state of the art approaches, but it is only tested on one single dataset, and only limited to H&E staining. It would be interesting to show, or comment, whether the same method would work for immunohistochemistry as well, where stain artefacts are presents, and detection of cells grouped in clusters is challenging.\n\nAdditionally, only examples of positive results are reported. Reported F1-score is good, but not perfect. This should be discussed, for example show cases with failure, if there are common causes of failure, how to address them, and whether this relates to an imperfect reference standard. Would also be good to compare with one of the methods based on region proposal mentioned in the introduction, such as Faster r-cnn, or YOLO, which showed pretty good performance at lymphocyte detection (only limited to IHC) at MIDL 2018 (M. van Rijthoven et al., 2018).\n\nAbout quantitative performance, the authors claim that the performance ""largely improved"": from 0.879 to 0.886, and from 0.882 to 0.887. Is this considered a large improvement in this setting?\n\nThree maps are produced and combined to create an accumulator map, which is post-processed in order to obtain the final detections. Since the three maps are generated for the training set as well, did the author check whether this gives F1-score = 1.0 on the training set? I guess it does, but it could be that the contribution of Wt underweights some locations and lowers their final score. If this is the case, it would be good to assess the performance of the post-processing step on the training set as well (without using the model), which could be a good indication to understand the upper bound of the performance of this approach.\n\n\nOther comments:\n\n* The caption of Table 1 should be improved, it does not describe what is in the table (the description is in the text though).\n\n* How is the average accuracy computed? Only single pixels manually annotated as foreground and the rest as background? And how are the weights computed, if used, and the scores averaged? This is not clear from the text.\n\n* What type of functions are the losses?\n\n* lambda_1 and lambda_2 are introduced but then set to 1, authors could consider removing them from the formula. I acknowledge they mention investigating this effect in the future, but I wonder what is the utility of these two parameters in this paper.\n\n* A receptive field equivalent to the size of a single cell is used; would a slightly larger receptive field improve the performance, allowing to include some more context?\n\n* The architecture relies on an encoder-decoder model, but skip connections used in the U-Net architecture are not used here. Was this a specific design choice, and would using skip connection improve the performance of the method?\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		Byg6tbleeE	BJxl-BooQV	MIDL.io/2019/Conference/-/Paper51/Official_Review	[]	3		['everyone']	Byg6tbleeE	['MIDL.io/2019/Conference/Paper51/AnonReviewer2']	1548625144211		1548856745863	['MIDL.io/2019/Conference/Paper51/AnonReviewer2']
147	1548625078650	"{'pros': ""A well-written paper proposing an adversarial network for pseudo-healthy image synthesis by explicitly separating the healthy and pathology domain. Method has been compared to two baseline methods (CycleGan and conditional GAN) on two publicly available datasets with superior results evaluated using their newly proposed metrics for 'healthiness' (measuring size of predicted pathology) and for 'identity' (measuring similarity of non-pathological regions). This is very interesting approach to an important problem, particularly by the lack of desired image pairs (image with and without pathology of same patient) and potential wide number of applications. "", 'cons': '- Implicitly, their approach does a detection and segmentation of pathology. Why not evaluating their method on how well this part has been done, and not only on healthiness and identity?\n- how were both cycles (Figure 3) trained? Does the following order matter?\n- it is unclear if the method was trained and evaluated using the images as 3D or 2D slices. I suppose as 2D otherwise the number of patient data seem too limited\n- I suggest to have expert reader or radiologist to evaluate how well the healthy synthesis has succeeded\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'oral_presentation': ['Consider for oral presentation']}"		H1gMYCQxg4	HJl1TEij7E	MIDL.io/2019/Conference/-/Paper91/Official_Review	[]	2		['everyone']	H1gMYCQxg4	['MIDL.io/2019/Conference/Paper91/AnonReviewer1']	1548625078650		1548856745594	['MIDL.io/2019/Conference/Paper91/AnonReviewer1']
148	1548624600867	{'pros': 'The paper presents an implementation of the U-Net for medical image segmentation applied on two open-access datasets. \n\nInstead of using two decoder network for Mask/Contour (or Mast/Distance Map), the author proposed that using a signal decoder structure, would achieve comparable results and reduced number of parameters compared with previous works.\n\nThe comparison of 3 distance maps is of clear interest to be discussed, as well as the information provided by features maps before the output layer.\n \nThe paper is well organized and clearly written.\n', 'cons': 'The first main drawback of the work: the evaluation is performed on randomly split 30% testing data without cross-fold validation. Based on experience, the variation of metrics among different training-testing splits is considerable. This makes the conclusion of “better performance than state-of-arts” a bit week.\n\nSecond, for the loss functions (sec 2.3), illustration of the changing and convergence of L_mask and L_contour (or L_mask and L_distance) during training epochs is lacking. Should present their magnitudes during training to verify if adding them together is a good and balanced choice.\n\nFig 1, small correction for the decoder part: the red blocks and their neighboring blue blocks have different channels (ex. 768 = 256+512, separating the number of channels), and thus, they should be of same length, height but 1:2 width.\n\nTo compute the contour, the authors dilated the boundaries from ground truth segmentation of radius 5, how does the dilation affect accuracy?  (Sec 2.2)\n\nIt is not clear if the cup and disc segmentations are trained separately.\n\nIt is a bit confusing (sec 2.1, end) “for distance map… 1 feature map” for method 1Enc1dec+Conv MD, while the loss function contains two terms.\n\nRef. Sarker et al.: title missing.\n\nThe novelty of the paper is not so prominent.\nAlthough the parameter reduction is interesting, by “minimalistic”, I would encourage the authors to explore the feasibility of further reducing number of parameters or removing certain layers compared with U-Net (Ronneberger et al.) while maintaining the performance. \n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		SJxWBKR1gN	SklZyXjjmV	MIDL.io/2019/Conference/-/Paper47/Official_Review	[]	2		['everyone']	SJxWBKR1gN	['MIDL.io/2019/Conference/Paper47/AnonReviewer3']	1548624600867		1548856745379	['MIDL.io/2019/Conference/Paper47/AnonReviewer3']
149	1548624134527	"{'pros': 'The paper presents a deep learning method for the super-resolution of 4D flow MRI. The proposed architecture combines wide activation and squeeze & excitation strategies in a residual block to encourage information flow while reducing redundancy. A mutual projected (MP) l1 loss is proposed to provide more sensitivity to the direction of flow vectors.  \n\n- First application of SR to 4D flow MRI. Note: deep SR methods have been used to resolve vector fields in diffusion MRI (e.g., see Albay et al.2018, Tanno et al. 2017 below)\n\n- Novel SR loss function presented for reconstructing vector fields, which gives more importance to vector direction.\n\n- Comprehensive experimental validation on real and simulated data. Experiments measure the impact of several factors, including loss function and use of squeeze and excitation blocks.\n\n- Significant improvements are observed compared to basic techniques like cubic spline interpolation.\n\n- Except for a few typos, the paper is well-written. \n\nAlbay, E., Demir, U. and Unal, G., 2018, September. Diffusion MRI Spatial Super-Resolution Using Generative Adversarial Networks. In International Workshop on PRedictive Intelligence In MEdicine (pp. 155-163). Springer, Cham.\n\nTanno, R., Worrall, D.E., Ghosh, A., Kaden, E., Sotiropoulos, S.N., Criminisi, A. and Alexander, D.C., 2017, September. Bayesian image quality transfer with cnns: Exploring uncertainty in dmri super-resolution. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 611-619). Springer, Cham.', 'cons': '- Methodological contributions are a bit incremental. The proposed architecture combines ideas from previous works (i.e., wide activation and squeeze & excitation). The main contribution lies in the novel loss function, however this loss offers no significant gain when used by itself compared to l1 (see Table 1). \n\n- The technique employed to generate LR images can have an important on performance. Authors should better justify their approach of removing high frequency components in k-space (i.e., smoothing), instead of more traditional techniques for SR. Note: reducing the number of k-space samples is more related to compressive sensing than super-resolution. \n\nOther comments:\n\n- ""We use voxel shuffling layer (3D pixel shuffling (Shi et al., 2016) layer) to rearrange features from channel dimension to increase spatial dimension"": Provide more details on voxel shuffling.\n\n-  ""We argue that although wide activation helps in better feature propagation throughout the residual blocks, they often share similar information because of the channel expansion with a 1×1×1 convolution kernel."": This may not be obvious to the reader. Authors should elaborate.\n\n- Figure 3 is very dense and hard to understand. Moreover, it should be made clear in the text that the local minimum is only w.r.t. v (i.e., assuming u fixed) \ny\n- Differences in Table 1 do not appear to be statistically significant. Likewise, Fig. 5 does not really illustrate the advantage of the proposed method.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		S1g0Vms-lE	rJxyz-ioXE	MIDL.io/2019/Conference/-/Paper140/Official_Review	[]	1		['everyone']	S1g0Vms-lE	['MIDL.io/2019/Conference/Paper140/AnonReviewer1']	1548624134527		1548856745160	['MIDL.io/2019/Conference/Paper140/AnonReviewer1']
150	1548620190035	"{'pros': ""The author proposes to use the output of the softmax function (here denoted the softmax prediction) to be used for two purposes: \n1) to be used as an uncertainty measure that can flag (classify) 2D slice over-segmentations of the heart and,\n2) to be used as a feature/predictor, along with the slice number and its mean intensity, in order to train a regressor that can predict a slice's dice score. \n\nThe problem that the author tackles is of significance and of high relevance. As the author mentions, heart segmentation has achieved a high level of accuracy. However, there are cases where the segmentation fails. In the presence of large volumes of data, it is desirable to have a way to automatically identify such cases so that, for example,  a human expert can intervene. For this matter,  there is currently a lot of interest in developing methods that can assess the quality of a segmentation in the absence of ground truth. This problem has been very well explained by the author. I would just recommend to cite O. Bernard et al, TMI 37(11), 2018, which is the journal publication associated to the Automated Cardiac Diagnosis Challenge, rather that the proceedings of STACOM (Pop et al, 2018) when referring to the challenge.  \n\nWhile this problematic has recently been addressed by other authors, I do not consider that the problem is yet completely solved. Therefore, I consider this work to be original and of value for the community.\n"", 'cons': '- State of the art: The author misses to position herself against very similar work aiming at the same task. The work of Valindria et al, TMI 36(8) and Robinson et al. MICCAI 2018 aim at the same task, same anatomical structure (the heart) and similar type of images (MRI, although the sequences are not exactly the same). By positioning this work w.r.t with these previous one, it is easier to understand the value of the current work.\n\n-  Clarity: The paper is not always easy to read. First because there are many steps involving different algorithms (i.e the U-Net, a Random Forest and finally a Boosted Tree). For this I would recommend adding a diagram where one can see how the different steps of the framework are interconnected, which algorithms are used and which data is used (sample sizes) for each of them. \nSecond, there are several there some mistakes and missing elements in the methodology which are not properly introduced. This makes difficult to follow and to understand the value of your contribution. Some examples include:\n** In Section Methods the author talks about the segmentation of the left ventricle (endo and epicardium) to then talk about the myocardium for the remaining of the paper. What is it that you aim for? Moreover, when reporting results on table 2 it is never mentioned which structure was being segmented.\n** In Section 2.2 the author refers to a "" previously trained"" CNN model to separate the slices to remove from those to keep"". More details should be provided 1) on the CNN model (as no further information is given) and 2) on the separation of slices as the problem is never formally introduced and one has to infer what the author aims at from the text.\n** Before equation 1, the author refers to a "" corresponding mask"". This is never introduced.\n** In Equation 1, refer to the pixel as x_{i}. It is confusing to call it pix_{i} as the i acts also as the index.\n** The slice number as a feature seems, as it is right now, a strange choice. The author should provide more details: \n      1) What numbers are assigned to slices that do not contain the heart? \n      2) Perhaps these are excluded at training but, then how does it work at testing? Is it needed to only provide slices containing the heart? How do you achieve this?\n      3) Is the image resolution taken into account for the normalisation? Please provide details. \n** As previously mentioned, the author selected to use three different algorithms. The first is the modified U-Net that was previously proposed for the segmentation, second there is a random forest for classifying keep vs. remove slices and finally a boosting tree to predict the dice score. Could you please discuss the reason why you chose to use something at each step?\n** It is not clear how detecting the erroneously segmented slices improves segmentation accuracy (Table 1). Do you remove those identified and then measure again? This should be better explained.\n** Provide details on implementation of U-Net and Boosting tree as done with the random forest.\nI consider that the author has sufficient space to provide further details about the methodology. Moreover, the section on the MRI sequences (1.1) seems too long for this type of paper. I would recommend to leave only section 3.1 (which seems slightly redundant at the moment) with may be a couple of more details to introduce the data. \n\n- Methodology & Evaluation:\n** Robinson et al MICCAI 2018 discussed the problem of highly imbalanced data for the accuracy prediction with the dice score as most of the dice scores were concentrated in a small range. How did you address this problem? It would be interesting to see reported the range of dice scores used for training.\n** The dataset used for evaluation is rather small consisting of only 40 cases. As the method contains several steps, it would be interesting to see how the data is distributed among all of the training/testing tasks that are required.\n** Why the validation strategy is different (5 fold vs 10 fold CV) when evaluating the classifier and the regressor? Is this related to the data limitations? Please clarify.\n\n- Results: \n** It is strange not to see some visual example of the results that are expected from this work. One would expect to see examples of slices kept/removed along with the ground truth. Similarly, one would expect to see segmented slices along with predicted dice score, uncertainty measure and real ground truth.\n** From Figure 2, one could say that there is no need to use a classifier to keep or remove slices. One could argue that by defining a threshold around 0.995 it would be possible to obtain a good classification of keep vs. remove slices. Could you please comment on this?\n** Figure 3 shows that the uncertainty measure you propose is always very high (> 0.992 in the worst case). This means that your measure is always very confident about the segmented pixels. However, the dice scores either:\n   1) Run over a large range of values. For example, for an uncertainty higher than 0.994 the measured dice score is slightly higher than 0.2 (first plot). There are several cases presenting a similar score  or,\n   2) For a same (high) uncertainty value present different dice scores that run over a rather large range of values. This can be seen from the ""columns"" of predicted and measured uncertainty-dice value pais piled along the same abscissa value. \n\nFrom this behaviour, one could conclude that the proposed uncertainty measure might not have a significant contribution to the regression model, as the author states. The relevance of the uncertainty measure should be better demonstrated.', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		rJe3YQqgxE	r1e8i-cjQE	MIDL.io/2019/Conference/-/Paper125/Official_Review	[]	1		['everyone']	rJe3YQqgxE	['MIDL.io/2019/Conference/Paper125/AnonReviewer3']	1548620190035		1548856744905	['MIDL.io/2019/Conference/Paper125/AnonReviewer3']
151	1548616665877	"{'pros': '\nSummary: Authors present a new dataset with 850 images and manual expert segmentations of surgical instruments from (non-robotic) minimally-invasive brain tumor neurosurgery. On this dataset and the MICCAI 2017 challenge (DaVinci robotic tool segmentation), authors compare instrument segmentation using three well-established approaches: manually handcrafted features + Grabcut algorithm, vanilla U-Net and U-Net with transfer learning (VGG16 features trained on ImageNet).\n\nPros:\n- A new dataset with 850 images of 8 classes of instruments was collected and hand-labeled by surgeons. The dataset will be made public. Such sharing of data is welcome and will benefit the field, since it adds to the variety of instrument appearance next to other public datasets like the MICCAI 2017 instrument segmentation challenge.', 'cons': '\nRemaining questions / clarity:\n- Next to the 5 video sequences collected for labeling, authors ""additionally collected 6 more videos to increase the size of our dataset"" - are those 6 further videos entirely unlabeled? Were they used in any of the experiments in this work?\n\nCons:\nWhile the authors present a new dataset and an evaluation study on an established challenge dataset and their new dataset, the employed methods are not very novel and the investigated comparison does not really yield new insights. Numerous works in the past years have shown that hand-crafted features are not competitive to representation learning in deep neural nets. The comparison of a vanilla U-Net to U-Nets with domain-transfer (i.e. feature injection from ImageNet-trained networks like VGG16 in the encoder path) has also been investigated in multiple other works in literature, and in many of those work, pre-training/domain-transfer was beneficial. Further, in many of those works, a more thorough evaluation was presented than here, where a mean Dice overlap measure serves as the sole measure of comparison.', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		BylPGrhVxN	rkgf1EYjm4	MIDL.io/2019/Conference/-/Paper147/Official_Review	[]	2		['everyone']	BylPGrhVxN	['MIDL.io/2019/Conference/Paper147/AnonReviewer1']	1548616665877		1548856744654	['MIDL.io/2019/Conference/Paper147/AnonReviewer1']
152	1548616485503	"{'pros': '\nSummary: \nAuthors present AnatomyGen, a CNN-based approach for mapping from low-dimensional anatomical landmark coordinates to a dense voxel representation and back, via separately trained decoder and encoder networks. The decoder network is made possible by a newly proposed architecture that is based on inception-like transpose convolutional blocks.\nThe paper is written clearly. Methods, materials and validation are of a sufficient quality. There are certain original aspects in this work (latent en-/decoding, inception-based decoder network, latent space interpolation, generalization to previously unseen shapes etc.), but the work may not be as original as authors suggest, since they may not be aware of a very similar work (see Cons), where some of the discussed concepts have already been proposed and explored.', 'cons': '\n- Authors explicitly that the work is not intended for segmentation, but many previous shape modeling works (including SSMs) were used as regularization in segmentation. Authors could comment on how their model could be incorporated into (e.g. deep) segmentation approaches, because I do not see an immediate way to do that without requiring the (precise) image-based localization of mandible landmarks in a test volume.\n- I would recommend weakening or at least toning down certain ""marketing"" claims like ""3 times finer than the highest resolution ever investigated in the domain of voxel-based shape generation"", or ""the finest resolution ever achieved among\nvoxel-based models in computer graphics"". First, it is not fully clear where this number 3 comes from, and second, the quality of the work speaks for itself. Further, there is always the chance that authors are not aware of every piece of related literature (in all of computer graphics), as it might be the case here.\n- Authors claim to introduce many concepts for the first time, such as the ""first demonstration that a deep generative architecture can generate high fidelity complex human anatomies in a [...] voxel space [from low-dimensional latents]"". However, I am aware of at least one work where such concepts have been proposed and explored already. CNN-based shape modeling and latent space discovery and was realized for heart ventricle shapes with an auto-encoder, and integrated into Anatomically Constrained Neural Networks (ACNNs) [1]. Their voxel resolution is only sligthly smaller than in this work (120x120x40), with a similar latent dimensionality (64D, here: 3*29=87). Smooth shape interpolation by traversal of the latent space was also demonstrated, and some of their latents also corresponded to reasonable variations in anatomical shape, without being ""restricted"" to statistical modes of variation as discussed here. \n- Compared to the proposed work, where latents represent clinically relevant mandible landmarks, an auto-encoder approach as in ACNN is more general: relevant landmarks as in the mandible cannot be identified for arbitrary anatomies, and a separate training of decoder and decoder as proposed here crucially depends on a semantically meaningful latent space with a supervised mapping to the dense representation (e.g. hand-labeled landmarks vs. voxel labelmaps). In contrast, ACNN auto-encoders train their encoder and decoder in conjunction. How do authors suggest to apply their approach to anatomies where it is impossible (in terms of feasibility and manual effort) to place a sufficiently large number of unique landmarks on the anatomy (e.g. smooth shapes, such as left ventricle in ACNN)?\n- Authors suggest that their solution ""is not constrained by statistical modes of variation"", as e.g. by PCA-based SSM methods. While I agree that the linear latent space assumption of PCA is too simplistic and the global effect of PCA latents on the whole shape often undesirable, the ordering of latents according to ""percent of variance explained"" is actually desirable in terms of interpretability. \n\n[1] Oktay O, Ferrante E, Kamnitsas K, Heinrich M, Bai W, Caballero J, et al. Anatomically Constrained Neural Networks (ACNNs): Application to Cardiac Image Enhancement and Segmentation. IEEE Trans Med Imaging. 2018;37(2):384–95. ', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		BkltUK71xV	Skxp77YiXV	MIDL.io/2019/Conference/-/Paper31/Official_Review	[]	2		['everyone']	BkltUK71xV	['MIDL.io/2019/Conference/Paper31/AnonReviewer1']	1548616485503		1548856744439	['MIDL.io/2019/Conference/Paper31/AnonReviewer1']
153	1548616837231	"{'pros': '\nSummary: The authors present an approach for surgical activity segmentation using fully convolutional neural networks (FCNN), i.e. an hourglass architecture i) in its vanilla form, ii) with direct skip connections from the down-sampling to the up-sampling path, and iii) skip connections with incorporated convolution+pooling+normalization blocks.\nThe paper is written clearly. Methods, materials and validation are of a sufficient quality. There are certain original aspects in this work (hourglass-networks with skip connections, once direct and once with additional convolution operations), but overall, the novelty is limited. The evaluation is performed on the publicly available JHU-ISI JIGSAWS dataset, for which competitive methods and results are available.  \n\nPros:\n- Good overview of related literature on action segmentation from kinematic data\n- Validation on JIGSAWS data is well comparable to other methods in literature.\n- Comparison of several hourglass architectures and kinematics representations in the experiments. ', 'cons': '\nRemaining questions / clarity:\n- While HMMs and RNN/LSTMs are designed to handle temporal sequences of varying length T, a FCNN architecture as proposed here requires a fixed-length input. Authors try different lengths in this work (10/20/.../50), but it is not clear 1) what the unit of this temporal window length is (10 seconds, or 10 samples of 76D kinematic vectors), 2) if 10 means ""10 samples"", at what framerate were  kinematics recorded, and how many seconds of kinematic data are covered by 10/20/.../50 samples?, 3) whether the inference was performed in a sliding window fashion with striding, and what the striding factor was (dense sliding, or every n samples, or windows with 50% overlap)? \n- The comparison evaluation to other methods (Table 3) does not feature results for ED-TCN, but authors could include this with little effort, by removing the encoder-to-decoder skip connections from the ED-TCN-Link network and re-training.\n\nCons:\n- The ED-TCN-Link network architecture is an hourglass network with skip connections from the down-sampling to the up-sampling layers. This idea is not novel though, and the resulting architecture is in principle identical to a 1D U-Net with summation instead of concatenation of feature maps in the up-sampling path [1]. Could the authors please discuss this similarity and explain whether and in which way their architecture is different from a 1D U-Net?\n- Three different kinematics representations were tested (All/Slave/PVG), as originally proposed by Lea et al. Results confirm the previous finding by Lea et al. that PVG performs better than the other two representations, but no further insight beyond this is won from this experiment. For example, in future work, it could be more interesting to investigate whether more efficient latent representations of ""All"" can be achieved. One interesting direction could be e.g. deep bayesian state space models [2]. \n- The ED-TCN-ConvLink architecture is similar to ED-TCN-Link, but with convolutional and pooling layers put into the forward links. In the experiments, this architecture almost consistently performs worse than ED-TCN-Link. I can imagine that this is due to the incorporation of pooling (downsampling), and I would recommend trying to leave them out and perform only convolution instead (the link needs to be summed into one higher layer in the up-sampling path though, right after the up-sampling layer, to match resolution). In U-Net and comparable architectures, horizontal links preserve spatial resolution and high-frequency features from the down-sampling path. Maybe the loss in accuracy is due to this loss in resolution.\n\n[1] Ronneberger O, Fischer P, Brox T. U-Net: Convolutional Networks for Biomedical Image Segmentation. Miccai. 2015;234–41. \n[2] Karl M., Soelch M., Bayer J., van der Smagt, P., Deep Variational Bayes Filters: Unsupervised Learning of State Space Models From Raw Data, ICLR, 2017, https://arxiv.org/abs/1605.06432', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		B1g5dgfee4	BkepY4ti7V	MIDL.io/2019/Conference/-/Paper69/Official_Review	[]	2		['everyone']	B1g5dgfee4	['MIDL.io/2019/Conference/Paper69/AnonReviewer2']	1548616837231		1548856744187	['MIDL.io/2019/Conference/Paper69/AnonReviewer2']
154	1548612934529	"{'pros': 'The paper presents a dense 3D-FCNN for segmenting multi-modal infant brain MRI. The main contribution is a modified training strategy which optimizes the prediction of tissue classes separately with sigmoids, instead employing a traditional softmax function. This enables a finer control of the precision-recall tradeoff, via a custom F-beta loss. \n\n- The paper proposes a somewhat novel strategy for dealing with high-overlapping classes, for which the trade-off between precision and recall for each class has an significant impact on performance.  \n\n- Authors report state-of-art performance of the challenging iSeg dataset, where different class regions exhibit low contrast. Statistically significant improvement is also obtained compared to the traditional single-label (i.e., softmax) approach.\n\n- The paper is well written and easy to follow. In particular, authors did a good job motivating the problem and their proposed method. The method and experiments are clearly described and could be reproduced fairly easily.\n', 'cons': '- Contributions w.r.t. to existing work are not entirely clear. The proposed loss is similar in terms goal to the Generalized Dice Loss (Sudre et al., 2017 -- see bottom ref.), where the precision/recall importance of each class is weighted by its size. Moreover, the strategy of processing 3D images in separate patches (both in training and testing) is actually implemented in several segmentation methods, for instance DeepMedic and HyperDenseNet. In fact, this random region crop strategy is fairly standard when training deep segmentation networks from large images.     \n\n- The proposed method seems to be tailored to this specific dataset (iSeg), i.e., three classes, two of them having a large overlap. A stronger validation could have been achieved by testing the proposed method on other brain MRI segmentation datasets (e.g., MRBrains), or problems where class imbalance is more pronounced (e.g., brain legion segmentation).      \n\nMinor comments:\n\n- The proposed architecture merges modalities in the first layer, however recent studies have shown that later fusion could lead to better performance (e.g., Dolz et al., 2018). Perhaps authors could motivate this architecture choice.    \n\n- ""calculating sigmoid is less computationally cumbersome for a processing unit compared to softmax especially for large number of labels."": I doubt this makes a real difference in computation time.\n\n- ""... our 3D FC-DenseNet architecture which is deeper than previous DenseNets with more skip-layer connections and less number of parameters"": This is a bit misleading. For instance, HyperDenseNet introduces skip connections across all layers and all paths (one per modality), therefore has a maximum number of skip connections for a equivalent number of layers.\n\nCarole H Sudre, Wenqi Li, Tom Vercauteren, Sebastien Ourselin, and M Jorge Cardoso. Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations. In Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, pages 240–248. Springer, 2017.', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		Byg-krBi14	B1ekIruoXN	MIDL.io/2019/Conference/-/Paper4/Official_Review	[]	3		['everyone']	Byg-krBi14	['MIDL.io/2019/Conference/Paper4/AnonReviewer1']	1548612934529		1548856743936	['MIDL.io/2019/Conference/Paper4/AnonReviewer1']
155	1548611858151	{'pros': 'In this paper, the authors present a comparison of three different deep architectures for the automatic diagnosis of pneumothorax in chest radiographs. In particular, they compare a convolutional neural network ResNet50, a U-Net-like fully convolutional architecture, and a multiple-instance learning. \n\n- In a time where a new architecture is proposed every day, I think that a structured and organized comparison of the actual performance of different models is necessary. \n\n- In general, the paper is well written and well structured, being easy to read and understand. \n\n- They make a good job describing how automatic CAD systems could improve the workflow in a real radiology department. \n\n- It has some short of “encyclopaedic” spirit, comparing the performance of different alternative architectures, and providing a brief, but clear description of each one.\n', 'cons': '- In fig. 1, they show that additional patient-specific meta-data information was also incorporated into the network. Was something similar done for the rest of the alternative models? Please, clarify.\n\n- Number of parameters per model? Is it a fair comparison between the models?\n\n- In the same way they provide a graphic representation of the ResNet-50 and the multiple-instance learning network, they should also show the representation of the FCN.\n\n- For ResNet50, they trained the network from the scratch using the images using in the experiments. Why was a pretrained VGG used for the MIL network?\n\n- How many form the 566 cases without pneumothorax where healthy and how many pathological?\n\n- Why only 305 out of the 437 images were manually annotated? Does it mean that only 305 images were actually used? I’m not sure if the comparison between the three models is entirely fair. Not only the number of parameters, input image size, and tasks are different, but also the number of parameters and even the training process (e.g., CNN was trained from the scratch while a pre-trained network was used in MIL).\n\n- It is also hard to compare visually the ROCs curves of each method, being depicted in four different graphs. I would suggest to include a comparison between all the methods in another figure.\n\n- Please, provide additional details of the training process for each model: number of epochs, training time, etc. \n\n- Please provide the Dice’s coefficient values for the FCN model.\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		B1lpH4jkxN	SkgczZOiQE	MIDL.io/2019/Conference/-/Paper41/Official_Review	[]	2		['everyone']	B1lpH4jkxN	['MIDL.io/2019/Conference/Paper41/AnonReviewer2']	1548611858151		1548856743677	['MIDL.io/2019/Conference/Paper41/AnonReviewer2']
156	1548609972895	{'pros': 'The paper proposes a deep learning framework for pseudo healthy synthesis based on the factorization of pathological and anatomical information. The network is trained following two different settings namely, the paired and unpaired. To enable quantitative performance evaluation the “healthiness” and the “identity” metrics are proposed. The method has been validated on two different datasets and its performance has been compared to two baselines, the conditional GAN and the CycleGAN.\n\nThis is an interesting work which fits well to the scope of the conference. The paper is well written and easy to follow. The contributions of the paper have been clearly defined. The presented work is of sufficient technical novelty and seems technically and theoretically sound. The references are adequate. The figures could be improved as it is explained below in detail.\n', 'cons': 'Suggestions for revision\n\n1. In the 4th paragraph in Section 3.3, it is mentioned that “a pathology mask for a real healthy image cannot be defined”. It is not clear why a black mask can not be used in this case. \n\n2. In the experimental results, why has the ISLES dataset been divided so unevenly into the training (22 volumes) and testing sets (6 volumes)?\n\n3. Figures 2 and 4 are quite small, making it difficult to distinguish the differences between the subfigures. The size of these figures should be increased.\n', 'rating': '4: strong accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		H1gMYCQxg4	SkxThtvi7N	MIDL.io/2019/Conference/-/Paper91/Official_Review	[]	1		['everyone']	H1gMYCQxg4	['MIDL.io/2019/Conference/Paper91/AnonReviewer2']	1548609972895		1548856743457	['MIDL.io/2019/Conference/Paper91/AnonReviewer2']
157	1548610822048	{'pros': '1. Finding appropriate mixing ratios at the layer scale for multi-task model merging in an adaptation stage is a novel approach\n2. The model was tested on an appropriate dataset and shows an improvement over previous methods\n3. The paper is well-written and clear', 'cons': '1. It would have been valuable to see the distribution of alphas (mixing ratios) that were learned in the adaptation stage for the experiments\n2. How does fine tuning affect the optimal mixing ratios? Is alpha still close to the optimum after fine tuning? One imagines that iterating between the adaptation and fine-tuning stages until alpha convergence could give a superior result\n3. In the case of larger data sets (100%), the model shows only a marginal improvement over existing methods. There are no error bounds on the accuracies so it is difficult to judge whether this is a statistically significant difference\n\nOther issues:\n- Table captions should appear above table\n- Page 3, last line: Q is not defined\n- Introduction: “cause” -> “because”\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		rklPRjjlxV	HJxCb6DjmN	MIDL.io/2019/Conference/-/Paper131/Official_Review	[]	3		['everyone']	rklPRjjlxV	['MIDL.io/2019/Conference/Paper131/AnonReviewer1']	1548610822048		1548856743238	['MIDL.io/2019/Conference/Paper131/AnonReviewer1']
158	1548610804869	{'pros': 'The paper presents a semi-automated data generation pipeline and a deep learning (DL) framework to segment potentially cancerous areas in prostate biopsies. Six different models were trained using needle biopsies and generated prostatectomy data. The proposed framework has been validated on biopsy data. \n\nThis is an interesting work which fits well to the scope of the conference. The paper presents validation of a DL framework which was originally introduced at [Pinchaud and Hedlund, 2018]. The figures are clear, and the references seem adequate. The paper is well structured, but I found it difficult to follow as many technical details regarding the data generation are missing as explained below. Also, the presented framework has not been compared to any baseline method. ', 'cons': 'Suggestions for revision\n\n1. In Section 4.1, it is not clear how WOB areas are detected on H&E images. Is this done with manual segmentation? Details about the density estimation filter should be given. Why is it required to “distribute the local information evenly within a local neighborhood of the image” when applying the density filter? The authors should explain in detail how the heatmaps are generated.\n\n2. In the “Using consecutive slices” section, which method has been used to register the consecutive H&E stainings to the original H&E images? How does the performance of this registration affect the accuracy of the generated ground truth data?\n\n3. In Section 4.2, the number of pathologists who annotated the biopsy data should be specified rather than defining them as “several”. How were the ground truths of these pathologists combined? What is the level of expertise of the pathologists who annotated the data and of those used for the comparison in Section 5.4?\n\n4. The performance evaluation study would have been more robust if it had included comparison to other segmentation approaches.\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		SJxVA7xleE	HkxpgTPj7E	MIDL.io/2019/Conference/-/Paper53/Official_Review	[]	2		['everyone']	SJxVA7xleE	['MIDL.io/2019/Conference/Paper53/AnonReviewer3']	1548610804869		1548856743018	['MIDL.io/2019/Conference/Paper53/AnonReviewer3']
159	1548610216071	"{'pros': '- This work combines four processing pathways for pathological images in order to predict their Gleason score\n- The fusion of a pre-trained network and a network trained from scratch is an interesting approach\n- Saliency map generation from expert guidance adds to the information available to the model', 'cons': ""Issues:\n- The authors compare their model to previous work by Jafari-Khouzani and Soltanian-Zadeh [1]. The authors claim that Jafari-Khouzani and Soltanian-Zadeh's model achieves a classification accuracy of 0.82 (table 1). However, in the original paper [1] the accuracy is stated as 0.97. I find this difference, and lack of explanation for it, quite concerning. 0.97 also happens to be the accuracy that this work presents. Both papers use the same dataset.\n- The stated contributions include: fine-tuning a VGGNet to make the model versatile as to process any kind of microscopic image that is captured in any lighting background or angle. I do not see evidence in results that support this claim, particularly since all of the images in the dataset are captured in the same lighting conditions\nThe authors say that they augment the data (section 3.2) and split into training/validation/test sets. I have two issues with this A) It is not conventional to augment test sets and the authors provide no motivation for it. B) If the augmented data were split at random into the three sets, then different augmentations of the same source image could exist in both training and test sets causing obvious data leakage\n- No proper citations for works that are pivotal to the author’s model and implementation including VGGNet, DCGAN and TensorFlow\n- The pages beyond the recommended eight were not put to good use\n\nThis work provides some methodological contributions but they lack explicit motivation and the experiments are insufficiently explained. Additionally, the paper itself is lacking in both quality and clarity.\n\nMinor issues\n- Numerous spelling and grammatical errors (too many to list) that make the paper hard to read\n- Table 3 (confusion matrix). Grade 3 adds up to 100.25%\n- Section 3.2: learning rate of 1 - e^-4. A peculiar learning rate, possibly a typo\n- Some abbreviations (e.g. ROI, FCL) are not defined or explained\n\n[1] Jafari-Khouzani, K., & Soltanian-Zadeh, H. (2003). Multiwavelet grading of pathological images of prostate. IEEE Transactions on Biomedical Engineering, 50(6), 697-704.\n"", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BJgXUQLBgV	S1llnqwjXV	MIDL.io/2019/Conference/-/Paper156/Official_Review	[]	1		['everyone']	BJgXUQLBgV	['MIDL.io/2019/Conference/Paper156/AnonReviewer2']	1548610216071		1548856742804	['MIDL.io/2019/Conference/Paper156/AnonReviewer2']
160	1548608019396	"{'pros': 'This paper addresses the problem of scarcely available dense manual annotations for supervised learning in histopathology image segmentation, and propose using parse annotations in a framework based on few-shot learning.\nThe problem tackled by this paper is very relevant, because manual annotations are time consuming and very expensive, specially when pathologists have to be involved, whereas sparse annotations are easier to make.\n\nThe title suggests that a framework for collaborative annotations, possibly by involving multiple users, is presented, which is a novel approach in the context of the Camelyon challenge and to metastases detection in lymph-nodes in general, to the best of my knowledge.', 'cons': 'The paper lacks clarity in the order and in the details in which components are introduced and applied, and several parts of the paper are difficult to understand.\nFurthermore, it is not clear where the ""collaborative"" part of the whole methodology takes place.\nThe only point where this is mentioned is in the section about ""late fusion"", but I do not understand how new annotations added during inference can make the model collaborative. What would be a good use case scenario? This should be explained in the paper.\n\nAdditional comments:\n\n* In section 3.1, a training set of s samples and a test set of t samples are introduced. What is the size of s and t, and what should be their order of magnitude to make this method effective? I guess t << s, otherwise one could just rely on dense annotations from the test set and use them for training. Experiments with different ratios t:s should be performed\n\n* What is the method actually tested on? What is called test set seems to be used during training, so there should be another set used for the actual validation of the method, but it\'s not introduced.\n\n* Figure 2 is not clear. I think the direction of the arrow between g and m is wrong. Furthermore, components are used here that are described later in the paper, which makes it very difficult to understand. Those components are actually introduced in the section about experiments, instead of in the method section.\n\n* Patches are labeled as lesion ""if at least one pixel in the center window of size 224x224 was annotated as lesion"". Is this the central pixel or any pixel in the patch? The way it\'s written it seems that it is any pixel in the patch, which I don\'t think is a good choice.\n\n* ""Bilinear interpolation for downsampling"" sounds a bit odd.\n\n* Table 1 and Table 2 show results with sparse and dense annotations, but it\'s not clear if dense refers to FCN-32. If it does, why are the numbers in the text different from the ones in the table?\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		HJl6fpzgg4	Syxjffvo74	MIDL.io/2019/Conference/-/Paper81/Official_Review	[]	3		['everyone']	HJl6fpzgg4	['MIDL.io/2019/Conference/Paper81/AnonReviewer3']	1548608019396		1548856742587	['MIDL.io/2019/Conference/Paper81/AnonReviewer3']
161	1548598347085	"{'pros': 'This paper explores the use of deep learning algorithms to identify contrast enhancement phases. The algorithm design is reasonable. The problem was explored from both a classification and regression perspective. The results are also very promising.\n\nWhile many other works on medical imaging only reports classification accuracy, this work also uses saliency map to analyze the rationale behind the classifier. This step is very crucial in medical applications as it always provides intuitive clinical insights to physicians.', 'cons': 'The description of the training/testing data is confusing to me. The authors made 10-fold split on the 60,000 patients\' data, but they further sampled or screened (by free text DICOM analysis) subjects in the training, validation and test datasets. It is important for the authors to make explicit how many subjects exactly are used for training, validation and testing. Are the same subset of subjects used for chest and abdominal analysis?\n\nThe authors need to clarify ""Even for these unambiguous cases, we find that the labels obtained are noisy, showing 90.5% consistency with a visually tagged sample by an expert radiologist."" If I understand correctly, there are approximately 90% subjects in the training and validation sets whose labels are correct. If this is the case, how can we interpret the >90% validation accuracy? \n\nThe authors used 2D convolution instead 3D. Is there a rationale why 2D features would suffice for 3D images?\n\nIt is still not clear to me why a regression model would be chosen over a classification model. There is no direct comparison between the two models. Does the classification model perform equally well for finer granularity 7 phases?\n\ntypo: Section 3.1 ""The high top-2 accuracies show the that the ground truth ...""\n\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		SylDbkUlxE	r1e7824s7N	MIDL.io/2019/Conference/-/Paper104/Official_Review	[]	2		['everyone']	SylDbkUlxE	['MIDL.io/2019/Conference/Paper104/AnonReviewer1']	1548598347085		1548856742367	['MIDL.io/2019/Conference/Paper104/AnonReviewer1']
162	1548594396295	"{'pros': 'Considering the fact that in most medical applications there are very limited training data, the preprocessing of images is crucial for utilizing deep learning algorithms. In general, the discussion on this topic is important and should be encouraged. \n\nThe presentation of the paper is clear and easy to follow; the three important preprocessing steps explored in this paper are quite reasonable as most existing works differ on these aspects. The conclusions made the authors seems rationale. ', 'cons': 'Compared to the critical motivation of the work, the present results are hard to interpret. I believe the results and conclusions can be further strengthened if the authors can provide p-values.\n\nMajor:\nUnless I misunderstand the results, many of the authors\' conclusions don\'t match the tables. E.g. ""regardless of resolution or skull-stripping, a smaller intensity window has higher performance"", but from the ""1*1*1 level set"" row in Table 1, the smaller intensity window performs worse.\n\nEven for those with improvements, Figure 4 indicates they are not statistically significant, so the conclusions are less convincing. Again, the claims can be validated by p-values, e.g., paired t-tests.\n\nMinor: \nHow can we make sure that the results are invariant to different CNN architectures? Will the results replicate when another CNN model is used?\n\nIs figure 6 cited anywhere in the text? what additional value does it provide?', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		HylG9MfleV	rye4kpQomV	MIDL.io/2019/Conference/-/Paper72/Official_Review	[]	3		['everyone']	HylG9MfleV	['MIDL.io/2019/Conference/Paper72/AnonReviewer1']	1548594396295		1548856741956	['MIDL.io/2019/Conference/Paper72/AnonReviewer1']
163	1548595214848	"{'pros': 'The authors present an analysis of several machine learning methods for prediction of neurodegenerative syndromes from volumetric brain measurements. The paper is very easy to follow and clearly written. This is a comparative study, with minimal methodological novelty, which is not an issue since the application task seems very interesting and relevant (note however, that I am not very familiar with this subfield). ', 'cons': 'The authors used Bayesian optimisation for the parameters of the gradient boosting, random forests and support vector machines methods, however, not for neural networks. Was there a particular reason for this? In principle, the hyperparameters defining the regularisation (dropout) and neural network architecture could also have been subject to optimisation.\n\nThe neural network architecture is not particularly clear from the text (were there two hidden layers?, perhaps ""the second hidden layers"" is a typo). \n\nThe expression for Cohen\'s kappa seems incorrect. The correct expression is k = (observed agreement among raters - probability of chance agreement) / (1 - probability of chance agreement).\n\nIt would be to see an analysis for confounding factors such as age and center of origin. \n', 'rating': '3: accept', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		BJlZBmBrl4	rklwzxNiXN	MIDL.io/2019/Conference/-/Paper154/Official_Review	[]	2		['everyone']	BJlZBmBrl4	['MIDL.io/2019/Conference/Paper154/AnonReviewer1']	1548595214848		1548856741738	['MIDL.io/2019/Conference/Paper154/AnonReviewer1']
164	1548594833310	"{'pros': '\nThe paper presents a method to use multi-modal images within a neural network-based approach for medical image segmentation.\nThe different datasets do not need to have the same spatial resolution. In fact, the method is designed to leverage the high resolution of one image (even if it is not labeled) when paired with another one with a low resolution but labeled.\n\nThe motivation of the paper is clearly defined in the Introduction.\nThis is a relevant topic for the community since data annotation is tedious and time consuming; therefore, being able to train a machine learning model with only one label per patient would indeed be useful.\n\nTwo different approaches are being investigated, one of them being inspired by collaborative learning, and the other one by distillation learning.\nThe experimental setup is sound. The selected dataset, as well as all pre-processing steps, is correctly described.\nResults show that the collaborative learning approach seem to yield more true positives and fewer false positives and negatives.', 'cons': 'Here are my main concerns about the paper:\n\n* The experimental results, which basically consist of Table 1, are quite brief and not particularly convincing.\nMore statistics on the distribution of the statistics (mix/max/quartile/etc.) would have been helpful. Only reporting the mean and standard deviation is not sufficient, especially when they are so close to each other.  With such results, running statistical tests seems an absolute requirement to me, in order to understand whether any difference is statistically significant or not.\nRegarding the statistics on TP/FP/FN, it would also have been useful to include the actual number of lesions in the average dataset (otherwise it is difficult to assess how useful is this +3.15 TP).\n\n* It is also surprising that the proposed methods have better true positive and lower false positives/negatives, despite having a lower average of Dice coefficient. I think this should have been discussed by the authors.\n\n* The authors refer to a number of related literature throughout the paper but didn\'t compare their approach to any of them.\n\n* All experiments have been run on 2D slices, while state of the art methods for segmentation typically use 3D networks. \nFurthermore, it is not clear how the final results have been recombined, and whether the statistics on the number of lesions have been computed in 2D or 3D.\n\n* Another major problem of the paper is the writing of the method description.\nFigure 2 should probably be either refined or split into two smaller ones to make the difference between the two approaches more apparent.\nSection 3.1 is overly complex, with notations that are not useful (introducing a genetic \\phi while it is merely a downsampling operation) or only defined two sections later (e.g the ""temperature"" \\tau).\nI also find the distillation part a bit confusing, for instance in (4) I don\'t understand why tau is 1 in the first term but T in the other ones.\nIn general, the link between the equations and what the authors want to model needs to be more explicit.\n\n* The conclusion is also a bit confusing: \n- The authors admit that the ""type of problem chosen"" was inadequate.\n- The distillation approach did not seem to perform any better than a standard U-Net, but the authors wrote that they did not optimize some hyperparameters (why ?).\n\n* There are a number of typos and arguable statements, for instance:\n- Page 2 ""typically low-resolution scans are annotated"". \nI don\'t see why this would be the case - this seems like a counterintuitive approach. I don\'t know which tools were used to label the images but I don\'t think using a higher resolution image would increase the labeling time that much.\n- Typo on page 2 ""to the a common coordinate system""\n- Page 3 the sentence ""Shared representations [...] follows the network illustrated in Figure 2"" is a bit confusing with the two references to Figure 2.\n- Page 3 ""Usually in medical imaging, there are complimentary inputs with targets defined on one of the inputs"". While this is indeed true for MR imaging (especially for brain), I would not say that this is the common case in medical imaging.\n- Typo on page 5 ""In the our case""\n- Page 6 Wrong reference to ""Figure 2"" (should be Table 2)\n- Typo on page 6 ""For a give threshold""\n\n\nAll in all, I do believe that this paper has some merits, but I do not find it suitable for publication in its current state.\nI would advise the authors to re-write the description of the methods, enhance the experiments section by providing more statistics - perhaps also select a more suited dataset, and get rid of the distillation approach that does not seem to bring anything - and resubmit to another venue when the paper is more mature.\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		SJgw_QIlx4	rygt50QjmE	MIDL.io/2019/Conference/-/Paper106/Official_Review	[]	1		['everyone']	SJgw_QIlx4	['MIDL.io/2019/Conference/Paper106/AnonReviewer3']	1548594833310		1548856741525	['MIDL.io/2019/Conference/Paper106/AnonReviewer3']
165	1548593196683	{'pros': 'In this paper, the authors propose a deep multi-task network for the automatic estimation of clinically relevant information in the assessment of tuberculosis using thoracic CT volumes. In particular, they divide the diagnosis into 25 different tasks, including the number of nodules in each lobe, and the presence of up to 4 types of manifestations in each region. They use a multi-task approach, combining classification and regressions models with a dedicated multilayer perceptron for each task, and defining a combined loss function weighted by the task homoscedastic uncertainty. Additionally, the authors include self-normalizing blocks by using scaled exponential linear units as activation functions. Despite the limited database used in the experiments (a total of 14 cases and 56 volumes), the proposed model seems to present better properties than alternative and more conventional approaches using batch normalization, in terms of convergence. The use of clinical reports as ground-truth is an interesting approach. However, further validation needs to be done. \n\nPros:\n- The use of clinical reports as ground truth is interesting, as alternative to the more traditional and  time-consuming manual segmentations.\n\n- The proposed network seems to present good convergence properties, thanks to the use of self-normalizing blocks.\n\n- The paper is well structured and it is easy to follow. ', 'cons': 'According to the figures presented in the paper, it seems the model did not reach good convergence and further training was needed. Moreover, the learning curves of the alternative model were not even converging for some of the experiments (and yet they showed acceptable performance). In this context, it is hard to properly evaluate the performance of the models presented.\nSee more comments below:\n- I would recommend the authors to review the way the manuscript is written. There are some expressions that doesn’t sound quite right to me. For example: “[…] this approach is unfeasible. As it is extremely time consuming, subjective and prone to errors. “ —> I don’t think the use of two separate sentences is grammatically correct. Please, correct. \n- “However, these techniques are usually limited to a particular application and are quite sensible to the implicit high variance of medical images. ” —> I’m afraid these same issues could be raised about a DL-based network: data and task specific. \n- Data is non-isotropic and their size is not homogeneous (201 to 270) ??\n - There is a typo in the definition of SELU in equation (1). I think the second part should be “x <= 0”, not “x <= 1”.\n- How can it be a 5-folds CV if each fold corresponds to one single case, and there are 14 patients? Each fold should contain 3 cases, with one of the folds containing only 2. Please, clarify. \n- According to figure 2, it seems that traditional BN+PReLU suffers overfitting in some cases, while others it seems to converge better. How did the authors tried to optimize the hyper-parameters to deal with this? Is there any significant difference between cases that could be responsible of this behaviour.\n- Please, adjust the scale of all the figures in Fig.2. It is difficult to compare different graphs if the y-axis has a different scale-range on each one.\n- Also, from Figure 2 it is clear that 10K iterations wasn’t enough to converge. I would strongly suggest the authors to repeat the experiments since the reported results might be sub-optimal.\n- I would suggest to provide a separate evaluation of recall and precision, instead of the combined F1. Also, \n- How is it possible that in those folds where the BN+PReLU model is clearly diverging (e.g., fold 1 or 4), it is obtained a similar performance to the SELU model? Moreover, in Table 2 it can be seen that RMSE value for fold 1 is lower than fold 2, where the BN+PReLU was showing better convergence.\n- In the conclusion, the authors claim that the proposed model presents a lower computational complexity. Please, provide the computational cost for both models (training and execution time). \n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		ryxPCJNexV	BylBE_mi7N	MIDL.io/2019/Conference/-/Paper92/Official_Review	[]	3		['everyone']	ryxPCJNexV	['MIDL.io/2019/Conference/Paper92/AnonReviewer2']	1548593196683		1548856741273	['MIDL.io/2019/Conference/Paper92/AnonReviewer2']
166	1548584783581	"{'pros': '\n- The paper is well written, and easy to understand.\n\n- The authors propose to solve the domain adaptation problems by inserting another two type of layers, namely geometric transformation, and intensity transformation.\n\n- Ablation studies have been done extensively to validate the function of the proposed layers.\n', 'cons': '\n- The training process is not clear for the geometric transformation, where does the ground truth comes from?\n\n- I don\'t understand why the performance for the original source domain (adults) can drop, did you also use these new layers on testing the source domain ? if so, should you first train them when training on adults data ?\n\n- The idea of transforming the data is not new,  there are several missing references:\n1.  ""FiLM: Visual Reasoning with a General Conditioning Layer"", E. Perez, F. Strub, H. Vries, V. Dumoulin, A. Courville, In AAAI2018\n2. ""Learning multiple visual domains with residual adapters"", S.A.Rebuffi, H. Bilen, A, Vedaldi, In NIPS 2017\n3. ""Omega-Net: Fully Automatic, Multi-View Cardiac MR Detection, Orientation, and Segmentation with Deep Neural Networks."", D.M. Vigneault, W. Xie, C.Y. Ho, D.A. Bluemke, J.A. Noble, In Medical Image Analysis, 2018.\n\n- Why only use the intensity transformation only on that specific layer ? have the authors tried to put them every layer along the whole network, it won\'t introduce that many parameters, and if you put strong regulariser, they are unlikely to lead overfitting.\n\n ', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		HyxhwW_6k4	rJxdLw-jQN	MIDL.io/2019/Conference/-/Paper9/Official_Review	[]	1		['everyone']	HyxhwW_6k4	['MIDL.io/2019/Conference/Paper9/AnonReviewer1']	1548584783581		1548856741020	['MIDL.io/2019/Conference/Paper9/AnonReviewer1']
167	1548592860368	"{'pros': '\n- The paper is well-written, and easy to read and understand.\n\n- The authors consider the problem of nuclei detection, and propose to decompose the task into three subtasks, trying to predict the confidence map, localization map and a weight map.\n\n- I think the effort of disentangling a complicated task into simpler ones makes sense, and the experiments have shown promising results.', 'cons': '\n- In my view, the proposed methods are not completely novel, I think the authors are suggested to cite them, just name a few.\n\n- Predicting the confidence map with fully convolutional networks was initially done by :\n""Microscopy Cell Counting with Fully Convolutional Regression Networks"", W. Xie, J.A. Noble, A. Zisserman, In MICCAI 2015 Workshop.\n\n- The proposed localisation map is actually the result of distance transform, and has been initially used in : \n""Counting in The Wild"", C. Arteta, V. Lempitsky, A. Zisserman, In ECCV 2016.\n\n\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		Byg6tbleeE	B1xE1wQjQN	MIDL.io/2019/Conference/-/Paper51/Official_Review	[]	2		['everyone']	Byg6tbleeE	['MIDL.io/2019/Conference/Paper51/AnonReviewer1']	1548592860368		1548856740809	['MIDL.io/2019/Conference/Paper51/AnonReviewer1']
168	1548511111484	"{'pros': 'The paper presents a unique framework for generating a set of “explanations” for the prediction of a CNN classifier and demonstrates its efficacy in mammogram classification.\n\n- the idea of providing the pair of visual explanation (defined as saliency maps with respect to the most influential features in the last layer) and text explanation (“human interpretations” of such features provided by experts) is new and more informative than providing only one of the two as done in the prior work.\n\n- some convincing qualitative evidence of the method\'s utility is provided (e.g. making sense of failure cases with conflicting explanations) while the quality of ""explanation"" is quantified as the Greedy Matching Score with respect to radiological annotations. \n\n- the paper is by and large well-written. \n', 'cons': '\n- the method imposes some architectural requirements e.g. global average pooling, the number of fully connected layers, etc. \n\nMinor:\n- Correlations between features (or units) could be important for predictive performance. The method considers only the first-order “explanations” i.e. association of each unit to an explanation.  \n\n- I found the method section somewhat difficult to parse; pseudocode might be helpful for reproducibility. \n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		Bkxms_x1lN	SyxkcPJ9X4	MIDL.io/2019/Conference/-/Paper28/Official_Review	[]	2		['everyone']	Bkxms_x1lN	['MIDL.io/2019/Conference/Paper28/AnonReviewer1']	1548511111484		1548856740595	['MIDL.io/2019/Conference/Paper28/AnonReviewer1']
169	1548428682699	{'pros': 'The paper presents an application of a recent few-shot learning algorithm (Guided Network) to the problem of lymph node segmentation in histopathological images. \n', 'cons': '\n- no methodological novelty; the presented method is an application of an existing work (“Guided Network”) to a lymph node segmentation dataset.\n\n- no error bars are provided. For example, the model trained with (5 shots, 10 points) annotations performs better than the model trained with (1 shot, dense) annotations, which seems strange given that dot annotations are generated from dense annotations.\n\n- the details of optimisation are incomplete e.g. optimiser, learning rate, etc. \n\n- two of the requirements stipulated in the introduction are not empirically validated; 1) “be collaborative and easy to use”; 2) “requiring minimal maintenance.” \n\nOverall, despite the well-communicated relevance of the topic, the paper lacks both methodological novelty and empirical validation of its utility in the considered application. \n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		HJl6fpzgg4	BJeXcBiu7V	MIDL.io/2019/Conference/-/Paper81/Official_Review	[]	1		['everyone']	HJl6fpzgg4	['MIDL.io/2019/Conference/Paper81/AnonReviewer1']	1548428682699		1548856740380	['MIDL.io/2019/Conference/Paper81/AnonReviewer1']
170	1548587075908	{'pros': '\nThis paper applies dropout to different UNet-based architectures during training to tackle the problem of missing modalities in the inference. The presented method was validated on the public BRATS dataset for multimodal glioma segmentation.\n\n+ The paper is well motivated and clearly written;\n+ The method is validated on one publicly available dataset;\n+ The idea of this paper is straight-forward;\n+ Studied the combination of dropout and three different network architectures.\n\n', 'cons': '\n- Innovation of the paper is relatively limited. The utilization of the dropout technique and the three network architectures are not new in dealing with missing modalities and has already been used in studies published in MICCAI and TMI;\n- The paper is lack of discussion about existing works, as well as comparison with them. There are indeed some very good works towards addressing the issue of missing modalities;\n- The experimental validation is not comprehensive enough. Only the scenario of one modality missing was considered. The authors didn’t report the performance when more modalities are missing. Also, as mentioned in the discussion section, the information from one MR modality may not be entirely removed in the late fusion network, which could affect the results. \n- Although dropout increases the network robustness to missing modalities, the network performance on full dataset decreases.\n- Since the ensemble and late fusion network are trained for each modality separately, do they cost four times more training time than the single UNet network? \n\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		S1lXP-nJxE	B1e2BeMjQN	MIDL.io/2019/Conference/-/Paper43/Official_Review	[]	2		['everyone']	S1lXP-nJxE	['MIDL.io/2019/Conference/Paper43/AnonReviewer3']	1548587075908		1548856739947	['MIDL.io/2019/Conference/Paper43/AnonReviewer3']
171	1548585927532	{'pros': 'The authors aimed to investigate the effect of incorporating attention modules to various CNN architectures for automatically grading knee radiographs based on the OA severity. Supervised training using clinically accepted KL-grade as a ground truth is combined with unsupervised attention module training proposed by Mader2018 to achieve the goal. Related work and attention module structure are explained in great detail; however, the experiments and results require better explanation and further refinement.\n\npros:\n1-\tAutomatically grading the knee OA severity based on KL-grade will reduce the work load of radiologists and it could potentially enable automated OA progression measurements in clinics.\n2-\tIt was shown that attention modules can be inserted into various locations of the CNN architecture. \n\n\n', 'cons': '1-\tThe study proposes to use attention modules to remove the need for localization of the knee joints prior to classification. It is mentioned that the need for knee joint localization step affects the quantification accuracy negatively and adds further complexity to the training process. Even though the attention modules remove the necessity for knee joint localization (proposed by the authors), it still adds further complexity to the modelling and training process compared to previous approaches (multi-loss, concatenation of features, where to locate attention modules and so on). Moreover, the results presented using attention modules are performing worse than the CNNs without attention modules ( ~ 6% lower accuracy in Table 2 vs Table 3, and as mentioned by the authors in the Conclusions.). This reduction in the accuracy needs to be properly investigated and the reasons should be identified in detail. These are the fundamental issues with this paper which needs to be addressed in detail. Some suggestions/required improvements:\n1.a.\tWe need to know if the attention module improves the accuracy or not. This could be done by comparing CNNs without attention modules (which accepts either the full knee image or localized knee joint images) and with attention modules in a systematic way. In the current manuscript, this improvement, if any, cannot be distinguished from other factors. \n1.b.\tThe authors should provide results for the Resnet-50 and VGG-16 architectures without attention modules. This information is missing in the current Tables 2 and 3. Because of this reason, the readers cannot be clear if the improvement over Antony et al.’s models are due to attention modules or changes in the CNN architecture.\n\n2-\tIt is not clear if the data used in calculating the Kappa from 150 subjects from OAI dataset is in the test set of the original split or not. If these images were used for training or validation, this raises a question on the validity of the results presented in Table 3.\n\n3-\tFurther details are required for Section 3.3. I expect that the size of the fully connected (FC) layer after channel-wise concatenation will have an effect on the training. The size of the FC was not defined and its effect on the accuracy was not experimented. In addition, it was mentioned that several multi-branch combinations are tested in multi-loss training without giving details. It is not clear if this was achieved by some sort of a grid search or using a few empirical combinations. Please add required details to improve our understanding of the effect of attention branch locations and their corresponding weights to the loss function.\n\n4-\tDataset generation needs corrections/explanations: \n4.a.\tIt is mentioned that the OAI dataset has 4,476 participants at the baseline, but it has 4,796 subjects.\n4.b.\tTraining/validation/testing set are generated from images. Are there any specific reason for the authors not to generate these sets based on subjects? Is it possible that data splitting from images could add a bias to the results presented?\n\n5-\tThe manuscript has several typos, please fix them. For example: page 1: bony --> bone, page 5: focuse --> focus/focused, page 9: Table ?? --> Table 3.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		B1epyN8rlV	B1lg0ibsQ4	MIDL.io/2019/Conference/-/Paper157/Official_Review	[]	2		['everyone']	B1epyN8rlV	['MIDL.io/2019/Conference/Paper157/AnonReviewer3']	1548585927532		1548856739735	['MIDL.io/2019/Conference/Paper157/AnonReviewer3']
172	1548585308444	{'pros': 'The paper is relatively concise and clearly written. It describes and interesting methodology for handling image dataset of the same imagining modality and target, but with different annotations. The methodology is based on adversarial training of deep neural networks for segmentation. While the application domain (segmentation of different target in retina fundus images) is not terribly relevant as many of these applications are well-addressed, the wider context of the manuscript (dataset with disparate annotations) is relevant.', 'cons': 'The evaluation was not done properly or in a very transparent manner. \n\nFor the vessel segmentation  task, the author state that: “SE, SP and ACC are reported as the maximum value over thresholds.” This is poor practice and leads to overoptimistic estimates of the performance  of the method. Performance measures such as specificity, sensitivity and accuracy are highly dependent on the selection of the operating point. The selection of the optimal operating point should be part of the model design and ideally selected based on a validation set. \n\nFor the tasks related to the IDRiD challenge, the authors compare their performance to other methods submitted for the evaluation. However, the challenge seems to be closed for new submission and I assume that the performance for these tasks was evaluated by the authors themselves, and not by the challenge organisers. In my opinion, this disqualifies direct comparison with the official leaderboard of the challenge, unless the authors release the complete code based that reproduces the results described in the paper. \n\nFurthermore, the official leaderboard features results submitted by one of the authors of the paper. The results for the optic disc segmentation are similar as reported in the paper, however, all other results are significantly worse. Was this the same method as described in this paper? I', 'rating': '1: strong reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		HJe6f0BexN	Hkx4vtbsQN	MIDL.io/2019/Conference/-/Paper102/Official_Review	[]	2		['everyone']	HJe6f0BexN	['MIDL.io/2019/Conference/Paper102/AnonReviewer1']	1548585308444		1548856739481	['MIDL.io/2019/Conference/Paper102/AnonReviewer1']
173	1548584608513	"{'pros': 'Although the technical novelty of the presented work is not high (using a CycleGAN to ""style transfer"" digital histopathology slide images), this paper is an excellent example for using an established science or method in an applicable manner for a medical application. They have done it while designing, running and presenting a very well-thought-out set of experiments and evaluation metrics. I just enjoyed reading the paper! The presented results show that the the proposed CycleGAN achieves better performance than the common solutions, while boosting the performance of the segmentation network. \n', 'cons': ""1. It's a known fact that in the medical field the lack of (training) data is a major limitation to evaluate new methods. However, in this work, the small number of centers (only two) and the fact that the method was only trained to transform 1=>2 and not vice versa is a big drawback. I urge the authors to collect more data, from more centers, while using cross validations to evaluate their method to the most.\n\n2. The authors did not compare their results to the other CycleGANs methods they cited (Gadermayr et al., 2018; Shaban et al.,\n2018)."", 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'oral_presentation': ['Consider for oral presentation']}"		BkxJkgSlx4	rklujIbimV	MIDL.io/2019/Conference/-/Paper99/Official_Review	[]	1		['everyone']	BkxJkgSlx4	['MIDL.io/2019/Conference/Paper99/AnonReviewer3']	1548584608513		1548856739269	['MIDL.io/2019/Conference/Paper99/AnonReviewer3']
174	1548581605029	{'pros': 'A nicely written paper about material decomposition in spectral CT using a CNN (U-Net). A novel and promising application for a CNN.\nThe results are promising and show the feasibility of employing such out-of -the-box CNN to solve a complex inverse problem without many assumptions. Compared to regularized Gauss-Newton (RGN) method, the proposed method achieved better/comparable results.\n\n', 'cons': '1. The authors presented results of a phantom only. No real clinical data was used, not even for a qualitative evaluation. \n2. A major drawback (or lack of clarity?) is the fact that the training set is basically an augmented version of the test set. This questions the presented results and conclusions. \n3. Although the authors presented a projection-based method and compared it to the state of the art (RGN),  there was no comparison with the other deep-learning image-based material decomposition methods (in the image domain) that were cited by the authors (Chen and Li, 2017 and Clark et al., 2018).\n4. In all figures showing the error images, while the errors of the RGN are almost uniform (white?) noise, for the U-net there is a clear correlation between the error and the input image. This needs to be discussed thoroughly while clarifying the reasons and the implications of it. ', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		HkgoYYqJlV	Byg6kigsQV	MIDL.io/2019/Conference/-/Paper35/Official_Review	[]	1		['everyone']	HkgoYYqJlV	['MIDL.io/2019/Conference/Paper35/AnonReviewer3']	1548581605029		1548856739056	['MIDL.io/2019/Conference/Paper35/AnonReviewer3']
175	1548578295576	"{'pros': 'This paper proposes a deep-learning-based approach for dynamic pacemaker artifact removal. In the context of deep learning, the proposed method is somewhat novel. In point of view of medical imaging, the paper is very interesting and valuable.  \n \n', 'cons': '--The paper is not easy to follow.  \n\n--The method is incremental and results are very limited.\n\n-- There are several hyper-parameters in your method such as patch size and etc. Discussion and more analysis of these parameters are needed.', 'rating': '3: accept', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		rkx5InjA1N	HyxeWCyiQN	MIDL.io/2019/Conference/-/Paper20/Official_Review	[]	2		['everyone']	rkx5InjA1N	['MIDL.io/2019/Conference/Paper20/AnonReviewer2']	1548578295576		1548856738836	['MIDL.io/2019/Conference/Paper20/AnonReviewer2']
176	1548561755450	{'pros': 'The paper applies model quantization technique to the U-Net architecture. Model quantization is a method that represents weights and activations with a lower bit resolution when compared to their high precision floating point counterparts, thus reducing memory cost while keeping a comparable accuracy compared to a full precision model. \n\nMemory quantization is not a new concept, and has been introduced to the biomedical imaging community (Xu et al., 2018b). Considering this, I am not sure if the paper brings enough contribution. Nevertheless, the U-Net architecture is one of the most popular models in medical imaging so reporting quantization results on this network could be of practical use. The quantization method was also adjusted to work for the U-Net architecture. \n\nThe paper is well written and has a good summary of quantization methods in the context of neural networks. \n\nExperimental results on data from Spinal Cord Gray Matter Segmentation Challenge show good performance compared to full precision model.\n', 'cons': 'Lack of novelty. Quantization methods have been previously used in medical images. \n\nExperiments are done with only one dataset. Does the parameters of the proposed quantization method depend on the data ? How is the fixed point precision chosen for a different dataset such that a comparable accuracy with full precision is maintained ? The reason for the choice of fixed point precision has to be explained. \n\nMinor\n- Need to explain the meaning of “pJ”, “nJ” and “mJ” in the Table 2 and Table 3.\n\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		rketmBuel4	BJgQvpo5QE	MIDL.io/2019/Conference/-/Paper118/Official_Review	[]	2		['everyone']	rketmBuel4	['MIDL.io/2019/Conference/Paper118/AnonReviewer3']	1548561755450		1548856738625	['MIDL.io/2019/Conference/Paper118/AnonReviewer3']
177	1548549076174	{'pros': 'The paper is technically sound and propose an interesting approach to fuse two otherwise separate steps --localization and classification/regression -- that are necessary of knee OA severity assessment. \n\nAlthough the paper is too long, and it could have been shortened in some parts and sections, the authors included a lot of material to better position their findings (appendix etc). The structure of the paper, and especially the state of the art analysis is very good. \n\nThe approach has some novelty to it. It uses multiple losses that are combined with weights chosen manually according to the rationale that deeper layers learn faster and overfit more. Using attention is not per-se new and the authors position their paper nicely with respect to previous approaches, but attention modules in this context have the potential of simplifying training and inference as well as improve results. \n\nThe results that are presented are well related with state of the art results and are convincing. Some comparisons with other approaches have in fact been shown and crucial aspects of the algorithms and method explored.\n\nThe result section is well structured and I have liked that the authors showed the performances of att0 att1 and att2 separately to give an idea of the behavior of these predictions heads. The results seem interesting and I agree with the authors when they say that this research brings a valuable contribution to the community. ', 'cons': 'The paper seems not to respect the conference format which dictates a maximum number of pages that is smaller than the number of pages of this submission.\n\nThe method of Tiulpin et al 2018 which uses a siamese network could have been explained better, what does it mean that they use symmetry of x-ray knee images. \n\nThere are repetitions and long sentences that take up a lot of space without conveying anything strictly useful. Both introduction and experimental sections can be shortened without changing much of the meaning.\n\nThe network architecture is not very clear. I would like to see a schematic representation of the network which in this moment seems to have prediction branches due to the presence of attention mechanism at different points in the network. A schematic example of this would clarify much of what actually happens in this method. Figure 2 clarifies something but it would be nice to see where are the prediction layers placed together with respective losses. In other words, the authors need to structure their method section, prioritize things they want to explain, give a panoramic view on their approach and then zoom in to the details about how they define their losses etc.\n\nFigure 1 is confusing because the image gets rotated and N (number of channel) shifts place with another axis. Would be better to keep it consistent.  \n\nThe results are not state of the art, although the method is much more difficult to implement and train due to the presence of the early fusion or multi losses (that require manually picked weights). ', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		B1epyN8rlV	HyghCj_qmE	MIDL.io/2019/Conference/-/Paper157/Official_Review	[]	1		['everyone']	B1epyN8rlV	['MIDL.io/2019/Conference/Paper157/AnonReviewer1']	1548549076174		1548856738410	['MIDL.io/2019/Conference/Paper157/AnonReviewer1']
178	1548545393313	"{'pros': 'The paper presents an approach to aid interpretation of pathology images coming from confocal microscopes (CM images). The clinical value of CM images has been highlighted in previous work, but although effective towards the goal of detecting the presence of cancer, these images are hard to interpret by humans. The authors propose to use a cycle-GAN to shift the distribution of CM images towards more standard H&E images which are easier to interpret. They present an architecture making use of two network, a de-noise/de-speckle network (trained independently on one of the two types of CM images used in this work) followed by a generative network (cycle gan). \n\nThe general organization of the paper is sound\n\nThis paper tackles a problem that is relevant to the whole medical community. It has the potential to improve pathology and cancer diagnosis by making it simpler and quicker\n\nThe results of this work look visually convincing. Both the de-speckle network and the GAN appear to deliver very good results, at least at first glance. The quantitative results delivered by the de-speckling images, which seem to be computed using simulated realization of random speckle noise, look also convincing.\n\nI agree with the authors statement in the end of the paper where they say they could train both GAN and de-speckle network end to end. I think this joint training might result in even better outcomes. \n\nThe study has potential and could have interesting applications in clinical settings.', 'cons': 'One issue, from a purely organizational standpoint, is the fact that information about previous work is either omitted or scattered around the text. I understand that the available space is limited and therefore it\'s difficult to bring in the paper all the information that would be necessary, but the introduction should be extended to include previous work both in terms of DL and medical research. \n\nThis paper still represent a niche application of a more general DL technique that has been already used for a large number of similar applications. The contribution is therefore incremental, building on top of well-known techniques. \n\nAfter the publication at MICCAI 2019 of the work ""Distribution Matching Losses Can Hallucinate Features in Medical Image Translation"" and similar other works, it has started becoming apparent that the simple visual similarity between samples generated by a GAN and true samples from a specific distribution doesn\'t ensure that diagnostic value is kept. \n\nThis doesn\'t mean that cycle-GAN type of techniques are not suited for medical imaging since they might wipe out their diagnostic value, but it means that every study around this topic needs to prove that the diagnostic value is indeed kept! Unfortunately the authors didn\'t report indications in this sense in their paper. \n\nThe main contribution of the paper is scarcely justified by the statement ""...they confirmed that the images were similar to those in routine"". I feel it would have been extremely interesting to evaluate the performance of those same clinicians (and others) diagnosing cancer using both H&E stained image and CM images of the same patient (or patient distributions) vs a control group. A lengthy study, I agree, but a necessity in light of other recent works highlighting how dangerous is to use GANs for this kind of tasks. \n\nThe choice de-speckle network architecture is somewhat not sound, with the multiplicative residual connection near the outputs of the network and the median filtering operation. Is there some reference for multiplicative residual connections? How do we know that the network is learning 1/F (inverse of speckle noise)? Can we prove that at least visually? Is the math right? \n\nIt is necessary to prove that the generated images retain their important diagnostic value. It is necessary to run a study to confirm that in a similar way that CM images were confirmed having diagnostic value and could therefore be used instead of H&E stained images.', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		rJxj3vUel4	r1lF_av5QE	MIDL.io/2019/Conference/-/Paper109/Official_Review	[]	3		['everyone']	rJxj3vUel4	['MIDL.io/2019/Conference/Paper109/AnonReviewer1']	1548545393313		1548856738156	['MIDL.io/2019/Conference/Paper109/AnonReviewer1']
179	1548235247550	"{'pros': '-\tThis is a very well-written paper that contains many details that would be very useful for reproduction of the results.\n\n', 'cons': ""-\tCutting the U-Net halfway leads to two separate networks with different architectures. Although it looks aesthetically interesting in Fig. 1, would it not be better to just use the same architecture for both tasks? \no\tThe first network -- the registration network -- is a regression network that has two input channels and predicts a three-channel image. It only has a contracting path and is thus similar to the Fully Convolutional Network (Long et al., CVPR 2015). This is in itself not very novel, and the registration part of the paper is very similar to previous work by the same authors (https://doi.org/10.1109/TMI.2018.2878316).\no\tThe second network -- the transformation network -- is a classification network that has four input channels (x, y, z + segmentation) and predicts a one-channel segmentation image. None of this is directly based on image information.\no\tThe input of the first network is only indirectly connected to the second network. This kind of defeats the purpose of a U-Net, as skip connections are cut and all image feature maps obtained at different resolution levels are thrown away. Have you tried concatenating these feature maps to the input of the transformation network? \n-       The most important finding in the paper seems to be  that a CNN (the transformation network) can be trained to apply a transformation on an image given a deformation field (guided by a Dice loss). This is an interesting insight, but using a neural network for this seems overly complex. What do results look like when the obtained deformation field is directly applied to the moving image segmentation?\n-       I'm not convinced that the comparison to a 'standard' segmentation network is entirely fair. In the propagation approach, the network gets two images (fixed and moving) of the same patient + a reference segmentation (moving) as input to predict the fixed segmentation. In the segmentation approach, the network only gets one fixed image as input. For a fair comparison, the 'standard' segmentation network should also be evaluated with the same amount of input information, i.e. three input channels. I can imagine that the moving image segmentation provides valuable information.\n-\tIt would be interesting to see an example of a fixed and moving image and the aligned images based on the predicted deformations, to assess the quality of the registrations.\n-       Does the segmentation task also have an effect on the quality of the registrations?\n-\tThere is no reference to Fig. 3 in the text.\n-\tMinor comments\no\tSmall grammatical errors in the second paragraph of Sec. 3.1.\no\tMissing ‘128’ in caption of Fig. 1.\n"", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		B1evn4blx4	Hyx_lz2BXV	MIDL.io/2019/Conference/-/Paper62/Official_Review	[]	1		['everyone']	B1evn4blx4	['MIDL.io/2019/Conference/Paper62/AnonReviewer2']	1548235247550		1548856737893	['MIDL.io/2019/Conference/Paper62/AnonReviewer2']
180	1548062340666	"{'pros': '-\tThe authors compare RNN approaches with iterative CNN approaches, leading to the insight that for this task iterative application of a CNN performs much better than an RNN.\n-\tA synthetic vessel data set is generated to develop the method. This is a potentially useful contribution for development of such methods, but should also be put into context with similar existing works (e.g. https://doi.org/10.1016/j.compmedimag.2010.06.002). \n', 'cons': ""-\tI don't think the proposed method is an ‘image-to-tree’ method. It actually performs an iterative segmentation of the voxels that make up the vessel centerlines in the image, i.e. deep learning-based region growing. The result is a segmentation mask which is in principle simlar to a thinned version of a segmentation of the retinal vessels. A similar result might be obtained by first obtaining a binary retinal vessel segmentation (for which many DL methods have been proposed, e.g. https://doi.org/10.1007/978-3-319-46723-8_17, https://doi.org/10.1109/TMI.2016.2546227) and then applying a conventional morphological thinning operation.  The obtained segmentation does not contain information about the topology of the vessels (e.g. separation of veins and arteries, branching points, individual segments) that would facilitate more advanced tree analysis (e.g. https://doi.org/10.1109/TPAMI.2012.265). \n-\tI don’t agree with the authors that this is an ‘end-to-end’ method. The best performing method is found to be an approach in which a CNN iteratively provides a prediction of the most likely prediction to a tracker. This is not end-to-end, as the CNN is used many times to provide a prediction.  \n-\tReferences to related work are missing. Vessel segmentation/tracking has a long history, see e.g. the review by Lesage et al. (https://doi.org/10.1016/j.media.2009.07.011), multi-orientation tracking by Friman et al. (https://doi.org/10.1016/j.media.2009.12.003), work by Bekkers et al. (https://doi.org/10.1007/s10851-013-0488-6). DL methods for vessel tracking include simultaneous orientation classification and radius prediction (Wolterink et al. https://doi.org/10.1016/j.media.2018.10.005) and LSTM-based methods (Poulin et al. https://doi.org/10.1007/978-3-319-66182-7_62). \n-\tThe method is evaluated on the DRIVE data set, which is an old data set consisting of relatively small and old fundus images. It would be interesting to see how the method fares on larger images such as the High-Resolution Fundus (HRF) image data set or the REVIEW data set. In addition, as many vessels are visualized with 3D imaging it would be good to evaluate the method on a 3D data set, e.g. http://coronary.bigr.nl/centerlines/ or http://image.diku.dk/exact/). \n-\tIt is unclear how the method deals with vessels running in parallel. Based on Fig. 4 and the description in the text, the method would be trained to jump from one vessel to the other. This would be highly undesirable when differentiating between e.g. arteries and veins. In fact, the example results in Fig. 10 show a lot of cyclic structures, which indicates that the tracker connects arteries and veins.\n-\tAdditional constructive feedback:\no\tFigs. 3 and 6 are a bit unconventional. It would have been nicer to show precision and recall in one plot (as you actually do in Fig. 3B) and use isolines to indicate Dice/F1 scores in those images.\no\tThe deep convolutional network (DCN) is not described anywhere.\no\tThere is no description or discussion of the results shown in Fig. 3, while these may actually be the main insight of the paper."", 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		B1geHYXgx4	ryg6F0-7QE	MIDL.io/2019/Conference/-/Paper88/Official_Review	[]	2		['everyone']	B1geHYXgx4	['MIDL.io/2019/Conference/Paper88/AnonReviewer1']	1548062340666		1548856737679	['MIDL.io/2019/Conference/Paper88/AnonReviewer1']
181	1548537201084	"{'pros': 'Overall, the idea is clearly presented. The problem of irregularity detection by unsupervised methods is highly relevant. If one could come up with a method that does not require any annotation, but solely extracts normal healthy tissue (patches) from a database of e.g. mammographic images, thus allowing to discriminate normal image patches from benign/malignant ones through the distance to a generative model of the normal ones, such a method would have a huge impact (in case it works at high discrimination accuracy). \nIt is potentially possible that the proposed method could work if trained on a one to two orders of magnitude larger dataset of healthy tissue image patches, but this is hard to achieve and not discussed in the present work. ', 'cons': 'I have a number of concerns regarding this work:\n\n- Most importantly, I think the proposed GAN is not able to distinguish normal from benign or malignant (i.e. potentially cancerous) tissue, as the authors propose. While the definition of an anomaly from the clinical point of view demands that the respective image patch contains benign or malignant tissue, what the authors define as an anomaly (according to Figure 1) is those patches, that the autoencoder R is not able to reconstruct appropriately. I would argue, that the latter distribution of image patches is a lot larger than the distribution of image patches containing benign or malignant structures (a confirmation for this proposition can be found in Figure 4). Essentially, the authors intend to teach the discriminator what are cancerous image patches, but during training they solely provide image patches labelled as fake for the discriminator, which have a large reconstruction error. In my opinion, the discriminator can only learn to distinguish normal patches, from patches that R cannot reconstruct. Although these may be called irregularities, this is not the same as cancerous tissues (as the title and all the motivation presumes). \n\n- From the description and experiments, I do not find any hints if the authors have actually explored the empirical distribution of normal healthy tissue image patches, which they learned during the training of R. Since the generative model of healthy tissue is the crucial component for the irregularity detection score, I would expect that this learned distribution is deeply investigated, e.g. by randomly modifying the embedding values generated in the bottleneck layer of R, or by interpolating between the embeddings of different image patches. GANs need huge amounts of training data, while the datasets used in here come from a subset of 322 images of the MIAS database, which is very limited in terms of size. I do not expect that from such a small number of images, the sought for distribution can be learned with GANs.\n\n- Going one step further on the problem of limited training data for learning the generative model, the recent literature has given a lot of reason to criticize GANs of actually not really providing a meaningful distribution. See e.g. Arora et al., ICLR 2017 and Arora et al., ICLR 2018, but also other recent works at NIPS 2018 have shown theoretical doubts about the use of GANs for modelling image distributions or assessing out of distribution samples, even in the case of large datasets. The mode collapse problem, which is especially problematic for the traditional GAN formulation, is an indication for that. However, the authors do not discuss these doubts at all, or how they probably may be overcome in their proposed work due to the autoencoder step. Additionally, according to our own experiments in our group, the use of more recent Wasserstein GANs has proven much more stable in terms of generated distributions and the mode collapse problem, compared with the traditional GAN formulation. So it is not clear, why the authors pursue the traditional GAN formulation, which is not considered as state-of-the-art anymore.\n\n- I think the experimental evaluation, despite going into the effort of comparing with other methods, does not give convincing results regarding the performance of the method. First, the results in Table 1 on the MIAS dataset are clinically questionable. While the fully supervised methods give a reasonable performance in terms of AUC and F1-score (up to 0.91 AUC), all the unsupervised methods perform pretty poorly on the task at hand. From the practical point of view, I would not consider an AUC of 0.76 or F1 of 0.71 as a performance that is good enough to be used clinically, even if there is a lot less annotation effort involved with the method. So although the proposed approach is slightly better than the compared unsupervised ones, the absolute performance of all unsupervised methods is clinically not relevant. Given these results, I would invest in improving the supervised methods further, on the same annotated data, and forget about the unsupervised ones. Furthermore, the results shown in Table 2 are also far from acceptable, both for the supervised and unsupervised variant, since with an AUC (or 1-AUC) close to 0.5, these results for binary classification are essentially the same as by random guessing. \n\n- My criticism regarding methodology and evaluation results are also confirmed in Figure 4. From the heat maps, it is not clear what the thresholds are to perform detection of irregular structures. There seem to be a lot of false positive regions in very clearly healthy tissue regions, which makes the method\'s performance questionable (note that this problem is in line with quantitative results mentioned above). It seems that the generative model is actually not picking up the distribution of healthy tissue as it is supposed to do. Another issue regarding Figure 4 is the question, why the authors did not compute a  quantitative evaluation in terms of AUC/F1 against the ground truth segmentation (see third row of Fig. 4) available for this data?  \n\nOverall, I do not agree with the statement in the discussion that the proposed method obtains ""acceptable results"" despite being unsupervised. I think the supervised state-of-the-art methods (I am certain there are tailored supervised methods that perform even better than plain AlexNet or VGG-Net on MIAS or CBIS-DDSM - these should also be mentioned), deliver far better results than the unsupervised ones shown here. If this performance drop is the price to pay for getting no annotation effort, I think a clinician will not be willing to pay it. \n\nMinor comments:\n\n- the manuscript is not easy to read, since there are quite a number of grammar mistakes and typos still contained in it\n- I would not agree that it is relevant to say in the discussion that ""accuracy scores of all unsupervised methods are significantly better than chance (i.e. p-value <0.01 in Fisher\'s exact test (Fisher, 1935))"", since presumably the p-value is low due to the large N, and of course it is the distance to the chance classifier that is relevant in the end. regarding statistical significance, it would be more interesting to ask if there are significant differences between the performances of the different unsupervised methods in Table 1\n- in the introduction, the authors claim two contributions (1) and (2), which in my opinion are very similar\n- in 2.4 the authors state that their R+M training is similar to GANs from Goodfellow et al., 2014. Why is it not the same? What is the difference?\n- in the description of the MIAS dataset, it is not clear how many of the 322 provided images are normal, and how many contain a breast containing benign/malignant tissue', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		Ske2oyiye4	SJgt_6H9XV	MIDL.io/2019/Conference/-/Paper38/Official_Review	[]	2		['everyone']	Ske2oyiye4	['MIDL.io/2019/Conference/Paper38/AnonReviewer3']	1548537201084		1548856737422	['MIDL.io/2019/Conference/Paper38/AnonReviewer3']
182	1548535359229	"{'pros': '- The problem addressed in this paper is well motivated and typically found in realistic scenarios. ', 'cons': ""- There are a lot of inconsistencies in the paper, both in the methodology and in the evaluation. For example, the author says in section 2.2 that various losses have been proposed, while this paper uses the negative log version of dice. Nevertheless, in the results section all the models employ cross-entropy. \n- A major concern I have with this paper is the evaluation. First, I feel the evaluation is insufficient. Second, reported results do not correlate with claims of the paper. And third, some choices are not  motivated. Find below my comments concerning the evaluation:\n- 1. Only 5 different losses are used, while the combination of them gives more configurations. For example, loss_xent_pos, or loss_xent_or alone. Without evaluating those, one cannot see where the difference in performance is coming from. \n- 2. 'Specialists models are only trained for approximately 9k steps to ensure fair comparison'. Why? Author should let the model train until convergence, as in the proposed models. Seen from other perspective, 'specialist' models can be trained until convergence (let's say 15k) and proposed models until 45k. That also would be fair according to the reasoning here. Related to this author also mentions that training his models is more efficient than training 3 'specialist' models. To me, this does not make sense at all (at least this was not demonstrated in the results)\n- 3. Author claim that in Fig 2 it is shown that models trained with their models (mode_or or mode_plus) achieves better results than with the baseline (mode_neg). Nevertheless, one cannot distinguish if this improvement comes from the proposed versions or from the combination of cross-entropy + dice loss functions. \n- 4. Author only includes the 'specialist' versions trained with loss_xent_neg + loss_dice. Its counterpart from the proposed models (loss_xent_neg + loss_dice) does not perform better than the 'specialist' CNN in liver and spleen segmentation tasks, showing that training a dedicated CNN in full annotated images typically outperforms the proposed method. \n- 5. Author also removed results for cancerous tissues, alleging that results were not good. I believe it should be interesting to see those results with both his models and 'specialist' CNNs to see whether this approach only works for easy tasks.\n- Does w_c have any impact on the final result? w_c is set to 1 in all the classes except to the background. An ablation study on the different values of w_c would be interesting, specially for different values for different classes.  \n- What is for you p_{i,j}^c in equation 1? I guess it is the softmax and not predictions_{i,j}^c.\n- As images are processed in 2D, in the sum of equations should be pixels instead of voxels.\n- As an aside note, I am surprised by the low performance of the baseline network, specially when segmenting the spleen and pancreas. Even though I understand the goal of this paper is not achieve state-of-the-art results but to demonstrate the proposed training strategy I feel these results are pretty low. "", 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		S1gXSzNexN	BJxDHUrqQN	MIDL.io/2019/Conference/-/Paper93/Official_Review	[]	3		['everyone']	S1gXSzNexN	['MIDL.io/2019/Conference/Paper93/AnonReviewer2']	1548535359229		1548856737163	['MIDL.io/2019/Conference/Paper93/AnonReviewer2']
183	1548534065341	{'pros': '- definition of a novel CNN approach on the fODF, by applying convolutions to data that lives on SO(3).', 'cons': '- geometric distortions in diffusion data are significantly larger than in traditional T1w and T2w data. I am not aware of any studies that would acquire diffusion data only and then employ that data for structural volume analysis. Thus, the need for tissue segmentation from dMRI is significantly lower and really only necessary for the purpose of masking/seeding in diffusion analyses. \n- diffusion MRI Segmentations are compared to FSL-Fast segmentations on structural MRI. That is NOT a reference segmentation as FSL-Fast can fail, be significantly imperfect at given parts in the images, etc. References should be of manual or semi-manual order. Evaluation data is small (16 subjects)\n- Since fODF segmentations are compared to structural MRI segmentation (where the latter is used as segmentation), the obvious question is why not solve this on the structural MRI\n\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		BJgm2DkJgN	BJxFNWBc7E	MIDL.io/2019/Conference/-/Paper27/Official_Review	[]	3		['everyone']	BJgm2DkJgN	['MIDL.io/2019/Conference/Paper27/AnonReviewer2']	1548534065341		1548856736905	['MIDL.io/2019/Conference/Paper27/AnonReviewer2']
184	1548532961206	"{'pros': 'Nice novel fCNN DenseNet strategy to improve segmentations in data with heterogenous appearance and features, such as in infant MRI data at iso-intense stage (images that are particularly difficult to segment). The strategy employs a novel multi-label, multi class classification layer, a novel similarity loss function (that allwos balancuing of precision vs recall) and patch prediction fusion This reduces complexity and increases speed. New methods achieves highest performance in challenge dataset, with surprisingly high dice accuracy and slower surface distances.', 'cons': ""Novelty is somewhat iterative. \nWhile computational efficiency/speed is improved, that's not really an issue for this type of segmentation "", 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'oral_presentation': ['Consider for oral presentation']}"		Byg-krBi14	BkgKkaE5mE	MIDL.io/2019/Conference/-/Paper4/Official_Review	[]	2		['everyone']	Byg-krBi14	['MIDL.io/2019/Conference/Paper4/AnonReviewer3']	1548532961206		1548856736687	['MIDL.io/2019/Conference/Paper4/AnonReviewer3']
185	1548531375119	"{'pros': 'The paper addresses an important problem in medical image analysis when compared with natural image segmentation : the lack of training data. To address this issue often data augmentation is used (ex. UNet is using traditional elastic transformations for data augmentation)\n\nThe paper proposes a new augmentation method for the endoscopy images taken during minimally invasive surgery. Rather than doing the usual geometric transformation of the training images, the proposed method synthesizes the new labeled images by collating parts representing organs or instruments from other images on the background given by one training image.\n\nThe approach is interesting but, theoretically, there is little contribution. Results are good showing detection improvement as compared with a traditional augmentation method based on geometric transformations. Two deep architectures are tested. Nevertheless, I found the practical importance this method not well supported by the experiments and the scope (target type of images) rather narrow (see below). \n\n', 'cons': 'While the method shows good improvement for the type of images it is tested, the scope of this method is rather narrow. Are there any other type of images where this particular data augmentation is useful ? The experiments are rather weak and unclear, and therefore the method is not convincing. \n\nThe presentation and especially the evaluation not very clear. There are also some typing errors have to be corrected.\n\n* In 5. Performance Evaluation, more details have to be provided such as image size, classes, image numbers of training and testing before and after  augmentation.\n\n* In 5.1 Evaluation Results, ""The column labeled as SDA represents the mIoU when the models are trained by either random mirror, random distortion, and  random rotation."" What does this sentence mean? Does that mean the original dataset is augmented by using each of the listed methods above respectively? If  so, did the authors try different combinations of augmentation techniques, because people usually use a set of augmentation tricks such as flipping, rotation and contrast tuning at the same time to get augmented dataset rather than using only one of them. By the way, regardless of the space problem, it\'s better to put all the results from datasets augmented by different methods into table 2. Otherwise, the way you present results make experiments not convincible.\n\n* ""Specifically, the enhancement of SDA over DeepLab is much significant than that over PSPNet. This is because that the DeepLab algorithm uses all of the  three methods and the PSPNet only uses one of the three methods."" Why use different settings for DeepLab and PSPNet? I think the settings should be the same? \n\n* ""Result of this class specific augmented collages is shown in table 4, which suggests that this naive combination is not suited for boosting network performance."" The detailed configurations of each classes have to be described.\n\nMinor:\n\n*  Figure 1 has to be polished. For example, straighten the arrow lines. There are several zigzag arrow lines which are ugly and unnecessarily been like that, such as arrow line between Training Annotations and Loss Calculation, one between Data Augmentation and Augmented Data and another one between Augmented Data and Classifier.\n\n* Figure 2.: the caption of figure 2 is too simple and the author should move the description of figure 2 to its caption. In addition, what does the color (red, green yellow, black and gray) mean in images (d),(e) and (f) has to be described\n\nTypos: \n\n* The first paragraph of the 1. Introduction, mostlyed->mostly\n* The first sentence of 2. Background and Related Work: a process of recognizing \n* ""Specifically, the enhancement of SDA over DeepLab is much significant thatn that ov\n\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		HJl-FXD4eN	BylD3LV9QN	MIDL.io/2019/Conference/-/Paper146/Official_Review	[]	2		['everyone']	HJl-FXD4eN	['MIDL.io/2019/Conference/Paper146/AnonReviewer3']	1548531375119		1548856736475	['MIDL.io/2019/Conference/Paper146/AnonReviewer3']
186	1548529763485	{'pros': 'The work presents the use of label based copy-paste between images as data augmentation in a robotic surgery semantic segmentation problem. The presented methodology is simple and hence very easy to implement.', 'cons': 'The proposed methodology is not really novel, it involves pasting objects of interest into different backgrounds as a means of data augmentation. The network architecture is basically ResNet101. The way the method is validated is too simplistic, for example the authors only compare to three other types of augmentation: horizontal mirroring, scaling and elastic distortion (from manuscript, see Simard et al. 2003). There is a serious problem with using elastic distortion on rigid objects (such as surgical instruments) as they will introduce unrealistic variations of the data, this could be overcome by constraining distortions to only happen outside rigid objects, this however was not mentioned. There are still many other forms of very simple data augmentation techniques that could be used, such as rotation or color jittering. On the selected data, the authors use 12/16 available sequences from a challenge while not offering any explanation as to why 4 sequences where omitted. The 12 sequences are split into 9 training and 3 for testing in a single experiment, it would have been far better if there would have been some sort of cross validation. The manuscript is very poorly written, with far to many typos to cite here.\n\n', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		HJl-FXD4eN	BkgoDlVcQV	MIDL.io/2019/Conference/-/Paper146/Official_Review	[]	1		['everyone']	HJl-FXD4eN	['MIDL.io/2019/Conference/Paper146/AnonReviewer1']	1548529763485		1548856736222	['MIDL.io/2019/Conference/Paper146/AnonReviewer1']
187	1548529187354	"{'pros': 'This work proposes a reinforcement learning (RL) strategy for tracking elongated structures, specifically neural axon structures from two photon microscopy 2D images. The paper is well structured and easy to follow. Up to my knowledge, the specific idea of using reinforcement learning to track elongated structures given a seed point is original, and the authors propose an implementation using a state-of-the-art RL technique utilizing deep CNNs for the path predictions of the actor and the prediction of the expected risk in the value function. As such it follows recent successful ideas proposed by Mnih et al., 2015 (please note that reference Mnih et al misses the publication year), and adapts them to the tracking task. \n\nA strength of this work is that by using RL, the authors are able to train their tracking algorithm from an entirely synthetic dataset, and show that this trained agent is in principle able to solve the tracking task on their real-word two-photon microscopy image segmentation/tracking task, which depicts the axons of a mouse somatosensory cortex. Another strength of this work is the clear description of their algorithm (pseudocode), which makes it very likely for others to reproduce their results, as well as the fact that the authors promise to release their training data and code to the public in case of manuscript acceptance. ', 'cons': 'In my opinion, there are a number of stronger issues, mainly regarding the experimental evaluation, that diminish the scientific value of this work:\n\n- While the overall idea of the manuscript looks interesting, the evaluation is very much limited regarding choice of dataset and general applicability of the proposed original concept. The introduction is written in a spirit that claims the proposed method being able to (quote) ""alleviate the need for hand-engineered trackers for different biomedical image datasets"". This is not shown in the experiments, since a single real-world dataset containing thin, elongated structures is used, after training from a synthetic dataset that looks very similar to what kind of information is expected to be visible in the real-world dataset. To evaluate the claim, that the proposed method is generic, and kind of a meta strategy for learning how to track thin, elongated structures, performance on different, additional datasets would need to be shown. As it is presented in the manuscript, unfortunately only a very limited experimental evaluation is given, from which I do not get the impression that a novel concept has been developed, but solely that a very specific problem has been solved with a method, which may be overly complex for the task at hand.\n\n- Reinforcement learning has been used for medical image analysis tasks, e.g. the work of Ghesu et al., PAMI 2017 and Ghesu et al., MIA 2018, where actually the goal of localizing landmarks by letting an agent follow appearance information in volumetric CT data until a multitude of different landmark locations is reached, reminds me a lot of the work proposed in this manuscript. While I see room for further exploration of these RL based concepts in medical image analysis literature, it is necessary to mention these approaches in the related work section and to discuss commonalities and differences. \n\n- In the introduction, the authors argue about tracking vs. segmentation to justify their tracking approach formulated in the RL framework. I do not fully agree with their arguments. I think by solving the segmentation problem robustly, tracking starting from a seed point - thus deriving e.g. structural information on the geometry of relevant data - would be trivial. Therefore, I think the authors miss the comparison with pure segmentation strategies for solving the task that they show in their evaluation. In the recent years, there was a lot of work in enhancement of vascular structures using deep learning based methods, both in 2D (optical retinopathy) as well as 3D. For example DRIU (MICCAI 2016), but also other works following up on that have provided state-of-the-art benchmarks for segmentation, which are able to overcome missing structures. This body of work is totally ignored by the authors. I would not consider the Vaa3D algorithm as a fair, state-of-the-art comparison to show the benefits of the proposed method. In addition, the proposed method is not able to outperform the Vaa3D method on this dataset, so the question has to be asked, what are the practical implications of the propsed method? (As stated above, there are no other use cases demonstrated, to show more generic applicability.)\n\n- The authors argue about subpixel accuracy, however, this is misleading. In my opinion, using the continuous outputs of the predicted actor displacement locations, which are modelled by continous distributions, they solely are able to operate in a subpixel environment. However, from their experimental evaluation, where coverage is defined in a three pixel radius and mean errors are slightly below 2 pixels for their method, and most importantly, the segmentation ground truth is defined on the pixel grid, I would not consider the outcome of their algorithm as having subpixel accuracy. \n\n- Another criticism of the evaluation is the fact that the authors state that they can finetune their method on ""a very small amount of labelled data"" to improve their performance, compared with solely training from synthetic data. However, in their experiment they fine-tune on three quarters (15 out of 20) of the available labelled datasets. Therefore, I would consider this conclusion as incorrect.\n\nMinor issues:\n\n- I think repeating the six numerical values in Table 1 is redundant, since those are already stated in the text.\n- From the results of the methods and their discussion in the paper, it is not clear what a relevant error would be for the downstream tasks regarding the two-photon microscopy image dataset. Is Vaa3D already there, or is a higher accuracy still needed?\n- In Fig. 1 and its explaining texts, it is not clear what are the start and manually labelled end points of the trackers, and what the different colors in the middle subfigure mean. \n- In 3.1, I do not fully understand the details of the - very important - reward function, giving the negative of the base reward if an action means there is a 90 degree or greater change seems to be a heuristic, please explain that in more detail.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		HJxrNvv0JN	H1eomCX57E	MIDL.io/2019/Conference/-/Paper13/Official_Review	[]	2		['everyone']	HJxrNvv0JN	['MIDL.io/2019/Conference/Paper13/AnonReviewer2']	1548529187354		1548856736001	['MIDL.io/2019/Conference/Paper13/AnonReviewer2']
188	1548528661544	"{'pros': 'The paper is tackling an interesting problem and I also share the belief that imaging cell variations holds potential to learn representations which are predictive of function.\n\nThe motivation of this work is to have a method which is able to visually represent cells with high fidelity while also having a latent representation which captures information regarding impact of being treated with a compound.\n\nIn section 4.3.2 the paper discusses studying the difference in reconstructions by the AE vs VAE. This is very interesting. These differences should be studied more ask they could provide insight into what is different about the models and what image features are captured given what the models are designed to capture.\n', 'cons': 'The method does not perform better given the literature on learning unsupervised representations that are predictive of the compounds used. The main baseline in this paper is ""the best reported GAN model"" and not the non-GAN methods by Singh and Ando which are SOTA for this task. This is potentially misleading.\n\nGiven the motivation it is not clear why a new method VAE+ is proposed without sufficient evaluation of existing work. The most needed baseline is ALI (https://arxiv.org/abs/1606.00704) which uses an adversarial loss to learn a latent space with a gaussian prior. Also, InfoGAN (https://arxiv.org/abs/1606.03657) is another baseline to try and report results on.\n\nAlso, the results from the previous GAN method are not compared in table 1. It is important to put these numbers side by side given the same evaluation.\n\nAlso, the evaluation does not report variance. The evaluation should include a randomized train/valid/test split selection together with random model initializations.  Given the evaluation now there is no guarantee that the VAE+ model improvements are significantly better. There is no way to compute a p-value.\n\nIf the VAE+ model offers a significant improvement a better venue is ICLR/NeurIPS/ICML with evaluations on multiple datasets to confirm that the method works. \n\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		HyxX96_xeN	B1g0M37cm4	MIDL.io/2019/Conference/-/Paper121/Official_Review	[]	2		['everyone']	HyxX96_xeN	['MIDL.io/2019/Conference/Paper121/AnonReviewer2']	1548528661544		1548856735746	['MIDL.io/2019/Conference/Paper121/AnonReviewer2']
189	1548528061795	{'pros': 'The paper addresses the problem of brain tumour segmentation from an unsupervised viewpoint which is a very useful approach in medical imaging where data annotation is expansive. The method segments tumours as outliers  from a learned representation of healthy images. The tumour detection is done by solving a MAP problem, where the prior distribution of data was approximated using Gaussian Mixture Variational Autoencoder (GMVAE). The data consistency term is optimized by using Total Variation norm. \n\nThe paper is well written and clear. A nice summary of VAE and GMVAE is presented followed by the description of the contribution.  \n\nResults are promising. The method is compared with few other deep learning based unsupervised methods and achieves good performance. ', 'cons': 'The majority of the method was proposed by an earlier paper from the same group [Tezcan et al. 2017]. This paper applied that method to the new context of brain tumor lesion segmentation with small modifications due to different task. \nThis group also had a similar paper in MIDL 2018 where they applied slightly different methods (VAE, AAE, as opposed to the GMVAE in this paper) on the same problem: https://openreview.net/forum?id=H1nGLZ2oG\nThis makes the contribution rather incremental. \n\nWhile experiments are good for comparing the method with other similar unsupervised methods, it is not shown how the method compares with state of the art on this competition data. This would help determining if this is practically a very useful approach. \n\nThe experiments also lack some details which made it hard to understand:\n- Description of DSC-AUC wasn’t clear.\n- Two of the baseline models VAE-256, VAE-128 weren’t described.\n- In the histogram equalization part, a subject was randomly chosen from CamCANT2 dataset as the reference. It was not shown in the paper whether it’s a sensitive parameter. Thus a potential issue here is that not knowing which specific subject was chosen might make it hard to reproduce the result.\n\nTypos:\n-Paper 2 top: “patients making them attractive”, ->  “patients,  making them attractive”\n-Page 8: in conclusion, line4, DCSs -> DSCs\n\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		S1xg4W-leV	rklI6F7c7V	MIDL.io/2019/Conference/-/Paper57/Official_Review	[]	2		['everyone']	S1xg4W-leV	['MIDL.io/2019/Conference/Paper57/AnonReviewer1']	1548528061795		1548856735527	['MIDL.io/2019/Conference/Paper57/AnonReviewer1']
190	1548525493308	{'pros': 'Summary\n\nThis paper proposes a new method on segmenting point cloud of intra-oral scans (IOS). This method contains three components: 1. Applying convolution neural networks (CNNs) on teeth semantic segmentation; 2. Proposing a non-uniform resampling strategy for better spatial learning; 3. Training loss combines with the auxiliary adversarial loss.\n\nThe proposed method achieves very good performance.', 'cons': '1. The idea of this paper is not novel, segmentation network is based on PointCNN and dDiscriminator network is identical to the part of Point-Net. However, the author utilizes CNNs on teeth semantic segmentation, which is important.\n\n2. The comparison with different method and settings are straightforward.  \n\n3. Applying non-uniform resampling strategy will generate different segmentation mask according to the chosen fovea point. How to get the final completed segmentation result should be specified.\n\nMinor:\n-----------\n1. Figure. 1 shows the model framework, but this figure is hard to follow, please explain it properly in the paper.\n\n2. In equation (6), the author applies an adaptive weight between segmentation loss and adversarial loss. However, the network is not shared between segmentation and discriminative network. The weight may not be necessary here. No ablation experiment is done to identify how that affects performance.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		ByxLSoblgV	r1eT2kXq7E	MIDL.io/2019/Conference/-/Paper65/Official_Review	[]	1		['everyone']	ByxLSoblgV	['MIDL.io/2019/Conference/Paper65/AnonReviewer1']	1548525493308		1548856735310	['MIDL.io/2019/Conference/Paper65/AnonReviewer1']
191	1548525320814	{'pros': 'The article proposes a novel structured prediction method for semantic edge detection in CT and X-ray images.  A fully convolutional network extracts sparse sample locations from original input images. Sparse prediction network takes patches from these sampling locations as input. This SSPNet contains a CNN path to produce edge features and a GCNN path to weight these patches. Hough voting is used to accumulate the predictions to obtain dense semantic edge map. The authors evaluate the method on two datasets against standard baselines. The results from the experiment indicate a significant improvement over the baseline methods. \n\nPros:\n1. A novel deep learning method for pixel level prediction by processing image data on sparse and irregular instead of dense grids.\n2. Few sparse samples for structure prediction reduce the memory and time limitations for edge detection tasks in medical images\n3. The experimental results evaluate the effectiveness of the work on two datasets.\n4. The proposed model has 2.5 times fewer learnable parameters than baseline(UNet-L), yet performs 1 and 1.6 percent better on two datasets.\n5. Figure 1. and Figure 2. helps to understand the article better.', 'cons': 'Minor Comments\n1. How does the Fully convolutional CNN influence the prediction of SSPNet? Can similar performance be achieved with fewer samples? Or Can other sample selection mechanism make any difference?\n2. Would increasing the training samples or doing data augmentation help the baseline methods (UNet)? \n', 'rating': '4: strong accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'special_issue': ['Special Issue Recommendation'], 'oral_presentation': ['Consider for oral presentation']}		SkxccsWxxE	B1gWGkX5mN	MIDL.io/2019/Conference/-/Paper66/Official_Review	[]	2		['everyone']	SkxccsWxxE	['MIDL.io/2019/Conference/Paper66/AnonReviewer1']	1548525320814		1548856735095	['MIDL.io/2019/Conference/Paper66/AnonReviewer1']
192	1548525110207	{'pros': 'The paper proposes a novel CNN architecture for dense segmentation in reduced dimensions with applications to OCT images. The architecture contains a series of downsampling layers with residual connections in an encoder-decoder fashion with funneling subnetwork providing a global and local context for dense segmentation. The authors illustrate the use of the proposed architecture on segmentation of geographic atrophy and retinal layers. The results from the experiments indicate a significant improvement over the baseline methods.\n\nPros:\n1. Novel CNN architecture for boundary extraction in OCT images. \n2. The method is evaluated for segmentation (GA) and regression (retinal layers) tasks.\n3. The results from the experiment indicate a 3% and 21%(Dice) performance improvement over the two baseline approaches. The method also shows a similar superior performance for the other application (GA segmentation).\n4. Figure 4. is helpful as it shows how the baseline works fail to segment retinal layers around the drusen. \n', 'cons': 'Minor comments:\n\n1. The dataset for layer segmentation application contains 115Normal and 269 AMD samples. The training set includes only 5 Normal samples vs. 159 AMD samples. What is the reason for choosing the training set with this class imbalance? \n2. The performance of the model (MSE)  training with very few Normal samples has better performance compared to AMD samples. This performance difference could be highlighted in the discussion section.\n3. It would be good if the authors could clarify the number of parameters for the proposed model and the two different baselines.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'special_issue': ['Special Issue Recommendation']}		Hkx5C9QeeN	HylCERfcXE	MIDL.io/2019/Conference/-/Paper90/Official_Review	[]	2		['everyone']	Hkx5C9QeeN	['MIDL.io/2019/Conference/Paper90/AnonReviewer1']	1548525110207		1548856734881	['MIDL.io/2019/Conference/Paper90/AnonReviewer1']
193	1548523946099	"{'pros': 'The paper presents what appears to be an interesting approach for beamforming in US imaging. The work proposes a novel way to construct ultrasound images by learning both transmission (Tx) and reception (Rx) parameters of a US imaging system, where the parameters are modelled as a two path (Tx and Rx) neural network which is trained end-to-end. The manuscript is well written and more or less well structured.', 'cons': 'Although an interesting approach, the manuscript really lacks in way of validation. The very small data set used in this work contained only 6 patients with 4-5 cine loops per patient (the total number of cine loops is not mentioned) and each sequence contains 32 frames. To validate the methodology a single sequence from a patient was left out, which to this reviewer\'s understanding means there are still 3 or 4 sequences belonging to the test subject in the training data. This is of great concern for two reasons. First, the network could be simply learning the anatomy of the patient visible in the other sequences. Second, even assuming that no sequences of the same patient where left in the training data set, a single test sequence is not nearly sufficient way of validation, and hence the methodology efficacy cannot really be concluded. \n\nOn a different note, the manuscript is missing key implementation details, e.g. the network used to simulate and train the Rx and Tx parameters is never discussed and left only as boxes in a diagram. Images that are (presumably) meant to highlight the results can be hard to interpret, e.g. some ovals are marked in the ""ground truth"" images, however they are never mentioned or pointed at in corresponding locations at the result images, which again makes the reader struggle how to best judge the proposed methodology.', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		Skxq9YqJg4	H1lMnYzqXE	MIDL.io/2019/Conference/-/Paper36/Official_Review	[]	2		['everyone']	Skxq9YqJg4	['MIDL.io/2019/Conference/Paper36/AnonReviewer1']	1548523946099		1548856734665	['MIDL.io/2019/Conference/Paper36/AnonReviewer1']
194	1548522735984	"{'pros': 'The motivation of the paper is good quality science. The paper performs a validation and comparison of 3 different approaches all trained using the NIH X-Ray dataset and evaluated on a privately collected holdout dataset. This is an important exercise to ensure reproducibility. They address the issues of detection as well as the ability to localize the cause of the prediction.\n\nThe evaluation seems robust yet has some issues discussed below. The paper performs cross validation training ensuring patients are not shared between splits. The dataset has the same data augmentation performed for each model.\n\nI think this paper has value to serve as a reference mini-survey paper. It is well written and very clear. I enjoyed reading this paper. Reproducibility is important. ', 'cons': 'The evaluation is more about which existing paper performs best and not about which approach provides the best performance. The MIL approach uses a CNN which has less capacity so it doesn\'t seem like a fair comparison. This makes it hard to accept the conclusion that the CNN performs best. Although I would believe this because the MIL approach has a smaller receptive field so larger patterns are not possible to detect but the paper should provide a more convincing argument to draw this conclusion (Although the wording is only ""we achieved"" it is the assumed conclusion of the work).\n\nThe new dataset used does not appear to be public. I think having this dataset which is isolated from the NIH dataset would be very useful in future studies. If the dataset were to be made public it would add more contribution and impact to the paper.\n\nThe take away is limited which makes it hard to enumerate what should be changed. It is missing more analysis of the difference between the model predictions. The authors state that there is differences and that an ensemble helps but discussing and showing these examples would help to understand how to improve these models.\n\nI think it is important that the paper release the source code for their unified evaluation. If this is the case I would change my review to accept. Without it I am concerned that the impact of the paper will be too low.\n\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		B1lpH4jkxN	BJguxBGcXN	MIDL.io/2019/Conference/-/Paper41/Official_Review	[]	1		['everyone']	B1lpH4jkxN	['MIDL.io/2019/Conference/Paper41/AnonReviewer3']	1548522735984		1548856734381	['MIDL.io/2019/Conference/Paper41/AnonReviewer3']
195	1548519497740	{'pros': 'This paper proposed an innovative deep learning architecture fusing supervised and unsupervised model to improve white matter lesion segmentation performance. Unsupervised Anomaly Detection was used to provides optimization targets for supervised segmentation model from unlabeled data. The experiment results showed the feasibility in a semi-supervised setting and an unsupervised setting. The motivation and methodology parts were clearly written. The paper is easy to follow.', 'cons': 'The essential details of the AE and segmentation network architecture are missing. Dataset seems too small without knowing the details of the network.\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		ryxNhZGlxV	SyeGI_b5XE	MIDL.io/2019/Conference/-/Paper71/Official_Review	[]	3		['everyone']	ryxNhZGlxV	['MIDL.io/2019/Conference/Paper71/AnonReviewer3']	1548519497740		1548856734165	['MIDL.io/2019/Conference/Paper71/AnonReviewer3']
196	1548517229816	"{'pros': 'This paper presents a deep network that could screen large set of WSIs of sentinel lymph nodes by segmenting out the areas with possible lesions. It is hypothesized that such network can even help to correct and adapt its behavior from a limited set of examples, which is an important limitation in medical AI applications today. \n\nThe idea of guidance is promising (however prior in DL is not a new idea), and combination of the guidance within the episode learning can be strong once its dynamics is shown in relatively more difficult problem where its additive value is proved. ', 'cons': '-- Almost entire paper is dedicated to describe the late-fusion technique and unfortunately there is very little description of early fusion. It’s not clear how early fusion variation model is trained. More information will address the ambiguity.\n\n-- The function ""f"" (which integrates the representation into the second network) is not clearly defined. \nIt would be necessary to clarify how this representation is integrated into the second network?\n\n--Although authors think the results are encoring, the overall results do not seem promising, and there is a lack of comparison with other methods. Hard to understand where this method stands compare to other available ones.\n\n--There is a lack of valid explanations/ justification on why the results for dense labels worse than that of 5 or 10 points ? \nIsn\'t that dense labels should be the ideal case of the sparse annotations? \n\n-- if human annotations and network output are overlaid, it could be easier to see where the mistakes are, but in the current form, it is hard to analyze images (see figure 3)\n\n--technical novelty is questionable, because few shot learning model is taken from Rarely et al 2018, and applied to histopathology images, and results are ""not"" presented in an elaborative way, therefore the application novelty remains questionable due to unjustified claims. ', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		HJl6fpzgg4	BklLd1bqmV	MIDL.io/2019/Conference/-/Paper81/Official_Review	[]	2		['everyone']	HJl6fpzgg4	['MIDL.io/2019/Conference/Paper81/AnonReviewer2']	1548517229816		1548856733950	['MIDL.io/2019/Conference/Paper81/AnonReviewer2']
197	1548514895900	{'pros': '1. The major contribution of this paper is that it evaluates a binary classification network for MS progression on a large clinical data set, which is quite useful for treatment planning. A minor contribution is that it incorporates uncertainty estimation for the classification result in order to remove highly uncertain results.', 'cons': '1. The methodological contribution is a bit limited. It uses a classical convolutional network for binary classification.\n\n2. It seems that all the patients also have their EDSS scores. It would be interesting if some discussion/exploration could be added to show how regression compares to classification on this task, as that may give more information for the network to learn.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		rkliARLel4	SkeO8Ig97E	MIDL.io/2019/Conference/-/Paper112/Official_Review	[]	3		['everyone']	rkliARLel4	['MIDL.io/2019/Conference/Paper112/AnonReviewer2']	1548514895900		1548856733737	['MIDL.io/2019/Conference/Paper112/AnonReviewer2']
198	1548514411303	{'pros': '--Motivation: multiphase CT scans are acquired at predetermined times after contrast administration, and the phase of these scans are sometimes wrongly labeled (although from appearance it is very clear), authors suggest to use Deep Learning (DL) method to automate this classification task. For large scale data analytics problem, especially for retrospective data where many mistakes are available, (meta data may not include this information) an automated tool classifying phase of the CT scan might be useful. However, we need to keep (still) in mind that the problem is an easy one! \n\n--Paper is written clearly, easy to understand. \n\n-- (several) experimental details are covered appropriately for reproducibility of the research.\n\n--(minor) examining where/what CNN looks at is an important subsection, perhaps not that suitable for this application which is considerably easy, but it can be powerful for some other more challenging applications. Nevertheless, it is another way of validating the results, and can be considered a strength.\n', 'cons': 'The paper has several weaknesses, major and minor.\n1) no comparison with the referred work (phibrick et al), which already achieved a very strong result.\n\n2) no cross validation performed (red-alarm!)\n\n3) motivation is given weakly, only practitioners and experts can easily grasp the idea because of their own experiences in the field, so a stronger motivation should be given (clear write up in that part is necessary).\n\n4) Figure 6 and 7 are not referred in the paper at all! (did authors forget to refer them? or am I missing something?)\n\n5) Authors try to argue from Figure 4a that since the predictions are overlapped in validation that indicated high variation in the appearance of the scans. This is quite weak since it might just be the classifier doing a poor job of separating the classes. You can easily measure directly the variance in HU values in different regions of interests in the scans without approximation.\n\n6) From one phase to another one, there are significant HU changes in various regions. Then, one very rough localization (let say aorta) can solve the problem extremely well instead of diving into deep learning field where solving a simple problem can be a nightmare due to introducing extreme non-linearities.  (see figure 7 as an example)\n\n7) no theoretical contribution, no novelties in network design,  network is very basic CNN, extracting slices and doing a classification, and even clinical benefit is questionable in the way it is written in the motivation. ', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		SylDbkUlxE	Hkx7OEl57N	MIDL.io/2019/Conference/-/Paper104/Official_Review	[]	1		['everyone']	SylDbkUlxE	['MIDL.io/2019/Conference/Paper104/AnonReviewer3']	1548514411303		1548856733526	['MIDL.io/2019/Conference/Paper104/AnonReviewer3']
199	1548514070136	{'pros': '1. The contribution of the paper is that it proposes a framework to perform semi-supervised learning for white matter lesion segmentation. Essentially, it is a self-training method, where the unlabelled images are first segmented using a thresholding method and then fed to a segmentation network for training.\n\n2. The method is solidly evaluated, demonstrating its performance on a few different datasets and for domain adaptation problems.\n', 'cons': '1. The basic assumption to perform segmentation using the difference map between the input image and the auto-encoder (AE) reconstruction is that the difference is only attributed to lesions. I am not quite sure whether this assumption always holds. For example, if the AE is able to reconstruct brain images with lesions as well, then the segmentations of the unlabelled images would not be available. In addition, if the difference is not caused by lesions, but instead by other structural abnormalities, then the lesion segmentations would be wrong.\n\n2. In the introduction, the paper mentions that one major challenge in lesion segmentation is the wide variety of lesion appearances. Maybe it would be helpful to show that the AE segmentation method can cope with different lesion patterns.\n\n3. Two thresholds or operation points (OPs) are used for segmentation. It is understandable for the segmenting the unlabelled images, the first threshold may be required. However, for training the segmentation network, why is the second threshold needed?\n\n4. It would be easier to read Tables 2,3,4, if an additional column could be added to show the number of training samples (#labelled and #unlabeled) for each method.', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		ryxNhZGlxV	ByxAzXx9XN	MIDL.io/2019/Conference/-/Paper71/Official_Review	[]	2		['everyone']	ryxNhZGlxV	['MIDL.io/2019/Conference/Paper71/AnonReviewer2']	1548514070136		1548856733311	['MIDL.io/2019/Conference/Paper71/AnonReviewer2']
200	1548508712080	"{'pros': '- It is interesting to see how authors studied the problem of ""forgetting"" in neural networks during incremental learning tasks in terms of attention and saliency. Interpretability is indeed a still existing problem, and the idea of utilizing the shift in the saliency maps [15] is highly appreciated and considered as original.\n- The motivation behind the proposed methods is very clearly put and rationalized.\n', 'cons': '- Experiment set up is either flawed or not well written in my opinion.\n- Results do not focus sufficiently on the novelties proposed in this manuscript.\n- Figures should be improved and extended.\n\nBelow, questions and comments are formulated in more depth:\n\nPlease elaborate on the major differences with your other submission “Continual few-shot learning for medical image analysis”, which is similarly motivated and also evaluated on the same dataset.\n\nThe main contribution of this work is to exploit saliency maps [15] for both interpretations of ""forgetting"" and exemplar sample selection during class-incremental training, which is very interesting. Unfortunately, the only time the reader gets a glimpse of falling quality in the saliency maps is in the Appendix for a vanilla finetuning approach. It would be of great value if the authors dedicate a full page (if not more) showing how the saliency map for a group of representative images change for the compared methods across incremental tasks (ideally with the corresponding KLD scores of the instances). In addition, the authors should consider showing a few images with high and low OSQ, since this metric lies in the foundation of the methodology.\n\nOne of my major concern is on the experiment setup. If the dataset is split by 80:20 (as stated in the Appendix), does this imply that during a new task schedule, same images/instances are being trained on, only with different ground truth annotations (of the actively trained 3 classes)? -If not: \nPlease clarify either that the instances of 4C, LVOT, and 3V are mutually exclusive sets, or how the training set was split during the successive incremental tasks. \n-If it is; then the results raise a lot of questions. \nIf at the end of a stage of a task, the network manages to perfectly fit the ground truth, then the stored logits will essentially be the same as the ground truth annotations. Furthermore, this would also raise questions for the motivation, where it is stated that the labels for old tasks are no longer available.\n\nAnother major concern is of how the exemplar samples are chosen. In Page 6, authors say ""After each task schedule we generate class activation maps using [9] for the validation data per class [...]"", and use these class activation maps when picking the exemplar samples for that stage. Unfortunately, it looks like the same validation set was used to compute model accuracy in the successive stages (cf. Fig 3). For a proper experiment, the model accuracy should be evaluated on data which was never seen, however, exemplar samples were picked based on information from the data which is later used for evaluations. In addition, unless the authors used the validation set samples to compute the “class means” for the baselines, compared methods such as iCaRL have an unfair disadvantage.\n\nIntroduction:\nI feel an abrupt change in the flow of text in the first paragraph; ""[...] been proposed for fetal sonography [1,2].  As with other applications of deep networks in medical image analysis and traditional computer vision [...]"" Perhaps the authors can smooth the transition with an additional sentence.\n\nLiterature Review:\nWhile it is understandable that authors reference the fundamental literature or surveys on literature review, authors say ""In medical imaging, incremental addition of new data has largely been unaddressed despite clinical systems often acquiring data in non-deterministic phases."" Then, they proceed with examples of shifted topics such as transfer learning or multitask learning from the medical domain.\nIt seems that the authors have missed referencing the recent work https://link.springer.com/chapter/10.1007/978-3-030-00937-3_42 on MR segmentation in a class-incremental setting, which is tackling a similar problem, the difference being segmentation as opposed to object detection.\nFurthermore, it seems to me that the exemplar selection methods proposed on Page 6 as well as objective function on Page 7 have similarities with this earlier work; i.e. exemplars are picked in a way to keep class samples with high confidence while also maximizing coverage of sub-clusters of class latent space representations.\n\nImplementation Details:\nIt is not clear how the classification network produces a bounding box for object detection in terms of bounding boxes. It seems that sufficiently small patches were generated and classified with the CNN, but this information is either missing from the manuscript or not emphasized enough such that I missed it.\n\nFig 1:\nConsider replacing with vector graphics. Figure barely readable even on digital environment.\n\nMethods:\nAlthough I believe ""stage of task"" and ""task schedule"" refers to training on a new set of classes (i.e., the first stage on 4C view classes and second stage on LVOT structures, so on), it would be better if defined formally in the text.\n\nExperiment Setup:\nOn Page 5, authors state the image inputs are 224x224, and map quality is estimated as a cumulative metric over 16x16 grids. However, later in Appendix, it is written that the instances are 100x100 images cropped from the dataset. Please clarify.\n\nDiscussions:\nAuthors point out that exemplars help improving performance. However, comments on OSQ vs KLD and ADRS vs DEDS would be appreciated, especially if a connection between the methodology and results can be speculated.  \n\nPage 3, Fig. 2: CAM Module (Class Activation Mapping?) is never defined in the manuscript. \nPersonally, I have a hard time following the images used in Fig. 2; e.g. what do the nature images resemble? There seems to be a small patch of heart image going in, then saliency map of a full resolution nature image, which goes into saliency maps of rotated shark images. The caption should include all necessary information to properly understand the figure.\nPerhaps authors can consider replacing it with a simpler (maybe abstract drawings?) images or simply an explanatory block schematic. Also, authors should pay attention to the flipping ""CNN"" text inside the classifier.\n\nPage 4: ""classwise average saliency maps"": This seems to imply that all images have to be aligned? How sensitive are the results to the manually cropped heart region and the 10 degree rotation used for data augmentation? \n\nPage 5: ""Each of these grids is evaluated by the trained model after a task for their prediction probabilities by themselves"". Please clarify whether the prediction probability is referring to classifier prediction or the saliency map.\n\nPage 5: ""A threshold can be imposed and all n grid regions contributing can be summed up to express the Overall Saliency Quality"" Does this imply that the value of n changes for every instance based on how many of its grid regions pass the threshold? Please clarify.\n\nPage 6, 3rd line: ""[...] it is not always true in case of classes with significant diversity and multiple sub-clusters of exemplars"". Can the authors elaborate on ""multiple sub-clusters of exemplars""? Do they refer to some classes possibly having different clusters in the ""input distributions"", hence class means may lead to suboptimal samples being picked as exemplars?\n\nPage 7: variable ""n"" was already used in Page 5 for a different term. Furthermore, it is ambiguous (is it ""new"" as in new instances and ""o"" is ""old""?) and should be defined for second and third equations on the page.\n\nPage 7: ""For the distillation terms, the original labels not being available [...]"". Authors should comment on why the original labels would not be available at an incremental task schedule in their motivation.\n\nPage 7: ""For the distillation terms, the original labels not being available, y_o^{ij} is computed with new and retained examples and compared to stored logits for the old examples p_o^{ij}, giving a loss term [...]"" Is it possible that the variables are confused? In a typical distillation loss between teacher and student predictions, predictions from the teacher (a.k.a. stored logits for the old examples prior to the new stage of task) are replaced with the labels, while predictions of the student (a.k.a. logits evaluated during the ongoing stage of task) are log-scaled. From the text, one would interpret the contrary.\n\nPage 7: Distillation loss is defined as L_{KD}, which is misleading since the only connection with KD seems to be how the stored exemplar instances were picked. However, L_{KD} is evaluated for the union of the set of exemplar instances and the set of new instances. Authors should consider renaming this loss term.\n\nPage 7: Parameter regularisation for already trained weights using ""Frobenius norm"": Please define how \\mu is computed or picked. In addition, please comment on how sensitive the proposed method is on the picked \\mu, as one would imagine it can have a high impact on the plasticity of the network for new training instances.\n\nPage 7, Discussion line 4: Please define ""no retn"". The corresponding sentence can be changed with ""Variants of our approach without storing rehearsal exemplars (i.e. no retn) are also considered in Table 1."" Also, It is not clear if the authors are referring to ""Fig. 3"" or Table 1 in the Appendix.\n\nPage 9, ""Fig 3"": Figures/tables should be self-explanatory. Please indicate the metric for Map Quality in the caption. In addition, please clarify Task 1, 2 & 3 somewhere. Initially, in Methodology, the sets of classes were introduced in the order of 4C view, LVOT, and 3V view. However, soon after, it is said that 6 of these classes (in a different grouping of the previously defined groups) are learned as base categories (clarification needed for ""base categories""), and remaining 3 are used for class incremental learning; reducing the number of tasks to a total of 2. Later in the appendix, the order of tasks is switched to 4C view, 3V view, and LVOT.\n\nPage 11, Appendix, Training: Please clarify that the split of datasets ensures frames from a subject (out of 12) does not get split between training and testing sets.\n\nPage 12, Appendix: There are 2 Fig.1s.\n\nAdditional questions:\n- Page 4, line 4: Are the specific classes of 2 stages; first 6 and then the other 3, purely picked as a design choice?\n- Is the exemplar size of 30 instances found empirically? Can the authors comment on whether the number of exemplar instances is a sensitive parameter?\n- How are the network weights initialized for a new stage of task?\n\nGrammatical/typographical errors:\n- Pay attention to \'.\' (full stops) and \',\'; some sentences are missing a \' \' (space).\n- Page 4: ""These structures are considered for demonstrating our approach because of their relevance in the assessment of congenital heart disease [17]."" [17] is the Distillation work from Hinton et. al.\n- Page 4, last sentence of the 2nd paragraph: Typo ""om"".\n- Page 4: The sentence ""We consider only the maps resulting from correct predictions because the explanations are generated even otherwise but is suboptimal for further inference"" is not clear. Perhaps replace with ""While attention maps can be generated for all inputs, incorrect predictions are suboptimal for further inference. Thus, we consider only the maps resulting from correct predictions.""\n- Page 5, line 12: Missing a comma, ""We extend from the SSR concept, and instead of thinking in terms of concentration of information \',\' consider the extent of regions of informative content.""\n- References are not properly indented. Additional care should be taken for the syntax of references, please follow natbib reference guide.\n- Page 9, Figure 3: This is a table. Please replace with a table and ensure it is not a compressed image.\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		S1gyUgN7eE	ryxlERAtmN	MIDL.io/2019/Conference/-/Paper142/Official_Review	[]	1		['everyone']	S1gyUgN7eE	['MIDL.io/2019/Conference/Paper142/AnonReviewer1']	1548508712080		1548856733087	['MIDL.io/2019/Conference/Paper142/AnonReviewer1']
201	1548242505229	{'pros': 'The paper presents a method for continual few-shot learning for medical images analysis. Few-shot learning is an interesting topic and is very valuable for medical applications.  The idea of this paper is to understand the based classes by a network for feature representation coupled with a classifier.  The output of last fully connected layer of a CNN and the weights between this layers and classification layer is important to calculate the class score confidence. This paper proposes weight generation for new incremented classes.  The novelty of the paper is limited and then there are several concerns about both method and writing of the paper. ', 'cons': '\n1-The paper is not easy to follow. The organization of it is needed to be improved. Further, there are several considerable mistakes in the paper. For example, The symbols in the paper are not used as the same format in the whole of text, or you have not referred to any of Figures in your text. The paper is carelessly writhed.  \n\n2-Most important papers on the topic of few or low shot learning are ignored in this paper. It is expected to compare your work to the state-of-the-art method in the topic. \n\n3-in contrary to your method, in the deep learning community, it is very important to propose an end-to-end network.\n\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		BkeM-yMgeE	rkebI06HQV	MIDL.io/2019/Conference/-/Paper68/Official_Review	[]	1		['everyone']	BkeM-yMgeE	['MIDL.io/2019/Conference/Paper68/AnonReviewer3']	1548242505229		1548856732787	['MIDL.io/2019/Conference/Paper68/AnonReviewer3']
202	1548482584878	"{'pros': 'This work presents to use different strategies of data augmentation and the authors introduced a ""perturbed normalization"" strategy. ', 'cons': ""The proposed perturbed normalization is nothing but rescaling the image intensities overall. It's like shifting the histogram of an image and to this reviewer's knowledge, it is mostly not helpful in learning network parameters.\n\nIt is required to report performance with K-fold cross-validation techniques applied, e.g., K=10.\n\nThe performance of the proposed method is much lower than that of the state-of-the-art methods in the literature."", 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		r1gvQWLglV	B1lb7__t7E	MIDL.io/2019/Conference/-/Paper105/Official_Review	[]	4		['everyone']	r1gvQWLglV	['MIDL.io/2019/Conference/Paper105/AnonReviewer1']	1548482584878		1548856732574	['MIDL.io/2019/Conference/Paper105/AnonReviewer1']
203	1548481784447	{'pros': 'This paper proposed an interesting idea to correct the mislabeled labels by examining the training loss of each sample. The motivation for this work is good.', 'cons': 'Essential details of the experiment are missing. For example, the loss function and learning rate were not specified. In the 1st paragraph of page 4, the authors claimed: “For those correctly labeled samples, the sequences of loss functions always present a similar decreasing trend”. However, this can be affected by the shape of the loss function and learning rate. Also, overfitting could be another factor. More experiment details and solid mathematical theory proof will make this paper more convincing.\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		rJxCyxj1xN	BygeZSdFQN	MIDL.io/2019/Conference/-/Paper39/Official_Review	[]	2		['everyone']	rJxCyxj1xN	['MIDL.io/2019/Conference/Paper39/AnonReviewer3']	1548481784447		1548856732359	['MIDL.io/2019/Conference/Paper39/AnonReviewer3']
204	1548479662082	"{'pros': ""In this work, the authors present to leverage an attention mechanism to single-shot detector networks and to apply it to pulmonary nodule detection. They emphasize their world's largest dataset of CT scans with annotations of varying types and sizes of pulmonary nodules. \n\nMethodologically, the main contribution of this work is to design a group attention network that can be injected in the existing network architectures. "", 'cons': 'While the reported performance presents the superiority of the proposed method, the detailed information in implementation  is missing such as the complete network architecture, number of groups in group convolution, loss functions, etc.\n\nTo better justify the effectiveness of the proposed method, it is highly recommended to experiment over the LUNA16 dataset and compare with the scores listed in the leaderboard.\n\nTable 2 and the statement of ""~ using the GA module could help the model learn more important feature layers."": Regarding these, it would be interesting to see the weights estimated by GA modules and how those affected the performance.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		H1x-pmF0JE	HklLh2DF7E	MIDL.io/2019/Conference/-/Paper15/Official_Review	[]	1		['everyone']	H1x-pmF0JE	['MIDL.io/2019/Conference/Paper15/AnonReviewer2']	1548479662082		1548856732146	['MIDL.io/2019/Conference/Paper15/AnonReviewer2']
205	1548479501371	{'pros': 'The authors describe a novel method for medical image segmentation when there is a high inter-rater variability. They modified a segmentation U-net by conditioning it on a scaling parameter, which enables the model to provide multiple proposals for segmentation. The conditioning was done by addition of a dynamic leaky RELU and a feature-mapping layer. The description of the method and model is clear.\nThe work shows results of the study in three examples: 1. synthetic vanishing shapes data,  2. volumetric laser endomicroscopy dataset and 3. ARGOS dataset. For the first two studies, the results are shown qualitatively. The segmentations produced as a result of this methodology are heat maps or probability maps which show the most likely regions of interest, as opposed to regular segmentation outputs that do no show uncertainty. The results in the third experiment are quantitative - they show comparison between five models for biopsy localization. The proposed method performs better in terms of almost all the metrics. \nIn general, this method could be useful to solve the problem of segmentation in the presence of variable ground truth. This method also shows uncertainty of the segmentation  output, which might be beneficial to clinicians.\n', 'cons': 'The description of experiments and results is unclear. The authors do not mention any details about training protocol or the number of training and test samples used. The results for studies 1 and 2 are qualitatively shown in just one example. Please include details of number of test samples, and aggregate metrics of general performance. In the third example, the evaluation is performed based on metrics to measure biopsy localization. However, the details about number test samples this was performed on is missing. \n\nIt is hard to assess the general performance of the model lacking these details.\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		SJl27gYRkN	HygBfhvYQN	MIDL.io/2019/Conference/-/Paper14/Official_Review	[]	2		['everyone']	SJl27gYRkN	['MIDL.io/2019/Conference/Paper14/AnonReviewer3']	1548479501371		1548856731927	['MIDL.io/2019/Conference/Paper14/AnonReviewer3']
206	1548479361297	{'pros': 'The authors describe a method to track biological structures using deep reinforcement learning (DRL) when there is little to no training data available. In this paper, the use case is microscopy images of cortical axons in mice. They employ the PPO algorithm, which is a state-of-the-art reinforcement algorithm, to learn policy and value updates using CNN. The model is learnt in two phases: In the first, they train the networks on a huge (32, 000) synthetic dataset, which simulates microscopy imaging, to learn hyper-parameters. This model is tested on 1000 synthetic images and 20 microscopy images. In the second phase, they use 4-fold cross validation on the 20 microscopy images to fine-tune the networks. They compare the results to a state-of-the-art tracking software, Vaa3D. The two measures used for comparison are coverage and mean absolute error. The following are the main pros of the paper:\n1. The background description is clear and informative. They clearly distinguish tracking from segmentation. They clearly describe the reinforcement learning methods\n2. The authors show that a DRL that is trained to track structures on a synthetic dataset can be generalized to real world biological data with reasonable accuracy. The DRL model does not perform as well as Vaa3D. However, the results do show value of synthetic training datasets to train DRL problems.\n3. They introduce a new metric based on the entropy of their stochastic training process, which automatically indicates the uncertainty of the tracking.\n', 'cons': '1. The main issue of the paper is the re-use of testing dataset in phase 2 to fine-tune the parameters. From the description, it seems like the 4-fold cross validation in phase two uses the same 20 images that were used to select the best hyper-parameters in phase 1. Given that n=20 is very low, the improvement seen in phase 2 might have been due to over-fitting, since model has already been hand picked for this data. An alternative could be to follow through the 4-fold cross validation in both steps 1 and 2 while holding the subsets constant. \n2. It is unclear what the authors mean when they state that this model approaches the performance of Vaa3D. The normal range of error measures such as coverage is not immediately evident. Perhaps comparison with a baseline method will be useful to show that this model is indeed close to state-of-the-art performance.\n3. The claims made in this paper are very broad. The authors claim that this method could be extended to any tracking task. Further experiments need to be performed to ascertain this claim.\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		HJxrNvv0JN	HkxFYsPtmN	MIDL.io/2019/Conference/-/Paper13/Official_Review	[]	1		['everyone']	HJxrNvv0JN	['MIDL.io/2019/Conference/Paper13/AnonReviewer3']	1548479361297		1548856731602	['MIDL.io/2019/Conference/Paper13/AnonReviewer3']
207	1548477010156	{'pros': 'The authors proposed a SRCNN-based dMRI up-sampling method. While the results look good, more details about the method and the results should be provided.', 'cons': 'Since there is a lot of spare space, I suggest the authors add more details in the paper. For example, how did the authors plugin a 4D dwi volume to make it work in waifu2x? How was the 0.625x0.625 resolution image obtained? What information a reader can get from Figure 6? The current paper looks like a technical report, but not a research paper.', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		B1gETswelE	BJx9LGvtQV	MIDL.io/2019/Conference/-/Paper115/Official_Review	[]	2		['everyone']	B1gETswelE	['MIDL.io/2019/Conference/Paper115/AnonReviewer1']	1548477010156		1548856731226	['MIDL.io/2019/Conference/Paper115/AnonReviewer1']
208	1548459956714	"{'pros': ""1, This paper is interesting enough and well written. It covers an interesting topics on understanding the CNN activation maps and units for interpretation. The technical aspects are based on authors' previous work extensively.\n\n2, Within human-in-the-loop annotation is a relevant aspect along this line of work and may be worth more investigation. \n\n3, The performance of the CAD is close to the state-of-the-art reports [Shen, 2017].\n\nIf the authors can do rebuttal properly and adequately, I can change my rating and reviews."", 'cons': 'In all, this paper is interesting but probably only stays at the ""interesting"" level without real clinical impacts. It is a loose technical setup to investigate the top 60 units to see what they represents, but there may not be strongly semantic coherence for each unit (based on judging their most representative image patches). There is no thorough numerical experiments on this aspect. This is a borderline paper and from the clinical or medical imaging usefulness perspective, I intends to recommend ""rejection"" otherwise it is too easy to be accept. I do think the paper has value and merits but this is not a sufficiently high quality of technical submission yet.\n\n1, The techniques are mostly authors\' previous work, not much of new development but a new subjective study maybe. Not sure this will be sufficient for MIDL. This is not a critical criticism however.  \n\n2, The main reason is that, if you look at the false classified visual examples, for example, Fig. 4(b) and Fig. 5(b), and the explanations from the highly-ranked units, it may be depressing. The units report ""confusing"" and ""misleading"" explanations which means that the ResNet network trained/employed actually can not  tag properly about what\'s going on and make significant errors. This kind of explanation can be more misleading I believe if the human observers read these ""Interpretable"" phrases. ', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		Bkxms_x1lN	SJeT31mYQN	MIDL.io/2019/Conference/-/Paper28/Official_Review	[]	1		['everyone']	Bkxms_x1lN	['MIDL.io/2019/Conference/Paper28/AnonReviewer3']	1548459956714		1548856731013	['MIDL.io/2019/Conference/Paper28/AnonReviewer3']
209	1548452482583	{'pros': '\nThe paper is well-written and easy to follow.\n\nThe experiment is very well described.\n\nThe proposed approach could potentially be used in other applications.\n\n', 'cons': '\n1.\tHaving two extra neighbor patches (Extended 2D) as inputs to the network does not make the network structure different, as it will be the exact same way as inputting a multi-modality image (3 modalities in this case) and therefore not a novelty for the network architecture.\n\n2.\tAugmentation and residual connections have previously been used in the literature and in a vast scale. Using 3 planes of 2D and combining the results have also been used by previous literature where 3D training was not feasible.\n\n3.\tResidual Unet (Residual connections in a Unet architecture) have been introduced and used before, therefore not a novelty for the network architecture.\n\n4.\tUsing VGG11 weight transfer for initialization seems like a good idea, however it needs to be compared to its own category (weight initialization), which has not been done in this project. There are many different random and non-random initialization techniques that are working quite well for recent cutting-edge methods. How does VGG11 compare to them?\n\n5.\tThe visual comparison of the proposed method and Hippodeep seems attractive, however there is no reported statistical comparison between the two, especially since they were both applied to a third publicly available dataset. The comparison would have been valid, on point and quite necessary.\n\n6.\tThe fact that the authors have tried using a smaller number of layers as well as different architectural changes to the network and did not even get to convergence, while only this specific architecture had successful results is very questionable and needs more clarification.\n\n7.\tAxial results seem to have much less accuracy than Sagittal and Coronal results, therefore calculating a consensus out of those three may only mean that you might be selecting the intersection of the two better models. Therefore, if that is the case, there won’t be a need for the axial model to train at all.\n\n', 'rating': '1: strong reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		rkg6MIRa1V	HkgjKGbYm4	MIDL.io/2019/Conference/-/Paper11/Official_Review	[]	3		['everyone']	rkg6MIRa1V	['MIDL.io/2019/Conference/Paper11/AnonReviewer1']	1548452482583		1548856730789	['MIDL.io/2019/Conference/Paper11/AnonReviewer1']
210	1548436708504	"{'pros': 'This paper presents a convolutional capsule-based generative adversarial network, similar to pix2pix, that is applied to a simulated and a real microscopy dataset. Adding the synthetic examples generated by the model to train a segmentation network improves the performance of the segmentation model, with a performance improvement comparable to or better than that of the pix2pix network.\n\nI liked reading the paper. The various components are explained well and the approach is relatively easy to follow. The addition of capsules to the pix2pix seems to be a novel approach. The experiments look fairly solid (although I am not sure of the number of repetitions, see below).', 'cons': 'Section 2 notes that LaLonde and Bagci (2018) restricted the dynamic routing to small spatial neighbourhoods, but that the proposed method uses full dynamic routing instead. Does this restrict the size of the images that can be processed by the network?\n\nTo some extent, the proposed synthesis method provides a fancy way to do data augmentation or to add regularisation to the model. It might have been interesting to include one or more of these simpler methods in the comparison in Table 1.\n\nThe Discussion is relatively brief. Although the authors provide at the features extracted by the networks in Figure 2, there is not much more in the way of analysis or discussion of how the capsule networks are able to outperform the non-capsule baseline. Is it the fact that they are more efficient? (This isn\'t really measured in the paper, I think.) Is it because they learn fewer redundant features? There are some hints to the answers in the Abstract, but these things are harder to find in the paper itself.\n\nIt is not clear to me whether the results shown in, for example, Table 1, are based on multiple runs of the algorithms, or whether we are looking at the performance of a single run for each setting. (I obviously hope that we are looking at averages.)\n\nMinor point in Section 2.1: ""D is shown both real and synthetic label-latent pairs"". Shouldn\'t the discriminator also receive the real and synthetic images?\n', 'rating': '4: strong accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		rJen0zC1lE	SJlhJBpdQN	MIDL.io/2019/Conference/-/Paper46/Official_Review	[]	3		['everyone']	rJen0zC1lE	['MIDL.io/2019/Conference/Paper46/AnonReviewer2']	1548436708504		1548856730572	['MIDL.io/2019/Conference/Paper46/AnonReviewer2']
211	1548434010231	"{'pros': ""This is a well-written, clear paper that uses cycle-GANs for image augmentation of bone lesion pathology. The experiments are thorough and convincing that their proposed approach improves upon the baseline.\n\nIn their approach, they train a patch-augmenting tool that adds bone lesions to normal patches. These patches are blended back into their image of origin using alpha-blending, to ensure that changes to gross image features such as contrast are not noticeable. Then, a fraction of these images (the ones that score as the most 'hard-positive') are selected to augment the training set. This can be done on a bone-wise basis (e.g. for the humerus, tibia) or learned from other bones in the case where it is impossible to train a bone-specific translation model (femur). The best scores were obtained using the humerus translation model for inference and pseudo-labelling with the humerus baseline model, which shows that their approach was amenable to transfer learning. This has the potential to dramatically increase the effective size of the training set and hints at ways to train on conditions that are rarely seen.\n\nThe paper overall has a high level of experimental conduct and is persuasive. In particular, I found the 'hard-positive' mining surprisingly effective and it was well-demonstrated to help."", 'cons': ""While the paper is well-explained in general, the precise application of the tool is not really clarified in the writing. Is it for building a bone-lesion classifier to be used in a general setting? If it is, it's not clear how training with this augmentation would interact when testing on a dataset that included a range of other pathologies or whether it would still improve on the baseline.\n\nI would also like to see some discussion of the merits of the proposed method for other applications. VAEs are known to produce blurry images, which may (or may not) make them suitable for x-ray problems and not others.\n\nIt would also have been nice to see slightly more comparison with other augmentation or curriculum learning methods: the baseline is fairly crude (balanced sampling) and it's not clear if any more standard image augmentation techniques were used in the baseline evaluation."", 'rating': '4: strong accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'oral_presentation': ['Consider for oral presentation']}"		rkxOKaj0yV	HkxMvc3u7E	MIDL.io/2019/Conference/-/Paper21/Official_Review	[]	1		['everyone']	rkxOKaj0yV	['MIDL.io/2019/Conference/Paper21/AnonReviewer3']	1548434010231		1548856730360	['MIDL.io/2019/Conference/Paper21/AnonReviewer3']
212	1548433239848	"{'pros': '\nThis paper presents a study on the impact of data pre-processing on the training of neural network for lesion segmentation in brain CT images.\nWhile network architectures are the core of many research papers, data pre-processing is often overlooked even though they might have an even stronger impact on the result.\nI therefore believe that papers like this one are actually quite useful and should be welcome in the community.\n\nIn particular, three different aspects are being investigated: the resampling of the images and their registration to a common coordinate system, the intensity normalization and the skull stripping.\nBased on their experiments, the authors are able to provide some guidelines regarding data pre-processing.\n\nOne interesting point is that the authors applied their experiments to two different tasks/datasets, which gives more credibility to their findings.\n\nFinally, this is also the first work to report segmentation results on contusion core, blood and oedema, which means that the paper can serve as a baseline for future work.\n\nThe paper is clear, references and figures are adequate.\nFor all those reasons, and despite my other remarks below, I would recommend its acceptation.', 'cons': '\nIt is a bit surprising that no statistical test seems to have been performed in such an experimental paper.\nReporting the average and standard deviation is not enough to draw solid conclusions, especially when the numbers are not dramatically different from one experiment to another.\n\nThe paper is a bit short - more experiments could have been performed.\nFor instance, it would have been interesting to also investigate the sampling of the patches, which I expect to also have a strong influence on the training.\nA third dataset/task would also have been beneficial to include in the study in order to support some of the conclusions (e.g. ""registration benefits challenging tasks"").\n\nThe Dice coefficients on the TBI datasets are still quite low (~0.5) and the difference between the various methods is also quite small.\nI understand that this is a very challenging problem and my point is definitely not to criticize the results, but to raise the issue of a potential ""premature optimization"".\n\nDoes it never happen that the skull stripping method actually masks out the lesion (especially when it is located  at the periphery of the brain, or when the patient has severe pathologies), therefore making the information unavailable to the network ?', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		HylG9MfleV	SJgxvw2d7N	MIDL.io/2019/Conference/-/Paper72/Official_Review	[]	2		['everyone']	HylG9MfleV	['MIDL.io/2019/Conference/Paper72/AnonReviewer3']	1548433239848		1548856730149	['MIDL.io/2019/Conference/Paper72/AnonReviewer3']
213	1548432660195	"{'pros': 'This paper deals with the issue of missing MRI modalities in multimodal glioma segmentation. This issue is more commonly addressed by replacing the imaging modality with e.g. the average of the remaining images in the dataset or by synthetically generating these modalities (e.g.Bowles et al. International Workshop on Simulation and Synthesis in Medical Imaging, 2016).', 'cons': ""The main concerns lie in the originality/novelty of this work and the evaluation. For more details see below:\n\n- The concept of dropout has long been used to improve generalisation of models and to tackle sparse inputs.\n- It is unclear how the label fusion model works exactly. Figure 2 diagrams could be improved to reflect what the models are actually doing.\n- I understand that the authors want to avoid data imputation of any kind, but it would be useful to see a comparison of the two approaches. If data imputation is performing significantly worse, then it is worth the extra effort of generating synthetic images or computing the missing modality from remaining data.\n- In terms of evaluation, results seem a bit unstable across folds so it would be useful to show results for all folds of the cross-validation. I understand the time constraints but it doesn't serve much to the user to only see results from 2 folds.\n- The boxplots in Figure 3 only favour the ensemble approach for the FLAIR images. The advantage of using ensemble or late fusion is not clear in this case."", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		S1lXP-nJxE	Skg2MB3OXN	MIDL.io/2019/Conference/-/Paper43/Official_Review	[]	1		['everyone']	S1lXP-nJxE	['MIDL.io/2019/Conference/Paper43/AnonReviewer2']	1548432660195		1548856729936	['MIDL.io/2019/Conference/Paper43/AnonReviewer2']
214	1548430334015	{'pros': 'The authors propose a novel unsupervised anomaly detection and segmentation approach that utilizes Gaussian mixture variational auto-encoder (GMVAE) to learn the prior distribution on healthy subject images. The images containing anomalies are restored using the learned prior, incorporating total variation for data consistency, and the residuals are computed by subtracting the restored from the original image. By thresholding the residuals the pixel-wise anomaly detection map is obtained. The proposed method was trained on 652 images of healthy subjects and the anomaly detection approach applied to BRATS 2017 challenge datasets containing brain tumors as the anomaly. \n\n- The unsupervised approach is well motivated and literature review is extensive\n- The reconstruction methodology incorporating the normative prior seems novel and, in general, is described clearly and concisely ', 'cons': '- Method was applied to 2D image slices and does not take full 3D information into account\n\n- Results in Table 1, for instance the DSC_AUC, for brain tumor detection are rather poor compared to the equivalent Dice_WT score for most other tested methods (see https://www.cbica.upenn.edu/BraTS17/lboardValidation.html)\n\n- Some validation metrics are not defined, for instance, DSC_AUC is not defined in section 2.4, but which, presumably, is obtained by maximizing the TPR-FPR value; please clarify\n\n- There are two approaches to residual map computation, but results are not reported consistently; namely, the signed difference based residual map calculation is proposed ad hoc at the end of results section 4.2, indicating much improved results, while unfortunately the results are not reported in the same manner as before', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'oral_presentation': ['Consider for oral presentation']}		S1xg4W-leV	B1l8-ho_X4	MIDL.io/2019/Conference/-/Paper57/Official_Review	[]	1		['everyone']	S1xg4W-leV	['MIDL.io/2019/Conference/Paper57/AnonReviewer3']	1548430334015		1548856729720	['MIDL.io/2019/Conference/Paper57/AnonReviewer3']
215	1548430257520	"{'pros': 'This paper evaluates a brain extraction method that is pretrained with automatic segmentations from a large MRI dataset and subsequently fine-tuned on a small number of manual annotations. This fine-tuning is performed both on data from the same dataset and, in a kind of transfer learning setting, to data from two other datasets. The authors show that the segmentations produced by the fine-tuned model are closer to those of the target dataset than the segmentations from the pretrained model.\n\nThe paper is fairly well-written and overall easy to follow. It provides a reasonable overview of related work.\n\nThe paper provides compares with a relatively large number of alternative brain extraction methods and show that their proposed method achieves an improved Dice score on their test dataset.\n\nThe experiments are done on three datasets, which allows the authors to do some transfer learning experiments. The main dataset is reasonably large and covers a range of scanner vendors and field strengths.\n\nI like the fact that the authors are sharing their code in a public repository. (Although the way they do this is perhaps not ideal, see below.)\n', 'cons': 'From a technical perspective, the proposed method is not very novel. It follows a common pattern of pre-training on a similar dataset, followed by fine-tuning on a smaller dataset to better adapt to the target data.\n\nThe paper does not provide many details of the model used or how it was trained. The authors provide a link to a source code repository of a Machine Learning 101 course that does include a ""Skull-stripping (3 part demo)"", which I suppose gives some of the information, but I think some of this should also be described in the paper itself. What choices did the authors make, why did they make them? (E.g., why choose pseudo-3D patches, why take patches of 64x64 pixels?)\n\nAt present, the authors only train with the STAPLE-combined annotations from all methods. What would happen if the model would be trained with the individual segmentations from all methods? (Perhaps that would allow the network to learn from a larger range of slightly different segmentation approaches?)\n\nI have some concerns about the overlap with the authors\' earlier work (Souza et al. 2018b, https://doi.org/10.1016/j.neuroimage.2017.08.021), which presents the same dataset, the same set of automatic segmentation algorithms and even a supervised method that learns from the automatic segmentations and is fine-tuned on a small target dataset. It is not for me to compare the results, but the Dice scores reported there do seem to be fairly close to the ones of the currently proposed method. It would have been useful to include them in this paper, as an alternative non-deep learning method.\n\nSimilarly, it might have been nice to compare with the results from the authors\' ISBI paper (Lucena 2018a), which also applies a deep learning method to the same dataset. The present paper only provides a comparison with slightly older algorithms, so I would have liked to have seen how the proposed method compares with more recent approaches.\n\nIn the present paper, I would argue that the results are not surprising. It seems fairly obvious that a model fine-tuned on data from a particular annotator or algorithm will produce segmentations that best matches those segmentations also in the test set. We see this in all results in Table 1.\n\nThis raises the question whether we are looking at meaningful differences -- is the improvement in Dice clinically significant? are the segmentations objectively better? -- or whether each annotator or algorithm just uses a slightly different annotation approach, and the fine-tuning method just adapts to this systematic but harmless bias. The improvements are quite small.\n\nIn summary, I think the paper certainly has merits, but the proposed method is not particularly novel and the machine learning component not well described, the improvements in performance are modest, and there is a significant overlap with other papers by the same group of authors.\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		Syen8HNxeN	B1x92oiuQE	MIDL.io/2019/Conference/-/Paper95/Official_Review	[]	1		['everyone']	Syen8HNxeN	['MIDL.io/2019/Conference/Paper95/AnonReviewer2']	1548430257520		1548856729502	['MIDL.io/2019/Conference/Paper95/AnonReviewer2']
216	1548428154577	"{'pros': 'In this paper, the authors propose a modification of the res-net to reduce its computational complexity at minimal cost to its performance.\n\nThe paper is in parts clearly-written with a well-motivated method. However, the main claims are broadly unsubstantiated and at times, missing any evidence at all.', 'cons': ""In the abstract, the authors state that they report 'significant computational savings' and reassert this claim in the conclusion: 'the proposed method... reduces the computational burden'. However, I could find no experiments in the paper that support this assertion at all.\n\nThe second of the author's claims, that they achieve state-of-the-art results on Endovis 15 challenge data, is supported in the experiments. However, given that the authors explain their modifications of res-net for the best part of 4 pages, it is surprising that they don't compare the results with those achieved by implementing the vanilla res-net. Given that the focus of the paper is improving this architecture, I would like to see the proposed method vs vanilla res-net in terms of computational burden and performance (neither is performed).\n\nFor the comparisons that they do offer, While their network outperforms the state-of-the-art for the 'binary' and 'grasper', the shaft is actually worse than any of the other methods. This is not discussed in the manuscript and should be -- is there a good reason for this poor performance here?\n\nIn summary, while the method is novel and of interest, the work is not ready for presentation. There is no 'discussion' section and the results section is only one experiment, which as previously noted, does not give any evidence that their proposed architectural modifications improve the residual network. I would encourage the authors to extend this work and submit to a different venue, because the core of the method is interesting and potentially useful."", 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		r1xJrU9llV	rJgmYXjuXN	MIDL.io/2019/Conference/-/Paper126/Official_Review	[]	2		['everyone']	r1xJrU9llV	['MIDL.io/2019/Conference/Paper126/AnonReviewer3']	1548428154577		1548856729250	['MIDL.io/2019/Conference/Paper126/AnonReviewer3']
217	1548421016924	{'pros': 'The authors propose to stratify the input magnetic resonance scans according to magnetic field strenght and vendor with a hypothesis that this can be used to reduce the impact of scan variability onto automatic image analysis tools by specializing each tool to the image source. Their approach to stratification is a straightforward transfer learning, in which features output from the final convolutional layer of  the VGG16 network, pretrained on ImageNet dataset, are employed in SVN and kNN classifiers. Input to the network are the mid-volume 2D slices in three orthogonal axis-aligned orientations. Experiments on two datasets showed that the proposed stratification can be performed with accuracy of >98%, clearly showing that interscanner differences in image quality, contrast, etc., are so large that they enable to classify image source directly from the image-derived features.\n\nMajor strengths:\n- Relevant problem in neuroimaging is addressed with a rather novel strategy\n- Experiments on classification are well-designed and involve relevant and sufficient amount of data ', 'cons': 'The novelty in the manuscript is in the proposed application, while the employed machine learning methodology is straightforward and well-known. I feel that the utility of the proposed methodology should be explored to the extent to answer the underlying hypothesis made by the authors: i.e.,  specializing each CAD tool to the image source may be used to address the problem of interscanner differences. \n\nFurthermore,  I see plenty of difficulties with implementing the proposed strategy in clinical practice, since this requires huge effort in tuning and validating the performance of CAD tools for each dataset. The main drawback is that the burden of demonstrating  repeatability and reproducibility is tied to each image source. \n\nFrom the present manuscript it remains unclear how effective is the strategy in leveraging the performance of CAD tools. Namely, the authors could have demonstrated on a particular application, e.g. using skull-stripping tools, that the dataset variability reduced through stratification does indeed allow for an easy and somewhat significant improvement of the tool performance.\n\nMajor weaknesses:\n- Methodology is straightforward and novelty is very limited\n- Utility of the proposed methodology was not demonstrated, thus benefits are not clear \n- Classification tasks seem trivial and redundant in practice, because vendor and magnetic field strength is usually readily available from image meta-data (e.g. Dicom headers, even in the case of anonymization)', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		HylnnSQlgV	B1gZivFuQN	MIDL.io/2019/Conference/-/Paper86/Official_Review	[]	2		['everyone']	HylnnSQlgV	['MIDL.io/2019/Conference/Paper86/AnonReviewer2']	1548421016924		1548856729035	['MIDL.io/2019/Conference/Paper86/AnonReviewer2']
218	1548416112361	"{'pros': 'This paper presents a method for learning ultrasound transmission patterns together with the corresponding image reconstruction pipeline. The authors performed experiments on a small echocardiographic dataset. The results show an improvement of image reconstruction using the learned settings, compared to the standard procedure.\n\nThe method and application are interesting and definitely relevant to MIDL. The methodology could also be useful to other imaging modalities.\nThe results are good for a proof-of-concept.\n\nThe paper is very well written except 3.3 paragraph “Convergence”, which should be revised.', 'cons': 'The evaluation is relatively limited. It could be improved on the following points:\n- The method is evaluated by testing on a single cine-loop (32 frames). At least a leave-one-out cross-validation should be performed.\n- The low-resolution acquisitions are simulated from the ground truth single-line acquisition images. Is this equivalent to actually performing an acquisition with the corresponding transmission profile? This should be included in the discussion, and ideally, one could perform a test by acquiring images with the learned parameters.\n- It is not clear why the reconstruction part of the network is pre-trained before training the transmission part, instead of training everything from scratch. This is probably why the network converges to locally optimal solutions near the initial beam profiles.\n- Differences in performance are described as “significant”, which would need to be backed up with a more thorough evaluation (using cross-validation).\n- Why is the L1-error for the DAS methods missing? It should be reported for completeness.\n- The image quality metrics used (PSNR, contrast…) are quite basic and the authors do not explain how they relate to image interpretability.\n\nThe following papers seem relevant, and could be included in the literature review:\n[A] El-Zehiry, Noha, et al. ""Learning the manifold of quality ultrasound acquisition."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Berlin, Heidelberg, 2013.\n[B] Abdi, Amir H., et al. ""Automatic quality assessment of echocardiograms using convolutional neural networks: Feasibility on the apical four-chamber view."" IEEE transactions on medical imaging 36.6 (2017): 1221-1230.\n\nFinally, is there any potential application of the method beyond ultrasound imaging (other types of sensors)?\n\nMinor comments:\n\nPage 3:\n- “4-MLA” is not defined – it is only defined on page 6\n- “BFTransform” is not used later, “Rx beamformer” is used instead. Layer names should be harmonised throughout the text and with Figure 2.\n- The notation for the raw signal should be phi without a hat.\n- The standard symbol for seconds is “s”, not “sec”\n\nPage 4:\n- Figure 1 (right) is Figure 2\n- “finite-sample approximations”\n\nPage 5: “ground truth”\n\nPage 7:\n- Text in Figure 3 should be in bigger font\n- In Table 1 the results for “10-MLA” are reported twice. This could be reformatted to save space and include more results.\n\nPage 8:\n- Table 4 is Table 2 or 3\n- Y-axis labels are missing in Figure 4 (bottom)\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'oral_presentation': ['Consider for oral presentation']}"		Skxq9YqJg4	H1ld_Nud7V	MIDL.io/2019/Conference/-/Paper36/Official_Review	[]	1		['everyone']	Skxq9YqJg4	['MIDL.io/2019/Conference/Paper36/AnonReviewer3']	1548416112361		1548856728812	['MIDL.io/2019/Conference/Paper36/AnonReviewer3']
219	1548415461459	"{'pros': '- This study is well-motivated: reconstructing MR maps from MRF fast scans provide a useful tool for wide-range of medical applications.\n- Clarity: the text is very clear and understandable with enough necessary technical details.\n ', 'cons': '- Limited contribution with respect to the previous work: this manuscript offers limited methodological contribution with respect to an already published paper from the same authors (Balsinger et al., 2018). The extra contributions of this manuscript are 1) application on a larger dataset (from 6 to 95); 2) a \'yet\' new architecture; and 3) extra analysis on the size of receptive fields and temporal frame importance.  While the first extra contribution is very valuable, the second and the third does not add much due to technical soundness, sub-optimal experimental design, and partial reporting of results (see the followings).\n- This study suggests a new CNN architecture for patch-wise MR reconstruction from spatio-temporal MRFs using the under-experiment data with limited size. This has recently become a common practice, and an unfortunate pitfall, especially in the medical imaging context due to ""Architecture-Data Bias"" problem in comparison with other existing methods. In other words, the authors tend to compare their already very-fine tuned architecture (with a massive number of parameters and hyperparameters) -on a rather small benchmarked data- with existing architectures that are again fine-tuned with ""other"" datasets. Without any surprise every time we try this we will come up with better results from every respect compared to competing approaches (same as table 2). There is a pressing need in the community to raise awareness about this problem. The author might convince me to some degree to change my mind about the presented results if they also present the results for the proposed architecture in Balsinger et al., 2018 on this dataset.\n- The authors proposed to concatenate the real and imaginary parts of input in the time dimension. This choice seems to oppose the natural spatio-temporal structure of data and blurs the temporal frame importance analysis. I would suggest to consider the real/imaginary features as a new dimension in data and use 3D-CNN. Analyzing the importance of real and imaginary parts in the reconstruction process would also be a nice research question.\n- Analyzing the influence of receptive field is very interesting but some arbitrary choice made by the authors in the analysis and reporting results put the results under a question. For example, the authors opted to only report the results for T1_H2O and not for FF and B. Another example is to use 3 \\times 3 convolution for 13 \\times 13 receptive field! Why? I would also like to see results for bigger receptive fields e.g., 20 \\times 20.\n- A minor comment is the quality of Figure 2. The left-panel is not informative as it shows similar reconstruction across different methods. The numbers in the colorbars are so tiny and not readable.', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		HyeuSq9ke4	S1g6kGOdQ4	MIDL.io/2019/Conference/-/Paper37/Official_Review	[]	2		['everyone']	HyeuSq9ke4	['MIDL.io/2019/Conference/Paper37/AnonReviewer1']	1548415461459		1548856728548	['MIDL.io/2019/Conference/Paper37/AnonReviewer1']
220	1548413190617	{'pros': 'The authors combine DL and computer vision methods to digitally stain confocal microscopy images to generate H&E like images. The aim for this work is to provide an image that is familiar to the pathologists such that it will remove the need for specific training for CM interpretation. \n\nPros:\n1-\tIf this approach is accepted by the community, it could remove the need for additional training to the pathologists. This will potentially bring us closer to rapid evaluation of lesions during surgical operation using fast CM.\n2-\tTwo step approach combining despeckling and generative networks are reasonable for the task.\n3-\tQualitative stained image results look promising \n\n', 'cons': 'Cons:\n1-\tMedian filter is used after the despeckling network, however it is not clear the added benefit of using median filter in despeckling process. Error measures presented in Table 1 needs to help readers to identify the benefit of the proposed neural network. The authors should validate their selection of two step approach (NN + filter) compared to an end-to-end FCN (with an additional loss like TV) for the despeckling network.\n2-\tIt is not clear why the histology images were used for denoising network training. Even though it is mentioned by the authors that these images resemble to noisy RCM, this should be either referenced or shown. \n3-\tPlease provide an evidence to support the positive effect of choosing an augmentation of size 512x512 after 50 epochs in Section 3.2. \n4-\tThe authors conclude that the despeckling NN is crucial to obtain realistic images, however, the results presented in Figures 8 and 9 do not provide enough information to support this conclusion. For example, it is not clear what are the non-desirable artifacts, where are the eliminated nuclei and why the network has a harder time to learn. The authors should provide support to these conclusions. For instance, Figure 9 needs to use the same images presented in Figure 8 to provide enough support for the need of despeckling network.  In addition, images representing eliminated nuclei using noisy RCM images should be presented with their counterpart using despeckling network. \n5-\tObtaining quantitative comparison results for staining accuracy is not feasible due to the reasons clearly defined by the authors. It is necessary to provide more qualitative information regarding the staining results in addition to confirmation from two expert pathologists. Please provide results of the inter-rater reliability of two pathologists using a point scale on the quality of image digital staining.\n6-\tI suggest the authors to use train validation and test split or a cross-validation, since the results presented here are from a validation set without a test set. This could potentially add a bias to the results presented here.  \n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		rJxj3vUel4	r1lyzKDOXN	MIDL.io/2019/Conference/-/Paper109/Official_Review	[]	2		['everyone']	rJxj3vUel4	['MIDL.io/2019/Conference/Paper109/AnonReviewer2']	1548413190617		1548856728261	['MIDL.io/2019/Conference/Paper109/AnonReviewer2']
221	1548406506946	{'pros': 'This nicely written and enjoyable paper proposes a loss function focusing on boundary errors as a complement to classical regional scores of segmentation overlap and presents experiments on data\n\nThis paper is well written and easy to follow. The motivation of the proposed work is well presented and the adopted method clearly detailed and well illustrated.\n\nResults are very encouraging and the potential generalisability of the use of this additional loss term is high increasing the potential significance of this piece of work.', 'cons': 'A few points remain questionable and would benefit further clarification\n\nMethods\n- From equation 5 it seems that the absence of segmentation output will yield a null value for the loss. Is this a truly desirable behaviour?\n- In case of multiple, potentially coalescing objects and/or when the border function is of a complex shape, the closest point in distance may not be the appropriate one to consider for the comparison. Is an object defined constrained possible in that situation?\n\nExperiments\n- Could you please confirm that in the validation set for the WMH challenge, elements from all three scanners were used? Could you give the range of lesion load in these cases? Was it chosen to reflect the existing distribution?\n- Although the choice of 2D may seem reasonable for data with highly anisotropic resolution as in the ISLES challenge, this choice is more questionable in the WMH challenge where data is 3D. Moreover, the objects to segment being volumetric, the experiment would be much more interesting when going to the tridimensional complexity.\n- To complement the experiments, it would be interesting to observe the behaviour of the boundary loss alone and of a case of training with fixed weight.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		S1gTA5VggE	SkxQgkLuQV	MIDL.io/2019/Conference/-/Paper97/Official_Review	[]	2		['everyone']	S1gTA5VggE	['MIDL.io/2019/Conference/Paper97/AnonReviewer1']	1548406506946		1548856728047	['MIDL.io/2019/Conference/Paper97/AnonReviewer1']
222	1548401414210	"{'pros': ""- The authors present a new architecture for the segmentation of the hippocampus from 3D brain MRI images, trained on an in-house manually annotated dataset. Three separate FCNs are trained to segment 3D MR volumes, one for each 2D view in the sagittal, coronal and axial directions, and a consensus method is used to select a class for each voxel. Worthy of note is the  proposed 'extended 2D' approach which uses input patches that include neighbouring slices to the current 2D slice (for a given direction), combined with the use of pre-trained weights from a VGG11 network trained on ImageNet.\n\n- Methods are clearly explained, including thorough reasoning of the choices for the architecture and hyper-parameters.\n\n- Results show clear improvements in segmentation Dice with the addition of each selected design choice, namely data augmentation, the inclusion of residual blocks, using an 'extended 2D' input, and using pre-trained VGG11 ImageNet weights. Results also show promisingly high Dice scores for hippocampus segmentation, although only on a limited in-house dataset without direct comparison to other state-of-the-art models trained and tested on the same data.\n\n- While further experiments described below would strengthen the evaluation of the proposed approach, the proposed novelties and clarity of explanation would still benefit the community.\n"", 'cons': ""- While the authors use Hippodeep as a benchmark against which they compare results of the proposed model on a public dataset, namely CC-359, they do not actually report the Dice values achieved. This comparison is also weakened by the absence of ground-truth segmentations in the public dataset, and Hippodeep do not claim to produce human-level segmentation accuracy. The example shown in Figure 5 could simply be a hand-picked case where Hippodeep and the proposed method produce a very similar result. If the aim was to show that the proposed method can generalise to another dataset, a more thorough evaluation is required. The ADNI dataset is accessible to researchers, used in (Xie and Gillies, 2018), and contains ground-truth segmentations of the hippocampus for such an evaluation to be made.\n\n- 3D CNNs are used in the referenced papers for the models Hippodeep and DeepNAT, and have been used in many of the top performing approaches for different 3D MRI image segmentation tasks `(e.g. by winners of the Brain Tumour Segmentation Challenge 2018, and Ischemic Stroke Lesion Segmentation Challenge 2018). The advantages of using pre-trained VGG11 weights + a tri-planar 'extended 2D' approach are clearly shown by the authors over using tri-planar 2D FCNs with or without pre-trained weights. However, a head-to-head comparison with an off-the-shelf 3D U-net, or the 2.5D triplanar approach in (Xie and Gillies, 2018), trained and tested on your in-house dataset would strengthen the evaluation.  \n\n- Was there some advantage of using a sigmoid activation on the network output over the more common choice of a softmax activation?  \n\n- A final spell-check would benefit the readability of the manuscript."", 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		rkg6MIRa1V	ByxCWo4uQN	MIDL.io/2019/Conference/-/Paper11/Official_Review	[]	2		['everyone']	rkg6MIRa1V	['MIDL.io/2019/Conference/Paper11/AnonReviewer3']	1548401414210		1548856727828	['MIDL.io/2019/Conference/Paper11/AnonReviewer3']
223	1548308068992	"{'pros': 'This paper addresses a significant problem that entails automatically detecting the magnetic resonance imaging sequence, while focusing on T1-weighted, contrast enhanced T1-weighted, T2-w, PD-w and T2-FLAIR images. This is a time-consuming process in several large-scale biomedical studies and authors made their trained models and code publicly available to help researchers tackle this problem more efficiently.', 'cons': ""- Some of the preprocessing steps are unclear. It is mentioned that all images are resampled to 256 x 256 x 25 and then split to 25 slices. Since the voxels in the resulting images are not isotropic, I guess there is a significant impact of the acquisition plane's orientation on the resulting image quality. \n- Using a deep neural network for such a task seems a bit of an overkill. I wonder how this approach compares to purely using the sequence parameters along with some statistics of the image intensities (in total or per slice).\n- Based on the results in Table 4 and the overviews of the data in Tables 1 and 2 it could be that what is actually predicted is the orientation of the image.\n- How would the authors go about the data imbalance? it would be very valuable to provide insights on that.\n- Why is the reason behind using a much smaller training set compared to the test set?\n- Relevant literature is not sufficiently covered."", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		rkg7DD-gl4	Hke6wR6UX4	MIDL.io/2019/Conference/-/Paper63/Official_Review	[]	3		['everyone']	rkg7DD-gl4	['MIDL.io/2019/Conference/Paper63/AnonReviewer2']	1548308068992		1548856727575	['MIDL.io/2019/Conference/Paper63/AnonReviewer2']
224	1548378321734	"{'pros': 'The application for the breast MRI is new and may have some potential clinical impact.', 'cons': '1. The novelty of the proposed method is very limited using a GAN based model.\n\n2. The authors have neglected large amount of previous research in this field and therefore also overlooked the existing research have already done the GAN based MRI reconstruction. For example, the following work must be considered carefully with proper citations and the authors must point out what are the novelties of the current study compared to the previous ones as the proposed synthesise is essentially a similar inverse problem as the MRI reconstruction and super-resolution.\n\nJo Schlemper, Jose Caballero, Joseph V Hajnal, Anthony N Price, and Daniel Rueckert.\nA deep cascade of convolutional neural networks for dynamic mr image reconstruction.\nIEEE transactions on Medical Imaging, 37(2):491-503, 2018. ISSN 0278-0062.\n\nGuang Yang, Simiao Yu, Hao Dong, Greg Slabaugh, Pier Luigi Dragotti, Xujiong Ye,\nFangde Liu, Simon Arridge, Jennifer Keegan, and Yike Guo. DAGAN: Deep de-aliasing\ngenerative adversarial networks for fast compressed sensing MRI reconstruction. IEEE\ntransactions on medical imaging, 37(6):1310{1321, 2018. ISSN 0278-0062.\n\nTran Minh Quan, Thanh Nguyen-Duc, and Won-Ki Jeong. Compressed sensing MRI re-\nconstruction using a generative adversarial network with a cyclic loss. IEEE transactions\non medical imaging, 37(6):1488{1497, 2018. ISSN 0278-0062.\n\nSchlemper J. et al. (2018) Stochastic Deep Compressive Sensing for the Reconstruction of Diffusion Tensor Cardiac MRI. In: Medical Image Computing and Computer Assisted Intervention – MICCAI 2018. (pp 295-303). Springer, Cham.\n\nYu, Simiao, et al. ""Deep de-aliasing for fast compressive sensing MRI."" arXiv preprint arXiv:1705.07137 (2017).\n\n3. The proposed Human Reader Test has been proposed as \'Mean Opinion Score\' in this study:\n\nMaximilian Seitzer, Guang Yang, Jo Schlemper, Ozan Oktay, Tobias Wrfl, Vincent Christlein, Tom Wong, Raad Mohiaddin, David Firmin, and Jennifer Keegan. \nAdversarial and perceptual refinement for compressed sensing MRI reconstruction.\nIn International Conference on Medical Image Computing and Computer-Assisted Intervention,\npages 232-240. Springer, 2018.\n\n4. The W-GAN based MRI reconstruction and super-resolution has been applied in this study:\n\nZhu J et al.. Lesion Focused Super-Resolution. arXiv preprint arXiv:1810.06693. 2018 Oct 15.\n\n\n5. There are still concerns of GAN based methods that they may hallucinate unrealistic features.', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		BJlpmkJxl4	HygcRgkOQN	MIDL.io/2019/Conference/-/Paper48/Official_Review	[]	2		['everyone']	BJlpmkJxl4	['MIDL.io/2019/Conference/Paper48/AnonReviewer3']	1548378321734		1548856727360	['MIDL.io/2019/Conference/Paper48/AnonReviewer3']
225	1548377423292	"{'pros': '1. The paper is well written with cleared method description and experimental settings.\n\n2. Well-organised comparison studies.\n\n3. The proposed method is novel.', 'cons': '1. The major problem I found for the experiment is that the undersampling pattern is less realistic. For example, the 2D Gaussian undersampling.\n\n2. Some important and relevant studies are neglected, and should be considered to add into the references:\n\nSchlemper J. et al. (2018) Stochastic Deep Compressive Sensing for the Reconstruction of Diffusion Tensor Cardiac MRI. In: Medical Image Computing and Computer Assisted Intervention – MICCAI 2018. (pp 295-303). Springer, Cham.\n\nYu, Simiao, et al. ""Deep de-aliasing for fast compressive sensing MRI."" arXiv preprint arXiv:1705.07137 (2017).\n\n3. The authors should stick on \'undersampling rate\' or \'sampling rate\', don\'t mix them to create some confusion.\n\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'special_issue': ['Special Issue Recommendation'], 'oral_presentation': ['Consider for oral presentation']}"		HJeJx4XxlN	SJgPI6RD74	MIDL.io/2019/Conference/-/Paper85/Official_Review	[]	3		['everyone']	HJeJx4XxlN	['MIDL.io/2019/Conference/Paper85/AnonReviewer3']	1548377423292		1548856727148	['MIDL.io/2019/Conference/Paper85/AnonReviewer3']
226	1548371393988	"{'pros': ""The paper is of high quality, clarity, and originality. I have not seen a combination of activation maps, with a statistical shape prior and unets for segmentation before and I think it is a smart idea. The experiments highly support this idea. I'm not familiar with the task at hand so I can not judge on this. For the medical vision community, it seems to be significant for me since it helps to deal with few data points for complex problems.\n\nI'm not sure if I got it right that M-2DUnet is basically the same network but trained with manual delineations instead of the weakly-supervised approach with activation maps. In case that is correct it is actually nice and impressive that the Wilcoxon test does not show a significant difference for that case and I think it is worth to stress that more - even in the abstract and the conclusion!\n\nThe paper is slightly above the page limit, this is mainly due to figures and table and I think it is adequate."", 'cons': 'The paper does not state to release source code and the data is not publicly available. Therefore it might be hard to reproduce.\nOne experiment that could be interesting would be to compare the approach to using the activation maps directly instead of the tumor location - that way we would get an insight if the shape prior actually helps.\nIt has some minor typos and should be proofread or put through grammarly (e.g. methods has been, software( Varian, Figure7)\nThe figures have artifacts from a spellchecker.\n', 'rating': '4: strong accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'special_issue': ['Special Issue Recommendation'], 'oral_presentation': ['Consider for oral presentation']}"		ByldG19Ry4	SJgcaSaw7N	MIDL.io/2019/Conference/-/Paper16/Official_Review	[]	2		['everyone']	ByldG19Ry4	['MIDL.io/2019/Conference/Paper16/AnonReviewer3']	1548371393988		1548856726935	['MIDL.io/2019/Conference/Paper16/AnonReviewer3']
227	1548370817916	{'pros': 'The paper aims to classify skin lesions as accurately as possible with fewest labeled observations in an active learning setting. It presents two criteria used to incorporate the informativeness and representativeness of an image in order to select the next most useful one to label. The authors train a ResNet-101 network and compare their results with state-of-the-art method (AIFT) on two binary classification problems from ISIC 2017 Skin Lesion Classification Challenge.\n\nThe authors have done a good job disentangling the effects of informativeness and representativeness in their Selective Annotation(SA) part as well as characterizing the effect of aggregation in Aggregative Supervision (AS). The differences in accuracy improvement of SA, SA combined with AS, and AIFT selection methods are demonstrated. A problem of introducing potential correlations in the selected observations when the selection criteria are based on the same CNN model is exemplified. The authors’ solution is to decouple the SA part so that the informativeness criterion depends on the CNN prediction uncertainty, while the representativeness criterion does not take CNN predictions into account.\nAggregative Supervision is essentially a simple and working intra-class clustering (stacking images of the same class into 2 x 2 collages)  used for image augmentation, which is shown to improve the results further.\nAdditionally, an illustrative introduction with the overview of the existing Active Learning strategies utilized in CNNs is provided. In conclusion, the authors achieve as good results (in AUC, accuracy and other metrics) as the challenge winners, but with at most half of the training data.\n\n', 'cons': 'Weaknesses:\nThe main criticism of the paper is the lack of novelty in the mentioned methods.\nSelective Annotation method combines observations selected according to informativeness (S_I) and representativeness (S_R). The informativeness criterion used in the paper is already known as the least confident uncertainty sampling, while the representativeness criterion which combines PCA and LSH is very similar to clustering from a similarity matrix. Combining them together into SA yields a generic information density heuristic, which has been extensively studied (see e.g. Burr S.“Active Learning”, pp 47-49 and references therein). The paper also lacks a basic mathematical rigor and clarity in a number of paragraphs and definitions. Similarly, the writing style and existing errors (see the list below) make the paper difficult to understand and may require an additional effort from the reader. Major revision and proofreading are recommended.\n\nList of Errors:\n    1) Section 2.1, p.3. The definition of least confident uncertainty sampling is not mentioned or explicitly cited, while its description is vague. Eq. (1) must include prediction probability.\n    2) Section 2.1, p.4. “In order to avoid using the same CNN features both for informativeness and representativeness sample selection, we compute the first principal component of the image as its data diversity feature representations.” - unclear formulation. What justifies taking only the first PC of an image?\n    3) Section 2.1, p.4. “As the PCA features are independent of the trained model” - unclear formulation. While it’s clear that PCA is applied to the raw images in the unlabeled set D_U and the model M is trained on the annotated images set D_A, it’s unknown if the Principal Components are statistically independent from the model’s features.\n    4) Section 2.3, p.5. Algorithm 1, line 8. To generate augmentations, IntraClassAug is applied on the set D_A*, while figure 1 on page 3 shows and describes that “we generate augmentations D_{A^aug} of all the gathered annotated data pool D_A” - contradiction.\n    5) Figure 1, p 3. Caption. S_R and S_I are mixed up.', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		rygLeM3C1N	rJl5YQ6v7N	MIDL.io/2019/Conference/-/Paper23/Official_Review	[]	1		['everyone']	rygLeM3C1N	['MIDL.io/2019/Conference/Paper23/AnonReviewer1']	1548370817916		1548856726718	['MIDL.io/2019/Conference/Paper23/AnonReviewer1']
228	1548368486686	"{'pros': 'The problem seems interesting to me, and it has been hardly tackled in the literature. Training a model to discard uninteresting slices in a 3D CT scan, so that a doctor could focus attention on suspicious slices is meaningful. In addition, incorporating a bidirectional LSTM to gain consistency in terms of classifying consecutive slices is a logical step in this context.', 'cons': '1) In my opinion, there is a concerning lack of comparison against other approaches in the experimental evaluation section. The authors claim in the abstract that the method outperforms existing CAD methods, but no comparative results are reported to support this claim. At the very least, a standard CNN should have been trained for the same problem and results reported as a baseline, in my opinion.\n\n2) The method was tested on two different datasets. It seems to me however that the model was trained/tested independently on each dataset. It would have been more interesting to analyze if a model trained on the 3AGHC dataset performed well also in the LIDC-IDRI dataset without retraining, in a cross-dataset experiment.\n\n3) Figures 5 and 6 are hard to read. There is no legend and the fontsize for the numbers in the x/y axis is tiny.\n\n4) The threshold for declaring nodule presence was manually selected to be t=0,4. I guess it may have been better to use a principled approach to select this threshold, through ROC analysis.\n\n5) The discussion section seems to me a combination of a literature review plus an introduction to the problem, rather than a discussion on the positive and negative aspects of the approach with potential future work.\n\n6) Few details about model architecture, hyperparameter tuning and training are given, specially for the LSTM module. Since apparently there is no open-sourced code/data, this hinders reproducibility substantially.\n\n7) Some claims in the introduction do not have any supporting reference. For instance, ""The centers for medicare and medicaid services proposed that medicare beneficiaries ... will have coverage for lun cancer CT screening""', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		B1gQDNA3JE	r1gJd92wmN	MIDL.io/2019/Conference/-/Paper7/Official_Review	[]	2		['everyone']	B1gQDNA3JE	['MIDL.io/2019/Conference/Paper7/AnonReviewer1']	1548368486686		1548856726469	['MIDL.io/2019/Conference/Paper7/AnonReviewer1']
229	1548360505256	{'pros': '- The authors propose a novel method to jointly perform the segmentation of six brain tissue classes and white matter lesions employing the hetero-modal MRI volumes available in different datasets. The proposal allows to combine datasets composed only by labelled T1 scans (usually related with subjects without anatomic lesions) together with datasets composed by T1 and FLAIR acquisitions (related with subjects with brain injuries) in which only the lesions are labeled. \n\n- Due to the nature of the clinical datasets, this kind of approach must be welcome. The need is clear and is becoming a hot topic in this field. In fact, another similar approach for the same problem has been presented for MIDL2019 → https://openreview.net/forum?id=Syest0rxlN \n\n- As the authors remark, they elegantly cope with a real problem in which three branches of machine learning: Multi-Task Learning, Domain Adaptation and Weakly Supervised Learning meet. \n\n- The model is tested with T1 and FLAIR volumes but should already work with more modalities. \n\n\n', 'cons': '- As I mentioned above, a closely related paper have been submitted for the MIDL. I truly believe that the authors should compare themselves against https://openreview.net/forum?id=Syest0rxlN denoting advatages and disadvantages. This could be really helpful in order to help the chairs to make a decision. \n\n- The sections 2.4 and 2.5 are halfway between proper mathematical justification of the employed tools and the purpose of using them, which sometimes makes the text difficult to understand (even taking into account that the concepts are not the simplest). Due to the recommended conference page limit, simpler sentences along with the maths could help with this issue.\n\n- The work employs the statistical formulation for the loss definition, cite the beautiful Kendall & Gal and Bragman jobs but at the end, employs the mode of the distribution as predictor. Could the authors go all the way and provide (in a near future) a whole probabilistic solution?. Besides, in my personal opinion, the method would be better understand it employing this kind of formulation.\n\n- The evaluation is OK but it would be more complete by adding a comparison (where is possible) with the traditional approaches (SPM, FSL, etc.) and specially, providing any measure of how the results are distributed (standard deviation, boxplot, etc.)\n\n- Could the authors comment on the possible effects of including more modalities?\n\n- The work is nice, could be talk material (and for sure will be part of Medical Imaging Analysis or a similar journal soon) but the authors have employed 11 pages. The text contains some unnecessary blank spaces, overdimensioned tables and figures which could be structured much more efficiently in order to save space. Summarize an interesting work is always difficult but is must be done in order to ease the work for the scientific community. For this reason, I cannot propose the paper for a talk.', 'rating': '4: strong accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		HJeZW_QxxN	rklZHs5wmE	MIDL.io/2019/Conference/-/Paper87/Official_Review	[]	1		['everyone']	HJeZW_QxxN	['MIDL.io/2019/Conference/Paper87/AnonReviewer1']	1548360505256		1548856726252	['MIDL.io/2019/Conference/Paper87/AnonReviewer1']
230	1548349712470	"{'pros': 'The authors investigate a neural architecture search (NAS) framework for U-Net optimization in the setting of 3D medical image segmentation. \n\nTo be more specific, what is subject to the automatic tuning is the precise arrangement of operations within the network ""cells"" (the sequence of pooling / concat / conv / skip connections within a layer); the high-level design of the encoder-decoder path is fixed (in line with that of a standard U-Net).\n\n| Strengths:\n- I find this line of research interesting. It has also objectively received a lot of recent interest in the ML community, with the potential to design new architectures that are both simple and with more expressive power than what has been proposed so far.\n- The related work seems to be adequately cited in parts 1 and 2.\n- The application to 3D image segmentation presents serious challenges in itself, and it indeed seems to have good novelty, although I am not an expert. The method is sound, in particular the use of the Gumbel-softmax trick is a good answer to the challenge of scaling to large architectures.\n\n| Weaknesses:\n- Key claims are made without basis. The interpretation of the experimental validation is distorted to suit the claims.\n- It is unclear how to interpret the (slightly) higher score of ""SCNAS (transfer)"" in Table 1; and whether it actually makes a case for SCNAS as stated in the paper.\n- There is a lot of repetition/verbosity in the first 6 pages. The proposed approach is reintroduced 3 times in similar terms. The respective focus of introduction vs. related work is unclear. The contributions of the paper are in turn less clear. Whether the contribution is specifically in the application to 3D medical image segmentation or also w.r.t. the methodology itself could be clearer.\n\n| Main comment:\nFrom the abstract to the experimental section, to the conclusion, the authors make a repeated claim w.r.t. performance that is contradicted by experimental validation:\n- Abstract: ""On the 3D medical image segmentation tasks with a benchmark dataset, an automatically designed 3D U-Net by the proposed NAS framework outperforms the previous human-designed 3D U-Net as well as the randomly designed 3D U-Net""\n- Introduction: ""Experimental results [...] show that in comparison to the previous human-designed 3D U-Net, the network obtained  by the proposed scalable NAS leads to better performances""\n- Experiments/Results: ""Table 1 shows that the SCNAS produced better architectures than the (human-designed) 3D U-ResNet as well as the randomly designed 3D U-Net in terms of the overall performances on all three tasks.""\n- Conclusion: ""Empirical evaluation demonstrates that the automatically optimized network via the proposed NAS outperforms the manually designed 3D U-Net.""\n\nHowever, in Table 1. all the scores are within (plus or minus) a fraction of dice or a dice point (SCNAS transfer excluded, see comments below.).\n\nThe paper would be stronger (i) without the contradiction between claims and results; and (ii) if the emphasis was shifted away from the (lack of) experimental evidence for improved performance of the NAS, to a more thorough empirical analysis of the auto-ML mechanism, with an open discussion. \n\n\n| Miscellaneous:\n- ""It is noted that unlike these architecture hyperparameter optimizations, we use the complete NAS to obtain the entire topology of the network architecture in this work."" Is that the case? My understanding is that the high-level architecture (U-Net) is fixed. The distinction is not a minor one in terms of outcome. The proposed approach optimizes over cell architectures, where a cell is e.g. an encoding unit in the encoder path. Same nature cells are further restricted to the same topology. Such optimization appears to yield rather intricate cell designs (cf. appendices) but does not significantly improve performance (Table 1.). \n- It is unclear how to interpret the (slightly) higher score of ""SCNAS (transfer)"" in Table 1. Are baselines trained on 20 (heart) / 32 (prostate) images, vs. ""SCNAS (transfer)"" being trained on 400+ images and fine-tuned on the relevant datasets? If so, what numbers are obtained for baselines when (pre)training and fine-tuning in a similar fashion? \nRight now, as SCNAS performs similarly to the baselines, with only the transferred architecture earning a couple of DSC points, a natural interpretation is that the experiments make a (limited) case for pretraining on a bigger dataset (rather than for autoML).\n- The paper could elaborate some more on Algorithm 2 (the sampling of two operations for computational reasons) and what the concrete effects of this compromise are.', 'cons': '.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		S1lhkdKkeV	S1xdzbdD7N	MIDL.io/2019/Conference/-/Paper34/Official_Review	[]	3		['everyone']	S1lhkdKkeV	['MIDL.io/2019/Conference/Paper34/AnonReviewer1']	1548349712470		1548856726032	['MIDL.io/2019/Conference/Paper34/AnonReviewer1']
231	1547834520516	{'pros': 'The paper introduces an autoencoder-like network architecture to be used for atlas building/application purposes in a LDDMM setting. Essentially, a deep architecture is defined that:\n(1) allows to estimate an unbiased atlas/template of an image population during network training in an unsupervised fashion\n(2) when trained can be used to estimate mappings between formerly unseen images and the atlas\n\nTo do so, an approximation of the conventional LDDMM atlas building objective is proposed, which is solved by the network/training process presented in the paper. The architecture itself consists of an encoder and a decoder part. The encoder maps an input image to a low-dimensional latent space while the decoder maps a point of the latent space to a deformed version of the atlas that is most similar to the input image. It is important to note that the decoder is composed of three different components (1. latent space to momentum field mapper, 2. EPDiff solver, and 3. atlas image warper) and only the first component is actually learned.\n\nOverall, the paper addresses two important problems (diffeomorphic image registration and atlas building) that have not gained much attention by the deep learning community so far. I, therefore, agree with the statement made in the paper that it is the first to introduce a deep learning-based atlas building method using LDDMM (DL-supported LDDMM registration itself was also used by Yang et al./Quicksilver). The approach presented is quite interesting and well in the scope of MIDL as it allows the integration of components of the  well-known LDDMM framework (i.e. EPDiff integration) directly into network architectures to facilitate, for example, deep learning-based computational anatomy methods where the use/estimation of diffeomorphic mappings is crucial. Furthermore, I also like the fact that the authors actively support the idea of open and reproducible research by making their source code available on github (I took a look, but did not review the code) including a PyTorch-module for EPDiff. The evaluation presented can be characterized as somewhat preliminary with only limited experiments (only conventional vs. new atlas building methods for affine and non-linear atlas building are compared) and no real quantitative results. However, the main problem I see with this paper is related to its clarity about a key part of the method presented. \n\nAs I see it, the key part of the paper in terms of novelty is Sec. 2.2, which describes the new atlas building method and how it is solved by using diffeomorphic autoencoders. The introduction of Eq. 8 is easy to follow and the basic description of the autoencoder approach (bottom part of p. 4) is also intelligible. However, at least to me it is somehow unclear how \\overline{I} (the atlas image) is actually computed during the training process. Is it directly learned on a voxel/pixel basis, generated by applying the estimated transformations to the input images, or ...? Maybe I am missing something but this detail is crucial and needs to be added to the paper or clarified if present. I also recommend to describe the network architecture in more detail in Sec. 2.2 as it is hard to understand which parts of the decoder are actually learned and which are static without referring to Sec. 3. This could, for example, be done by moving parts of Sec. 3.2/3.3 to this Section and by improving Fig. 1.\n\nTo sum up, I like the paper and I think it should be presented at MIDL, but the part describing the training-based atlas building should be revised prior to publication.\n\n\nPros:\n\n- LDDMM-based deep learning approach for image registration\n- Atlas building problem solved by training an autoencoder network\n- PyTorch code publicly available', 'cons': 'Cons:\n\n- Description of the novel parts of the method (partially) unsatisfactory\n- Preliminary evaluation', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		Hkg0j9sA1V	SyWiNc1XN	MIDL.io/2019/Conference/-/Paper19/Official_Review	[]	1		['everyone']	Hkg0j9sA1V	['MIDL.io/2019/Conference/Paper19/AnonReviewer3']	1547834520516		1548856725774	['MIDL.io/2019/Conference/Paper19/AnonReviewer3']
232	1548242634700	{'pros': '\nThis paper proposes to use a CNN architecture  to classify the magnetic field strength and vendor type of brain MR images. The authors used a VGG architecture to extract features . These features are used by SVM and k-nearest neighbour methods to do the final classification.  A balanced dataset in terms of classes consisting of 359 brain MR images was used for training and validation. A separate dataset of 210 scans was used the evaluate the results. An average accuracy of %98 is achieved with proposed machine learning algorithm.\n\nI believe there is a need for further experiments with additional methods of comparison to truly understand the nature of the problem. Besides, vendor and field strength information is usually stored in the image tag of the datasets, so I have a difficult time to understand on what is the clinical relevance of the problem that is being answered. Moreover, the proposed machine learning framework is not novel and there are questions marks regarding design choices.\n', 'cons': '\n1-The main task of the paper is not a significant problem in my point of view. The problem of identifying field strengths and vendor types can be easily addressed using image information tag (e.g. DICOM tag).\n2-Theres is not any method of comparison provided in the paper. I strongly suggest the authors to do extensive analysis with state of the art machine learning methods for classification (e.g. Decision Trees, Random Forests etc.).\n3- What would be the results using a CNN directly with some dense layers at the end (e.g. LeNet architecture)? Why do the authors include kNN and SVM algorithms at the end of the network? Is there any performance increase achieved by this?\n4- How does the CNN trained to extract features? Are the authors using a pre-trained network architecture to extract features?\n5-Vendor type and field strength are classified separately in the current experimental setup. One additional test could be to generate 6 classes combining these two labels for each image. It would be interesting to see the performance of the algorithm using a single classifier for both tasks.\n6-As far as I understand the test dataset (CNS) only has GE vendor type and 3T images. I think to better evaluate the algorithm, the test dataset should include cases for all the classes. Please elaborate on this.\n\n\nMinor suggestions\na-Please break the first sentence in the abstract for clarity.\nb-Please increase the image quality of Fig.3 and Fig.4.\nc-For testing dataset abbreviation CNS was used. Please explain what CNS stands for.\n\n\n', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		HylnnSQlgV	HJeX0ApS7N	MIDL.io/2019/Conference/-/Paper86/Official_Review	[]	1		['everyone']	HylnnSQlgV	['MIDL.io/2019/Conference/Paper86/AnonReviewer1']	1548242634700		1548856725517	['MIDL.io/2019/Conference/Paper86/AnonReviewer1']
233	1548347524543	"{'pros': 'This paper proposed a method to classify histological whole-slide images into one of the six tissue types based on deep learning and multiple resolutions (magnification rates). It is a very clear paper, well written, and easy to understand. Details are adequately provided in the limited space of this conference paper. The performance is considerably high, and multiple magnification rates did provide additional information to make the classification even better than the mono-magnification strategy. The deep learning framework was reasonably designed and the parameter effect was sufficiently compared. The sample size is decently adequate and training/validation/testing are rigorously designed. Imbalance problem was correctly handled. Therefore, it is a really exceptional paper showing a good application of deep learning in real clinical problem, which deserves a praise. \n\nThe pros are: 1) large sample size, 2) rigorous experimental design, 3) correct model selection and parameter tuning, 4) the implementation and integration of multi-resolution images. \n', 'cons': ""Some minor issues, constructive suggestions, and future works:\n1. The intensity calibration or normalization to the WSI and tiles should be described. \n2. One rater labeled the image, which could lead to some extend of bias and human errors. Thus, the accuracy could be not “the higher, the better” but could have bias. More raters should be enrolled and their consistently labeled results should be considered as gold standard. \n3. Application issue: The whole task is relatively easier than grading task (identification of different tumor cells). The future work could be, based on the current model, another deep classifier should be trained to do this harder (but more useful) task.\n4. Does the method work on tiles that have been extracted from the WSI, or it directly works on the WSI?  It seems that a pathologist will first manually label the 400x images with 1 of the 6 labels using contours, then the non-overlapping tiles are extracted based on the contours and the algorithm works on these tiles. In future, since the model has been trained, can this whole process be fully automated and directly work on WSI itself?\n5. It's not quite reasonable to make images different magnifications include different areas simply in order to make sure their sizes are the same. The motivation should be provided. In my opinion, this is to introduce certain (larger-scale) context information so the algorithm could use to better make decision.\n6. Certain overlap can be used when extracting tiles so that the algorithm could give a heat map with probabilities and class labels so that all these information could be superimposed to WSI to train raters (a possible application) and to guide pathologists to focus on tumor cells to make further grading decisions. \n7. Please verify: Parameter optimization for each of the 3 models are carried out using training + validation sets only? The testing set is only used for final performance evaluation, correct?\n8. Minor: DI-CNN and TRI-CNN have more complex structures with more parameters (more neurons) than MONO-CNN, is this a reason of better performances of them? TRI-CNN has 0 dropout rate, could this affect model generalization and cause over fitting?\n"", 'rating': '4: strong accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'special_issue': ['Special Issue Recommendation'], 'oral_presentation': ['Consider for oral presentation']}"		rJlDYnoJlN	SkxaF_vvQE	MIDL.io/2019/Conference/-/Paper42/Official_Review	[]	2		['everyone']	rJlDYnoJlN	['MIDL.io/2019/Conference/Paper42/AnonReviewer1']	1548347524543		1548856725303	['MIDL.io/2019/Conference/Paper42/AnonReviewer1']
234	1548340757538	"{'pros': 'The paper is well and clearly written. It is somehow original since I have not yet seen networks reconstructing voxel-based shapes from landmarks and vice versa from that resolution. The resolution is impressive. The abstract and introduction are well written and motivated. The paper is slightly above the page limit but I think that is adequate. The paper is reproducible, especially since both, code and data is or will be made available.', 'cons': 'Coming from the shape modeling community the paper did some weird design choices and especially the validation of the approach should be improved.\nMy main criticism is the choice of landmarks as latent representation. This leads to a one-to-many mapping to shapes. The shape modeling community tends to model a posterior distribution in such a case. Usually, the aim is to learn this latent representation and I only see drawbacks in this explicit choice. The task is compared to other approaches fully supervised and therefore the spatial resolution is less impressive than for an unsupervised method learning the latent representation. The statement that this resolution was not reached before should at least be put in context to ""Octree Generating Networks: Efficient Convolutional Architectures for High-Resolution 3D Outputs"" at ICCV 2017 presenting a convolutional decoder reaching a resolution of 512^3.\nThe task of reconstructing a shape from landmarks is well studied e.g. by the modeling of posterior distributions. \nThe weakest part of the paper are the experiments. It is hard to estimate the performance of the approach based on those experiments and since the for me obvious experiments or visualizations (see later) are missing. I honestly expect its performance is pretty bad.\nI would suggest to further work on the paper and especially improve on the experiments.\n\n\nHere some detailed comments and suggestions:\n\nIntroduction: \n- The limitation of classical SSMs are not fair not all models are limited to variation by principal modes - a lot of models allow some additional deformations that are regularized not by the statistics of the training data (e.g. Gaussian processes).\n- The statement that the mandible is one of the most complicated and variable anatomies of the human body is weak - it is not obvious to me why that is the case. I would also not agree that cars, chairs and tables are well-formed shapes. The challenges are different, chairs or teeth for example have the challenge of adding or removing legs/roots.\n- The choice of the network architecture looks arbitrary. Choices in the methods part are not motivated and it basically only contains the architecture and the loss functions. The sentence ""we experimented with many deep neural architectures, one of which is depicted in Figure 1"" is perhaps honest but is supporting a trial and error approach instead of a deeper idea behind the architecture.\n- Section 3.3. is named Experiments  but contains a description of the choosen latent space. Since the learning is fully supervised I would expect that to be part of the methods section.. The actual  experiments performed are described in the results section.\n- The results section contains two different tasks, landmark estimation and reconstruction. I think additional structure with titles would improve readability.\n- Table 1 shows the reconstruction performance given landmarks. The presented values appear extremly high to me. Instead of comparing it to a proper baseline it is compared to the task of segmentation which does not make sense in my eyes. As a simple baseline I would propose to add the thresholded average of the original voxel maps or a reconstruction based on the average landmarks. This would indicate if and how much better the reconstruction is than just taking the average of the data.\n- The average fiducial-to-surface distance measured was 1.89 mm - this again feels quite big. Other publications working on mandible landmarks show the performance per landmark - this could perhaps be added to Figure 2 (since some landmarks are not well defined like the one on the tip of the teeth in front which is not available in the full dataset.\n- For the average surface distance 1.2 mm should also be set in the context of the average as prediction.\n- The shape modelling community came up with some measurements of modeling quality (generalization, specificity, compactness). A full loop would therefore be interesting: New unseen shape -> estimate landmarks -> reconstruct shape.\n- Figure 3 is missing the landmarks - without the given landmarks those reconstructions don\'t help in estimating the quality of the reconstruction.\n- The landmark reconstruction performance of 3.84 again is hard to evaluate without comparison or context. Since no values to compare are given I head to search for one - so I don\'t know if the comparison is fair, but ""Deep Geodesic Learning for Segmentation and Anatomical Landmarking"" (TMI 2018) estimated landmarks from images with an segmentation before - they reach ~ 1mm. Adding a figure to allow a qualitative estimation of the quality could help here. Again I would propose to add a landmarkwise number for this to Figure 2.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BkltUK71xV	rkeCMAHDmN	MIDL.io/2019/Conference/-/Paper31/Official_Review	[]	1		['everyone']	BkltUK71xV	['MIDL.io/2019/Conference/Paper31/AnonReviewer3']	1548340757538		1548856725051	['MIDL.io/2019/Conference/Paper31/AnonReviewer3']
235	1548340328554	"{'pros': '- The motivation behind weak/inexpert labels is indeed an existing issue. \n- It is interesting to see that the authors have proposed to use an additional feature (geodesic maps derived through Fast Marching on ground truth) when constraining the segmentation results. ', 'cons': '- It is not clear how the simulated synthetic inexpert/weak labels correspond to an actual inexpert annotator behavior. It is unlikely that a human inexpert would have a noisy contour in their labels. I would rather expect boundaries with a clear contrast to be well annotated while repeating and consistent errors being made at contours where borders are not so clear, and the expert experience is needed to interpret the actual class borders. Furthermore, if the synthetic weak labels have salt-pepper noise with uniform distribution, it would be expected that the network would learn the ""clean label"" given a sufficient number of training data. \n- It is not obvious to me that the Net_geo autoencoder (trained on transforming distance maps to binary ground truth) would have a comparable latent space representation for distance map (Enc_geo(G_i)) and probability map (Enc_geo(P_i)). The authors should comment on their hypothesis behind why P_i should look similar to G_i at the AE encoded/latent space. \n- Since the challenge results are available online, I would suggest authors to also compare with the scores on the scoreboard from https://acdc.creatis.insa-lyon.fr/#phase/59db86a96a3c7706f64dbfed such that state-of-the-art is visible for the expert labels. \n\n\nQuestions: \n- The term ""growth rate"" in Sec 3.1. Segmentation Network Architecture is ambiguous. Did the authors start with 16 filters and increase by 16 at each level; i.e. having 80 filters at the coarsest level? Please rephrase.\n- Despite having 24GB GPU (P6000), was using 3D UNet a problem for the memory? What was the batch size during training? \n- Is Eq. (1) missing term on output probability map for a class? It is not a binary problem after all.\n- Sec 3, authors should consider stating the full name ""Densely Connected Convolutional Blocks"" with the corresponding reference when mentioning ""dense blocks"" the first time.\n- Sec 3.1, authors say that they ""utilize dense layers to facilitate a better flow of low-level information towards the end of the network"". However, there are no dense layers in Fig 1a for the Segmentation network. In addition, authors should refrain from using ""dense layers"" in this manuscript and stick with FC layers since (1) FC are already defined, (2) it is prone to be mixed with Dense blocks.\n- Page 6, please elaborate on the hypothesis behind ""there is an FC layer in the middle of architecture (instead of convolution  layer) to generate the geodesic deep feature.""\n- Page 6, ""In order to increase the robustness of these features, we exclude the skip connections from the design."" Please comment on how dropping skip connections increase the robustness of features. Also, robustness in which sense?\n- Page 8, Sec 5, which of the compared in Table 1 is the ""state-of-the-art"" method? \n- Were the pathologies taken into consideration during the data split?\n\nComments:\n- Authors should consider updating Fig 1c F_geo with a more useful alternative.\n- Captions should be self-explanatory. Fig 1 caption is not referring to a,b,c. Plus, (c) is completely ignored in the caption. In addition, inside Fig 1, terms ""Groundtruth"", ""Groundtruth B_i"", and ""B_i"" are used interchangeably. Please pick one and stick with it. Furthermore, the legend behind color coding (green and red arrows) should be included in the caption.\n- Sec 3.1. ""This information flow does not only help for better performance but also help in faster convergence of the network."" Please elaborate on faster convergence. \n- Sec 4.3: When defining ""binary AE"", I would suggest the authors clarify that objective function stays the same and only G_i is replaced with B_i from Fig.1.\n- Please give more details on model training phase (e.g. number of training steps, differences in training between compared methods), convergence criteria (e.g. which model was used to evaluate the test set).\n- Results on Table 1 are hardly discussed (also mention bold refers to best score). Consider mentioning by what percentage is the proposed method better than the state of the art. \n- Authors should consider showcasing example geodesic maps generated for a given image in order to support their claim that distance maps created with Fast Marching could prove to have \n- L_geo is defined too late, please include the relevant definition soon after mentioning.\n- L_recon is not defined.\n\nGrammatical/typographical errors:\nPage 4: ""[...] as an extra [...]""\nPage 6: Section 4.3 is referenced instead of Table 1.\nTable 1: ""all methods""', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		r1lkO0u1lV	rkxb_hrw74	MIDL.io/2019/Conference/-/Paper33/Official_Review	[]	1		['everyone']	r1lkO0u1lV	['MIDL.io/2019/Conference/Paper33/AnonReviewer1']	1548340328554		1548856724804	['MIDL.io/2019/Conference/Paper33/AnonReviewer1']
236	1548334365908	"{'pros': 'This manuscript evaluates different data augmentation approaches in order to improve the generalization of CNNs when applied to clinical MRI data. Considering the limited access to data in this context, data augmentation provides a practical solution toward extending the application of DNN to neuroimaging data. Thus, I would consider the motivation behind this research valuable.', 'cons': 'However, this manuscript suffers from a major problem in the experimental design that puts the results under question. In section 2.3 the authors state that for a fair comparison they use a fixed epoch number across different methods during training. This is actually rather unfair than fair. Because as mentioned by the authors and widely known, regularization techniques and data augmentation postpone the overfitting of the model on training data. Therefore, such models obviously need more iterations to converge to the same performance than when they trained without regularization or data augmentation. When training all the models for 100 epochs, as well-shown in Figure 4, we let the model with no augmentation to overfit while we prevent the others from overfitting. Thus, comparing their performance in terms of accuracy is completely unfair. A more fair strategy is to use early-stopping strategy in all cases. Furthermore, the whole experiment must be repeated (at least 10 times) to evaluate the standard deviations of accuracies reported in table 2.\nFurthermore, the results reported in this manuscript are below the reported results in similar previous works on the same dataset (see for example ""Predicting Alzheimer’s disease: a neuroimaging study with 3D convolutional neural networks""). This affects negatively the significance of the results.\nFinally, the text suffers from many typos, grammatical errors, and inaccuracies in the presentation. For example:\n1) in the abstract ""Deep learning has become default choice ..."" => this is inaccurate as deep learning is not yet default choice in this context.\n2) Better to have abstract in one paragraph rather than 3.\n3) in the abstract: to generalize the deep neural networks => to improve the generalization of deep ...\n4) Throughout the text, the citations are used inappropriately as nouns for example: (Suk et al., 2017) recognizes ... => Suk et al. (2017) recognizes. To solve this problem you should use \\citet instead of \\cite or \\citep.\n5) Page 2 first paragraph: While preventing the lost of useful information =>  While preventing the loss of useful information\n6) page 2 last paragraph, I cannot undertand this sentence: ""For AD it\'s characteristic that the patients su\x0ber from progressive cognitive decline.""\n7) Page 3 first paragraph: in waste majority => in vast majority\nsection 1.2: About proposed method=> The Proposed Method\n8) Page 3: perurbet => perturbed\n9) Throughout the text: \'x\' or \'*\' are used to denote multiplication, better to use \\times\n10) Why in the perturbed normalization method the multiplication with standard deviation is used for perturbing the mean? Statistically and mathematically it would be more plausible to add some random values within the standard deviation to the mean.\n11) In Section 4: posptones overfitting=>postpones the overfitting\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		r1gvQWLglV	H1x8mrNP7N	MIDL.io/2019/Conference/-/Paper105/Official_Review	[]	2		['everyone']	r1gvQWLglV	['MIDL.io/2019/Conference/Paper105/AnonReviewer2']	1548334365908		1548856724552	['MIDL.io/2019/Conference/Paper105/AnonReviewer2']
237	1548334399082	"{'pros': '\n- The paper discusses an issue of clinical importance and tackles a difficult problem in medical imaging\n- The dataset is large \n- The lesion masks are provided by consensus of two raters\n- Accuracy is reasonable given the difficulty of the task and the short time-frame\n', 'cons': '\n\n- The paper ignores the extant literature specifically dedicated to MS clinical prognosis using machine learning on medical images. For example [Wottschel2015] use radiomics-style lesion features from T2 and PD fed to an SVM to predict CIS->CDMS conversion at 1 and 3 years (N=74). [Yoo2016] uses a CNN on lesion masks to predict CDMS conversion at 2 years (N=140). These or other specifically relevant papers should be mentioned and, ideally, one existing method should be tested to provide a competitive baseline.\n\n- The evaluation setup on the dataset described in section 4.1 gives cause for concern due to two major potential confounders.\n\t- The first potential issue (as in all multi-site studies) is that the outcome of interest (progression) may be confounded by the site. Even with harmonized acquisition protocols and sequences (this should be mentioned!), signal differences can persist and easily be picked up, especially by high-capacity models such as the one proposed. The authors should provide a table or statistical test on the equivalence of progression across sites.\n\t- The second potential issue is that visits seem to be considered independent of patients (e.g. first study \'624 inputs\' but \'312 patients\'). How is this handled in the cross-validation setup? Can subjects cross the fold? If they can, can the authors motivate why this is not an issue ?\n\n- The authors selected only patients that completed the trial. This raises the risk that the method presented will suffer from attrition bias, in particular if subject drop-out is related to the disease process - maybe people that start from higher EDSS and/or have more rapid progression drop out of the study more quickly ? This should be acknowledged as a limitation since it will impair real-world applicability of the method.\n\n- Figure 1 does a reasonable job describing the architecture of the model, but a few important details are unclear, in particular \n\t- How many convolutional kernels are used in each Conv3D of each ""parallel pathway""? \n\t- Related, how are the different modalities handled, presumably the same number of convolutional kernels for each modality? \n\t- Related, in table 2, does the \'lesion masks\' version have the same number of parameters as the \'proposed 3D CNN\'? Presumably not, but rounded off? This is confusing.\n\n- What is meant by \'an architecture analogous to VGG\'? Presumably this means a series of conv-conv blocks with various channel depths, but it would be useful if the authors could summarize briefly the main differences with the original VGG19.\n\nTYPOS\n- 3.1: \'difference dimensions\' -> \'different dimensions\'\n- 4.2: \'receiver operation characteristic\' -> \'receiver operating characteristic\'\n\nREFERENCES\nWottschel, V., Alexander, D., Kwok, P., et al.: Predicting outcome in clinically isolated syndrome using machine learning. NeuroImage: Clin. 7, 281–287 (2015)\n\nYoo et al., Deep Learning of Brain Lesion Patterns for Predicting Future Disease Activity in Patients with Early Symptoms of Multiple Sclerosis, LABELS 2016/DLMIA 201', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		rkliARLel4	HJxvBSNvX4	MIDL.io/2019/Conference/-/Paper112/Official_Review	[]	2		['everyone']	rkliARLel4	['MIDL.io/2019/Conference/Paper112/AnonReviewer3']	1548334399082		1548856724304	['MIDL.io/2019/Conference/Paper112/AnonReviewer3']
238	1548332164470	{'pros': 'This paper proposes an iterative two-stage approach based on weakly supervised and unsupervised training stages for histological image segmentation. The paper is clear, well organized, well written and easy to follow. The contributions are clearly stated and a thorough literature review is presented that allows a good insight into the stated contributions. Clever design choices are made such as clues 1 and 2 in stage 1 and 2, respectively, that allows improved promising performance. ', 'cons': 'Evaluation on the test images (IS3 - 5 WSIs) is missing and should be included to give more insight into the proposed approach.\n \nSection 4, Stage 2: Why sixth iteration of Stage 1 is used? Comment on this selection? And why 10th iteration is not used/reported where STD is comparatively low?\n\nFigure 5: Missing detailed caption making it difficult to follow the four plots. Explain what each plots are. \nFor each plot, use the same range for y-axis especially for (b), (c), (d) probably from 0.2 to 1.0.  \n\nPage 9, line 1: The scores of Stage 2 (Fig. 5(b)) --> The scores of Stage 1 (Fig. 5(b)) ', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		SklToCZ2J4	rygnYnQw7N	MIDL.io/2019/Conference/-/Paper5/Official_Review	[]	1		['everyone']	SklToCZ2J4	['MIDL.io/2019/Conference/Paper5/AnonReviewer1']	1548332164470		1548856724050	['MIDL.io/2019/Conference/Paper5/AnonReviewer1']
239	1548331947176	"{'pros': 'The paper proposes a data augmentation approach based on cycle-GAN and define a controlled loss for generating realistic cell-nuclei data in immunohistology.  \n \nThe paper is difficult to follow and is missing motivation. Moreover, there are several typos, errors and missing information throughout the paper which makes it difficult to follow and lacks in proper justification. ', 'cons': ""The paper presents a data augmentation strategy tailored specifically to the underline problem and the dataset at hand. It is not clear how the design choices are made and how they can be useful for similar related problems.  \n\nThe motivation for using Cycle-GAN is not clear and is not justified. It would be worth referring to other approaches that are using similar strategies for data augmentation. \n\nFor completeness, it should involve a clear motivation, literature review highlighting similar deep learning-based approaches and clear mention of the contribution compared to the existing approaches. \n\nPage 3: Table 1 is referred in the paper but is totally missing from the paper. Not clear what the random parameters are in this paper and what are their distributions. \n\nPage 4: 'While this loss enables a translation ... the image content may still change drastically'. Why? Please elaborate. \n\nThe abbreviation WSI is missing its first-time definition - Whole Slide Imaging. \n\nSeveral typos throughout the paper. It seems like no proofreading was done before the submission. \ne.g: dection, mutli-labeling, adress\n\nWhere in the results and discussion section the average performance of 82.9% reported in the abstract is discussed? \n\nWhen referring to Figure 1, Which one is the lower row or bottom row? Are you referring to the right-most column instead? "", 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		B1en70uU1V	HyxX3s7wQ4	MIDL.io/2019/Conference/-/Paper3/Official_Review	[]	2		['everyone']	B1en70uU1V	['MIDL.io/2019/Conference/Paper3/AnonReviewer2']	1548331947176		1548856723837	['MIDL.io/2019/Conference/Paper3/AnonReviewer2']
240	1548326190303	{'pros': 'The paper investigates the importance of explicitly using taxonomical structures of labels for classification tasks in medical imaging within the application of chest-x-ray classification into a number of hierarchically related diagnostic categories.\n\nThe key contribution is the formulation of hierarchical multi-label classification within a deep-learning framework, which is a novel and generally useful idea. They authors propose an intuitive and effective two-stage optimisation scheme which first encourages the model to capture label taxonomy and then maximise the end classification accuracy. They introduce a numerically stable implementation of cross entropy loss for unconditional class probabilities. Using a large labelled chest x-ray data based, they provide a first demonstration of hierarchical multi-label classification in medical imaging.\n\nThe paper is well written and well motivated. Results show improvement in classification accuracy over relevant baselines.\n', 'cons': 'Despite the new loss functions that accounts for the label hierarchy, a single\n\ndistributed model is used to predict all the classes. This means that the architecture does not respect the hierarchical structure in the data e.g. different features may be desirable for detecting ‘Abnormality’, and discriminating between ‘Pleural fibrosis’ and ‘Fluid in pleural space’.\n\nThe improvements shown in Table 2 are relatively small and no error bars are provided.\n\n\n\nIt would be interesting to see the benefits of capturing label taxonomy when the size of the training data is smaller. Injecting such domain prior knowledge may improve the data efficiency.\n\nIt would be also interesting to see how the model performs in the presence of incomplete labels i.e. each image is not necessarily labeled until it reaches one of the leaf nodes (e.g. Abnormality=> Pulmonary => Opacity, but Infiltration or Major atelectasis is not known) .\n\nIt would be more informative to reorder to the disease indices in the breadth-first order. This would help to see how the level within the taxonomy affects the performance. I also wonder what the plot would look like after the first optimisation phase based on the HLCP loss.\n\nOverall, the contribution of the paper is solid in terms of technical novelty and problem formulation. However, the paper could use stronger experiments as suggested earlier to bolster its claims. \n', 'rating': '4: strong accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'special_issue': ['Special Issue Recommendation'], 'oral_presentation': ['Consider for oral presentation']}		SJgNCUbke4	ryeLVHzPXN	MIDL.io/2019/Conference/-/Paper29/Official_Review	[]	2		['everyone']	SJgNCUbke4	['MIDL.io/2019/Conference/Paper29/AnonReviewer3']	1548326190303		1548856723621	['MIDL.io/2019/Conference/Paper29/AnonReviewer3']
241	1548322172091	{'pros': 'This submission presents a two-stage process for optic disk segmentation, for diagnosing glaucoma. They use a combination of YOLO and and a local encoder-decoder network, as well as Cycle-GANs to adapt the segmentation tasks from different cameras. Results on a standard data set used for a community challenge on the same task show an improvement over some classical segmentation networks.\n\nThe main ideas in the paper come across clearly. The work is well motivated as the segmentation enables accurate quantitative measurements that are informative of glaucoma but noisy and inaccurate with traditional image analysis - deep learning is a clear opportunity. The network proposed is a well-constructed and quite a neat idea (combination of VNET/UNET encoder-decoder paradigm with YOLO) and the Cycle-GAN data augmentation approach is interesting.\n\nThe data-set appears reputable, although of modest size (https://refuge.grand- challenge.org/Details/). Experiments show improvement over two baselines. The first U-Net, is a simpler version of the proposed network, without the YOLO object-detection. The second Tiramisu, was only ever described in the paper as ‘Future more, there are a lot of models [12][13] based on fully convolutional network are proposed for image segmentation and achieved best performance’. There is no explanation as to why this network was chosen as a baseline.\n\n\n', 'cons': 'Limited, but non-zero, methodological innovation, essentially combining three existing ideas.\n\nThe description of the experiments is incomplete - unclear how many data sets were available and used for training and testing. \n\nNo error bars reported in the results, which would add a lot.\n\nUnclear how much effort went into training the baseline networks and whether that is comparable to the training, hyperparameter tweaking, etc. for the proposed method.  Is it really a level playing field?  Some description of the tuning processes would make the benefits more convincing.\n\nNo analysis performed to show which of their innovations is key in achieving the increase in accuracy - is the CycleGAN or the YSNet most important or each component equally so?\n\nUnclear why the particular baseline networks are an appropriate choice - would have been nice to see a comparison with the winning entries in the REFUGE challenge (possibly available here https://refuge.grand- challenge.org/Results-ValidationSet_Online/ ) .\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		B1gV2yZ4g4	BkeNYr-PXN	MIDL.io/2019/Conference/-/Paper143/Official_Review	[]	1		['everyone']	B1gV2yZ4g4	['MIDL.io/2019/Conference/Paper143/AnonReviewer2']	1548322172091		1548856723403	['MIDL.io/2019/Conference/Paper143/AnonReviewer2']
242	1548269299808	{'pros': 'This paper proposed a new data augmentation method using superpixels (SPDA) for training deep learning models for biomedical image segmentation. \n\nThe proposed method can effectively improve the performance of deep learning models for biomedical image segmentation tasks.\nThe experimental results are detailed and solid.', 'cons': 'Some technical details are missing.', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		H1gLLOgxeE	rklhgDV8mV	MIDL.io/2019/Conference/-/Paper55/Official_Review	[]	1		['everyone']	H1gLLOgxeE	['MIDL.io/2019/Conference/Paper55/AnonReviewer1']	1548269299808		1548856723187	['MIDL.io/2019/Conference/Paper55/AnonReviewer1']
243	1548262160199	{'pros': '\nSummary:  A method for metal artifact removal in CT is proposed using a U-net based CNN architecture to segment the metal artifacts in the projection data, after which in-painting is performed and a new CT image is reconstructed with reduced metal artifacts. The method is trained on clinical data in which synthetic pacemaker leads are introduced. In the end, the method is also tested on clinical data that contains real pacemakers. \n\n-\tThe paper is understandable and clearly written\n-\tThe segmentation method itself is not very novel, a U-net based architecture is used for segmentation. However, the application to sinograms is novel and very interesting.\n-\tThe use of sinograms for segmentation has potential to generalize to other segmentation tasks.\n-\tThe evaluation on a clinical dataset with real pacemaker leads is interesting, especially considering that the method is trained with data that contained only synthetic pacemaker leads.\n\n\n', 'cons': '\n-\tIt is unclear why different thresholds are used for background and foreground? (table 1).\n-\tAre the synthesized pacemaker leads based on non-contrast enhanced CT scans? In Fig. 2, the first picture looks like a non-contrast enhanced CT scan, is this true? If yes, why weren’t contrast enhanced scans used to synthesize training data. \n-\tIf possible, it might be interesting to also show the Dice coefficient, sensitivity, specificity and AUC of the clinical test data with real pacemaker leads. This might give a better indication of how the method performs on real clinical examples.\n-\tWould the method also work on non-contrast enhanced scans?\n-\tHow does the method deal with calcifications and stents in for instance the coronaries?\n-\tThe use of the term “data set” is a bit confusing in combination with terms such as “training, validation and test set”, and “reference, target and test data”. Probably the authors refer to one CT scan and the corresponding projection data as “one data set” but perhaps in the future clarify what is exactly meant.\n\n\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'oral_presentation': ['Consider for oral presentation']}		rkx5InjA1N	H1gdzjGIQV	MIDL.io/2019/Conference/-/Paper20/Official_Review	[]	1		['everyone']	rkx5InjA1N	['MIDL.io/2019/Conference/Paper20/AnonReviewer1']	1548262160199		1548856722757	['MIDL.io/2019/Conference/Paper20/AnonReviewer1']
244	1548262585259	{'pros': '\nSummary:  \nThe authors propose a ratio based sampling method to train a 2D U-net based CNN architecture for liver segmentation in the LiTS challenge dataset (contrast enhanced abdominal CT scans). Three-fold cross validation is performed to evaluate different parameter combinations. A comparison is made between networks trained with different sample ratios for liver and background samples, and random based sampling. Furthermore, networks trained with the weighted cross entropy loss (WCE), in which voxels are weighted based on thresholding and the liver mask, are compared with networks trained with the Tversky loss. The best results are obtained with a network that was trained with the WCE loss, and a sampling ratio of 50% between liver and background samples.\n\n-\tThe paper is clearly written. \n-\tThe idea on how to choose your training samples to enhance your performance is interesting to explore.\n-\tNot only the Dice coefficient but also distance metrics such as the ASSD and MSSD are computed, giving a better understanding of the results.\n\n', 'cons': '\nSeveral details are missing in the paper:\n-\tIt is unclear which activation function is used in the output layer of the network.\n-\tWere the networks only trained on axial slices? If so, would other views not also be beneficial to use during training? \n-\tIt is unclear why three weights are used for the WCE. Why not use two (background and liver)?\n-\tThe paper states that only one or two patches per slice are extracted, based on whether the slice is classified as background or liver slice. However, are different patches extracted from the slices during every epoch? Are all possible image patches used during training? Or is a large part of the (background) patches not used at all? Furthermore, what is the patch-size compared to the size of an entire slice? \n-\tIt is not entirely clear how the amount of training iterations is obtained, based on the sampling method.\n-\tIs a validation set used to see whether the networks might be overfitting? \n\nGeneral remarks:\n-\tA comparison with other liver segmentation methods is missing. Did balancing the data also outperform previously presented liver segmentation methods? The method of balancing your data on itself is not very novel, therefore it is unclear what the contribution of this paper is.\n-\tBased on fig. 4, the network trained with TVE seems to have a higher FN rate (missing quite a bit of liver in the segmentation), would adjusting α and β improve results?\n-\tThe paper shows that a ratio (r) of 0.5 obtained the best performance. However, this seems specific for liver segmentation. Could the authors elaborate on how this r might change based on the organ of interest (for instance smaller organs probably need a smaller r due to the chance of a high FP rate otherwise). Furthermore, do the authors think these setting are also able to optimally perform on a test set that comes from a different dataset?\n-\tBecause the Tversky loss already deals in some way with class imbalance, it might be that the networks perform less good due to an overkill of balancing the data. It would therefore be interesting to see what happens if you use the Dice loss, in which the weight for FP is equal to the weight for FN.\n-\tIt would be interesting to mention the imbalance present in the data, for instance the number of liver pixels vs. the number of background pixels. Could there be a relationship between this ratio and the optimal r?\n-\tOn page 3 and 4, a reference is made to figure 4 but probably this should be a reference to figure 2.\n\n\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		BkeeIavee4	BylZTnG87N	MIDL.io/2019/Conference/-/Paper117/Official_Review	[]	1		['everyone']	BkeeIavee4	['MIDL.io/2019/Conference/Paper117/AnonReviewer3']	1548262585259		1548856722540	['MIDL.io/2019/Conference/Paper117/AnonReviewer3']
245	1548261243814	{'pros': 'The paper makes the case that existing NAS approaches, especially those that replace discrete variables with continuous approximations, are insufficient for very large architectures such as those used in 3D segmentation, and the authors select the widely used U-Net as their exemplary case. This makes a compelling case for the proposed method, which consists of drawing two entries from the probability vector of possible realizations (operations in this case) at a given iteration, clipping the remaining probabilities to zero and renormalizing, with the benefit of reducing the number of activations from N (8 in this work) to 2 for each operator that is part of the optimization process. While no quantitative analysis of computational savings is offered, it is clear that they are considerable.', 'cons': 'While the method certainly has merit, the experimental results do not do it justice. The segmentation performance obtained with SCNAS is below state-of-the-art and even compared to their own baseline (3D U-ResNet) the improvement is not convincing. The former could in part be explained by the missing data augmentation (the authors address this) while the latter could be the simple fact that the baselines can already cover a large enough function space to find a good approximation of the target function (given the limited amount of training data), so that the architecture search has negligible influence. I list my concerns in more detail below.\n\nMajor: \n\n1. The baseline models chosen for this study do not represent the state of the art. The reader can only be convinced of the merit of this paper if it can outperform the state of the art, which in the case for the heart and prostate dataset used here is nnUNet, the winning contribution to the Medical Segmentation Decathlon (Isensee et al., 2018). While Isensee et al. used an ensemble of different models for their final submission, their paper also reports five-fold cross-validation results with their 3D UNet (without ensembling) on these datasets. The 3D U-Net results reported by Isensee et al. are better than any of the results reported in this manuscript. In addition, the 3D U-Net used by Isensee et al. is very basic with no major architectural variations. As such, it could have been an ideal baseline candidate to demonstrate if SCNAS can really advance the state of the art. It should further be pointed out that, in general, the 3D UNet used in Isensee et al. is rather similar to the UNets used throughout this work with the sole big difference being the number of pooling operations ([3,3,3] in the case of this manuscript vs [5,5,5] (heart, brain tumor) and [2,5,5] (prostate) in Isensee et al., see Table 1). The statement “We conjecture that Isensee et al. (2018) might be benefit from complicated pre-/post-procedures and thus obtained slightly better performances than the SCNAS.“ does not sufficiently explain the difference in performance. In fact, the preprocessing used by Isensee et al. for MRI images is basically identical to the procedure used in this paper.\n\n2. In Equations 2 and 3, the authors state that the optimization of the edge weights is done on the ‘validation set’. It is unclear however to what set this actually refers to. In the experiments, the authors report results of a five-fold cross-validation. In order to draw any kind of meaningful conclusion of the this cross-validation results, it is important to clarify whether the validation set of the splits were used for this optimization or whether the training set was again split into two sets. If the performance of the models is estimated via cross-validation then the validation split cannot be used for any kind of optimization!\n\n3. “SCNAS produces a more generalizable neural architecture for the similar tasks of 3D MRI image segmentation “. There is no evidence in the paper that would support this statement. The authors must also transfer their baseline models and see if the performance of the transferred baseline models is better or worse than that of SCNAS.\n\n4. The authors state that SCNAS performs significantly better than the other approaches. Even if we ignore the previous concern, I would only buy that claim for the peripheral prostate zone. In all other categories the standard deviations (?, see also minor-8) are too large to support this statement without an actual test.\n\n\nMinor: \n\nThe authors do not give sufficient details about the “Random Search” result. How was this network architecture obtained? How many different configurations were drawn randomly and how was the best model selected?\n\nTraining large 3D segmentation networks is very computationally expensive. It is clear that the authors must have had access to a fairly large GPU cluster. If would be interesting to have more specific information about how many GPUs were used (in total and per model) and how long one of the models needed to train. \n\nHow did the authors handle the different number of input channels when transferring their network from brain tumour (4 channels) to heart (1 channel) and prostate (2 channels)?\n\nThe authors state that they compare SCNAS against a 3D U-ResNet and an attention U-Net, but no results are reported for the attention U-Net.\n\n“the input images were first resized for all voxel spacings to be physically equal using the given meta-data“. It is unclear what spacing the data was resampled to.\n\n“Note that unlike Isensee et al. (2018), any heuristic pre-/post-processing techniques including data augmentation, network-cascade, and prediction-ensemble were not adopted in this evaluation to solely examine the effects by the use of NAS in designing the network architecture.”: While it makes sense to drop ensembling and cascaded architectures for this work, the argumentation that the lack or data augmentation better isolated the effect of NAS is lackluster. In fact, including data augmentation would likely have improved the results somewhat across the board and thus made the results more convincing.\n\nFigure 2 a): in brain tumor segmentation it is more common to show contrast enhanced T1 sequences alongside with T2 or FLAIR to allow the reader to see all parts of the tumor properly. If only one of the sequences is shown then this should probably be the contrast enhanced T1 because enhancing tumor is not visible in the sequence presented here. \n\nWhat type of error is reported in table 1?\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		S1lhkdKkeV	BJeEFwM8QV	MIDL.io/2019/Conference/-/Paper34/Official_Review	[]	2		['everyone']	S1lhkdKkeV	['MIDL.io/2019/Conference/Paper34/AnonReviewer3']	1548261243814		1548856722281	['MIDL.io/2019/Conference/Paper34/AnonReviewer3']
246	1548260297124	{'pros': '- The authors transfer two recently published tools, MUNIT for data augmentation and Criss-Cross attention for semantic segmentation, from pure computer vision field to the medical imaging. Specifically they employ the tools to improve the  pathological lung segmentation employing CXRs images.\n\n- The authors present an extensive evaluation employing public available datasets, in which the lung lesions are mild, and their own dataset with severe lesions. The obtained results for public available datasets are similar to the state-of-the-art approaches, and particularly better for highly damaged lungs. \n\n- Employment of generative models as MUNIT for data augmentation is quite new in the medical imaging field. \n\n- Both, Criss-cross attention and specially data augmentation with generative models could be easily extended and useful in similar segmentation problems. Particularly, if the authors make publicly available a clean and robust code after publication.\n\n\n', 'cons': '- As I mentioned above, the employment of two novel techniques from the computer vision field must be welcome. However, the method has not been adapted for the presented problem. It seems like the authors have put two pieces of code together to subsequently employ their images. Computer Vision and medical images segmentation problems have a lot in common but as well several differences that should be reflected within the models. This issue could have been evaluated by publishing the code without the acceptance condition imposed by the authors.\n\nCriss-Cross Attention based Network for Lung Segmentation:\n - It is quite difficult to understand the section. The text and the figure 2 are insufficient (kind of disconnected even) in order to provide an adequate explanation for the work purpose, actually, it was a must for me to read the full original paper in order to understand the model. Besides, the figure is mostly the same that the one employed in the original work but is not explicitly cited.   \nIn this section, The weight decay and the batch size have changed with respect to the original work, why? Difference between images? Convergence issues? Etc.\n\nData Augmentation via Abnormal Chest X-Ray Pairs Construction:\n- Again, the employment of MUNIT for data augmentation is a nice approach but, is there a way to guarantee the realistic deformation of all lungs? How much does it care?\n\nDatasets:\n - Is there a single CXR per subject for all datasets? \n- There is a lack on the datasets description: Voxel size/resolution, etc. \n\n\nQuantitative Results:\n- To perform a fairer and more interesting comparison, it would be recommendable to employ an adapted version of the U-Net with the criss-cross attention modules in addition to the classic model. \n- In general, there are conclusions, subject to discussion, mixed with the results, e.g: “This demonstrates that the proposed XLSor based on the criss-cross…”, “suggesting the effectiveness of our data augmentation technique for lung segmentation...”, etc.\n- The results for severe lesions are better for the proposed model. The overlapping measures reach, in average, the results obtained for mild lesions but, surprisingly, the AVD is much smaller for the difficult cases, how do you justify this? I suppose that the lung shape is much more complex for your dataset, so similar results between mild and severe lesion would be already a big achievement but such favorable differences are weird and point to an ad-hoc model. Please explain it. \n- The AVD has not units.\n\n\n\n\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		B1lpb10Ry4	S1gZRQf87E	MIDL.io/2019/Conference/-/Paper26/Official_Review	[]	1		['everyone']	B1lpb10Ry4	['MIDL.io/2019/Conference/Paper26/AnonReviewer3']	1548260297124		1548856722026	['MIDL.io/2019/Conference/Paper26/AnonReviewer3']
247	1548258066027	"{'pros': 'This paper applies a densenet with less number of parameters for segmenting the infant brain at isointense stage.\n\n1. This paper is easy to understand and well formulated.\n2. The proposed method is validated on a public dataset (iseg).\n3. The proposed method achieves very good results.\n', 'cons': 'However, it needs to be improved in the following aspects:\n\n1. Though the authors claim they use less number of parameters, I cannot see the strategies to make it (using more hyper connection is actually quite trivial and cannot be used as novelty in my understanding; excluding label can indeed reduce the number of parameters? I doubt it). The authors should better list the number of parameters of all the compared networks, and the number of parameters if using and not using the proposed training strategy. Then we can see what happens.\n2. In addition, the number of parameters cannot represent how hard the network to be trained. Since we are not sure the freedom degrees of the network. Of course, this is only my personal understanding.\n3. I cannot learn much from this paper. The authors can point out what\'s the contributions.\n4. For the experimental part, I\'d like to see some ablation study to validate whether the proposed training strategy indeed works or the good performance is coming from excellent hyper-parameter tuning.\n5. ""The GM labels were concluded from the compliment of the already predicted CSF and WM labels."" This one is useful, but not actually new, I have read papers to use similar strategies to conduct infant segmentation to segment CSF vs. WM+GM and then WM vs. GM before. More importantly, the proposed strategy is not quite general, you can do the compliment to get the GM because the background for brain MRI is usually 0. In other applications, the background is usually not 0.\n\nIf the authors solves the above concerns, I\'ll choose to accept it.\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		Byg-krBi14	HkgcMj-IQV	MIDL.io/2019/Conference/-/Paper4/Official_Review	[]	1		['everyone']	Byg-krBi14	['MIDL.io/2019/Conference/Paper4/AnonReviewer2']	1548258066027		1548856721772	['MIDL.io/2019/Conference/Paper4/AnonReviewer2']
248	1548255777173	"{'pros': 'This paper proposes to train a nuclei segmentation network using pixel-level labels generated from points annotation by Voronoi diagram and k-means clustering. A dense CRF is trained on top of the network in an end-to-end manner to refine the segmentation model. The authors evaluate their methods on two datasets, Lung Cancer dataset (40 images/8 cases) and MultiOrgan dataset (30 images/7 organs). This paper is well-organized and easy to follow. The topic of learning from weakly annotated histology images is highly relevant for the community.', 'cons': ""- The proposed method uses (dilated) Voronoi edges as the 'background' label, which effectively depends on two assumptions: 1) the neighbouring nuclei have similar size and are non-toughing, 2) the point annotation is located at the centre of each nuclear. I feel these are also limitations of the proposed method. Simply saying 'point annotation' is therefore inaccurate and a bit of misleading here. In fact, the proposed method only allows one point per nuclear, and in the experiments the manual point annotations are simulated by computing the central point from the ground truth full masks.\n\n- Method of 'Full' + CRF should be included in the analysis, to show the possible 'upper bounds' of the segmentation performance.\n\n- The proposed training label extraction methods should be compared with the those evaluated by Kost et al. (2017), Training nuclei detection algorithms with simple annotations."", 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		H1xkWv8gx4	rygFXzWL7V	MIDL.io/2019/Conference/-/Paper108/Official_Review	[]	3		['everyone']	H1xkWv8gx4	['MIDL.io/2019/Conference/Paper108/AnonReviewer1']	1548255777173		1548856721545	['MIDL.io/2019/Conference/Paper108/AnonReviewer1']
249	1548255353555	"{'pros': 'The authors investigate a U-Net architecture for segmentation tasks whereby in addition to the probabilistic class label map, the contour map (resp. distance map) is classified (resp. regressed). The architecture uses a single-path decoder as opposed to e.g. Tan et al.\n\n| Strengths:\n- The paper is easy to follow.\n- Adequate comparison to baseline architectures.\n\n| Weaknesses:\n- The claim of ""shape learning"", presented as a distinct focus from that of segmentation (for instance in the title) feels rather misleading after going through the paper. The work learns to predict a signed/unsigned distance map on top of a segmentation map. To contrast, there is no attempt to model shape, or to provide evidence that working from the distance map allows the network to capture more information about the shape at training time.\n- The quantitative validation shows no significant change in accuracy compared to baseline architectures. The number of parameters is of the same order of magnitude (1e7). The method is 40% faster. Is the speed improvement really meaningful in the setting of interest to the authors?\n- The contour-based approach is highly dependent on the arbitrary ""thickness"" of the contour (related to class imbalance). \n\n| High-level comments:\nI am leaning towards rejection because the paper is written in a manner that emphasizes accuracy/performance when describing contributions and evaluating the method, but the results do not support this argumentation yet. The results involve fractions of Dice/Jaccard and give no clear incentive to pick a method over another one. \n\nInstead the authors could have shared insight into potential differences in ""behaviour"" between the different approaches. For instance, when learning to predict the segmentation+distance map, does the algorithm do ""better mistakes"" compared to the baseline ones? Does it qualitatively better respect the overall shape (maybe at the cost of some fine-grained accuracy); is the segmentation smoother? does it generalize differently/better to other datasets? It might be that there are compelling arguments to favour the proposed approach, other than a dramatic increase in performance, but the paper does not look into them. Why do the authors feel that the claim of ""joint shape learning and segmentation"" is justified, or that additional ""spatial and structural information"" is captured?\n\n| Misc:\n- In Table 1, are the results for the proposed approach MD with D3? (the one that performs slightly better in Table 3)\n- How is the 50% reduction in the number of parameters computed?', 'cons': '.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		SJxWBKR1gN	B1gzYe-874	MIDL.io/2019/Conference/-/Paper47/Official_Review	[]	1		['everyone']	SJxWBKR1gN	['MIDL.io/2019/Conference/Paper47/AnonReviewer2']	1548255353555		1548856721330	['MIDL.io/2019/Conference/Paper47/AnonReviewer2']
250	1548254604640	"{'pros': '0) Summary\n  The manuscript proposes to use neural processes for normative modeling of fMRI data in order to perform classification of healthy, schizophrenia, attention deficit hyperactivity disorder and bipolar disorder.\n1) Quality\n  The manuscript presents a conceptually simple idea as it proposes to replace a forward regression model by a different one.\n2) Clarity\n  The paper is mostly well written and the notation is clear.\n3) Originality\n  The use of neural processes for normative modeling of fMRI data seems to have not been explored before.\n4) Significance\n  The model could potentially be a valuable tool.\n5) Reproducibility\n  As the experimental evaluation is based on an public dataset available from OpenNEURO and there is some code available for the neural process, the results should be reproducible in principle.', 'cons': '1) Quality\n  One strange thing is the numbers reported in Figure 3 as compared to Figure 1 in [1]. Why does the sMT-GPTR(n,m) class perform much better here? Values of AUC=0.8 on SCHZ, AUC=0.7 for ADHD and AUC=0.85 for sMT-GPTR(5,3) are way better than the values reported in the manuscript. Also sMT-GPTR(5,3) does better than sMT-GPTR(10,5) there, which is not the case in the manuscript. Please explain!\n  The relative merit of using a neural process versus a Gaussian process (GP) remains only partly explored as the claim that GPs scale poorly is not substantiated by runtime experiments. In particular given prior work on scaling up GPs e.g. [2].\n  The manuscript does not discuss whether or not the proposed methodology reveals interesting/relevant spatial pattern.\n2) Clarity\n  There are a couple of typos.\n    - Section 2.2: ""In our application, in order to""\n    - Section 2.2: ""parametrized on an encoder""\n    - Section 2.2: ""In fact, in this setting""\n    - Section 2.3, Normative modeling: ""let $\\mathcal{Y}^* ...$ to represent""\n    - Figure 2, caption: ""3d-covolution"" -> twice\n    - Section 5: ""dropout technique in order to""\n  What is an ""amortized variational inference regime"" as mentioned in Section 2.2?\n  The description of the stochastic process formalism in Section 2.2 seems like an overkill here. In particular, the statement about the number of subjects N going to infinity needs to be translated into the setting of N=250 where the actual model operates.\n3) Originality\n4) Significance\n5) Reproducibility\n  The code for the comparison of Figure 3 i.e. columns 1+2 is not made public.\n\n[1] Kia et al., Scalable Multi-Task Gaussian Process Tensor Regression for Normative Modeling of Structured Variation in Neuroimaging Data, https://arxiv.org/abs/1808.00036\n[2] Kia et al., Normative Modeling of Neuroimaging Data using Scalable Multi-Task Gaussian Processes, https://arxiv.org/abs/1806.01047', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BJxTPziyeE	SklSq6eIXE	MIDL.io/2019/Conference/-/Paper40/Official_Review	[]	1		['everyone']	BJxTPziyeE	['MIDL.io/2019/Conference/Paper40/AnonReviewer1']	1548254604640		1548856721115	['MIDL.io/2019/Conference/Paper40/AnonReviewer1']
251	1548253523696	"{'pros': '- The paper uses a database with a good variety of diseases including PPA for which imaging studies are comparatively rare.\n- Several algorithms are evaluated\n', 'cons': '- The descriptions of the experimental setup could be more detailed, and including basic statements like ""this is an 11-class classification problem"" could help. In particular\n\t- It would be helpful to have at least one or two sentences on the atasing approach, not just the citation\n\t- SVMs are not natively multiclass. Did the authors use a one-versus-all or all-versus-all setup ?\n\t- The architecture of the DNN is unclear - what does \'... in each of the second hidden layers\' mean? Is this a network with just two hidden layers? Please at least mention the number of trainable parameters\n\n- The authors want to \'capture more complex atrophy patterns\', but the experimental design based on retrospective, unbalanced data almost guarantees that age, sex, and site confounds drive the results in large part, potentially as much as the real differences in atrophy locations and patterns. The authors provide useful statistics and plots on these differences, but it would be good to at least perform some classical statistics on the most important volumes found in Table 4. In particular\n\t- ANOVA on sex/site/age/disease would help partition the variance\n\t- A diagnosis by site table would be useful - e.g. are all bvFTD patients from a single hospital ? \n\n- There is very little discussion of the performance difference between diseases. In particular\n\t- Why is CBS so low ?\n\t- Is performance driven at least in part by sample size? Plotting F1 vs n might shed some light.\n\n- Neurodegenerative diseases are a pretty large class. The clinical motivation needs to be stronger, probably talking about the differential diagnosis angle - e.g. how often is PD in the differential diagnosis of AD? Which disease pairs often exhibit similar symptoms? \n\nTYPOS\n- 2.2 ""Atlas"" repeated after LBPA40', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BJlZBmBrl4	Bke38tx8XN	MIDL.io/2019/Conference/-/Paper154/Official_Review	[]	1		['everyone']	BJlZBmBrl4	['MIDL.io/2019/Conference/Paper154/AnonReviewer2']	1548253523696		1548856720901	['MIDL.io/2019/Conference/Paper154/AnonReviewer2']
252	1548237396751	"{'pros': '0) Summary\n  The manuscript proposes a methodology to locally normalize for rotation using image patch moments. The algorithm is applied to five 2d datasets: three public datasets from natural image classification and two (vaguely described) in-house datasets: one derived from volumetric CT data (mCRC) for classification and the other for liver lesion segmentation.\n1) Quality\n  The paper uses moment-based local patch normalization, which is a conceptually simple idea.\n2) Clarity\n  The technical content is well accessible.\n3) Originality\n  The idea of locally normalizing the 2d patches using image moments seems to have not been explored before.\n4) Significance\n  The rotation invariance property is important in DNNs. Hence the proposal has a certain value.\n5) Reproducibility\n  The method is evaluated on three public datasets using a standard baseline.', 'cons': '1) Quality\n  A proper comparison in terms of runtime is missing. The performance is only evaluated up to a certain size of the dataset which makes the overall judgement difficult. How do the plots in Figure 2 look for growing #Samples? Also, it is unclear whether the improved performance in some cases is really due to the rotation invariance. Control experiments with randomly rotated images are missing.\n2) Clarity\n  There are some typos.\n     - Abstract: ""continiously"", ""explorative tasks, realistic""\n     - Intro: ""perceived by human""\n  Certain parts of the paper can be shortened to meet the soft 8 page limit e.g. the discussion of invariance/equivariance and the sometimes vague description e.g. paragraph before section 2.1.\n3) Originality\n  The line of research around spatial transformer networks [1] is not discussed.\n4) Significance\n  The possible impact of the paper is limited by the fact that the evaluation is done on 2d datasets only and -- more principled -- the approach only provides local rotation invariance which is only a small step towards proper rotation invariance. The results on medical datasets are somewhat inconclusive.\n5) Reproducibility\n  Two datasets are not public and the implementation is kept closed which renders the results pretty hard to reproduce.\n\n[1] Jaderberg et al., Spatial Transformer Networks, NIPS 2015, https://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BJlVNY8llV	Sklp8qhBQN	MIDL.io/2019/Conference/-/Paper111/Official_Review	[]	1		['everyone']	BJlVNY8llV	['MIDL.io/2019/Conference/Paper111/AnonReviewer1']	1548237396751		1548856720680	['MIDL.io/2019/Conference/Paper111/AnonReviewer1']
253	1548249524132	"{'pros': ""The paper seeks to address a critical issue in medical imaging, namely the lack of training data for model training and evaluation, compared to other areas of machine learning such as computer vision where hundreds of thousands of images are routinely available. In particular, the paper focuses on data augmentation strategies to improve performance of large-capacity models. The authors propose to use a variety of data augmentation strategies, including an approach they call 'pertrurbed normalization'. This approach consists of shift-and-scaling data to values picked randomly rather than 0 mean and unit variance as commonly done. The authors also compare the performance of classical regularization strategies. The evaluation is done on an ADNI subset.\n\nOverall,  it is valuable to evaluate and compare various augmentation and regularization techniques for a medical imaging task. The use of open data is a good point of the paper. "", 'cons': ' I feel that there is neither sufficient originality to support the novelty claims nor sufficient empirical evaluation to enable readers to generalize findings from this paper. There are also several issues with clarity.\n\nI detail these points below.\n\nMAJOR POINTS\n\n1) An issue with the novelty claim for \'perturbed normalization\' is that the technique is equivalent to PCA-based data augmentation [Krizhevsky2012, section 4.1], which also results in random shifts and scaling of the intensity distribution of images. I have also seen the exact same random mean and standard deviation normalization technique used in practice, although I could not find a specific paper reference or options in popular deep learning packages (keras, tensorpack, mxnet...). Color jittering is also closely related, and implemented in several packages, e.g. torchvision\'s ColorJitter (random brightness, hue, etc...), mxnet (e.g. BrighnessJitterAug and others) and others. Torchvision\'s transforms.Normalize allow the user to custom-set the desired mean and std.\n\nThe authors should explicitely cite and compare their approach to Krizhevsky2012 and color jittering.\n\n2) As presented the paper lacks clarity. In particular\n- Page 5, perturbed normalization: are \\sigma and \\mu related to the estimated data standard deviation and data means? \n- Figure 4: Show loss rather than or in addition to accuracy. Keep vertical scales consistent between training and validation. It would help generalizability claims if authors showed results for other splits (cross-validation) - it\'s not necessarily the case that the behaviour would be the same given different sampling of the data.\n- The authors should provide a breakdown of sex and age and test for differences between groups\n\nThe authors should provide more details necessary to evaluate and reproduce the work.\n\n3) The empirical evaluation is insufficient.\n- The test set is unbalanced (around 1:1.5). The authors should report balanced accuracy or at both precision and recall in table 2.\n- How many new volumes where generated in each augmentation case? \n- How many new volumes were generated for the \'full augmentation\' case? It may be that the apparently superior performance is just due to having more data, rather than the complementarity of augmentations\n- No augmentation: what condition is that exactly, compared to the rows above?\n\nThe authors should also use at least another open dataset (Look at OASIS, FCON1000 etc.) to strengthen their analysis\n\n\nMINOR POINTS\n- Discussion ""we achieved high accuracy..."": AD versus controls on ADNI routinely reaches 85%+ accuracy using simple linear methods on e.g atlas volumetry. The authors may want to qualify this statement.\n\n\nTYPOS\npage 3 section 1.2 - \'perturbet\' -> \'perturbed\'\n\nREFERENCES\nKrizhevsky, ImageNet Classification with Deep Convolutional Neural Networks, NIPS 2012', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		r1gvQWLglV	Hyxhnt1U7N	MIDL.io/2019/Conference/-/Paper105/Official_Review	[]	1		['everyone']	r1gvQWLglV	['MIDL.io/2019/Conference/Paper105/AnonReviewer3']	1548249524132		1548856720461	['MIDL.io/2019/Conference/Paper105/AnonReviewer3']
254	1548248973351	"{'pros': '\n- This paper presents a clustering method using deep autoencoder for aortic value shape clustering. It is the first work to identify aortic value prosthesis types using a general representation learning technique.\n\n- This work has a remarkable clinical value. Clustering of aortic value prosthesis shapes has a high contribution to personalized medicine. \n\n- The entire workflow is quite clear and complete.\n', 'cons': ""\n- The introduction part is a little misleading for me. The authors emphasize that the objective is to cluster the geometric shape of leaflets, and it is hard to represent the shapes in high-dimensional space (last paragraph of introduction). I'm concerned that this would make the readers misunderstand the data are shape-models (point cloud dataset) before the description of dataset in Sec. 2.\n\n- One major concern is whether the results are reliable:\n    1. The experiments shown in Table 1 compare several different network settings. This kind of vertical comparison is insufficient to support the claims made in the study. Please compare to other representation learning methods such as sparse coding (e.g. spherical K-means, dictionary learning), dimension reduction (e.g. PCA, t-sne).\n    2. This study did not give a gold-standard for shape clustering (though it could be difficult). The experiments measure the recon accuracy. However, recon accuracy highly depends on decoder network. It is not convincing to claim that the clustering is correct since even a noise can be decoded into a normal image.\n\n- In the last paragraph of the introduction, authors say 'it is hard to define a feasible metric describing the similarity of the valve shape in general.'. However, authors use Jaccard coef. and Hausdorff distance to measure the recon accuracy between original image and reconstructed image. It is a self-contradictory statement.\n\nother comments:\n- The authors use 2D images to represent leaflet shapes, I'm concerned whether 2D photograph is precise enough. 3D scanner such as CT, MRI, optical scanner could be more suitable for this work? Though this is not the issue to be considered in this work.\n- The paper is not well organized. Details of training should be more clearly written. The hyper-parameters of autoencoder and the recon decoder should be more clearly stated for reproducibility. All architectures listed in Table 1 should be stated clearly in experiments section not only in method section.\n"", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BJesxFsA1N	SJxrcwyUXE	MIDL.io/2019/Conference/-/Paper18/Official_Review	[]	1		['everyone']	BJesxFsA1N	['MIDL.io/2019/Conference/Paper18/AnonReviewer2']	1548248973351		1548856720213	['MIDL.io/2019/Conference/Paper18/AnonReviewer2']
255	1548241172325	{'pros': '\nThis paper proposes to use a CNN architecture to reconstruct MR Fingerprinting parametric maps.  The authors test their algorithm on a dataset of 95 subjects for neuromuscular disease. They compare their method with two state of the art deep learning methods and illustrate superior performance on NRMSE, PSNR, SSIM and R2 metrics. Moreover, they have done some ablation studies to show the importance of the receptive field and temporal frames for MRF reconstruction.\n\nI believe the experiments are thorough and well designed to back the claims of the paper. The utilized network architecture can be better explained with an emphasis on specific design choices.\n\n1- This paper is well written and the message is clear to the reader. \n2- The extensive tests on a real dataset instead of phantom cases is definitely a strength of the paper.    \n', 'cons': '\n3- The description of the network architecture is not clear for the reader. How does the temporal and spatial blocks work? They seem to work in different dimensions of the signals. Even though the authors explain the details in the text I believe an additional illustration in each block (maybe in Appendix) might be helpful to reproduce the method in the paper for further research.\n4- How does the specifics of the network architecture influence the performance? Why do the authors reuse  the input of a temporal block to its output and how does this influence the performance?\n5- How is the complex component of the signal concatenated into a channel ? Does the order of concatenation influence the results? Did the authors considered to utilize complex valued networks for this task? \n6- The quantitative results are yielded using multiple segmentation masks due to MR physics related concerns. Are the results on Table 1 heavily dependent on use of these masks? Are the results on the entire parametric maps in line with the current results?    \n7- What is the number of parameters required for each method in Table 1? The reason for high performance of the proposed method can be explained with the required number of parameters to train the method. Please elaborate on this.\n8-The lack of scalability and the requirement of computational time is highlighted in the introduction and abstract. However, no quantitative comparisons are provided. I believe the computational time can be added for each method in Table 1. \n\n\nMinor suggestions\n\na- Some recent work on using the complex-valued neural networks (Virtue Patrick et al., arxiv), geometry of deep learning (Golbabaee et al., arxiv)and recurrent neural networks (Oksuz et al.,arxiv) for MRF dictionary matching can be mentioned in the literature review with their strengths and weakneses.\nb- Please explain (a.u.) term in Fig.2.\nc- Quantitative results can be mentioned in the abstract.\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		HyeuSq9ke4	BJxnzFaBmN	MIDL.io/2019/Conference/-/Paper37/Official_Review	[]	1		['everyone']	HyeuSq9ke4	['MIDL.io/2019/Conference/Paper37/AnonReviewer3']	1548241172325		1548856720001	['MIDL.io/2019/Conference/Paper37/AnonReviewer3']
256	1548212314421	"{'pros': 'The paper proposed Picky Residual Unit (PRU) as a modification to ResNet. ResNet has 1 skip connection and 1 refinement path in each ResBlock. PRU has multiple refinement paths, and during test phase, only 1 refinement path is activated. To train this network with randomness during forward pass, the author proposed discrete differentiable decision based on Gumbel trick for categorical reparametrization.', 'cons': 'This paper has the following drawbacks.\n1. I\'m confused by the purpose of this design, in the abstract, the author said ""It has a slightly bigger computational price as compared to the original residual unit but greater capacity"", then ""This allows to greatly reduce the computational burden\nof the deep network while partially preserving its capacity"". Does PRU reduce or increase computation burden? My guess is, in terms of computation, PRU during training  > ResNet with the same layer number > PRU during test\n\n2. I\'m not sure about the influence of PRU. If you want to reduce computation burden, why not try fewer number of layers in ResNet, or use DenseNet, which is significantly smaller?\n\n3. PRU randomly activates one refinement path during test phase, how does that compare to ResNeXt in terms of accuracy? And what is its advantage over other techniques such as Shake-Shake and Shake-Drop regularization? They all use extra refinement paths.\n\n4. The result is not very convincing. The author used ResNet 50 and achieved 97.8 accuracy in EndoVis 2015 binary segmentation task. I found ResNet 18 achieved 96 on EndoVis 2017 binary segmentation task. (link: https://github.com/warmspringwinds/pytorch-segmentation-detection). Although 2015 and 2017 datasets are different, I doubt if ResNet18 or ResNet38 will have similar performance as your PRU 50. ', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		r1xJrU9llV	HyxGvuUr7V	MIDL.io/2019/Conference/-/Paper126/Official_Review	[]	1		['everyone']	r1xJrU9llV	['MIDL.io/2019/Conference/Paper126/AnonReviewer1']	1548212314421		1548856719784	['MIDL.io/2019/Conference/Paper126/AnonReviewer1']
257	1548210363099	"{'pros': 'The author proposed Dense U-Net, which incorporates the idea of skip connection and dilated convolution into a standard U-Net, and get a light-weight model while retaining high segmentation accuracy. The paper is clearly written, and comparison with winning solutions of the challenge makes the results convincing. ', 'cons': ""More experiments can be added to better validate the proposed model. For example, what if all layers use dilated convolution? How does FCN with dilated convolution perform on this dataset, and what is the performance of DeepLab? I understand U-Net is the standard model in medical image analysis community, but I'm still curious how do models such as DeepLab and PSPNet from computer vision community perform in medical image problems. Overall, this is still a good paper."", 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		r1gBaP2Gg4	Bkxmpg8HQV	MIDL.io/2019/Conference/-/Paper141/Official_Review	[]	3		['everyone']	r1gBaP2Gg4	['MIDL.io/2019/Conference/Paper141/AnonReviewer1']	1548210363099		1548856719564	['MIDL.io/2019/Conference/Paper141/AnonReviewer1']
258	1548195047505	{'pros': 'The main idea of this paper is to utilize the learned model from previous T tasks to help the T+1 task. In this paper, the author used the brain segmentation tasks to leverage the brain tumor detection task. The topic is useful for the community and the idea is interesting. \n\nThe brain segmentation tasks are used as T tasks to help the brain tumor detection as T+1 task. \n\nThe adaptive weighted strategy was proposed and evaluated upon the equally weighted strategy.\n\nThe network is split into task-shared layers and task-specific layers.\n\nThe multi-stage (adaption and fine tuning) method is much easier to train compared with single-stage training.', 'cons': 'Only three tasks (basically 2) are employed in this study. Therefore, the “multi-task learning” idea could outperform the T-IMM. For example, if we have M output channels for segmentation and N for tumor segmentation, we can define a single U-Net with M+N output channels to learn from both tasks. Unfortunately, such strategy was not evaluated.\n\nThe method used the Fisher information to initialize the parameters for T+1 tasks using Eq.2. To make it work, all T tasks and the T+1 task should be similar to each other or with the similar hidden true distribution. However, the T+1 task in the paper is brain tumor detection while the T tasks are brain segmentation. That might not lead to good initialization using Eq.2 for T+1 task using T tasks.\n\nThe actual implementation of Eq.3 is not clear in the paper. For example, how to optimize A in deep network? Is that an end-to-end training?\n\nIn Figure 2, the batch-norm and instance-norm are defined as task specified S while the common layers P are convolutional layers. However, batch-norm could also be common layers while convolutional layers could also be task specified. \n\nThe training size for Task 1 and 2 are relatively small for a good initialization for a segmentation network. Meanwhile, they are much smaller than Brats. Therefore, when using 100% data, the improvements are not quite large.\n\n4% and 8% of Brats data are used in the evaluation to show the advantages of the proposed method. However, the Dice is relatively low (even better). So, a more meaningful and persuasive result could be, e.g., if we use 50% of Brats, the T-IMM hit 0.81 and much better than the traditional way.\n\nThe overall performance of the proposed method has not been shown to be superior compared with the state-of-the-art performances on Brats. Also, state-of-the-art benchmarks are not evaluated.\n\nThe training and validation details are not well described. For example, the network structure, epoch selection, hyper-parameter selection etc. Without such information, it would difficult for other researchers to use the proposed method.\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		rklPRjjlxV	H1x1xrGB7N	MIDL.io/2019/Conference/-/Paper131/Official_Review	[]	2		['everyone']	rklPRjjlxV	['MIDL.io/2019/Conference/Paper131/AnonReviewer2']	1548195047505		1548856719346	['MIDL.io/2019/Conference/Paper131/AnonReviewer2']
259	1548194588099	{'pros': 'This paper proposes the metastatic lymphnod segmentation using 2D U-Net.\n\nThe clinical application is important and has not been well investigated. Therefore, this paper is a pioneer in such field.\n\nAs only small number of training data are available, data augmentation technique to increase the number of training data.\n\nDifferent sampling strategies have been included and investigated for the un-balanced segmentation.\n\nThe implementation details have been provided, which is important for other researchers to repeat the method.\n', 'cons': 'This paper would be a great clinical centered paper. However, in terms of novelty in technology, the novelty is limited. Either U-Net or sampling strategies have been widely investigated.\n\nThe MSE loss is used and optimized in this study. However, the weighted cross-entropy loss or dice loss have not been validated. These two loss functions are prevalent for un-balanced data. Recently, the focal-loss and focal-dice loss have been proposed to further leverage the performance.\n\nThe data are only divided to training and testing. Therefore, the performance could be overfitted on testing images. A more proper way is to split the data by training, validation and testing. Then the epoch selection and hyper-parameter tuning should only use validation set and apply to testing set directly without tuning.\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		S1evJSAxgN	BygNQQzHXE	MIDL.io/2019/Conference/-/Paper136/Official_Review	[]	1		['everyone']	S1evJSAxgN	['MIDL.io/2019/Conference/Paper136/AnonReviewer1']	1548194588099		1548856719127	['MIDL.io/2019/Conference/Paper136/AnonReviewer1']
260	1547857698260	{'pros': 'The authors present a method for creating high resolution DWI images from acquired low resolution images, using a super-resolution convolutional neural network.\n\n- The paper is well written and very clear. \n\n- The approach to create synthetic data is good, well explained and results are clearly illustrated (especially Figure. 3).\n\n- The approach seems to work well and produce the expected results.\n\n', 'cons': '- Only 3 samples of real human data are considered. More data would be needed to validate the approach. Especially, if between-subject variability is evaluated.\n\n- A resulting image with resolution 0.625mm is shown, but one can only see how it performed qualitatively. Showing quantitative results would make the paper more solid.\n\n- Only DTI is used and it does not model multiple fiber populations in a voxel. It would be interesting to know what happens in regions where fibers cross. One of the main reasons for going to very high resolutions in DWI is to look at regions with crossing fibers. Tractography results could be checked, since they can be generated from DTI. Other models that consider multiple axon populations per voxel (HARDI methods) could be used and the resulting profiles analyzed, as well as its related tractography results. \n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		B1gETswelE	SyxqXkggQ4	MIDL.io/2019/Conference/-/Paper115/Official_Review	[]	1		['everyone']	B1gETswelE	['MIDL.io/2019/Conference/Paper115/AnonReviewer3']	1547857698260		1548856718910	['MIDL.io/2019/Conference/Paper115/AnonReviewer3']
261	1548190926276	{'pros': 'The authors present as main subject a study of the impact of the preprocessing on lesion segmentation employing head CTs. \n\n- Both, the segmentation problem and the analysis of the preprocessing for head CT images are kind of novel for this particular application. \n- It is good that the results are well summarized in figures and tables. The appendix with all the tables is specially useful for the work purpose.  \n- Patients with several scans are not mixed between the training and validation set which is omitted in a few works introducing a bias in the results.\n', 'cons': 'The paper is focused on the preprocessing motivated by the lack of literature for this kind of study for head CT segmentation problems. However, there are a few recent works tackling the effect of preprocessing as a previous step to the segmentation employing CNNs for similar applications. \nBesides, the lack of literature problem is extensible to the lesion segmentation in head CT, because of this, it would be much more interesting if the text pay more attention to compare different state-of-the-art models along with the preprocessing.\n\nIn my opinion, the work has potential but is incomplete and still immature. Following, I present some comments (minor and major) to justify my decision: \n\nIntroduction:\n- The work points to the need of preprocessing in order to “amplifies useful patterns for a task...”. However, the results don’t show evidences. Could the deep models be simpler with a proper preprocessing? Could the models be more stable and improve their convergence rate? Which is the trade-off between depper models and proper preprocessing? Ect. \n\n\nRelated work:\n- It could be appropriate to introduce some of the recent works about the preprocessing effects, which are not “virtually non-existent”.\n\nMethods. Data description:\n- Several scans from the same patient are employed in the training or alternatively the validation phase. How are the results affected by this decision? \n- It would be nice to use some of the images included in the paper to complete the description of the classes (blood, oedema, etc.), in order to guide readers without the needed anatomic knowledge.\n\nMethods. Spatial normalisation:\n- Image registration is a complex process which can vary the images in several ways. Due to the focus in the preprocessing, the employment of an in-house tool does not seem really appropriate. It would be better to employ one of the available validated frameworks for this task (or several in order to establish a fair comparison).\n- Usually images are rotated, translated, ect. to feed a CNN during training in order to generalize the model, in this work is the opposite. This effect should be measured.\n- Readers could not be familiarized with “MNI space”. A small introduction is needed. \n- In Figure 2 the slices in each column (before/after registration) are quite different, why?\n\nMethods. Skull-stripping and intensity windowing:\n- It seems like the windows are too small, which is probably reflected in the results similarity, specially taking into account that skull-stripping is an easy step in CT images and can be easily handled by a CNN. \n- Are there hyperintense areas in your sample? How are they affect by this process?\n\nMethods. Model architecture:\n- While the architecture is well defined and can be reviewed in the “DeepMedic” paper, there is a lack of information about the training phase: transfer learning? Optimization? Learning rate? Patches? Iterations? Etc. \n- As I expressed above, the paper should be extended in order to compare the segmentation with other models, so in my opinion, the details about the employment of the U-Net are insufficient. \n\nResults\n- There are not measures about how different are the results among the experiments which must be essential in order to establish the proposed conclusions.  \n- Figure 4, specially for TBI, shows some extreme outliers, why? In my experience these points are usually due to really small areas. Sometimes, these effects can be removed with the appropriate preprocessing steps which could be essential for the purpose of this work. \n\nDiscussion: \n- In my opinion, the results does not show any proper evidence of improvement under the employment of any kind of preprocessing. The differences are not clear. An hypothetical reader, who is looking for the best pipeline to segment head CT images would infer that is not necessary to perform any preprocessing. \n- “we achieve a maximum Dice score of 52.1...”. In average?\n- “which still falls behind the best result reported on MRI 64.5±16.3”. Would you use preprocessing to reach these results employing CT? Or a different model? \n\n\n\n\n\n\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		HylG9MfleV	r1gIR4-r7V	MIDL.io/2019/Conference/-/Paper72/Official_Review	[]	1		['everyone']	HylG9MfleV	['MIDL.io/2019/Conference/Paper72/AnonReviewer2']	1548190926276		1548856718686	['MIDL.io/2019/Conference/Paper72/AnonReviewer2']
262	1548182889610	{'pros': '1) the authors proposed a method for irregularity detection and applied it to cancerous tissue detection in mammography image patches, a reconstruction network and matching network were built and a irregularity score was proposed to present irregularity likelihood;\n2) the authors presented thorough experimental results, the proposed method was compared with both recent unsupervised and supervised baselines, showing improvement over previous unsupervised methods and also indicating the gap with the supervised methods; experiments were also carried out for varied experimental settings.', 'cons': '1) the improvement of the proposed method over methods such as GANomaly is minor in terms of AUC/F1-score, although the proposed method could yield larger reconstruction error than the other methods; on the other hand, the proposed architecture is similar to VAEGAN, it may be interesting to compare with how VAEGAN works for this task;\n2) from the visualized results (Figure 4), the obtained likelihood/irregularity score seems to be correlated with intensity (especially in MLO view),  while the cancerous tissues is not only characterized by intensity difference.', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		Ske2oyiye4	HygzuBkHQN	MIDL.io/2019/Conference/-/Paper38/Official_Review	[]	1		['everyone']	Ske2oyiye4	['MIDL.io/2019/Conference/Paper38/AnonReviewer1']	1548182889610		1548856718434	['MIDL.io/2019/Conference/Paper38/AnonReviewer1']
263	1548180680954	{'pros': '1) proposed a VAE-based method to learn representations of cell images for cell profiling with adversarial similarity constraint and progressive training procedure, the proposed method explains more biological phenotype variations and achieved better performance compared to current methods based on generative models in the downstream task of classification;\n2) modified the loss function of the original VAEGAN and applied adversarial loss at multiple layers of the discriminator for more realistic reconstruction results, the idea of progressive training is novel.', 'cons': '1) the authors compared the proposed method with AE and VAE, but VAEGAN, cited as Larsen et al. (2016) in the paper, is also a related method and should be compared with;\n2) VAE was also evaluated in the work of cytoGAN, which achieved 49% NSC, VAE in this paper achieved 82.5% NSC, are network architectures, experimental settings etc different in this paper?', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		HyxX96_xeN	BJgZAhC4mE	MIDL.io/2019/Conference/-/Paper121/Official_Review	[]	1		['everyone']	HyxX96_xeN	['MIDL.io/2019/Conference/Paper121/AnonReviewer1']	1548180680954		1548856718225	['MIDL.io/2019/Conference/Paper121/AnonReviewer1']
264	1548176704835	"{'pros': '\nSUMMARY\n\nThe authors compare different CNN architectures for detection and segmentation of ductal carcionoma in situ (DCIS) in breast histological images. The comparison includes SSD, Faster-RCNN, U-Net and Micro-Net.\n\nPROS\n\nAutomated detection and segmentation of DCIS in histological images is a highly relevant problem.\n\n', 'cons': '\nCONS\n\nUnfortunately, there are numerous weaknesses as outlined below:\n\n(1) It is unclear how hyperparameters were tuned (maybe even by using the validation set?).\n\n(2) It is unclear how exactly a segmentation was obtained from SSD and FRCNN.\n\n(3) Based on the information provided in the text, the experiments cannot be reproduced. Which loss functions, optimizers, learning rates, batch sizes etc. were used for the respective architectures?\n\n(4) What is the purpose of Experiment 3.3? This should be included in the discussion.\n\n(5) The authors claim that Micro-Net achieves the best segmentation and detection performance ""because of its use of receptive field and reduced information loss due to maxpooling used in other networks"". However, this cannot be concluded from the results.\nThis conclusion would only be valid if all architectures and pipelines would have been optimized in the same way with respect to their hyperparameters. Unfortunately, this is not the case, i.e. the difference in performanc could also be due to a poor choice of hyperparameters for the other methods.\n\n(6) The text contains many typos, incorrect spacing and punctuation and should be proofread by a native speaker.\n\n(7) There is no comparison to other SOTA methods such as the work by Bejnordi et al., even though it is referred to by the authors.\n\n', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		BJlBEUUHlV	HygFSapN74	MIDL.io/2019/Conference/-/Paper160/Official_Review	[]	1		['everyone']	BJlBEUUHlV	['MIDL.io/2019/Conference/Paper160/AnonReviewer1']	1548176704835		1548856718006	['MIDL.io/2019/Conference/Paper160/AnonReviewer1']
265	1548176134776	{'pros': '\nSUMMARY\n\nThe paper “Spherical CNN-Based Brain Tissue Classification Using Diffusion MRI” presents a neural network that classifies reconstructed diffusion weighted MRI signals into white matter, gray matter and cerebrospinal fluid. \nThe network utilizes spherical convolutional layers (sCNN) with rectified linear units (ReLU) as activation function and ends with fully-connected layers that perform the classification task. Training is based on constrained spherical deconvolution (CSD) orientation distribution functions (fODF) as input and anatomical FAST segmentations as label of a single (human connectome project) subject.\nEvaluation is performed in an inter- and intra-subject manner within the HCP project.\n\nPROS\n \nThe proposed approach utilizes a new and - for the field of diffusion imaging - very interesting method: the spherical convolution.\n\n', 'cons': '\nCONS\n\nUnfortunately, there are numerous weaknesses, hence only the most serious ones will be covered here:\n\n(1) The chosen network input:\nIn order to find a good response function (RF) for a CSD reconstruction, a meaningful white matter mask is required for big datasets due to computational constraints. Therefore, using CSD fODFs as input to predict the white matter mask, which is required during generation of the input signal, does not make much sense. \nFurthermore, it should be taken into account that the fODF was generated by deconvoluting the diffusion signal with a single RF. It should therefore be easily possible for a network to learn a convolution, while the plain diffusion signal can be utilized as input.\n\n(2) The networks structure:\nMain purpose of the sCNN layers is to keep the spherical signal structure from layer to layer.\nSince the goal is to classify the input, keeping the spherical structure does not seem important for a good classification. \nFurthermore, applying ReLUs to the Spherical Harmonic signal completely removes this spherical structure, since all values <0 are set to 0. Applying different activation function (e.g. sigmoid or tanh) would most probably keep the spherical structure, in case it might be beneficial for classification. \n\n(3) Evaluation:\nThe biggest drawback of the current evaluation is that no other method was evaluated for comparison. The easiest way to compute a segmentation would be to apply FSL’s FAST on the b=0 diffusion weighted signal. Another possible comparison would be a four-layer neural network with 16, 32, 128 and 3 neurons per layer. This would prove the possible improvement due to the spherical structure.\nThe statement that the network can also be applied to other datasets/subjects needs further investigation, since the HCP Project is a very homogeneous dataset. To this end, it would have been important to evaluate other scanners, different resolutions and different numbers of gradient directions. For a proper evaluation, at least the resolution and the number of gradient directions should be evaluated, as these have a direct influence on the fODF.\n\nCONCLUSION\nThis paper utilizes an interesting network structure for an important task within the field of diffusion imaging. Unfortunately, it doesn’t get far with it.\nAs the paper states itself, only preliminary results are presented. It would therefore be recommended to further improve this work.\n\n\n', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		BJgm2DkJgN	r1xJMipE7N	MIDL.io/2019/Conference/-/Paper27/Official_Review	[]	1		['everyone']	BJgm2DkJgN	['MIDL.io/2019/Conference/Paper27/AnonReviewer3']	1548176134776		1548856717783	['MIDL.io/2019/Conference/Paper27/AnonReviewer3']
266	1548167741591	"{'pros': 'Nice application of a (meaningful) combination of SOTA methods on thoracic Computer Tomography scans. Especially the use of uncertainty weighted multi-task loss is a great feature of this work. Also, the experiments to verify that SNNs outperform BN networks for these tasks, is a positive contribution. Especially given the fact that also standard deviations seem to have been reported.\n', 'cons': '\nAlthough this can be considered a valuable contribution there are two main points that need to improved / clarified:\n\n 1. (CRUCIAL) It is not clear whether the quality of the model has been assessed in a correct fashion. First of all, it is not reported how hyperparameters were selected. If a proper hyperparametersearch is not possible, that is okay, but these hyperparameters still have to be chosen and that is mostly based on some part of the data, which might introduce a positive bias if test data has been used. It is also not clear how exactly the CV is done. If each fold consists of exactly 1 subject and there are 13 subjects in the training data, it is not obvious how 5-fold CV can be applied. Finally, section 2.1. mentioned 14 subjects and from section 3. it could be deduced that 13 subjects are used for training. Is it correct to assume that 1 subject is used for testing purposes? If so, this should be clearly stated. In general, there should be some statement how the data was split up and how each part was used.\nThe reviewer bases his overall rating on the assumption that hyperparameter selection and assessment was performed correctly. The authors should add a paragraph/section on hyperparameter selection and assessment at a prominent position in the manuscript. \n\n 2. The paper is hard to read due to grammatical issues and typos in the text and might lead to misunderstanding. Also the formulas contain inconsistencies and typos/failures, e.g. equation (3). Since the formulas are mostly from the referenced work, it should not be a problem to correct this accordingly, however. \n\nOther remarks:\n  - Since V-net is a segmentation network, it might make more sense to talk about ""V-net encoder"" or ""V-net inspired CNN"".\n  - The ""residual"" connections in V-net are rather skip-connections with the sole purpose to aid in reconstructing the segmentation map for the original image. The improved gradient flow is rather a side-effect of residual connections, just as it is a feature of most normalization strategies (not regularization).\n  - For the weight initialization of SNNs, $n$ is the number of INPUT units to each layer. The units of a layer generally denote the output units.\n  - Kendall et al. denote the $\\sigma$-parameters as measures of uncertainty rather than noise.\n  - It is not clear what the last sentence of section 2.4. means. Is an extra loss term added to force weights towards a standard Gaussian distribution? This also seems to be contradicting the fact that weights should have a variance of 1/n. This needs some clarification.\n  - It is nowhere stated what the underscore numbers in tables 1 and 2 represent. The reviewer assumes standard deviations.\n  - In the conclusion (section 4.), the overfitting problem is mentioned. Is this the overfitting on specific tasks in the multi-task setting? If not, it might be useful to expand on how this loss function could avoid overfitting.\n  - Also in the conclusion, the performance subjects #3 and #2 are mentioned, but this is pretty meaningless without the explicit results for these subjects.\n\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		ryxPCJNexV	S1gLH9iVmE	MIDL.io/2019/Conference/-/Paper92/Official_Review	[]	1		['everyone']	ryxPCJNexV	['MIDL.io/2019/Conference/Paper92/AnonReviewer1']	1548167741591		1548856717527	['MIDL.io/2019/Conference/Paper92/AnonReviewer1']
267	1548167406941	{'pros': '1) The authors proposed a method based on conditional GAN for automated segmentation of prostate using MRI scans.  In this cGAN, the generator was used as a U-net based network to generator segmentation masks, and the discriminator was a PatchGAN classifier to retain spatial information with an optimal patch size of 70x70, as suggested by previous works. \n2) The authors performed several experiments to evaluate the proposed method and its robustness against noise using prostate MRI scans from multiple modalities including ADC, DW1 and T2W and different preprocessed datasets including de-noised, noisy and raw images.', 'cons': '1) the motivation and contribution of the paper are not very clear, e.g. the authors did not explain why specifically cGAN should be a good choice for this problem and what drawbacks of previous works have been overcome by this proposed method;\n2) notations for the specific problem in the paper are needed to describe the input and output of cGAN and how segmentation was performed, more explanation of segmentation using cGAN is needed, and the description of datasets and training procedure should be separated from methodology; \n3) the results section showed scores for several modalities and datasets, however the scores were not corresponded to visualized segmentation results, analysis of the scores was not accompanied with qualitative comparison among the modalities/datasets to illustrate why the scores were high/low in different experimental settings;\n4) segmentation is a very active field in the field of computer vision for natural images and medical images, however the baselines are methods proposed in 2012, it would be more convincing to evaluate more recent methods on the authors’ datasets and use them as baselines.', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		rJleFANp1E	H1ePxYjEQ4	MIDL.io/2019/Conference/-/Paper8/Official_Review	[]	3		['everyone']	rJleFANp1E	['MIDL.io/2019/Conference/Paper8/AnonReviewer3']	1548167406941		1548856717269	['MIDL.io/2019/Conference/Paper8/AnonReviewer3']
268	1548167376605	"{'pros': 'Overall, this is a well written paper presenting an interesting idea. Combining the results of multiple U-nets with different kernel sizes for the convolutions --- cf. the Inception network(s) --- intuitively makes sense, since detecting blood vessels requires different levels of granularity to be segmented well. Also the results look good.\n', 'cons': '\nUnfortunately, a very similar idea seems to have been used in [1], where multiple kernel sizes are used to extract features maps that eventually can be used, among others, to segment blood vessels with similar performance. It is not exactly the same, but since they start from the same idea and get similar performance with what the reviewer believes to be a smaller network (4 inception layers vs 3 U-nets), it is unclear whether this paper has much value to add to the field.\n\nOther remarks:\n  - No report of hyperparameter search (learning rate, number of epochs, size of image patches, architecture, ...)\n  - In figure 3, there seem to be quite some floating vessels in the IOSTAR segmentation maps. Is this naturally possible or are these wrong annotations / artefacts? Some comments on this phenomenon would have been useful. Could this be resolved by using structured prediction, cf. [2]?\n  - It is generally good practice to train multiple models and report standard deviations for your results to convince the reader that this is a good model, rather than a lucky run.\n  - From figure 4, it seems that the activation map for the 5x5 filters should be enough to get the output maps. It would have been interesting to use the single U-net(s) to get an idea of how important it is that these results are combined, or whether a single network could already get similar performance.\n\n[1] Tetteh, Giles, et al. ""Deep-FExt: Deep Feature Extraction for Vessel Segmentation and Centerline Prediction."" International Workshop on Machine Learning in Medical Imaging. Springer, Cham, 2017.\n[2] Liskowski, Paweł, and Krzysztof Krawiec. ""Segmenting retinal blood vessels with deep neural networks."" IEEE transactions on medical imaging 35.11 (2016): 2369-2380.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		Hyxj9HXrJV	HkxYAOiVXV	MIDL.io/2019/Conference/-/Paper1/Official_Review	[]	3		['everyone']	Hyxj9HXrJV	['MIDL.io/2019/Conference/Paper1/AnonReviewer1']	1548167376605		1548856717057	['MIDL.io/2019/Conference/Paper1/AnonReviewer1']
269	1548167267105	{'pros': 'This is a nice combination of interesting techniques for the segmentation / classification of micro-aneurysms.', 'cons': '- It is not clear whether the goal is to do segmentation of microaneurysms or classification of DR. Obviously a good segmentation might help classification, but it is not clear what the eventual goal of this work is and therefore no proper way to evaluate whether it succeeds to attain its goal.\n- It is not clearly stated what the classification task is for the refinement network. The reviewer assumes this would be the binary task of classifying whether a patch contains a microaneurysm or not, but has not been stated.\n- It is not clear how the segmentation maps are improved through the refinement network. It can be assumed that patches with false positives are fixed by removing the segmentation, but this is again guesswork.\n- Results for the lower resolution network are missing in table 2, making it impossible to compare the performance. Additionally, since the recall of the single network is higher and precision is lower, there must be more false positives / less false negatives and therefore its results should be better suited for refinement.\n- The triplet loss is claimed to improve performance, but from table 2, the reader can not decide anything. After all, the result of single runs is not informative and without error bars it is impossible to judge the significance of the improvement.\n- Figure 3 is non-informative due to the objectively poor choice of colours (cyan/green and red/orange are practically indistinguishable).\n- The classification results in the discussion are completely irrelevant when the ultimate goal is to segment the images. Classification of DR is a task by itself that does not necessarily require segmentation. There are also no baselines to compare these results again.', 'rating': '1: strong reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		BJlt-XMlgN	HJesPOjE74	MIDL.io/2019/Conference/-/Paper73/Official_Review	[]	1		['everyone']	BJlt-XMlgN	['MIDL.io/2019/Conference/Paper73/AnonReviewer2']	1548167267105		1548856716846	['MIDL.io/2019/Conference/Paper73/AnonReviewer2']
270	1548166535857	{'pros': 'The authors describe their approach CapsPix2Pix for synthesization of medical image data, that can be used as training data for machine learning. They reach state-of-the-art performance while reducing the number of network parameters by factor 7.\n\n- The paper is well written and gives a good overview of the issue.\n- They will release the synthesized dataset and their code to reproduce the results. #openscience\n- The authors do a good job explaining the background and related work. They provide a nice and clear overview on Capsule Networks.\n', 'cons': 'Abstract\n\nThe authors claim that “The field of biomedical imaging, among others, often suffers from a lack of labelled data.”. This statement is not totally clear. In which aspects does the field suffer from labeled data? E.g. “machine learning for biomedical imaging suffers from a lack of labeled training data...)” This is described well in the introduction, but could be made clearer in the abstract.\n\n\nThe authors compare features of pix2pix and their CapsPix2Pix approach in Fig. 2. It is not explained, what the presented features are supposed to demonstrate. How are the presented features of pix2pix selected? Please explain this figure better.\n\n\nIntroduction\n\n“A way to resolve this is…” -> What are other ways to resolve this/ are there other approaches? E.g. how does synthesizing images compare to more traditional data augmentation as e.g. described by Ronneberger et. al.?\n \nBackground\n\nThe authors use the value function V described by Isola et al. They chose the weighting parameter λ=1 instead of λ=0.1. This choice should be explained!\n\n\n“In initial experiments, we found that standard convolutional discriminators (Radford et al., 2015) performed as well as convolutional capsule discriminators, and so opted to use the former.” -> The authors should explain, how this was found? Please provide some information on the initial experiments and what makes you confident, that standard convolutional discriminators are sufficient.\n\n\nMethods\n\nThe role of the latent vector is explained in section 3.2. Please add a reference to Fig. 1 here. (p.6)\n\n\nThe authors describe their Discriminator very briefly. As they point out the effect of capsules in the Generator throughout the paper, it would be really interesting on why they chose DCGAN Discriminators.\n\n\nDatasets\n\nThe description on how the synthetic dataset is created is very sparse. The used methods are not explained or cited. It is not clear how the SSM or the PBAM works.\n\n\nExperiments and Results\n\nThe authors compare several training datasets for the U-Net in the Quantitative Analysis. Some information is missing here:\n\n\nWhat was the size of the used training datasets? (Same number of images in all training datasets?)\n\n\nWas some kind of cross validation performed? The test set of 20 images is rather small. How did you make sure, that the images represent the data distribution correctly.\n\n\nTable 1: Please provide the meaning of the Abbreviations in the caption.\n\n\nFigure 3: Please provide more information on what the red arrows are supposed to show/highlight.\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'oral_presentation': ['Consider for oral presentation']}		rJen0zC1lE	Hyeg9SjEmV	MIDL.io/2019/Conference/-/Paper46/Official_Review	[]	2		['everyone']	rJen0zC1lE	['MIDL.io/2019/Conference/Paper46/AnonReviewer3']	1548166535857		1548856716629	['MIDL.io/2019/Conference/Paper46/AnonReviewer3']
271	1548158410647	{'pros': 'The manuscript “An Inception Inspired Deep Network to Analyse Fundus Images” presents a new architecture for retinal-vessel segmentation. The manuscript is well-written, easy to follow and presents a relevant problem for the CAD community. The evaluation protocol is robust. Images help the reader understanding concepts easily.\n', 'cons': 'Manuscript readability could be improved (e.g., some concepts are repeated several times). \nThe abstract should be less qualitative and report at least the adopted evaluation strategy and some numerical results (possibly with a comment with respect to the state of the art). \nThe section on related work should exhaustively survey vessel-segmentation algorithms. In fact, it is not clear why only three architectures (i.e. U-Net, Inception Modules and Residual Connections) were described. The authors could refer to a recent review in the field (CMPB, 2018) for a more comprehensive overview on vessel segmentation. \nIn the results, dispersion measures should be reported, too. Why Dice similarity coefficient was not computed?\nFuture research directions should be suggested to foster research in the field.\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		Hyxj9HXrJV	B1lmCBYNXN	MIDL.io/2019/Conference/-/Paper1/Official_Review	[]	2		['everyone']	Hyxj9HXrJV	['MIDL.io/2019/Conference/Paper1/AnonReviewer3']	1548158410647		1548856716370	['MIDL.io/2019/Conference/Paper1/AnonReviewer3']
272	1548154995141	{'pros': 'The manuscript “Neurosurgical and Robotic Instrument Segmentation using Convolutional Neural Networks” investigates the use of state-of-the-art CNN architectures for surgical-tool segmentation in endoscopic images. The manuscript is well-written, easy to follow and presents a relevant problem in the field of computer-assisted surgery. The investigated CNNs are quite recent (> 2015) and the authors collected a new dataset that will be made publicly available for the community.', 'cons': 'The introduction completely lacks of references. Authors’ statements should be supported by the literature. \nThe section on related work should be focused only on the investigated topic (e.g., approaches to marker-based segmentation/tracking should not be included).  \nThe comparison with a threshold-based approach (Sec. 3.2) should be removed, as non informative for the reader. It is commonly accepted that deep-learning approaches outperform the threshold-sensitive ones. \nThe training procedure is not properly described (e.g., information on optimizer and initial learning rate is missing).\nValidation is poor. Only Dice similarity coefficient was computed, and only its global mean value was reported. Other metrics should be considered (e.g., sensitivity, F1-score, area under the ROC) and dispersion measures should be reported, too. Possible differences in the segmentation of the different tools should be highlighted (e.g. with visual samples) and commented.\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		BylPGrhVxN	SJeiOu_VQE	MIDL.io/2019/Conference/-/Paper147/Official_Review	[]	1		['everyone']	BylPGrhVxN	['MIDL.io/2019/Conference/Paper147/AnonReviewer3']	1548154995141		1548856716155	['MIDL.io/2019/Conference/Paper147/AnonReviewer3']
273	1548153001489	"{'pros': 'Deep learning GAN segmentation of prostate in MRI is rather novel\nReads well', 'cons': ""GAN performs MUCH worse (0.73) in Promise12 challenge (currently 0.89). Explaining this away with having ''other data'' is too simple. Needs a more convincing argument.\n\nGAN is claimed to be much better in general?, but performs worse for prostate segmentation. Why? What can we learn?\n \nThe authors only mention old Promise12 results while many Deep Learning algorithms entered the challenge. Add new results.\n\nThe Promise 12 result by Vincent (iMorphics was not 0.89), but 0.87. Fix.\n\nThe part about noise sensitivity is unclear. Is GAN less noise sensitive? Provide a proper experiment to show this. E.g. compare with UNet? \n\n"", 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		rJleFANp1E	Bkl-nguE7V	MIDL.io/2019/Conference/-/Paper8/Official_Review	[]	2		['everyone']	rJleFANp1E	['MIDL.io/2019/Conference/Paper8/AnonReviewer1']	1548153001489		1548856715940	['MIDL.io/2019/Conference/Paper8/AnonReviewer1']
274	1548113861077	"{'pros': 'The study is to propose a deep learning framework for MS progression prediction based on baseline MR images. The study was based on a rigorous dataset and stringent design. Rigorous cross validation was conducted. By including the lesion maps, the prediction could be even improved.  One of the pros is that the study features its novelty as the first application of prognosis study with a large sample of MS longitudinal data. And, it is not a simple application of DL algorithms but the DL framework they implemented was carefully designed (e.g., the usage of skip connections, and multiple depth of convolution\noperator is used to pass features from different levels with different spatial resolutions to the final output). In addition, in the end of the paper, the variability of the model was explicitly estimated and discussed. The paper is well written and well organized. The background and technical development of the same research group was found sound. ', 'cons': 'Perhaps due to the space limitation in the conference paper, some important details are not mentioned. For example, how to integrate information from multimodal imaging is not described. And, which imaging modalities are more important in the prognosis task was not investigated and mentioned. \n\nSince the inter-rater reliability is not good for EDSS, did the same rater rate the patients at different time points?\n\nIt is obvious that removing the most inconsistent samples could improve the result. The reviewer thinks that the model uncertainty estimation based on Monte Carlo simulation with dropout is reasonable, but the following comparisons (as shown in Fig. 3) do not mean model uncertainty. Instead, to run a better statistical analysis, the model should be trained with labels scrambled for multiple times to create a ""null hypothesis"", then, comparisons of model convergence and model accuracy distribution under the null hypothesis to the real application result could be carried out to make statistical influence.\n\nMinor:\nHow to draw the ROC curves was not mentioned, based on varying uncertainty thresholds?\n\nAll four modalities are registered to each other? Did the authors rule out center effect (different scanners, imaging centers, clinical trials, etc.)?  Did the authors consider missing data? How to deal with different follow-up schedules for the two data sets (1 year with 2 scans, 2 years with 3 scans)?\n\nFuture suggestions:\n1. Unified work for segmentation and prediction. As segmentation could be another important task that DL can accomplish, the segmentation network should be integrated and combined with the current prediction network in a multiple task strategy. In this sense, manually labeled images will not be needed anymore. And the segmentation could potentially help prediction.  Other clinical factors could also affect progression prediction, which could include some non-MRI or even non-lesion-related factors.\n2. Attention network for identification of  the important imaging features. The contribution of lesion label and why it helps the prognosis are not discussed very well. Although it could be included in an independent study in the future, the most contributing brain regions should be identified and discussed (maybe using attention neural network?) to further help clinicians. \n3. Include more scans from different follow-up times to consider the baseline differences among samples. Also please include the baseline clinical scores in the model.  Although the most intuitive idea of using this data set is to use baseline MRI data to predict changes in clinical scores. However, it\'s more proper to use changes in MRI (i.e., 4D image appearance changing trajectories) to do more accurate and reasonable prediction. \n4. In the future, the current model should be compared with a model predicting the normalized delta(clinical scores), i.e., changes/baseline_level, as the final output. Different subjects have different baselines (Table 1), if only using baseline MRI scans to build the relationship to recovery, the factor of baseline performance could be a non-negligible influencing factor', 'rating': '4: strong accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'special_issue': ['Special Issue Recommendation']}"		rkliARLel4	rkgpaDAm7N	MIDL.io/2019/Conference/-/Paper112/Official_Review	[]	1		['everyone']	rkliARLel4	['MIDL.io/2019/Conference/Paper112/AnonReviewer1']	1548113861077		1548856715729	['MIDL.io/2019/Conference/Paper112/AnonReviewer1']
275	1548107742359	"{'pros': '-Authors present a multiscale neural network that can analyze WS image patches at multiple scales and classify them into one of the 6 tissue classes. \n-Multiresolution approach makes a lot of sense. It actually mimics how the clinicians/pathologists analyze the images.\n-The results look promising. \n', 'cons': ""-Novelty is limited. Similar idea was presented at MICCAI 2018. Please check the paper titled; Improving WS segmentation through Visual context-a Systematic Study. The paper should be mentioned here. \n-Experimental design is limiting in the sense that TRI-CNN always has more representation power compared to DI_CNN and MONO-CNN as it uses 3 VGG networks and 3 times more data at the bottleneck layer (resulting in learning more parameters at the first fully convolutional layer). As the authors did not fine-tune the VGG network, this may not be a very big problem but at least authors should state the number of trainable parameters for each network so that the readers will have a sense of representation power of the individual networks.\n-Authors should carefully review the literature and compare their method against the state of the art methods in the literature. Comparison against simple autoencoder is not fair. \n-authors discarded patches that are at the region borders. Such selection may introduce bias and its effects must be discussed in the discussions section. What happens is a border tile is fed into the networks? As the network is capable of producing inference results, does the border tile's content will be rightly represented in the inference outcome (higher in prob for contained regions and less for the not contained ones)\n-The authors rely on the fact that VGG is a capable f extracting good features from each scale. Have the authors ever tried to fine tune the VGG for each scale and specialize. \n-Which level of VGG is used as features? How was that layer chosen?\n-What is the loss function? CE?\n-Few typos and grammar mistakes in the paper\n"", 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		rJlDYnoJlN	Hyl8kga7QV	MIDL.io/2019/Conference/-/Paper42/Official_Review	[]	1		['everyone']	rJlDYnoJlN	['MIDL.io/2019/Conference/Paper42/AnonReviewer3']	1548107742359		1548856715461	['MIDL.io/2019/Conference/Paper42/AnonReviewer3']
276	1548105235669	"{'pros': 'Authors proposed a novel method to combine the fluorescence and reflectance confocal microscopy images in order to generate H&E like images. As the more general cohort of (dermato)pathologists are trained to read such images, the presented method will surely help in faster adoption of the technique to the clinical practice. \n\nThe results look very compelling and more realistic compared to the available methods in the literature.\n\nAuthors took the results of the state of the method as the initial solution and improved over their results. This is a very good estimation and resulted in very good results. ', 'cons': '-Novelty in terms of technique is limited. Authors took the Cyclegan idea and applied it to their problem. \n-computational complexity and the processing time must be presented. The SOTA method is very simple and works in real-time. How about the presented method. \n-With the SOTA DHE method, the FCM and RCM images are linearly mapped to color images. On the other hand, authors do not have such control over the GAN network meaning that anything artificially introduced in the images may not be realized by the readers, potentially resulting in ""wrong diagnosis"". Authors should also comment on this issue and potentially mention it as a shortcoming of the methods (unless they can propose a method to regularize the results). \n-The statement on main is a little misleading. As the method takes the SOTA DHE images as input and makes their staining more like H&E slides, the presented method is a stain normalization method rather than a staining method. \n-Training partition is not clear. Did the authors slide wise or tile wise partitioned the data? \n-input-Output pairs of both denoising and staining networks should be clearly stated. For example in Figure 4, what is L_cyc, L_adv, L_id. All these should be clearly defined and explained. \n-In the abstract, authors state that Ex-vivo CM can be used to identify tumors with overall sensitivity if  96.6% and specificity of 89.2%. Authors should clearly state the disease that they give the statistics for (e.g. BCC)\n-Authors need to give a reference at the end of the first paragraph on page 2 after; ""... complex surgical operations in skin cancer""\n-In section 2.3.2, the total number of images does not add up. Authors state that their dataset consists of 8789 images. In section 3.1 they state that training is conducted on 7031 images and testing is conducted on 1748 images. \n-How did the authors obtain the grayscale images? Did they use color deconvolution to decompose the images into Hematoxin and Eosin channels? Which deconvolution method did they use? if they simply used RGB2YUV conversion, then what are the other steps that they used to make grayscale histology images appear similar to RCM images. \n-How do the authors obtain noise images? What are the parameters of the noise model?\n\nIn general, the paper lacks of implementation and experimental design details. Authors must include these details in the final version of the paper. \n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		rJxj3vUel4	B1l3GU3X7N	MIDL.io/2019/Conference/-/Paper109/Official_Review	[]	1		['everyone']	rJxj3vUel4	['MIDL.io/2019/Conference/Paper109/AnonReviewer3']	1548105235669		1548856715139	['MIDL.io/2019/Conference/Paper109/AnonReviewer3']
277	1548095271773	"{'pros': 'The paper describes a deep learning-based DICOM image sorting for separating various structural imaging modalities for potential tumor imaging studies. They treated sequence discrimination and contrast enhanced image discrimination as two separate classification tasks and tried to concatenate these two to let the first task help the second (more difficult) task. The accuracy of sequence classification is high, and that of the contrast image discrimination is lower but still acceptable. \n\nThe pros is that this study utilize a modern classification framework with better performance than traditional machine learning algorithms to deal with a new problem. DICOM sorting has been previously based on manual or toolbox-based methods (with the latter only used certain specific DICOM information, which could be easily removed during data sharing or not feasible in some cases). Generally, this paper proposed a good solution to this problem in real data analysis. \n\nAnother pros is that this study used a rigorous validation set instead of cross validation, which reduces the concern of over fitting. The performance is thus more reliable. \n\nThe third pros is that this paper, for the first time, proposes that DICOM sorting can be done based on images themselves, rather than only on the DICOM information. This is very innovative and should be encouraged in the MRI study field. Deep learning should be playing more and more roles in the traditional MRI processing field. ', 'cons': 'The biggest concern from the reviewer is that the paper, when introducing the background, did not mention currently existing software and toolboxes for DICOM sorting ""based on the information retrieved from DICOM files"". Actually, to the reviewer\'s own experience and best knowledge, there is always that one of these available toolboxes works well on DICOM sorting and they are designed based on very simple ""matching-based"" scripts. The results of these toolboxes is that the DICOM files belonging to the same scan (whatever this scan is for T1w, or T2w, or something else) will be put into a new folder with their common sequence name. The reviewer acknowledge and agree with the authors that these sequence names could be different for different scanners and imaging centers and even different imaging staffs. However, it is usually easy to make a decision on whether this folder is all T1w or T2w, or T1w or fMRI, or T1w or DWI, etc. In certain cases, researchers need to visually check the files in the folder to make sure of their modalities. For example, to make sure whether it is T1w or T1wC,  but again, this is not quite common, and application specific (e.g., tumor studies that both have T1w or T1wC but the decision cannot be made based on their DICOM information) -- in this case, image-based sorting could be of help.  In addition, the goal of DICOM sorting is not only for sorting the dicom files but also make them better transformed to NIFTI format, where the dicom files belonging to the same scan will be converted to a single nifti file (multiple slices to a single 3D image). The paper, however, seems more likely to treat each slice as a basic unit for the following analysis and do a simple classification task.  Therefore, the reviewer suggests that the paper should be revised by adding certain application scenarios into the story telling and acknowledge these existing automatic dicom sorting software & toolboxes. It\'s better to provide, with real application(s), why traditional dicom sorting software & toolboxes could fail but the proposed method could work.  The reviewer agree that it is all for future applications and the current version of algorithm subjects to future extension, but by adding some additional comparisons will further convince people to use or to contribute in it. \n\nSpecific comments are given below:\n1. The authors did not mention software for dicom sorting, e.g., MRIconvert, dicomsort function in REST. These toolboxes automatically read dicom info and, based on that, sort them into different folders. It usually works, and is really fast. It also works for data from different scanners or from different runs/scans.\n2. The application this paper proposed is quite specific to structural MRI data sorting. However, in more general cases, functional data (different scans of fMRI, DWI) are also needed to be sorted. This will add more complexity in the algorithm and could be hard sometimes. \n3. For image preprocessing, reslicing to the same dimension could be unfeasible because there exist thick-slice and thin-slice images. And, if a slice includes too much background, then, it could be calibrated too bright as the background is always very dark. Instead, a 3D-version of normalization could be better. In addition, z-scoring is feasible for whole training or validation set, however, it cannot be applied to a single testing image.\n4. The real data this paper used is highly specific. If imaging categories were changed, then the model cannot be used.  If the images are from high-grade glioma rather than LGG, the results could be different, as HGG has different contrast than LGG in terms of T1C.  \n5. The reviewer is not sure if the performance reported here is solely based on the validation set.\n\nSuggestion: \n1. The authors should compare the accuracy and timing of their method with traditional dicom sorting software.\n2. Since PD is not always scanned in other studies. If it is removed from the final target, the accuracy of sorting other modalities could be even better.\n3. Both PD and T2-FLAIR have big potential to be sorted as T1w or T1c, why? Did the authors check the inconsistent results?\n4. The lower performance in Table 6  is reasonable because T1w and T1wC look very similar in LGG images, i.e., glioma itself is not enhanced. The only differences could be in the big vessel and artery. Therefore, more complex neural network structure should be used to pay more attention to these regions.', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		rkg7DD-gl4	SJxlNk57XE	MIDL.io/2019/Conference/-/Paper63/Official_Review	[]	2		['everyone']	rkg7DD-gl4	['MIDL.io/2019/Conference/Paper63/AnonReviewer1']	1548095271773		1548856714883	['MIDL.io/2019/Conference/Paper63/AnonReviewer1']
278	1548089882251	"{'pros': 'The present work shows a brief analysis of what could be the performance of a convolutional neural network (U-net) when it is trained with weak annotations instead of manually annotated data. \nIn most cases, annotating biomedical data manually is expensive in terms of time and expertise (annotations should be done by experts). As deep learning has shown great potential in image processing tasks and it also requires a considerable amount of training data, it is necessary to search for new ways of obtaining training datasets. \nThe authors propose different methods to do it and they show that it is possible to achieve similar accuracy values when using new non-annotated data generated with vanilla U-net or manually annotated data. \n', 'cons': 'Some parts of the manuscript are not clearly explained:\n- When describing materials, authors talk about image patches instead of images (""5602 image patches...""), why? Were the original microscopy images cropped and then, annotated? Is there any specific reason to do it?\n- In section 3.3 a small test set was used to fit the parameters given. Is this set included later in the training of the U-net? The cost function used for the optimization of model-based segmentation is evaluated over a small set of the ground truth. Could it be said then, that in case of using this method, it would be necessary to have some manually annotated images?\n- In section 5.1, the following procedure is not clear: ""In order to obtain meaningful results, we separate our data into three subsets - two of which are used for training, while the other is reserved for testing. This is repeated 10 times for every test set""\n- Figure 1. Please, correct the caption and explain the pipeline briefly. The diagram is quite confusing. \n- In Figure 2, it would help to show the ground truth as well. \n- The authors repeat several times that the network ""has not been adapted to the kind of training data we generate"". What do you mean?\n- It is not clear at all the differences between the first experiment (Figure 3) and the second one (Figure 4). Are you referring to ""Computer-assisted annotations"" and ""Segmentation""? Please, be more explicit and explain clearly how each experiment was carried out. \n\nThe significance of the results shown in this work could be low due to the dataset used for it: The objects (i.e. cells) to segment are always rounded so it is easier to generate weak annotations. I would suggest as a future work evaluating the proposed approach using more heterogeneous datasets: images in which cells are not rouned, images from different microscopy modalities or even partially annotated images. Besides, it would be interesting to compare this performance with an unsupervised method in order to quantify the gain of weak annotations when training a network. \n\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BklpVE1eg4	r1gGm9dQmV	MIDL.io/2019/Conference/-/Paper49/Official_Review	[]	1		['everyone']	BklpVE1eg4	['MIDL.io/2019/Conference/Paper49/AnonReviewer2']	1548089882251		1548856714623	['MIDL.io/2019/Conference/Paper49/AnonReviewer2']
279	1548085000900	"{'pros': '- This work achieves competitive results.\n', 'cons': ""- Lack of novelty. As authors stated, dense connectivity and dilated convolutions have already been proposed in many other segmentation problems. Therefore, the technical contribution of this paper is very limited. In addition, there exist some other recent works that have included dense connectivity in UNet architectures.\n- Choice of hyperparameters is not motivated. For example: 'Compared to standard U-Net, most importantly, our model uses dense connection in layer seven to nine. For example, both layer one and two are concatenated back to layer nine after deconvolution'. Why to employ those connections and not connect all previous with subsequent layers? Why to employ only one dilated convolutional layer before the bottleneck? Which would be the effect of adding more dilated convolutions through the entire network? As one of the advantages of dilated convolutions against pooling is to increase the receptive field without scarifying resolution, replacing pooling layers by dilated convolutions would have a positive or negative effect?\n- Authors also state that one of the benefits of the proposed network is to reinforce the features reuse. Nevertheless, I somehow feel that authors fail to demonstrate this. In Table 4 they show that by employing 50% of the parameters performance only decreases by 2-3%. Despite this, authors never explained how these parameters are selected. There is a nice features reuse analysis in the original DenseNet paper (Fig. 5), as well as in this medical segmentation work, where authors show the importance of features reuse at different layers ([1], Fig. 6).\n- Two major concerns with the evaluation is the date when authors accessed the challenge results and the metrics employed for evaluation. Authors should show in Table 5 the updated results (at least from the date of submission) and not from June 2018. Further, evaluation of the challenge also provides results for other metrics, which are also important to assess the performance, such as Hausdorff and Average Surface Distances. Authors should include these metrics in their evaluation.\n- Authors fail to include a relevant literature on WMH segmentation related approaches, particularly CNNs.\n- Even though the paper is well structured, there are many unclear things. For example, Table 3, according to the authors, 'it reports scheme design of our architecture.'. However, I only see DSC values.\n\n[1] Dolz et al. 'HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation.' arXiv preprint arXiv:1804.02967. 2018 Apr 9."", 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		r1gBaP2Gg4	BygWMwDQQN	MIDL.io/2019/Conference/-/Paper141/Official_Review	[]	2		['everyone']	r1gBaP2Gg4	['MIDL.io/2019/Conference/Paper141/AnonReviewer2']	1548085000900		1548856714399	['MIDL.io/2019/Conference/Paper141/AnonReviewer2']
280	1548068268863	{'pros': '- The authors present a deep learning based approach for simultaneous optic disc (OD) and fovea localization and laterality prediction in high resolution ultra wide field of view scanning-laser ophthalmoscopies. \n\n- A residual architecture with multitask branches is introduced to perform this task.\n\n- The results show that the laterality prediction task is solved with an accuracy of 99.6%, which is in paired with the performance of human experts.\n\n- The draft is well-written, with minor organization issues that can be easily addressed.\n\n- This is the first study proposing to solve this problem in this imaging modality. All the existing approaches were used before in color fundus photographs.', 'cons': '- My main concern about this proposal is related with the evaluation and the results of the OD/fovea localization tasks. In particular:\n\n--> The evaluation using the estimated optic disc diameter (which is a function of each region coordinates) showed that the performance is still too poor compared to human experts (Table 1). Figure 4 also shows that the differences with the ground truth are way too high yet. The lack of baselines turns really important this comparison.\n\n--> The evaluation metric is not standard for this task, according to recent literature in the field [1]. Authors should evaluate the performance of the algorithm using the Euclidean distance of each predicted localization with respect to the ground truth, and also using the success rates applied in [1].\n\n--> It is clear that this task was never done before on this imaging modality. However, it is necessary to see if this novel architecture is the way to solve the problem or if adapting an existing approach is already enough. Authors could use the public implementation of [1] (as released in https://github.com/minesmeyer/od-fovea-regression) to have at least an automated method as a baseline. \n\n--> Table 1 includes too much information and it is really difficult to understand. As far as I see, the results of the observers were obtained on a subsample of 100 CP images. However, the CNN metrics were computed for all the images in the set. As a consequence, the values cannot be directly compared. Moreover, including the results of laterality classification does not aid this comparison. I would suggest to report the results in different tables: one for the OD/fovea localization and one for laterality classification. In the table for OD/fovea localization, I would suggest to report the performance of the algorithm on the full sample of images (authors can include there the results of the baseline, too). The comparison with the experts could be done using a box-plot, a new table or a Bland-Altman plot. \n\n- It is no clear why the authors did not use a single branch to accomplish both OD and fovea coordinate determination. Was this a design choice based on empirical evidence? Have they observed that separating both tasks do not help to improve performance? If that is the case, it would be interesting to report the results obtained by separating the tasks, too, for comparison purposes. Otherwise, I believe it is necessary to separate them: the fully connected layers are the only task-specific parameters, so each of the tasks might need their own parameters to obtain better results. Maybe one valuable alternative is to share a second fully connected layer, and then split the tasks on a third one. \n\n- One important thing to consider in multitask learning is the weighting between each of the tasks. According to the draft, authors did not weight each of the losses but summed them up to get a single value (which is equivalent to set a weight of 1). It would be interesting to see if balancing the losses using different multipliers can improve the results of the OD/fovea detection task.\n\n- Some other comments:\n\n--> The related works section did not include the recent approach presented in [1].\n\n--> Please, rephrase the second paragraph in Page 6. It is not clear if there are repeated subjects in the training/val/test sets.\n\n--> The laterality classification task looks certainly solved. It would be nice to include in Section 3 (Materials) the number of left/right images at least in the test set, to see if Accuracy was a suitable metric for evaluating the results. If the distribution is unbalanced, please report the performance in terms of other metrics such as the F1-score.\n\n\nReferences\n\n[1] Meyer M.I., Galdran A., Mendonça A.M., Campilho A.. A Pixel-Wise Distance Regression Approach for Joint Retinal Optical Disc and Fovea Detection. In: Medical Image Computing and Computer Assisted Intervention – MICCAI 2018, LNCS, vol 11071, pp 39-47, 2018. doi:10.1007/978-3-030-00934-2_5', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		rke8t4Gxg4	SkxB3r7QmV	MIDL.io/2019/Conference/-/Paper74/Official_Review	[]	2		['everyone']	rke8t4Gxg4	['MIDL.io/2019/Conference/Paper74/AnonReviewer1']	1548068268863		1548856714175	['MIDL.io/2019/Conference/Paper74/AnonReviewer1']
281	1548063833963	{'pros': 'The paper uses a generative adversarial model for the semantic segmentation of the prostate on different MRI modalities. Employing generative models for semantic segmentation and more broadly for scene understanding is an interesting and promising approach as it offers to provide uncertainty measures for the task at hand, increased robustness to label and image noise and other such desirable properties for vision models. As such, the paper is concerned with an interesting model and problem class.', 'cons': 'Although there are compelling arguments as to why one would like to generatively model perception tasks, the paper does not give a very clear motivation. The results do not appear coherent and are not presented clearly enough. Importantly the paper does not show the performance of baselines on the in-house dataset it employs, not even the simple and extensively used U-Net is evaluated comparatively. Furthermore, despite claiming otherwise, the paper lacks novelty as adversarial networks have already been employed on a prostate segmentation task.\n\nThe following list gives more detail to the above assessment:\n\n1.\tThe paper merely states ‘Among various deep learning approaches, Generative models have shown to be one of the most promising models for the segmentation tasks’, but doesn’t give any other motivation for why they choose this model class. There are good reasons why this could be attractive, but they are not explained here. This lack of motivation also weakens the experiments/results as there is no context that is specific to this model class (deterministic models could have been evaluated in the presented fashion, and in fact they should have, see below).\n2.\tThe paper states ‘To the best of our knowledge, no work has been done towards prostate MRI segmentation using generative models’. This is wrong, as there is a paper using conditional adversarial networks to segment prostate MRI: https://arxiv.org/abs/1702.08014\n3.\tThe introduction refers to different segmentation models employed for segmentation but omits a recent one based on variational inference, the Probabilistic U-Net (https://arxiv.org/abs/1806.05034).\n4.\tIn essence the paper trains cGANs on a training dataset which is altered by application of different filters (blurring & deblurring). It is unclear why this requires cGANs (again no motivation is given) and regular deterministic models could have been employed.\n5.\tAlthough at the core of the papers’ analysis, the exact composition of the different datasets (and the model training) remains unclear. It is unclear in what proportion raw and noisy images were combined on the respective datasets and whether any data-augmentation was used whatsoever. If no further data augmentation was employed it is rather unsurprising that adding blurred/de-blurred data to the training set improves performance.\n6.\tTable 1 results seem quite inconclusive and the discussion thereof does not seem to present it in a more structured form either.\n7.\tThe robustness analysis lacks clarity/context, e.g. why is $h_j$ mutiplied by 3? Also, on ADC & DWI, The DSC results for Denoised(h_i^1) look significantly different from Denoised(h_i^2), so it is unclear how the robustness claim can be made from these experiments?\n8.\tIt is unclear whether training images were sampled indefinitely when noising/denoising them. From the discussion section it could be inferred that this is not the case (as it is being implied that combining raw + noisy + de-noised incorporates more training examples than e.g. raw + noisy). Assuming they were not, the number of raw and the number of noisy images in their respective datasets should be equally large. If so, it seems hard to explain why the segmentation performance on the test set improves when trained on the same images but all of them are blurred by a filter (true for both ADC & DWI). Why is that?\n9.\tThe Pix2Pix model from ‘Image-to-Image Translation with Conditional Adversarial Networks’ was employed. In the paper it was found that the models learned to ignore the injected latent noise, which was mitigated by using ‘unignorable noise’, i.e. drop-out here. Fig. 2 suggests the present paper still injects latent noise. It would be interesting to learn whether the authors managed to condition the models on this noise rather than e.g. dropout.\n10.\tFig. 2 seems wrong as it is said to depict a cGAN but the generator is not conditioned on an input image, as shown presently, it is only conditioned on noise.\n11.\tIn Table 2 the method’s performance is compared to entries in the PROMISE12 challenge. The paper appears to compare its DSC performance achieved on the in-house dataset of 40 patients versus the PROMISE12 results. This does not make much sense and raises the questions of why a) the cGAN approach was not trained and was not submitted to PROMISE12 (the current DSC of 0.73 does not even make it into the top 200 on PROMISE12, so Table 2 is both misleading and very unfavorable) and b) why well performing approaches on PROMISE12 were not used as baselines on the in-house dataset. Simple U-Net approaches arrive at 0.89 DSC on PROMISE12 and should have been compared to in the considered tasks.\n\n\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		rJleFANp1E	H1lGP4fmmV	MIDL.io/2019/Conference/-/Paper8/Official_Review	[]	1		['everyone']	rJleFANp1E	['MIDL.io/2019/Conference/Paper8/AnonReviewer2']	1548063833963		1548856713907	['MIDL.io/2019/Conference/Paper8/AnonReviewer2']
282	1548035633681	"{'pros': '- FCN is trained using partially segmented images. The trained network can predict labels for the entire volume.\n- New loss function for the specific purpose is proposed.', 'cons': '- Definition of ""partially segment"" should be explained more clearly.\n * It is desired to cite some papers about semi- and weakly-supervised- segmentation.\n * Is your goal different from semi-supervised segmentation? You need to explain the difference. If they are same or similar, you should include experiments for comparison.\n\nOther small points:\n- There are many words in equations, such as ""loss"" or ""mode"". They should be replaced by one character. For example, a scalar by ""p"" or a function ""f({\\boldmath x})"". If you really need words, you should not use italic but roman style.\n- There are so many missing mathematical variables. For instance y^c_{i,j} in Eq. (1-3)\n- Equations also look weird. In Eq. (1), what are input variables? In Eq. (6), what does the arrow mean?', 'rating': '2: reject', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		S1gXSzNexN	Hygc4UoGQ4	MIDL.io/2019/Conference/-/Paper93/Official_Review	[]	2		['everyone']	S1gXSzNexN	['MIDL.io/2019/Conference/Paper93/AnonReviewer1']	1548035633681		1548856713650	['MIDL.io/2019/Conference/Paper93/AnonReviewer1']
283	1548035313881	"{'pros': 'The paper is well written, with following contributions:\n- Segmentation of seven cardiac anatomies on non-contrast CT (NCCT) is proposed. These anatomies are difficult to be divided on NCCT images due to their similar intensities.\n- It was validated that the FCN worked well for NCCT dataset, by training with virtual-non-contrast (VNC) dataset, which have corresponding CT angiography (CCTA) dataset. Anatomies can be manually trained on the CCTA dataset.', 'cons': '- For the paper of MIDL (Medical Imaging with Deep Learning), computational process should be enhanced and focused. Currently only section 3.1 explains that.\n * Preprocessing\n * More explanation of choice of basal network. Does the network for real-time style transfer work better than other commonly-used network, such as U-net? I don\'t mean you need to compare the performance, but just write the motivation of the choice.\n * Procedures of cropping patches and merging their outputs. Patches are cropped with overlaps or 5-slice interval? Input patch size is 256x256x5, but how about the output size? \n * Data augmentation procedure (if you are doing)\netc.\n- Figure of the original network structure and modified parts is desired.\n- Left part of Figure 2, why two ""ref. segm."" exist?', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BJxgkz-xeV	B1xclSof7N	MIDL.io/2019/Conference/-/Paper58/Official_Review	[]	2		['everyone']	BJxgkz-xeV	['MIDL.io/2019/Conference/Paper58/AnonReviewer1']	1548035313881		1548856713430	['MIDL.io/2019/Conference/Paper58/AnonReviewer1']
284	1547936323678	"{'pros': 'The authors have chosen to tackle an important problem in using unsupervised deep learning methods to reduce the amount of labelled data needed for classification of disease. Furthermore the problem of opacity detection/classfication in lung CT images is a good choice for this, as there are a range of potential abnormalities and this is an important and common clinical problem.\n\nThe adversarial autoencoder approach they have taken is generally well-motivated and promising, though I have some specific concerns (see below). Whilst the results are clearly not yet at the level of supervised methods, I believe they are promising for an unsupervised method and therefore the paper represents an important advance. The authors make a good case for considering both local and global representations when performing clustering.', 'cons': 'The most important downside of this paper is that insufficient detail is given to enable readers in the community to reproduce the experiments.\n\nConsider the CAAE schematic in Figure 4. The definitions of Block 1 and Block 2 do not give crucial details such as the number of feature channels at the output of each convolutional layer block, the size of the convolutional kernels, the size of the max-pooling operations, the stride used in the deconvolution operation, and how the convolutional feature map is reshaped into a feature vector at the end of the encoder (is it a global max pooling operation or a flattening operation?). I would recommend either including these details in Figure 4, or adding the additional details in an appendix, which is permitted by the MIDL format.\n\nAnother important missing detail is how non-lung ROIs, for example patches containing muscle/fat, spine, and/or mediastinum, are treated by the model during training and testing. It appears that they are not treated as the ""normal"" class, which judging by the images in the bottom row of Figure 7 is reserved only for normal regions within the lungs, but there is no mention of a ""non-lung"" class used during training. Are these ROIs simply omitted from training and testing? If so, this process would require initial segmentation of the lungs in the images at both training and test time. This could be done either manually, which would dramatically decrease the usefulness of this model as an ""unsupervised"" learning method, or automatically, but no details are given of this.\n\nHowever the most important missing detail is how the classifier is obtained once the GMM clusters have been found. The paper describes how the model is trained to produce 192D feature vectors for a given combination of ROI and slice. It then describes at the end of section 2.2 how these are clustered using a GMM algorithm to give 64 clusters. But then absolutely no detail is given about how the mapping is defined between these 64 clusters and the 6 classes. It is very important to explain this step in detail for reproducibility but also because it is important to understand how much labelled data is used for this process, as presumably there must be some. If the entire training set is used the this purpose, the method cannot be described as an unsupervised method. The method is only useful if this mapping can be learnt with only a few labelled examples.\n\nThere are also no details given of how the dataset is divided into training and testing sets. This is a very worrying omission. If it is simply an omitted detail then it can be easily fixed in the revised submission, but *if the images on which the precision/recall results are reported were also used to train the model then the results are close to meaningless as the generalisation performance of the model has not been demonstrated*.\n\nAnother weakness of this paper is that the dataset is highly imbalanced between the 6 classes, but there is no method for alleviating this problem described. It is notable that by far the worst performance is achieved on the nodular class, which is also the most under-represented class in the training set.\n\nSome of these missing details are critically important. If all these issues are resolved satisfactorily in a revised submission then there may be a case for accepting the paper, but in the current state it cannot be accepted.\n\nAs a comment on the chosen methodology, the authors\' choice to use a plain Gaussian distribution for the prior distribution in the CAAE and then perform post-hoc clustering of the feature vectors deviates from the original Adversarial Autoencoder paper (Makhzani et al 2015, Adversarial Autoencoders, https://arxiv.org/abs/1511.05644 ). This is especially surprising given that the authors cite the Makhzani paper as the primary source for the AAE method. In section 6 of that paper the authors describe a method to perform unsupervised clustering by incorporating a categorical \'y\' variable directly into the latent representation of an image, which learns the cluster assignment during the adversarial training process. It seems likely to me that this method would result in better class separation than the method used in the current paper. The authors should therefore justify their deviation from the methodology of their primary source.\n\nAs a minor detail, the authors appear to be be using the term ""linear"" to distinguish convolutional layers from what I guess are fully-connected layers. The authors should clarify this point. Technically convolutional layers (before the ReLU) are just as ""linear"" as fully connected layers, they just have different connectivity/sparsity patterns.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		HylqNO4RkV	Skl3rMX-7N	MIDL.io/2019/Conference/-/Paper12/Official_Review	[]	1		['everyone']	HylqNO4RkV	['MIDL.io/2019/Conference/Paper12/AnonReviewer2']	1547936323678		1548856713206	['MIDL.io/2019/Conference/Paper12/AnonReviewer2']
285	1548013996836	{'pros': 'The authors present a novel (to my knowledge) application of a CycleGAN in combination with a simple simulator model to augment a data set of manually labeled cell nuclei. They use a composite loss function to impose similarity constraints on the generated images, and evaluate the performance of a U-net to segment image pixels and identify cell nuclei, including immune cell nuclei. They demonstrate that training only on the simulated data achieves performance levels close to supervised training on labeled data, and that GANs provide benefit in settings with few available ground-truth annotations.\n\n-Given the time and labor cost required to annotate large data sets, it is important to develop effective methods for unsupervised training set generation / augmentation. The authors provide an interesting result demonstrating a promising approach for such tasks in pathology and similar settings.\n\n-Benefit of CycleGAN over simulation-only data is significant, with convincing rationale provided, and generated images which are plausible by inspection.\n\n-Benefit of CycleGAN in real-world context with small amounts of ground-truth labeled data (OAA scenario) is convincing.\n\n-Good set of experiments conducted to elucidate the strengths and weaknesses of this approach.', 'cons': 'In general, the paper lacks many important implementation details regarding network architecture, training procedure, and evaluation metrics, making it difficult to reproduce. See below:\n\n-In part 3) Evaluation, it is unclear exactly what the authors mean by “proximity” in the definition of the evaluation metric e.g. “TP means a prediction is in the proximity of a labeled nucleus”. Please define specifically how this is determined. \n\n-Please clarify additional details of your CycleGAN and U-Net architectures (number of layers, layer sizes, connectivity between layers, preprocessing steps required, etc.) – this need only be brief if they replicate existing architectures out of the box, but it does require clarification.\n\n-Please provide additional details on the network training procedures for the CycleGAN and U-Net, e.g. optimization procedure, learning rate, stopping criteria, total number of simulated images trained on. In particular, for the annotation+GAN augmentation training procedures, please explain in more detail how the generated data examples were combined with the real-world labeled data examples (e.g., in what proportion, etc.)\n\n-It would be helpful to know how long it took to train these architectures, and on what machines, for future implementation purposes.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		B1en70uU1V	SygBnWIGQV	MIDL.io/2019/Conference/-/Paper3/Official_Review	[]	1		['everyone']	B1en70uU1V	['MIDL.io/2019/Conference/Paper3/AnonReviewer3']	1548013996836		1548856712949	['MIDL.io/2019/Conference/Paper3/AnonReviewer3']
286	1547928727958	"{'pros': 'Summary:\nThis paper considers an alternative to region-overlap-based loss functions. Their boundary loss considers the integral of the area between the ground truth and predicted regions. It seems this loss function is less affected by class imbalance in the image, as it produces accurate segmentations for small and rare regions in their example figure.\n\nPros:\n- Novel idea for dealing with imbalanced classes. \n- Good reasoning for design of loss function.\n- Visualizations look nice.\n- Code is available online.\n\nQuestions:\n- You argue that most people only consider regional losses. But what about Hausdorff distance? That is based on the distance between boundaries of regions.\n- Sec. 1) ""..these regional integrals are summations over the segmentation regions of differentiable functions, each invoking.."". What are the differentiable functions here? Cross-entropy loss?\n- Sec. 1) ""... [graph-based] optimization for computing gradient flows of curve evolution."" Can you explain a bit more what this is about?\n- Sec. 2) What do you mean with \\mathbb{R}^{2,3}? That the image can be either 2D or 3D?\n- Sec. 2) You defined \'I\' as a training image and then didn\'t use it. Wouldn\'t it suffice to just say that Omega is your space of images?\n- Eq. 1) I assume the subscript B in w_B is from \'background region\', i.e. B = \\Omega \\setminus G? And not \'boundary\' as the subscript B in (5)?\n- Sec. 2) I don\'t understand why you would use the notation \'\\partial G\'. I would read that as \'change in the foreground region\'.\n- Sec. 2) Is q_{\\partial S}(.) unique? I can imagine that if \\partial G is not a circle (as in your example fig. 2), then multiple p would map to the same point on \\partial S.\n- Sec. 2) Is the signed distance between p and z_{\\partial G}(p) Euclidean?\n- Sec. 2) Is the sign in the signed distance necessary to flip the sign of the area of S in the interior of G (the part ""below the x-axis of the integral"" as it were)?\n- Sec. 2) What is actually the form of the level set function \\phi_{G}? Pixel distance?\n- Sec. 2) If the \'boundary\' is the sum of linear functions of s_{\\theta}(p), then is its gradient constant?\n- Sec.3) Are you sure you are allowed to use ISLES and WMH for this paper? For WMH at least, there is a rule in the terms of participation that you are not allowed to use the data for scientific studies other than that of the challenge.\n- Sec.3.2) Why do you need to start with the regional loss term and then slowly build up to the boundary term?\n- Sec.3.3) I am now quite interested in the performance of {\\cal L}_{B} in isolation. Why did you not report that?\n- Sec.3.3) You argue that the boundary loss helps to stabilize the learning process. But isn\'t the change in noise that you observe in Fig 3. coming from a difference in scaling in the loss terms? That is, if the scale of the boundary loss is smaller than that of the regional loss, and you\'re gradually shifting towards the boundary loss, then I would expect smoother curves over time.\nSec. 4) You say that the framework ""..can be trivially extended.."" to 3D. What would that entail? An element-wise product between the 3D pixelwise distance tensor and the prediction tensor from the network?\n\nOther comments:\n- Sec. 1) double use of the word \'common\'.\n- Sec. 2) \'s\' in ""Let .. denotes..""\n- Eq. 1) int_{p \\in \\Omega} should be int_{\\Omega}', 'cons': 'Cons:\n- The authors did not compare to other loss functions designed to handle imbalanced classes. These were mentioned in the related work section as relevant.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'oral_presentation': ['Consider for oral presentation']}"		S1gTA5VggE	Bkegj4W-QE	MIDL.io/2019/Conference/-/Paper97/Official_Review	[]	1		['everyone']	S1gTA5VggE	['MIDL.io/2019/Conference/Paper97/AnonReviewer3']	1547928727958		1548856712732	['MIDL.io/2019/Conference/Paper97/AnonReviewer3']
287	1547914590051	{'pros': 'The work seeks to model the interesting problem of inter-rater variability in semantic segmentation of medical images. The paper gives a good motivation for why it might be beneficial to take into account image ambiguity and tries to give an answer as to how current segmentation architectures could be endowed with the ability to predict multiple appropriate segmentations given an image. There is an effort to evaluate the model on 3 different datasets.', 'cons': 'The paper is based on a shaky assumption and some of its details with respect to the model and evaluation seem to lack in precision or appear questionable.\n\nFirst of all, the proposed architecture hinges on the assumption that linear interpolations between the intersection of all masks and the union of all masks for a given image, and thus variations in segmentation size, can be considered plausible segmentations. Although this is indeed one variation that is observed in medical images, there are many other forms of variation including more intricate differences in shape or even discontinuous ones, which this approach ignores. \n\nThe proposed model is trained to recapture linearly interpolated segmentations that are artificially obtained from 2 expert segmentations per image. The interpolation is parametrized by a size encoding factor \\rho and is further used to condition the model by means of i) scaling the negative slope of some of the used activation functions and additionally ii) scaling the length of a latent vector z. The model choices lack motivation, are not clearly presented and no evaluation of the individual model choices is presented, e.g. importantly for why a hypersphere autoencoder is used. The hypersphere loss appears to be no more than a regularizer on z’s norm and there does not seem to be any incentive for the network to encode semantics via z (because the U-Net can encode all of the semantics in higher resolution features that are skipped across the bottleneck by means of the employed residual connections). The reviewer would have appreciated an analysis for the impact of the z-scaling via \\rho alone, i.e. without scaling the dynamic ReLUs. As it stands it would not be surprising if the network fixes z, e.g. |z| = 1. This would thus not require a hypersphere auto-encoder, but the network could be conditioned on a scalar \\rho, e.g. as is already done via the dynamic ReLUs. Therefore there is suspicion that the hyper-sphere formulation does not have any bearing on the method.\n\nFor the first two of the datasets only qualitative results are discussed. On the third and final dataset the proposed model as well as U-Nets trained with different ground-truth segmentations are compared on localization performance. The centroids of up to 3 connected components for each model are matched and compared against ground truth biopsy locations. The significance of the improvements over the baselines is unclear. In particular, the amount of images for which the predicted ‘sweetspot’, n_{sws}, encapsulates the gt biopsy location seems comparable for both the proposed model (HAE) and a simple U-Net trained on the ‘sweetspot’ groundtruth. For a singular biopsy the sweetspot U-Net is even better. Standard deviations on all evaluation metrics should be reported to allow for an assessment of significance of the presented results. Moreover, the localization accuracy per image seems to be an inappropriate measure here, since a random model reaches 100% accuracy. This indicates that every test image had undergone biopsy, which means that a singular positively predicted pixel per image results in a correct detection according to this metric. As the paper is concerned with semantic segmentation and the models are trained via a Dice-loss, it would have been illuminating to also report the Dice test set results. \n\nThere are additional details that should be revisited:\n1) Mention more related work, e.g. works that condition networks by means of multiplicative interactions with feature-maps, which is essentially what the present work employs, as well as literature on hypersphere variational auto-encoders.\n2) Describe the training process in detail.\n3) There is neither a motivation nor an evaluation for why residual- rather than skip-connections are being employed in the U-Net.\n4) An architecture that models conditional segmentation variations, the Probabilistic U-Net, is mentioned, but not compared against on the given datasets. A glance at the corresponding paper also reveals that their method does indeed outperform a suite of others on a real world dataset with ambiguities contrary to the claim in the Introduction, and the work seems akin to what is proposed as future work in the Discussion section. Furthermore this paper employs a publicly available dataset, which could have also been used to compare on.\n5) How do the actual assessors’ segmentations perform in terms of $d_{biopsy}$ on the respective others segmentaion? This would give some more insight into the expected variability on this measue.\n6) A claim is made that ‘taking biopsies is harmless’. In the presented generality this statement does not hold, commonly one strives to reduce/avoid biopsies due to involved risks, discomfort etc.\n\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		SJl27gYRkN	HJl8PT6lQN	MIDL.io/2019/Conference/-/Paper14/Official_Review	[]	1		['everyone']	SJl27gYRkN	['MIDL.io/2019/Conference/Paper14/AnonReviewer2']	1547914590051		1548856712469	['MIDL.io/2019/Conference/Paper14/AnonReviewer2']
288	1547915684871	"{'pros': 'Summary:\nThe authors present ultra-wide field-of-view scanning laser ophthalmoscopic retinal images, with the task of classifying eye laterality and landmark coordinates. These landmarks are the optic disc and the fovea. They designed a loss for both tasks and trained a residual network. The paper is straightforward and clear, the novelty lies with the SLO images and the multi-task loss for this problem.\n\nPros:\n- I have not seen UWFoV-SLO retinal images before. I think this is an interesting modality for retina-based diagnostics.\n- The network\'s loss seems reasonable and it performs well, compared to the human observers.\n\nQuestions:\n- Are these SLO images open access?\n- It is mentioned twice that the OD is proportionally larger in fundus images than these SLO images. But that would make it easier to detect the OD, right? What is the advantage of SLO over fundus images then?\n- Sec. 2) What do you mean with \'fuzzy convergence\'? Approximate convergence? Or does this refer to fuzzy logic?\n- Sec. 3) How does the horizontal flip data augmentation affect the laterality class? Are horizontally flipped images of the right eye considered images of the left eye?\n- Sec. 4.1.2) Shouldn\'t y_{i,j}^c denote the ground truth eye laterality for image i? As opposed to ""whether the image i has been correctly classified.."".\n- Sec. 4.2) Why would you drop the red channel? If you wanted a single channel, you could still take some combination of the red and green channels. Now, you suffer information loss.\n- Sec. 4.3) Why is rho divided by 2.5? That seems like an arbitrary number.\n- Caption of Table 1) Don\'t you mean \'computed by rho\'? Instead of \'normalized by rho\'?\n- Are the observers independent from ground truth?\n- Fig. 4) Why do you think the error for optic disc coordination prediction is larger in x-coordinates than in y-coordinates?\n- Fig. 4) What does the legend stand for?\n- Fig. 5) In some of the images in the bottom row, there appears to be specks of light. Are these common in SLO images? Are they caused by patient movement or something?\n- Sec. 6) Why would it be interesting to transfer to an autofluorescence modality?', 'cons': 'Cons:\n- The authors did not explain why their choice of architecture, a combination of standard residual layers and \'convolutional residual blocks\' are an appropriate choice for this task.\n- The sentence ""... it is easier for a network to learn residuals from the mapping function required to the output than the mapping itself."" is vague. What do you mean with ""learning the residuals""? Or ""the mapping function required to the output""? As this sentence seems to be at heart of your network\'s design, I feel that it is important to explain what you mean.\n- Some paragraphs are only two sentences long. I find this a bit annoying as it breaks the flow of reading.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		rke8t4Gxg4	rJlTjb0gQN	MIDL.io/2019/Conference/-/Paper74/Official_Review	[]	1		['everyone']	rke8t4Gxg4	['MIDL.io/2019/Conference/Paper74/AnonReviewer2']	1547915684871		1548856712210	['MIDL.io/2019/Conference/Paper74/AnonReviewer2']
289	1547900943695	"{'pros': 'This paper presents an application of deep learning in Pap smear image classification. It conducts experiments on two publicly available datasets and shows good results.', 'cons': 'To begin with, the method does not show enough novelty.  \n\nThe authors argue that it is the first method to present a segmentation-free classification method for WSI. However, the method is similar to the conventional patch-based WSI classification method, which crops the giga-pixels WSI into thousands of patches with the size of H*W and feeds them into a classification network, such as google\'s paper ""Detecting cancer metastases on gigapixel pathology images."". In fact, the ""Whole Slide Image"" is not a suitable name to call images described in this paper (like images in Fig.2(a)). Instead, it is more appropriate to call them ""patches from WSI"" or ""the region of interest from WSI"".\n\nBesides, using transfer learning by loading ImageNet pre-trained weights is a common method to let the model have a good initialization.  It seems that the resulting gain is coming from the usage of ResNet architecture which introduces residual learning. \n\nMeanwhile, the description of visualizing and interpreting the model is ambiguous.  The paper does not describe which features are used to do PCA. In Sec.4.2.2 it does not describe what is Grad-CAM and lacks the citation for this method. From Fig.3, the saliency maps are not so meaningful since they almost include all the foreground regions.\nIn all, I think it can be considered as a deep learning application report instead of a technical paper.  ', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		S1lrzR0gg4	SJedz_cgQ4	MIDL.io/2019/Conference/-/Paper137/Official_Review	[]	1		['everyone']	S1lrzR0gg4	['MIDL.io/2019/Conference/Paper137/AnonReviewer1']	1547900943695		1548856711997	['MIDL.io/2019/Conference/Paper137/AnonReviewer1']
290	1547885832095	"{'pros': ""\n1. The method part is well-written and easy to follow.\n\n2. The authors formulate the problem into a multi-task learning framework which regresses centroid location, confidence map and classifies pixel-wise label simultaneously. The method makes sense that high-correlated subtasks benefit learning mutual information.\n\n3. The vector oriented confidence accumulation generates the accumulator map with the sparse response. It reduces the sensitivity of hyperparameter-radius' value in NMS, which may benefit in final prediction especially in densely nuclei cases where the proper radius value is hard to define.\n\n4. The experiment results are good on a publicly available dataset which is persuasive. \n\n5. Paper is also clear about reporting hyper-parameters for reproducibility. "", 'cons': 'The experiment analysis is not clear in some parts. \n1. In Sec 5.1, the authors use the pixel-wise evaluation metric. Though it explains the mutual benefit to some extent, it is best to also provide the results on final metrics (F1, Median Distance).\n\n2. There is a mistake in the explanation (the second paragraph, Sec. 5.1). The smooth-L1 loss is a combination of L1 and  L2 loss which is robust to the outliers. It is an L1 loss when the value is larger than the threshold, while it is L2 loss if value small than the threshold.  \n\n3. Fig.2 shows the probability map of the regression method. But the analysis is not very correlated to this image. Instead, it is better to show the comparison of (Conf+Loc+Wt) and (Conf+Loc), since the explanation is still ambiguous. \n\n\n\nQ: The L_loc value is about to equal to 4 according to the experiment, which seems to be a large value. How about the magnitude for L_conf and L_wt, since \\lambda_{1} and \\lambda_{2} are set to 1 in loss calculation? Will L_loc dominate the direction of optimization? \n\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		Byg6tbleeE	B1llMpIgQN	MIDL.io/2019/Conference/-/Paper51/Official_Review	[]	1		['everyone']	Byg6tbleeE	['MIDL.io/2019/Conference/Paper51/AnonReviewer3']	1547885832095		1548856711778	['MIDL.io/2019/Conference/Paper51/AnonReviewer3']
291	1547900652341	"{'pros': 'A well written paper with a clear motivation, interesting evaluations and good amount of detail.\n\nThe core idea of the paper is to optimize saliency maps during training in order to guide classification networks to attend to the expected image regions. According to the narrative of the paper, this aims at improving learning from few examples per class (although this should not be limited to class imablance scenarios). Explicitly optimizing to attend to salient regions appears to be the main novelty of the paper. The proposed approach is independent of choice of (deep) classification architecture, which of course is a nice property to have. The authors present a compelling analysis of how inter- and intra-rater variations, as simulated by bounding box tightness variations, affect the approach.', 'cons': ""The approach essentially solves the problem of having too few examples per class by using denser labels. Here the training of classification models -that rely on image labels- is improved by incorporating bounding boxes. This limits the method's applicability to datasets with bounding box labels, a domain on which object detectors are known to perform very well. One advantage though of the method here is that it allows to relax this requirement, in the sense that it is flexible to still be trained on classification alone for images/classes which are not labelled with bounding boxes. (As a meta comment: similar performance gains can be observed in recent object detectors which are typically trained using bounding boxes but can be improved by training on even denser labels, i.e. pixel-wise segmentation maps)\n\nThere are a few technical details that remain unmotivated or un-evaluated: Why is ImageNet pretraining required here? Why was the CARE-loss only used to finetune and not during training from the start? What data-augmentation (method 'DA') was employed? The appropriateness of the chosen metrics, recall and mean class accuracy, is not discussed either.\n\nThe impact of the learned attention/localization on the classification performance was evaluated. It would however also be interesting to evaluate the attention/localization itself in terms of appropriate metrics such as average precision instead of only showing a few test set examples in Fig. 2.\n\nThere exists a body of literature that employ saliency techniques (in part also building on Grad-CAM) in order to perform localization in the image space. Although these works appear to not explicitly optimize the obtained saliency maps, they could be discussed with the related work section.\n"", 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BJl2cMBHlN	HkxEeD5l7V	MIDL.io/2019/Conference/-/Paper153/Official_Review	[]	1		['everyone']	BJl2cMBHlN	['MIDL.io/2019/Conference/Paper153/AnonReviewer3']	1547900652341		1548856711557	['MIDL.io/2019/Conference/Paper153/AnonReviewer3']
292	1547854686776	{'pros': 'The paper introduces a new approach for deep learning-based reconstruction of spatio-temporal MR image sequences from undersampled k-space data. The novelty of the approach lies in the explicit use of motion information (displacements on a voxel level) during the joint reconstruction of all images of the dynamic sequence. It is assumed that this motion information provides useful temporal information and better exploits the dynamics of the underlying physiological process to finally improve reconstruction results.\n\nFrom a methodological point of view, the paper introduces a new objective function for the sequence reconstruction task that does not only include a standard image reconstruction term but also explicit motion estimation and compensation components. In this work, the objective function is minimized by using deep learning. The solution consists of three parts, which are sequentially applied to the results of the preceding part: (1) Initial image reconstruction using a recurrent network, (2) motion estimation using a FlowNet variant, (3) motion compensation by using a residual net. (2) and (3) are fused into one network. In the evaluation, the new approach is extensively compared to state-of-the-art reconstruction approaches in a simulation study based on cardiac image data. The results demonstrate the method’s superior properties in terms of image quality and temporal coherence.\n\nGeneral opinion:\n\nIn my mind the approach presented in this paper is novel and interesting. One might argue that the approach is (at least partially) a combination of different pre-existing methods/papers (U-Net, Data sharing layer, FlowNet, …). However, I think all choices are reasonable and according to the results of the extensive and convincing evaluation, this combination leads to excellent results.\n\nFurther comments:\n\n- I think the discussion of the state-of-the-art approaches most relevant to this work should be extended. In my mind, especially Schlemper et al., 2017 (why wasn’t their 2018 TMI paper used instead?) should be discussed in much greater detail as it is the key competitor in the evaluation. In this context, it remains also unclear why only Schlemper et al., 2017 was used in the evaluation as a representative of deep learning-based reconstruction methods (why not Qin et al., 2018 and Huang et al., 2018). This choice should be discussed in the paper.\n\n- In Fig. 3, the location of the axis captions seems to be odd. They should be placed directly adjacent to their axis to improve readability.\n\n- What is so special about frame #9 that all approaches struggle to reconstruct this image? I assume it is one of the two extrema (end-diastolic phase or end-systolic phase) of the cardiac cycle. Couldn’t this problem for MODRN be alleviated by choosing z_1=end-diastolic phase and z_T= end-systolic phase or vice versa?\n\n- Are the differences between MODRN and all other approaches statistically significant? Please provide results of statistical tests, if possible.\n\n- The original FlowNet paper should be cited.\n\n\nPros:\n- Novel method for learning-based reconstruction of spatio-temporal MR image sequences\n- Explicit inclusion of motion information in the reconstruction process by using a FlowNet-like motion estimation approach\n- Extensive evaluation with very good quantitative and qualitative results', 'cons': 'Cons:\n- Reasons for the selection of competing state-of-the-art approach in the evaluation unclear\n- Paper is sometimes hard to follow', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		Bke-CJtel4	S1xDD7kgm4	MIDL.io/2019/Conference/-/Paper122/Official_Review	[]	2		['everyone']	Bke-CJtel4	['MIDL.io/2019/Conference/Paper122/AnonReviewer3']	1547854686776		1548856711336	['MIDL.io/2019/Conference/Paper122/AnonReviewer3']
293	1547825292839	"{'pros': 'The paper presents a method to perform nuclei segmentation based on point annotations. The authors evaluate weakly supervised methods (with and without CRF loss) on two nuclei segmentation datasets and compare the performance with fully supervised and other state-of-the-art methods. The paper is well organized and clear, has a well-defined objective and shows experimental evaluation using segmentation metrics e.g. F1 score, Dice coefficient, AJI. The outcomes of the analysis are promising, as the segmentation achieved using weakly supervised learning is comparable to fully supervised counterparts and other investigated methods.', 'cons': ""The following concerns need to be addressed by the authors:\n-In initial stages, Voronoi diagrams extract the rough positions of cells and k-means clustering extracts the rough boundaries. From the k-means results, it seems the results do not provide strong priors for accurately segmenting the nuclei boundaries. Both Voronoi centers and clusters appear to be weak shape descriptors as boundary information is not fully preserved.  The authors may want to explain the limitations of this type of annotation and discuss why the AJI values are observed lower as compared to other evaluation metrics. Why is the highest AJI achieved for Weak/Voronoi method?\n-How is the 'ignored class' represented in the training set (0/1)? It is shown in Fig 2 but not in the results in Fig 3 and Fig 4. \n-The caption of Fig 1 could be made more clear to indicate the figure contents.\n-In Table 1, the best results could be highlighted for better readability.\n\n"", 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		H1xkWv8gx4	SklS9e_kXE	MIDL.io/2019/Conference/-/Paper108/Official_Review	[]	2		['everyone']	H1xkWv8gx4	['MIDL.io/2019/Conference/Paper108/AnonReviewer3']	1547825292839		1548856711077	['MIDL.io/2019/Conference/Paper108/AnonReviewer3']
294	1547825268765	{'pros': 'Summary:\nThis paper presents a method for automatically classifying the sequence of brain magnetic resonance images (MRI). The method has two steps. First, a convolutional neural network (CNN) classifies the images among 4 categories: “T1-weighted”, “T2-weighted”, “T2-weighted FLAIR” and “proton density”. This network uses information from DICOM tags as well as from the image. Second, another CNN further classifies “T1-weighted” images into two classes: “pre-contrast” and “post-contrast”. The networks are called “sequence-CNN” and “contrast-CNN”, respectively.\n\nClaims of the paper:\n1.\tThe proposed method can classify brain MRI into 5 sequence types accurately. Therefore, it can be used to sort a dataset of brain MRI faster than doing it by hand.\n\nPros:\n* This is the first time that the problem of brain MRI sequence classification is solved using CNNs.\n* The study is well written and easy to follow.\n* The authors validate their method using an independent publicly available dataset.\n* They provide access to the code used to reproduce the experiments.\n', 'cons': 'Cons:\n* There is no methodological novelty in this paper. Off-the-shelf CNNs are trained on brain MRI to solve two classification tasks. \n* There are strong limitations regarding the validation of their method in the presented application: \n    * Limited performance of sequence-CNN: sequence-CNN performs barely better than the baseline method that completely ignores the images (only about 3% relative improvement on average). It even underperforms the baseline for the T2-FLAIR category. Furthermore, the authors do not perform any test of statistical significance, e.g. retraining the CNNs with different weight initialization and comparing the performance scores with a T-test. Based on the presented evidence, it cannot be concluded that sequence-CNN is significantly better than the baseline. \n    * Limited performance of contrast-CNN: 17% of the brain MRI are misclassified by contrast-CNN. This result casts doubts on whether this method can be used in a practical setting.\n    * Since the goal of this paper is to introduce a method that can substitute or aid a human annotator, it would be very informative to report the performance of an external human observer in both classification tasks. This information is key to understand the potential as well as the limitations of the proposed method.\n* The authors claim that the proposed algorithm can be used to sort a dataset of brain MRI faster than by hand. It is not clear how they would use this method in practice given the imperfect performance reported in the paper. If the labels are not known in advance, and the algorithm does not classify all images perfectly (or indicate uncertainty), how can it aid a human annotator to be faster or more accurate? Knowing that there could be mistakes in the predictions, an annotator would have to go through all the images regardless of the algorithm’s predictions. Furthermore, there is no evidence in the paper suggesting that the annotator would complete this process faster due to having the model’s predictions. \n* Why are DICOM tags not used with the contrast-CNN?\n* An entire page is used for 3 figures with basic information. It can be summarized in a single figure.\n', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		rkg7DD-gl4	HJg6deuy7E	MIDL.io/2019/Conference/-/Paper63/Official_Review	[]	1		['everyone']	rkg7DD-gl4	['MIDL.io/2019/Conference/Paper63/AnonReviewer3']	1547825268765		1548856710852	['MIDL.io/2019/Conference/Paper63/AnonReviewer3']
295	1547814141128	{'pros': 'Summary:\nThis paper presents a method for conditional image generation based on the Generative Adversarial Network (GAN) framework. In particular, the authors extend the Pix2Pix model by including convolutional capsule networks (CapsNets) in the generator network, and called it CapsPix2Pix. They used Pix2Pix, CapsPix2Pix and a physics-based model to generate images of two datasets conditioned on their segmentation labels that look indistinguishable from the real images. Subsequently, they trained a series of UNet segmentation networks on several real and generated images, and compared their performance on a common test set. \n\nClaims of the paper:\n1. It is possible to train a successful conditional GAN with a CapsNet-based generator. \n2. Pretraining the segmentation network (downstream task for evaluation) with generated images from CapsPix2Pix improves segmentation performance compared to no pretraining. \n3. Training the segmentation network from scratch with generated images from CapsPix2Pix improves segmentation performance compared to training with generated images from Pix2Pix. \n4. CapsPix2Pix generates a large variation of images compared to Pix2Pix.\n\nPros:\n* This is the first paper demonstrating that it is possible to perform conditional image generation using convolutional capsule networks in the generator of a GAN. They managed to generate 256x256 grayscale images.\n* The authors write an extensive description of hyper-parameter values and implementation details. Furthermore, they promise to publish their code and data very soon. Doing so will definitely help other researchers to adopt GANs and CapsNets in the future.\n* The paper is well written and easy to follow in general.\n* The authors include plenty of images that provide context and help the reader to understand the methods and results.\n', 'cons': 'Cons:\n* There is limited methodological novelty in this paper. The authors took an existing network architecture (SegCaps from LaLonde and Bagci 2018) and used it as a generator model in an existing conditional GAN framework (Pix2Pix from Isola et al. 2017). Notice that other authors have used CapsNets in the discriminator before, but not in the generator.\n    * A significant part of the paper is devoted to explaining preexisting ideas such as GANs, CapsNet and dynamic routing.\n* There is limited validation regarding the application presented by the authors. In particular, I found that claims (2), (3) and (4) are not sufficiently supported by the evidence shown in the paper:\n    * (2): when comparing pretraining UNet with CapsPix2Pix versus no pretraining it, they show a relative improvement of 0.76% only (0.6876 vs 0.6824 Dice), less than 1% difference. A test of statistical significance would be required to justify this claim (see next bullet point for more on statistical tests performed in this paper). At most, it could be said that both techniques achieve similar performance. Furthermore, figures A3 and A4 show indistinguishable performance at convergence.\n    * (3): similarly as with (2), a small improvement between techniques is reported, inconclusive without a significance test.\n    * (4): this claim is based on Figure A6 where only 1 example is provided. Since there is no page limit on the Appendix, more examples could be shown. Crucially, these examples should not be cherry-picked but selected at random (the authors do not mention how they chose the reported example).\n* Regarding statistical significance, the authors perform T-tests and provide p values. However, variation between performance metrics should not be measured across test samples (Table A1). Instead, the authors should repeat the training of UNet networks multiple times with different weight initialization, obtaining a series of performance measurements where the T-test is performed. For example, let’s say the number of repetitions is 5, and we are interested in comparing PBAM-SSM with pix2pix-AR (first and second entries of Table 1). Then, 10 UNets should be trained, i.e. 5 networks for the first method and another 5 for the next, obtaining 2 series of 5 performance metrics (5 Dice scores per method, each one the average across test samples). Finally, significance would be studied by comparing these two populations with a T-test.\n* The qualitative results and analysis are difficult to follow given the variety of datasets, methods and metrics. A few changes could help the reader understand the paper faster:\n    * Figure A1 could be part of the “Dataset” section.\n    * Include standard deviations in the table.\n    * Be more explicit with Table 1 (use monospace font for better viewing):\n\t\t+--------+-------------+------------------+------+-----+----+\n\t\t| Labels | Images      | Pretrained       | Dice | ROC | PR |\n\t\t+--------+-------------+------------------+------+-----+----+\n\t\t| SSM    | PBAM        | No               | …    | …   | …  |\n\t\t+--------+-------------+------------------+------+-----+----+\n\t\t| SSM    | Pix2Pix     | No               | …    | …   | …  |\n\t\t+--------+-------------+------------------+------+-----+----+\n\t\t| SSM    | CapsPix2Pix | No               | …    | …   | …  |\n\t\t+--------+-------------+------------------+------+-----+----+\n\t\t| Real   | Real        | No               | …    | …   | …  |\n\t\t+--------+-------------+------------------+------+-----+----+\n\t\t| Real   | Pix2Pix     | No               | …    | …   | …  |\n\t\t+--------+-------------+------------------+------+-----+----+\n\t\t| Real   | CapsPix2Pix | No               | …    | …   | …  |\n\t\t+--------+-------------+------------------+------+-----+----+\n\t\t| Real   | Real        | Real-Pix2Pix     | …    | …   | …  |\n\t\t+--------+-------------+------------------+------+-----+----+\n\t\t| Real   | Real        | SSM-Pix2Pix      | …    | …   | …  |\n\t\t+--------+-------------+------------------+------+-----+----+\n\t\t| Real   | Real        | Real-CapsPix2Pix | …    | …   | …  |\n\t\t+--------+-------------+------------------+------+-----+----+\n\t\t| Real   | Real        | SSM-CapsPix2Pix  | …    | …   | …  |\n\t\t+--------+-------------+------------------+------+-----+----+\n\t\t| SSM(*) | Pix2Pix     | No               | …    | …   | …  |\n\t\t+--------+-------------+------------------+------+-----+----+\n\t\t| SSM(*) | CapsPix2Pix | No               | …    | …   | …  |\n\t\t+--------+-------------+------------------+------+-----+----+\n* According to Table 1, among the first 6 entries, it seems that training with real data always produces better performance than training with generated images. What are the consequences of such evidence? \n* The idea of pretraining the segmentation model with generated data appears without justification. Is there any hypothesis or intuition explaining why this could improve the performance? \n* The authors compare CapsPix2Pix and Pix2Pix in terms of the number of trainable parameters. However, CapsNets are historically slow and memory intensive. How do these two models compare in terms of GPU memory footprint (weights and activations) and training time (wall clock)?\n* How do you ensure that the generator does not generate images that look realistic to the discriminator but are not biologically plausible? What are the consequences of this potential behavior in the biomedical setting?\n* The caption of Figure 1 should say “CapsPix2Pix generator architecture” since the discriminator is not shown. \n* Is there any justification why the segmentation UNets are trained with 64x64 images whereas the generative models produce 256x256 images?\n\nMy acceptance rating is conditional to performing proper statistical analysis or a relaxation of the claims.\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		rJen0zC1lE	SyeHbrB1mE	MIDL.io/2019/Conference/-/Paper46/Official_Review	[]	1		['everyone']	rJen0zC1lE	['MIDL.io/2019/Conference/Paper46/AnonReviewer1']	1547814141128		1548856710590	['MIDL.io/2019/Conference/Paper46/AnonReviewer1']
296	1547792905700	{'pros': '1. to present a deep hierarchical multi-label classi\x0ccation (HMLC) approach for CXR CAD. \n2. to model conditional probability directly and with unconditional probabilities is key in boosting performance.\n3. formulate a numerically stable cross-entropy loss function for unconditional probabilities \n4. evaluate our approach on detecting 14 abnormality labels from the PLCO dataset, which comprises 198; 000 manually annotated CXRs. We report a mean area under the curve (AUC) of 0:887, the highest yet reported for this dataset. \n', 'cons': '1. There is no cross-validation, external validation, with a confidence interval for evaluating significant better method.\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		SJgNCUbke4	HJxGGGe1X4	MIDL.io/2019/Conference/-/Paper29/Official_Review	[]	1		['everyone']	SJgNCUbke4	['MIDL.io/2019/Conference/Paper29/AnonReviewer2']	1547792905700		1548856710296	['MIDL.io/2019/Conference/Paper29/AnonReviewer2']
297	1547727477281	"{'pros': '- The authors present an end-to-end approach that allows to retrieve vessel trees straight from images, segmentations or skeletonizations, based on a patch-guided U-Net.\n\n- Different deep neural network architectures are explored on a series of artificial images generated with two alternative, novel hand-crafted approaches. The observations indicate that the best performing algorithm is the U-Net.\n\n- This final model is studied in the context of vasculature characterization in fundus images. This is an important step in several clinical studies that are focused on analyzing correlation between vascular characteristics and disease progression.\n\n- Two techniques for visualizing graphs are applied on the outputs of the networks. To the best of my knowledge, this is the first time that these algorithms are applied to visualize retinal vascular trees.\n\n- The paper is written in an excellent style. It is easy to follow, the explanations are simple and therefore straightforward, and the experimental setup is well designed. The reader can certainly follow each experiments step by step, and comprehensible understand the contribution of each components of the proposal.\n\n- Despite the fact that the deep learning contribution is not too significant, this method can certainly contribute to the field of ophthalmic image analysis, specially in clinical studies where the anatomical vessel properties are analyzed.', 'cons': '- Authors refer to their approach as ""end-to-end image-to-tree"", but when evaluated on real images the results are not as good as when using segmentations or skeletonizations of the vessels as inputs. This is an important issue and I think that authors should take that result into consideration and modify the claims (and perhaps the title) accordingly. Provided that these modificiations are done, I believe that the article could be certainly accepted. Current performance of vessel segmentation algorithms is close to the one of human observers doing the task manually, so using a segmentation as input would not be really a problem.\n\n- It it not sufficiently emphasize that the methods for synthesizing vascular trees are novel and were not explored before. \n\n- The algorithm requires a starting point to extract the vascular graph. This position is by definition the central, top pixel in synthetic images. However, it is not clear which point is used when working on retinal images. This is also an important thing to consider. Using a single vessel from the optic disc is usually not enough, as some images might show more than one vessel spreading from this region. In [1-4], all the models solve the issue by taking root nodes in the intersection of the optic disc border and vessels. Did you follow a similar idea?\n\n- Is the model based on segmentations (not in skeletonizations) able to solve vessel crossings such as the one illustrated in Fig. 4 (c), bottom? Usually the skeletonization algorithms introduce a small piece of vessel there due to the overlap between vessels. If the proposed method is able to overcome that issue, then it might have really good implications in many applications, including blood flow simulation [1-4], where these ambiguities introduce false branching points that significantly affect the results.\n\n- Some other minor suggestions:\n\n--> It should be clarified in the introduction that Fraz et al. survey is focused only on retinal images and not in blood vessel segmentation in general.\n--> In lines 8 and 9 of the introduction, there is a repetition (""biomedical scans"").\n--> Although it is clear that the estimated vessel width is correlated with the manual annotations (Fig. 7 (b)), it would be interesting to complement those results with the R^2 value of a linear regression model and a Pearson correlation coefficient.\n\n\nReferences:\n\n[1] Liu, D., Wood, N. B., Xu, X. Y., Witt, N., Hughes, A. D., & Thom, S. A. (2009). Image-based blood flow simulation in the retinal circulation. In 4th European Conference of the International Federation for Medical and Biological Engineering (pp. 1963-1966). Springer, Berlin, Heidelberg.\n\n[2] Malek, J., Azar, A. T., Nasralli, B., Tekari, M., Kamoun, H., & Tourki, R. (2015). Computational analysis of blood flow in the retinal arteries and veins using fundus image. Computers & Mathematics with Applications, 69(2), 101-116.\n\n[3] Calivá, F., Leontidis, G., Chudzik, P., Hunter, A., Antiga, L., & Al-Diri, B. (2017). Hemodynamics in the retinal vasculature during the progression of diabetic retinopathy. Journal for modeling in Ophthalmology, 1(4), 6-15.\n\n[4] Orlando J.I., Barbosa Breda J., van Keer K., Blaschko M.B., Blanco P.J., Bulant C.A. (2018) Towards a Glaucoma Risk Index Based on Simulated Hemodynamics from Fundus Images. In: Frangi A., Schnabel J., Davatzikos C., Alberola-López C., Fichtinger G. (eds) Medical Image Computing and Computer Assisted Intervention – MICCAI 2018. MICCAI 2018. Lecture Notes in Computer Science, vol 11071. Springer, Cham', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		B1geHYXgx4	Bklp_zeAM4	MIDL.io/2019/Conference/-/Paper88/Official_Review	[]	1		['everyone']	B1geHYXgx4	['MIDL.io/2019/Conference/Paper88/AnonReviewer2']	1547727477281		1548856710077	['MIDL.io/2019/Conference/Paper88/AnonReviewer2']
298	1547323329337	"{'pros': 'The paper introduces a novel MEMC (motion estimation & compensation) refinement block to improve deep learning based dynamic reconstruction. The paper is a good contribution to the recon field as it opens up a new avenue of research for better understanding motion for the reconstruction. The idea is simple yet the result seems quite impressive. However, many details & comprehensive analyses are missing for one to appreciate the contribution of the proposed components. Overall, I feel that there are too many remaining questions for the paper to be published in its current form. However, conditioned on the fact that the concerns below will be addressed, I believe that the benefits can outweigh, making it acceptable for this conference.', 'cons': 'Although the authors introduce an interesting idea, my main criticism is the lack of comprehensive details. Completing these details will greatly improve the quality of the paper.\n\n1. Reference: The paper is well-referenced for MR reconstruction & optical flow, however, similar motion modelling has already been considered in video super-resolution. For example:\n\na. Caballero, Jose, et al. ""Real-time video super-resolution with spatio-temporal networks and motion compensation."" IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017.\nb. Makansi, Osama, Eddy Ilg, and Thomas Brox. ""End-to-end learning of video super-resolution with motion compensation."" German Conference on Pattern Recognition. Springer, Cham, 2017.\n\nIndeed, while this paper is the first to apply MEMC framework for DL recon, it is not a new idea for general dynamic inverse problem settings. I would suggest the authors to acknowledge their work.\n\n2. Network detail: while the overall architecture is well-described, there are many details that is lacking: for example, what are the detail & capacity of f_enc, f_dec and f_dec2? what are the resolution scale of these methods and how important is tuning these? How to balance beta and lambda in Eq. 6? Also, have you considered adding DC component at the end of motion compensation block? Wouldn\'t that further improve the result?\n\n3. Training: Are the DRN and the MEMC components trained end-to-end? From what I gathered, these were trained separately. Is it possible to train both at the same time? \n\n4. Choice of reference images for motion compensation: why z_1 and z_T and not the neighbouring frames? How sensitive is the network for selecting them? If we assumed that the CINE sequence is cyclic, wouldn\'t z_1 and z_T look similar? Isn\'t it better to consider, for example, end-systolic vs end-diastolic frames as a reference?\n\n5. Experiment: \n\n(a) the manuscript gave me the impression that only the magnitude component is considered for the motion compensation part. For the experiments, were the complex images used for all components? \n\n(b) it seems that MEMC component applies in general. For example, it can be applied to DC-CNN, or DRN w/o recurrence too. I wonder what the performance would be for them. The key question is, how sensitive is MEMC component to the initial reconstruction? How much can MEMC component compensate for/affected by imperfect reconstruction?\n\n(c) it is more informative to see the motion profile of sample pixel(s), rather than the frame wise RMSE for Fig 3. In this way, we can understand if certain methods under/overestimate the motion.\n\n(d) it concerns me that the method is only evaluated on three subjects. Can cross-validation be performed?\n\n6. MEMC Analysis: It seems that the difference between the performance of U-FlowNet-A & B is very small. I wonder if this is statistically significant? If not, I suggest the authors to remove this component, unless there is a good reason to.', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		Bke-CJtel4	ryxtTDTPfV	MIDL.io/2019/Conference/-/Paper122/Official_Review	[]	1		['everyone']	Bke-CJtel4	['MIDL.io/2019/Conference/Paper122/AnonReviewer2']	1547323329337		1548856709818	['MIDL.io/2019/Conference/Paper122/AnonReviewer2']
299	1547654519567	"{'pros': 'The manuscript describes semantic segmentation in microscopic images. The work consists of comparative evaluation of semantic segmentation methods using three state-of-the-art convolutional neural networks, namely U-Net, Tiramisu and Deeplabv3+. The manuscript is well organised, clearly written and has good motivation. The work mentions a custom U-Net inspired by original U-Net, however, the design process and differences from latter are not clear from the description.', 'cons': ""State-of-the-art methods are compared for semantic segmentation in microscopic images. A custom U-Net is applied but not clearly discussed. The application area is interesting but the experiments are performed on limited datasets and cross-validation is only performed for one method. One evaluation metric (dice) is used, based on which the performance evaluation is not conclusive. The following problems need to be addressed by the authors.\n1. The authors propose a custom U-Net. They could specify why this architecture was used and how is it superior to the original U-Net? What was the quantitative difference in their performance? \n2. The comparative evaluation seems incomplete. Why was only one method cross-validated? The custom U-Net was not compared to the original U-Net. Other evaluation metrics could be used as the current evaluation isn't conclusive. The standard deviation of cross-validations could also be specified.\n3.The dataset described is limited to 170 images but the deep architectures require learning from large-scale datasets. The authors mention augmentation, but the size of augmented data is not specified. Though the authors mention that learning curves did not show signs of overfitting; an example of such a curve could be illustrated and discussed.\n"", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		Sygt37F21E	B1gxYr0nME	MIDL.io/2019/Conference/-/Paper6/Official_Review	[]	2		['everyone']	Sygt37F21E	['MIDL.io/2019/Conference/Paper6/AnonReviewer1']	1547654519567		1548856709561	['MIDL.io/2019/Conference/Paper6/AnonReviewer1']
300	1547646999637	{'pros': 'This paper mainly studied image synthesis on a sequence of contrast-enhanced breast MR image patches, and the authors conducted their experiments on benign , malignant and healthy patches separately, using the common DCGAN, Wasserstein GANs and WGAN-GP for comparison. Moreover, they evaluated the results of the three GAN methods via FID and 1-Nearest-Neighbor accuracy, and did human reader test to evaluate the visual effect of synthesized image patches.\n\nThe paper is clearly written, described the data selection, the methods used, and recorded useful experiment results. However, the paper lacks originality, as the methods and evaluation metrics all have been studied before. It’s more like to compare the performance of the three methods of GANs on a dataset.\n\npros:\n1.\tThe authors synthesized six types of MR image patches simultaneously, and tried to characterize time variable, that’s interesting and maybe useful in the future;\n2.\tThey did experiments using three methods based on GAN, and used the evaluation metric to evaluate the similarity of synthesized images’ distributions and the real one\n3.\tThe conclusion of ‘FID scores correlate with the amount of training data’ could be interesting.\n\n', 'cons': '1.\tThe paper lacks novelty in terms of methodology or significance of clinical application. \n2.\tThe application value of the synthesized patches remains to be verified;\n3.\tIn abstract, it mentioned ‘de novo image synthesis’, what does it mean? And what are their advantages?\n4.\tOn page 5, it said in experiments ‘After we initially conditioned the GANs with a condition, {benign, malignant, healthy}, and achieved rather poor conditioning performance……’, This needs clarification.\n5.\tHuman reader test showed that there still existed large gap between the synthesized patches and real ones.\n\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		BJlpmkJxl4	r1gx7_h3z4	MIDL.io/2019/Conference/-/Paper48/Official_Review	[]	1		['everyone']	BJlpmkJxl4	['MIDL.io/2019/Conference/Paper48/AnonReviewer1']	1547646999637		1548856709343	['MIDL.io/2019/Conference/Paper48/AnonReviewer1']
301	1547641924043	{'pros': 'The authors propose to segment the non contrast cardiac CT via the training of a 3D FC using virtual non contrast CT images. They tested the performance of the four chamber and great vessel segmentation using the enhanced CT images. \nThe idea is simple, the method is simple, and the paper is easy to read.', 'cons': '1 There is little  novelty in terms of methodology in this work. The proposed method is the fully convolutional network which has been well developed and widely used for segmentation.\n2 The authors hardly knew about the literature of four chamber plus great vessel segmentation (there was a challenge on this topic in miccai 2017) or transfer learning/ domain adaptation. ', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		BJxgkz-xeV	rkgnSVshMN	MIDL.io/2019/Conference/-/Paper58/Official_Review	[]	1		['everyone']	BJxgkz-xeV	['MIDL.io/2019/Conference/Paper58/AnonReviewer2']	1547641924043		1548856709127	['MIDL.io/2019/Conference/Paper58/AnonReviewer2']
302	1547641988252	"{'pros': '- Transfer learning and dealing with small datasets is an important area of research\n- The paper proposes a novel method, enabling pretraining on several different tasks instead of only one dataset (e.g. ImageNet) like done most of the times\n- Results show clear performance increase on small datasets\n- Proper experiment setup and validation\n- Clearly written and comprehensible\n- Code is openly available\n', 'cons': '- Little comparison to other state-of-the-art methods for transfer learning. Only compared to IMM which is very similar to the proposed T-IMM. Comparison to (unsupervised) domain adaptation methods would also have been interesting (e.g. gradient reversal (Ganin et al. 2014, Kamnitsas et al. 2016)).\n- Method only evaluated on one dataset (BRATS). Often new methods are manually ""overfitted"" to one dataset. When used on another dataset they do not show gains anymore. The medical decathlon\n(http://medicaldecathlon.com/) would have provided easy access to more datasets and tasks.\n\n\nMinor:\n- Testing for statistical significance is only shown in the appendix. It shows that for ""100%"" T-IMM actually is not significantly better than most of the other initialization strategies. This should also be shown in table 2. The way table 2 is presented at the moment it seems like T-IMM is better than all methods also for ""100%"". But the higher performance is not significant.\n- How is training till ""convergence"" (section 4.3) defined?\n- Not 100% clear if the IMM method used in the experiments is the method described in section 3.2 (alpha=1/T) ?\n- in section 5: ""Table 2 shows, that both IMM and T-IMM..."". I guess this should actually be table 4.\n- Figure 1 could have been a bit more clear\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		rklPRjjlxV	HJg2YNo2GE	MIDL.io/2019/Conference/-/Paper131/Official_Review	[]	1		['everyone']	rklPRjjlxV	['MIDL.io/2019/Conference/Paper131/AnonReviewer3']	1547641988252		1548856708911	['MIDL.io/2019/Conference/Paper131/AnonReviewer3']
303	1547637682323	"{'pros': '- The authors present a deep learning method for fundus image analysis based on a fully convolutional neural network architecture trained with an adversarial loss.\n\n- The method allows to detect a series of relevant anatomical/pathological structures in fundus pictures (such as the retinal vessels, the optic disc, hemorrhages, microaneurysms and soft/hard exudates). This is important when processing these images, where anatomical and pathological structures usually share similar visual properties and lead to false positive detections (e.g. red lesions and vessels, or bright lesions and the optic disc).\n\n- The adversarial loss allows to leverage complementary data sets that do not have all the regions of interest segmented. Thus, it is not necessary to have all the classes annotated in all the images but to have the labels at least in some of them.\n\n- The contribution is original in the sense that complementing data sets is a really challenging task, difficult to address with current available solutions. The strategy proposed to tackle this issue is not novel as adversarial losses have been used before for image segmentations. However, it is the first time that it is applied for complementing data sets and have some interesting modifications that certainly ensures novelty in the proposal.\n\n- The paper is well written and organized, with minor details to address in this matter (see CONS).', 'cons': '- The clear contribution of the article is, in my opinion, the ability to exploit complementary information from different data sets. Taking this into account, I would suggest the authors to incorporate at least one paragraph in Related works (Section 2) describing the current existing approaches to do that.\n\n- It is not clear from the explanation in Section 3.1 how the authors deal with the differences in resolution between DRIVE and IDRID data sets. It would be interesting to know that aspect, as it is crucial to allow the network to learn to ""transfer"" its own ability for detecting a new region from one data set to another.\n\n- The segmentation architecture does not use batch normalization. Is there a reason for not using it?\n\n- The vessel segmentation performance is evaluated on the DRIVE data set. Despite the fact that this set has been the standard for evaluating blood vessel segmentation algorithms since 2004, the resolution of the images is extremelly different from the current ones. There are other existing data sets such as HRF (https://www5.cs.fau.de/research/data/fundus-images/), CHASEDB1 (https://blogs.kingston.ac.uk/retinal/chasedb1/) and DR HAGIS (https://personalpages.manchester.ac.uk/staff/niall.p.mcloughlin/DRHAGIS_database.htm) with higher resolution images that are more representative of current imaging devices. I would suggest to incorporate results on at least one of these data sets to better understand the behavior of the algorithm on these images.\n\n- The area under the ROC curve is not a proper metric for evaluating a vessel segmentation algorithm due to the class imbalance between the TP and TN classes (vessels vs. background ratio is around 12% in fundus pictures). I would suggest to include the F1-score and the area under the Precision/Recall curve, instead, which have been used already in other studies (see [1] and [2], for example, or Orlando et al. 2017 in the submitted draft).\n\n- The method in [2] should be included in the comparison of vessel segmentation algorithms. To the best of my knowledge, it has the highest performance in the DRIVE data set compared to several other techniques. It would also be interesting to analyze the differences in a qualitative way, as in Fig. 3 (b). The authors of [2] provided a website with all the results on the DRIVE database (http://www.vision.ee.ethz.ch/~cvlsegmentation/driu/), so their segmentations could be taken from there.\n\n- The results for vessel segmentation in IDRID images do not look as accurate as those in the DRIVE data set. However, since IDRID does not have vessel annotations, it is not possible to quantify the performance there. It would be interesting to simulate such an experiment by taking an additional data set with vessel annotations (e.g., some of those that I suggested before, HRF, CHASEDB1 or DR HAGIS) and evaluate the performance there, without using any of their images for training. That would be equivalent to assume that the new data set(s) does (do) not contain the annotations, and will allow to quantify the performance there. Since the HRF data set contains images from normal, glaucomatous and diabetic retinopathy patients, I would suggest to use that one. A similar experiment can be made using other data sets with red/bright lesions  (e.g. e-ophtha, http://www.adcis.net/es/Descargar-Software-Base-De-Datos-Terceros/E-Ophtha.html) or optic disc annotations (e.g. REFUGE database, https://refuge.grand-challenge.org). I think this is a key experiment, really necessary to validate if the method is performing well or not. I would certainly accept the paper is this experiment were included and the results were convincing.\n\n- It is not clear if the values for the existing methods in Table 2 correspond to the winning teams of the IDRID challenge. Please, clarify that point in the text.\n\n- The abstract should be improved. The first 10 lines contains too much wording for a statement that should be much easier to explain. I would suggest reorganizing these first line by following something like: (i) Despite the fact that there are several available data sets of fundus pictures, none of them contains labels for all the structures of interest for retinal image analysis, either anatomical or pathological. (ii) Learning to leverage the information of complementary data sets is a challenging task. (iii) Explanation of the method...\n\n\n\n[1] Zhao, Yitian, et al. ""Automated Vessel Segmentation Using Infinite Perimeter Active Contour Model with Hybrid Region Information with Application to Retinal Images."" IEEE Trans. Med. Imaging 34.9 (2015): 1797-1807.\n\n[2] Maninis, Kevis-Kokitsi, et al. ""Deep retinal image understanding."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2016.', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		HJe6f0BexN	HJgc3mq3z4	MIDL.io/2019/Conference/-/Paper102/Official_Review	[]	1		['everyone']	HJe6f0BexN	['MIDL.io/2019/Conference/Paper102/AnonReviewer2']	1547637682323		1548856708699	['MIDL.io/2019/Conference/Paper102/AnonReviewer2']
304	1547633050831	"{'pros': 'This paper describes the use of a deep learning network called ""DeepFormer"" that reconstructs a B-mode ultrasound image from the RF data. The Minimum Variance Beamforming B-mode images were used as a ground truth. A new object function was introduced to train the network. The authors use the trained network to train ResNet18 for classifying the anatomies using three different inputs: the ground truth images, the images created with the DeepFormer network and with the feature vector from the ""Botleneck"" of the network.\n\nThe DeepFormer network is actually the QuickNat network introduced by Guha Roy et al. (https://www.sciencedirect.com/science/article/pii/S1053811918321232), which goal was not to reconstruct US images, but segment brain structures. Next to this, the DeepFromer network was already presented by the same authors at IEEE IUS (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8579818&tag=1), so the network presented in section 2.2 is not novel. The novelty of this paper is the new object function and the anatomy classification. The ""cons"" section below explains why I think that novelty is limited and the evaluation lacks details. ', 'cons': '1. Object function: \n - The results in section 4.1 mention: ""An important observation is that L1 and LPSNR perform significantly worse than the hybrid losses"". The results in Table 1 show a limited improvement in both SSIM (0.781 ± 0.014 vs 0.784 ± 0.0132) and PSNR (26.27 ± 0.645 vs 26.34 ± 0.630). In addition, the authors did not describe any statistical tests, so the statement ""significantly worse"" is not proven. \n- The IEEE IUS paper of the same authors uses a larger part of the same dataset (19 healthy participants) and shows a higher SSIM and PSNR on a test set of 5 healthy subjects (0.9452 and 34.6326). It is unclear why only a subset of 6 healthy participants were used in this study and how many subjects were included in the test set of this study. \n\n- Figure 3 presents the qualitative evaluation, but only one image is shown. Only one line in axial direction is shown and no line in lateral direction. I agree that the borders seem to remain intact, but some structures also tend to be blurred out (for example the structure in axial direction at one third on the left side of the colored line). \n\n2. Anatomy classification:\n- This part describes the interesting finding that the image classification of the Bottleneck outperforms the other two methods for which the reconstructed B-mode image is used. This is a very interesting finding, because it could potentially speed up image classification. Unfortunately, a lot of details are missing to support this claim. \n- There are no example images shown of the eight classes. I do not understand why the left and right side of the body is used as a different class. No example images of each class are shown to explain the difficulty of the task. \n- The complete dataset consists of 6 subjects of 2541 images, but it is not mentioned how many are used in the training and test set. It is also not mentioned how many images there are per class. \n- The results in Table 2 shows an improvement in accuracy, but does the accuracy improve for every class, or only for some classes that are represented more in the training data? Is an accuracy of 0.754 good enough for this task? \n- The ROC of Figure 4 only shows one class (class 4, longitudinal carotid, while section 3.1 states common carotid artery) and it seems that there is only one cut-off point, why are there not more cut-off points (limited amount of data?) and if there is only one cut-off point, why do the authors not mention just sensitivity and specificity in a Table for each class? \n\n3. End-to-end DeepForming and Classification\n- This chapter is only included in the result section and comes out of nowhere. It mentioned that end-to-end training does not work (at this moment), so I do not understand why it is included in the paper.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		SygEcVLHl4	HygQjWYhz4	MIDL.io/2019/Conference/-/Paper158/Official_Review	[]	1		['everyone']	SygEcVLHl4	['MIDL.io/2019/Conference/Paper158/AnonReviewer1']	1547633050831		1548856708442	['MIDL.io/2019/Conference/Paper158/AnonReviewer1']
305	1547601440061	{'pros': 'The main contribution of the paper: The authors attempt to create automatic training label based on class activation map, which showed comparable results to manual-labeled training dataset. Specifically, the authors use CNN-based classification + Grad-CAM (along with ASM + Dense CRF) to automatically generate training data as the input of a 2D U-net. (Post-processing includes: ASM and Grad-CAM).\n\nIt’s an interesting attempt to alleviate the requirement manual labeling (although the result might only be dataset-specific). Plus, it shows a feasible application of combined network design in the field of of ophthalmic MR imaging.\n\nThe article is clearly written and structured. The presented figures well reflect the proposed framework as well as demonstrated the results with selected representative samples.\n', 'cons': '-\tThe activation map showed in Figure 2’s pipeline clearly demonstrated that the tumor should be the region that differentiates groups. However, in Figure 4, the entire sclera region is also activated, which is significantly different from Figure 2. It is unclear where such difference comes from, since that should not be different in the sclera region between the normal and diseased eye, and need to be explained more clearly.\n\n-\tGenerally, CAM can only be used to identify the differential region locations roughly, rather than delineating the segmentation even accurately (i.e. unsupervised CNN-based segmentation). The representing figure shown in Figure 4/7 indicate that the accuracy results depends heavily on the tissue contrast. That might indicate that the performance of the proposed methods maybe specific to the recruited dataset (e.g. more cases like shown in Fig 7 right-most column).\n\n-\tIn Figure 4 (c), it’s hard to see the improvement of applying ASM over to the dense CRF. It would be better to show a more representative figure or quantitative analysis the Dice when comparing them with the manual segmentation.\n \n-\tIn Figure 6: The author compared different segmentation approaches, and essentially showing that 2D network is better than 3D network, and 3D-CNN is better than 3D-Unet. I agree with the author that this should mainly be due to the small training set. The data augmentation with elastic deformation will help to alleviate the problem, which is used in both the original 2D U-net paper (by Ronneberger et al. 2015) and the 3D U-net (Çiçek et al. 2016. However, based on the method part, the authors doesn’t seems to use this data augmentation method.\n\n-\tFigure 7 mid-column, the author showed cases that their method (Grad-CAM+ASM+denseCRF) can “correct” the manual segmentation.\no\tI suppose they mean the automatic result is better than the “manual segmentation”, as they’re not training their method based on the manual segmentation.\no\tThis indicate there are errors in the manual segmentation. Then it’s questionable to use such manual label as ground truth. In that case, multiple manual segmentation with inter-rater variability analysis might be needed to construct and validate the ground truth.\n\n\nSome minor issue that need proofread:\n-\tThe figure seems to be screen captured without removing clean-up some software-based marks\no\tFigure 2/3 has red dot line indicating word correction mark from word\no\tFigure 7, the selection dots representing the selection window should be removed\n-\tPage 5: in section “Refinement”:\no\tthe word “and” should be put before k(fi,fj)\no\tparagraph after equation (3): shouldn’t use j as the subscript for wj, as j has already been used in the equation to represent the second pixel.\n-\tPage 6: in section “Unnet”: \no\teffectually => effectively\n-\tFigure 6, an additional “n” is placed before Grad-CAM-2DUnet\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		ByldG19Ry4	SyedQ8Zhz4	MIDL.io/2019/Conference/-/Paper16/Official_Review	[]	1		['everyone']	ByldG19Ry4	['MIDL.io/2019/Conference/Paper16/AnonReviewer1']	1547601440061		1548856708187	['MIDL.io/2019/Conference/Paper16/AnonReviewer1']
306	1547597129113	{'pros': 'This work proposes two additional structures -- Dense connection and Dilated Convolution to the standard U-net that are shown to improve the testing Dice Similarity Coefficient from ~71% to ~79.3% on test testing set. It also presents results of some other experiments on adding Batch Normalization and different methods of preprocessing. \nThe gain achieved is significant and the motivation behind the addition makes sense. Also, the ablation study (Table 4) shows how the gains stack up which would be useful to use in other architectures. \n', 'cons': 'The paper can benefit from better writing of some of its sections. For example, the last two paragraphs of Section 1 repeat the same thing (“..propose the dense u-net..”) without much change. Other examples are:\t\t\t\n\t\t\n“Sometimes the more layers, the lower DSC. This observation enlightened us to make our model more lighter”\t\n\t\t\t\t\t\n“Nevertheless, U-Net will lose more resolution as network goes deeper”\nand various other typos. \n\nOn the technical side, it is unclear to me why the training procedure doesn’t use data augmentation (as commented by the authors themselves) and some justification about it would be helpful. \nFurther, dilated convolutions have been found helpful in U-net in other works before too ([1]) and hence requires requires further justification if that is to be considered a novel aspect of the work. Similar is the case with Densely connected variant of U-net which have been shown to help with biomedical segmentation before this work ([2]). So, both the central contributions require some more effort. \nAlso, it would be useful to see the result of the proposed additions on some other similar dataset before strong conclusions on the applicability of the proposed structures. Moreover, Figure 1 of the paper is unclear in the current form and it can be made more instructive by an informative caption or redrawing.  \n\nLastly, if the authors are concerned about the size of the model, it would be useful to apply other model compression techniques like knowledge distillation and weight quantization rather than defaulting to a smaller but also less accurate model as shown in Table 4. \n\n[1]: https://mc.ai/u-net-dilated-convolutions-and-large-convolution-kernels-in-deep-learning/\n[2]: https://arxiv.org/pdf/1812.00352.pdf\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		r1gBaP2Gg4	rJx-8SenMN	MIDL.io/2019/Conference/-/Paper141/Official_Review	[]	1		['everyone']	r1gBaP2Gg4	['MIDL.io/2019/Conference/Paper141/AnonReviewer3']	1547597129113		1548856707936	['MIDL.io/2019/Conference/Paper141/AnonReviewer3']
307	1547164425180	"{'pros': 'The paper attempts to train a unified model usable across different segmentation tasks (liver, pancreas, spleen, for a total of 5 classes including background). This might be problematic since the different datasets labels only their organs of interest, and set the other organs as background. Hence, one dataset will label a liver as liver, while the other two will label it as background, making the network forget its previous task. This is related to both weak supervision and continuous learning. \n\nPro:\n- The dice figures provide error bars', 'cons': 'The paper completely ignores the state of the art in weak supervision and continuous learning ; while still managing to cite Leslie Lamport\'s paper introducing LaTeX. Relevant literature for weakly supervised segmentation includes at least, but is not limited to:\n- Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation, Papandreou et al., ICCV 2015\n- What’s the Point: Semantic Segmentation with Point Supervision, Bearman et al., ECCV 2016\n- Scribblesup: Scribble-supervised convolutional networks for semantic segmentation, Lin et al, CVPR 2016\n- On Regularized Losses for Weakly-supervised CNN Segmentation, Tang et al., ECCV 2018\n- Constrained-CNN losses for weakly supervised segmentation, Kervadec et al., MIDL 2018\n- DeepCut: Object Segmentation from Bounding Box Annotations using Convolutional Neural Networks, Rajchl et al., TMI, 2016\n- Constrained convolutional neural networks for weakly supervised segmentation, Pathak et al., ICCV 2015\nThe main claim of the conclusion (""In conclusion, training of deep learning model by using partially segmented images is demonstrated"") has been known for quite some time.\n\nThere is a lot of unsupported claims that need references, for instance:\n- ""Although there exist methods [...]""\n- ""Most methods require completely segmentated images [...]""\n- ""Hence, w_c is often enforced in loss_xent by first computing [...]""\n\nThe presented method is barely readable and understandable, both due to notation and lack of explanations on how the author came up with the proposed losses. Some of them do not make any sense on a theoretical point of view, and unsurprisingly the results are so bad the author does not report them. Why include it then ? There is also a lot of details that would be relevant in an engineering report, but not in a scientific paper. This only add noise to the message and makes everything harder to read. Releasing the code if much more efficient for the tiny details. \n\nThe author would gain much to introduce new variables to express his sums and losses. For instance, using $\\Omega_L$ for the set of labeled pixels, $\\Omega$ for the whole image and $\\Omega_P$ for the set of pixels with a predicted class that we supervise. $n_class$ could simply be $C$, $voxels$ could be $|\\Omega|$. The $w_c$ could be completely removed, since it is simply a binary value across the whole paper ; and the notation for the cross-entropy could be greatly simplified:\n$  \\mathcal{CE} = - \\sum_{x \\in \\Omega_L} \\sum_c y_x^c\\log{p_x^c} $\nLoss_mode_or could be:\n$ \\mathcal{CE_{or}} = - \\sum_{x \\in \\Omega_L \\cup \\Omega_P } \\sum_c y_x^c\\log{p_x^c}$ \netc. The difference between each loss would not only be much more explicit, but also easier to explain and justify. \n\nDuring evaluation, the unified model gets performances higher than the dedicated models for each task. This actually seem to indicate errors in the evaluation.', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		S1gXSzNexN	SkgZzj8Hf4	MIDL.io/2019/Conference/-/Paper93/Official_Review	[]	1		['everyone']	S1gXSzNexN	['MIDL.io/2019/Conference/Paper93/AnonReviewer3']	1547164425180		1548856707720	['MIDL.io/2019/Conference/Paper93/AnonReviewer3']
308	1547591606077	"{'pros': 'This paper attempt to do nuclei segmentation in a weakly supervised fashion, using point annotations. \n\nThe paper is very well written and easy to follow ; figure 1 does an excellent job at summarizing the method. The idea is to generate two labels maps from the points: a Voronoi partitioning for the first one, and a clustering between foreground, background and neutral classes for the second. Those maps are used for training with a partial cross-entropy. The trained network is then fine tuned with a direct CRF loss, as in Tang et al. \nEvaluation is performed on two datasets in several configurations (with and without CRF loss, and variation on the labels used) ; showing the effects of the different parts of the method. The best combination (both labels + CRF) are close or on par with full supervision. \nThe authors also compare the annotation time between points, bounding boxes and full supervision, which really highlight the impact of their method (x10 speedup).\n\nFew questions:\n- Since the method is quite simple and elegant, I expect it could be adapted to other tasks. Do you have any ideas in mind ?\n- How resilient is the method to ""forgotten"" nuclei ; i.e. nucleus without a point in the labels ? Could it be extended to work with only a fraction of the nuclei annotated ? \n- Is using a pre-trained network really helping ? Since there is so much dissimilarity between ImageNet and the target domains, I expect it to be mostly a glorified edge detector. It is improving the final performances, speeding up convergence, both ?', 'cons': 'Minor improvements for the camera ready version, in no particular order:\n\nTang et al. 2018 was actually published at ECCV 2018, the bibliographic entry should be updated. \n\nSection 2.3 should make the differences (if any) with Tang et al. explicit.\n\nThose three papers should be included in the state-of-the-art section:\n- Constrained convolutional neural networks for weakly supervised segmentation, Pathak et al., ICCV 2015 \n- DeepCut: Object Segmentation from Bounding Box Annotations using Convolutional Neural Networks, Rajchl et al., TMI, 2016\n- Constrained-CNN losses for weakly supervised segmentation, Kervadec et al., MIDL 2018\n\nSince the AJI and object-level Dice are not standard and introduced in other papers, it would be easier to put their formulation back in the paper, so the reader does not have to go look for it.\n\nReplacing (a), (b), ... by Image, ground truth, ... in figures 2, 3, and 4 would improve readability.\n\n\n\n', 'rating': '4: strong accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'oral_presentation': ['Consider for oral presentation']}"		H1xkWv8gx4	S1lRhyyhGE	MIDL.io/2019/Conference/-/Paper108/Official_Review	[]	1		['everyone']	H1xkWv8gx4	['MIDL.io/2019/Conference/Paper108/AnonReviewer2']	1547591606077		1548856707461	['MIDL.io/2019/Conference/Paper108/AnonReviewer2']
309	1547574265373	"{'pros': 'The authors present a method for identifying potentially incorrectly labeled data in a training, testing, or validation set by observing the loss profile of individual samples during training. To demonstrate effectiveness, they intentionally mislabel 6% of the data, use the algorithm to identify candidate samples that are potentially mislabeled, then update the dataset to the correct labels for only those samples caught by the algorithm. They demonstrate the performance improvement in their algorithm when going from 6% mislabeled samples in their training set to 1.5% mislabeled samples. Comparison was done using a validation set that contained no mislabeled samples.\n\nMethods for correcting mislabeled data have been presented in the past and definitely have value when used correctly. I have yet to see a similar method for identifying mislabeled exams, so the method appears to be novel. The method itself, likely has value. However, it must be presented in the proper context when published.', 'cons': 'There are several problems with the paper of varying magnitude. The most important omission is a discussion of the bias introduced by such a method. The paper lists that the method may be used to clean training, testing and validation data leading to increased performance. However, if this method is used in testing or validation, tremendous bias is introduced in the evaluation of the network. Potentially mislabeled images are identified as those that the algorithm struggles to get correct, and are subject to correction. If this is done in the validation or testing datasets, then labels are being switched only for those exams that the algorithm gets wrong. On the other hand, images that the algorithm gets ""right"" are unchanged, even if the label, and algorithm, are wrong; this is the source of the bias. To prevent bias, the entire dataset should be regraded blind to how the algorithm performed. While the paper only deals with changing labels in the training data, which is ok, a thorough discussion about bias is warranted and claims about this methods applicability to testing or validation datasets should be omitted and strictly discouraged.\n\nThe methods in this paper introduce bias that will inflate the performance of the algorithm in a manner that will not be realized in real-world applications. \nReal-world labels are not randomly incorrect. 6% of the images were intentionally changed, assuming randomly. In the real-world, the vast majority of mislabeled exams will contain very few obvious exams. By randomly changing the labels the paper only demonstrates that training data with better labels will perform better, and says nothing about the method of under evaluation.\nWhen the labels in the paper were identified, they should have been blindly re-graded rather than reset to the predetermined correct value. This would much more effectively evaluate how the algorithm would perform in the real-world.\nOne correct way to conduct this study is to use the original labels when gathered from their source rather than starting out by regrading all images to have the correct truth then randomly flipping labels. I know nothing of the hospital data, but the Kaggle competition data came with labels, many of which are incorrect. This would have served as a real-world evaluation of the model.\n\nThe title of the paper is misleading: ""An automated method of identifying incorrectly labelled medical images"". The method automatically identifies candidates for which the label may be wrong, but requires manual determination to see if the algorithm is correct. In the dataset analyzed, the positive predictive value of the algorithm was slightly less than 50%, suggesting that if the labels were assumed wrong in an automated fashion that the algorithm would be ineffective.\n\nThe accuracy point for model A"" is in the incorrect location in Figure 4A.\n\nFeature F6 in the K-means clustering is completely dependent on F4 and F5. Only independent features will add value in the k-means algorithm.', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		rJxCyxj1xN	Byg-Wh9jzN	MIDL.io/2019/Conference/-/Paper39/Official_Review	[]	1		['everyone']	rJxCyxj1xN	['MIDL.io/2019/Conference/Paper39/AnonReviewer2']	1547574265373		1548856707240	['MIDL.io/2019/Conference/Paper39/AnonReviewer2']
310	1547567293016	"{'pros': 'This study analyzes a novel network architecture for the segmentation of objects with a lower dimension than the input data. In medical images this corresponds to hyperplanes in 3D images or lines from 2D images. Analysis of the network was performed in 2 use-cases: segmentation of retinal layers in OCT B-scans and segmentation of A-scans containing geographic atrophy from 2D bscans.\n\nThe quality, clarity, and originality of this work is good.\n\nThe paper is very well-written and very clear. In particular, Figure 1 is excellent in simultaneously describing three different network architectures in a concise manner; very well done! Figure 2 is also exceptionally done. \n\nThe motivation for the work is clear and the results are described in two very relevant applications.\n\nI have not seen a similar network architecture and believe it to be unique. The network architecture is also very relevant to the task at hand with few arbitrary design decisions. It is unclear why the number of iterations was fixed rather than allowing to optimize on the validation network, but I commend including all data necessary to replicate the study.\n\nOverall, the network architecture is novel, the experimental design is mostly well-done. Nice paper.', 'cons': 'There are some claims made in the paper that are questionable. These claims do not affect the acceptance of the paper, but should be addressed prior to final publishing.\n\n""Neither classification networks nor segmentation networks are suitable for these tasks [tasks being segmentation of 1D lines in 2D image]"". I understand the point that this narrative is trying to deliver, and believe that the narrative should be in there, but as written the text is untrue. In the narrow definition of classification and segmentation networks provided, this would be true. However, the definition provided misses a wide range of published networks that do not fit the criteria and are able to segment 1D lines from 2D data. In the paper, base model 1 is able to segment 1D lines from 2D images and is very similar to AlexNet. Examples of this have been published in the context of retinal layer segmentation as well: ""Shah et al. Multiple surface segmentation using convolution neural nets: application to retinal layer segmentation in OCT images"".\n\nIn the Results section comparing algorithms for GA segmentation, there is a comparison of Dice scores compared across OCT-volumes in the dataset. A single Dice score was calculated per OCT volume. The proposed model had a mean and std dev of 0.49 +/- 0.21 while base model 1 had a mean and std dev of 0.46 +/- 0.22. The sample size, in number of volumes, is 20. The next sentence indicates that the proposed model is significantly better with a p-value < 0.01. I do not see how this can be true. I am not a statistic expert, but comparison of two algorithms on the same OCT volumes could use a paired student t-test. Since I do not have the paired data, an unpaired t-test gives a p-value of 0.66. It is possible that each B-scan, or even each A-scan, was used to calculate statistical significance, but that would not be the correct approach as well as the data within a single volume would be highly correlated. More information is required on how statistical significance was calculated.', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		Hkx5C9QeeN	S1er6eKszN	MIDL.io/2019/Conference/-/Paper90/Official_Review	[]	1		['everyone']	Hkx5C9QeeN	['MIDL.io/2019/Conference/Paper90/AnonReviewer2']	1547567293016		1548856706980	['MIDL.io/2019/Conference/Paper90/AnonReviewer2']
311	1547455786315	{'pros': '- The authors investigate how the predictions of several standard convolutional neural network architectures trained for dermoscopic image classification change when artificial elements are added to the skin area before taking the image. This is certainly an interesting topic on which there is not much prior work in the medical domain.\n\n- Several network architectures and several types of attacks are compared.', 'cons': '- The authors only investigated whether the confidence of the network is affected or whether the predicted lesion category is changed. However, it seems more logical that actual attacks would aim at changing the output in a specific way, for instance to a specific output category. These kind of attacks are not attempted and there are also no details on how the network output changes (do the networks all favor a certain category, i.e., if the category is changed due to an attack does the output always change to that category?).\n\n- Initially, the question is posed: “Can physical world attacks from the clinical setting severely affect the performance of popular DL architectures?” - I believe it would make the paper stronger if this would be toned down a bit. The answer is obviously yes since basically out-of-distribution examples are presented to the networks so that a lower/different performance is the expected result. I think it would improve the paper if the authors would instead just write that they are interesting in evaluating how such examples affect the performance.\n\n\nMinor comments:\n\n- In section 3.1, it is not clear what “The fine-tuned architecture consists of …” refers to. Is this the architecture of MobileNet, or are these some additional layers attached to each network?\n\n- Class weights are mentioned, but please explicitly state how different classes were weighted in the loss function.\n\n- It is confusing that the datasets are described relatively late in the manuscript. I would suggest moving section 3.3 before section 3.1.\n\n- In the caption of Table 1, it could be explicitly mentioned why no experiments with red lines were conducted.\n\n- In the PADv1 dataset, how was the ground truth verified? \n\n- In the results section, I found it confusing that first the results of the attack experiments and thereafter the baseline results of the clean images are presented.\n\n- The caption of Table 2 could mention (preferably in words, not as formula) what the robustness score expresses.\n\n- When referring to Tables and Figures, the words “Table” and “Figure” should be capitalized everywhere.\n\n- It is not really clear why calculating a weighted accuracy is not possible for the PADv1 dataset.\n\n- In the discussion, the authors write “We show small artifacts captured from the real world can significantly reduce the accuracy of DL diagnosis where dermatologists would not be impacted.” - this should be toned down as well (e.g., “would LIKELY not be impacted”) since it was not actually shown in this work that dermatologists are not impacted in their diagnosis.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		Byl6W7WeeN	rylM4pptfN	MIDL.io/2019/Conference/-/Paper60/Official_Review	[]	2		['everyone']	Byl6W7WeeN	['MIDL.io/2019/Conference/Paper60/AnonReviewer1']	1547455786315		1548856706725	['MIDL.io/2019/Conference/Paper60/AnonReviewer1']
312	1547321269115	{'pros': '- The authors propose a method that labels slices of a CT scan as “contains pulmonary nodule” or “no pulmonary nodule” \nbased on a pretrained CNN as feature extractor and a bidirectional RNN. Posing nodule detection as a slice labeling task allows for efficient use of the available training data since each slice is a different sample. A relatively low number of scans with manually labeled slices can result in a sizeable dataset for training.\n\n- The method is evaluated on two independent datasets.\n\n- The paper is overall well written and structured, it was easy to follow and it is detailed in describing method and experiments.\n', 'cons': '- While the methodological approach seems sensible for the task of finding slices with pulmonary nodules, this task itself seems not very useful. The observer would still need to search the entire slice and would likely scroll back and forth anyway. The method does also not give any indication how many nodules are visible in each slice. The authors could have investigated saliency maps or similar approaches for visualizing which parts of positive slices contributed to the decision, which might be more useful output for the observer.\n\n- The underlying deep learning approach is simple and many similar approaches have been used for similar tasks before. No novel methodology is presented in this paper.\n\n- The analysis of the results/performance is not very either detailed. For instance, there are various types of pulmonary nodules (ground glass, …) and nodules of many different sizes, with very different probabilities for malignancy. It would therefore be relevant to include a more detailed analysis of the detection performance stratified by such variables.\n\n- A comparison with other nodule detection methods would help to better understand how good 88% sensitivity and 0.5 FP/scan are.\n\n- The squared error is used as loss function even though this function is more commonly used for regression problems while this is a classification problem. This choice should therefore be better motivated.', 'rating': '1: strong reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		B1gQDNA3JE	H1gTh1TwzE	MIDL.io/2019/Conference/-/Paper7/Official_Review	[]	1		['everyone']	B1gQDNA3JE	['MIDL.io/2019/Conference/Paper7/AnonReviewer2']	1547321269115		1548856706475	['MIDL.io/2019/Conference/Paper7/AnonReviewer2']
313	1547510354453	{'pros': 'This paper presented a neural architecture search to optimize the structure of each layer of a 3D U-net. Besides the methodology, the authors also provided stochastic sampling algorithm to find the optimal parameters. Through benchmarking, the proposed method showed superior results and compact output model compare to other methods. ', 'cons': 'The experiment result is not strong, the link to in the paper leads to the competition website: http://medicaldecathlon.com/results.html. Most of the results posted was better than the results in the paper.\n\n1. Over reference. In my opinion, citing a paper once at where the paper is first mentioned is enough).\n2. I suggest add more explanation of the data and an picture example in the experiment section. \n3. I would see the run time difference for each method as well.\n4. Good to remind reader the evaluation metric again in table 1 ', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		S1lhkdKkeV	B1xcUzicGV	MIDL.io/2019/Conference/-/Paper34/Official_Review	[]	1		['everyone']	S1lhkdKkeV	['MIDL.io/2019/Conference/Paper34/AnonReviewer2']	1547510354453		1548856706259	['MIDL.io/2019/Conference/Paper34/AnonReviewer2']
314	1547501888856	"{'pros': 'This paper presented a redesigned the RCNN model to detect and classify extremely small objects in MRI. This paper also presented an experiment results to show the good sensitivity of this methodology. ', 'cons': ""General comments:\n* Grammar/sentence revision and proper introduction in the background can improve the readability of this paper significantly\n* I would also suggest to position in the Figure at the beginning of the explanation rather than at the end. The picture can better guide reader understand the specific content. Such as Figure 1 and Figure 2. \n* I think the paper is too specific and maybe hard to generalize to other datasets or problem \n    * The data is specific, I don’t see any discussion around the resolution of the image, noise level of the image \n    * The parameters are specific: to me, there are quite a few adhoc hyperparameters \n    * The dataset is small. The data description and the data cleaning process is not clear to me.\n\nSpecific comments: \n* Motivation: \n    * I recommend the author add more information on the significance of doing ESO detection \n        *  Specific examples/articles to support 'these markers reflect tissue damage and need to be accounted for to investigate the complete phenotype of complex pathological pathways'  \n* Please elaborate the purpose of the Fig 1 along with the meaning of the red dots.\n* Please explain the reason why the HighResNet was selected Backbone network. \n* In the DL equation, \n    * what is the meaning of the r_n? Is that a single number distance or a vector of 3 \n    * what is the unit for the distance r_n?Does that cutoff change as the resolution changes? \n    * Is the scale factor the solution to deal with resolution changes? \n* 'All input data was bias field corrected, skull stripped, and then z-scored to the white matter region statistics'.\n    * Was the white matter region known at the point or some estimation was applied\n* On page 5,  don’t understand 'the skeleton maxima of the smoothed regressed distance map (p score map >0.25)’ \n* I am confused with sec 3.1.  I am assuming 'Out of the initial 4147 considered elements, 2442 were used as gold standards for training. ‘ means for all 16 subjects, 2442 elements was used for training and testing. Among 2442, the elements belongs to 14 subjects were used for training, and the element belongs to 2 subjects were used for hold-out testing. Please confirm or clarity.\n* Not clear to me what is the input of this algorithm? \n* I don’t understand Figure 7, where is the ground truth? The first row boxes in Lacune and undecided look exactly same to me "", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BJlXpDh1gV	BkeYBWFcz4	MIDL.io/2019/Conference/-/Paper44/Official_Review	[]	1		['everyone']	BJlXpDh1gV	['MIDL.io/2019/Conference/Paper44/AnonReviewer2']	1547501888856		1548856706039	['MIDL.io/2019/Conference/Paper44/AnonReviewer2']
315	1547485444951	"{'pros': '-\tA relevant topic: I think the idea of using WOB as a biomarker towards the automation is interesting and worth to explore. \n-\tThe paper is generally well-written and easy to read and understand. They have also provided more details in the appendix which make it easier to cover missing parts from the body of the main paper. I have some suggestion to improve the quality of the paper.\n', 'cons': 'I would like to write this part as the suggestions:\n-\tSome details seem to be missing (sorry if they are there and I overlooked): For example, what is the final number of samples which is used for training? I can see the final number of 295 images, but I cannot find detailed information about the data division. \n-\tFurther explanation about the ""years of experience"" of the pathologist is useful in putting things into a perspective and also to factor out the human effects. Did they ask for the 4 or 5th pathologist to regrade? Also, for the manual annotation, who have done it? And do they have any confidence value for this annotator?\n-\tComparison of the method with the modality based approach such as MRI and Ultrasound would be interesting.\n-\tThe whole paper would benefit from mentioning the clinical goals and values of the research.  \n-\tIt would be interesting in Fig. 3, Sensitivity part, to also look at non-WOB regions sensitivity of P1, P2, P3. \n-\tFigure 1, is not more logical to train first on the coarse image 2 mpp, then refocused training on fine grain image 1 mpp? What are the reasons for the current setting? \n-\tWhat is the reported confidence rate of WOB, especially as a biomarker for indication of the acinar prostatic adenocarcinoma? \n-\tAnalysing and using other methods to control the receptive field is beneficial.\n-\tComparison with other methods which are using other biomarker is missing. \n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'special_issue': ['Special Issue Recommendation']}"		SJxVA7xleE	rJgpZ-HcfV	MIDL.io/2019/Conference/-/Paper53/Official_Review	[]	1		['everyone']	SJxVA7xleE	['MIDL.io/2019/Conference/Paper53/AnonReviewer1']	1547485444951		1548856705823	['MIDL.io/2019/Conference/Paper53/AnonReviewer1']
316	1547485084041	"{'pros': '1-\tA relevant topic: More exploration around the idea of transfer learning and the benefits of it in medical image application it is useful.\n2-\tThe proposed method seems quite well-engineered and convincing but there are some shortcomings (see below).\n3-\tThe evaluation in the simulation is relatively thorough but there are some weaknesses and addressing them it could be helpful in increasing the quality of the newer versions of this research.\n', 'cons': '1-\tNovelty is somewhat limited: The idea of evaluation and exploration of transfer learning approach for multi-stage setting is valuable, and the community can benefit from such comparative and analytic studies. However, the current idea partially has already been explored in Tajbakhsh paper in 2015. And the current authors’ evaluation is also limited and not aligned with their early claims. Moreover, at some point, the authors propose the use of focal loss as one of the contribution of the current paper (which is borrowed from another paper) to address the issue of the class imbalance. However, the benefits, pros, and cons of this idea are not evaluated properly in experiments. This mixture of contributions is so confusing for the reviewer to draw a clear line about the required evaluations.  \n2-\tIt is not clear what to conclude from the experimental results.\n3-\tWriting looks rushed and incomplete, some details seem to be missing (sorry if they are there and I overlooked): \na.\tFor the pathologist mentioning the ""years of experience"" is crucial to make any conclusion. \nb.\tData division is somehow incomplete and complicated; finally how many positive cases do we have in the training set? This is crucial for making a conclusion about the novel proposed loss for the class imbalance training.\nc.\tThere is no figure or diagram to show the process of different multistage transfer learning setting. At some point in the middle of reading, you are getting confused that why SVM? Why RF or how the choice of middle-level features will happen? A simple diagram is always beneficial.\nd.\tSection 2.3: ""Data augmentation is selectively used for lesions to improve the variety and quantity of positive patches"", what are these selection criteria?\n4-\tThe overall quality of the paper will benefit from rewriting some of the parts, like the introduction, contributions, and conclusion:\na.\tFor example, we have several too long sentences which make it so hard to follow. (e.g. We found that adopting a deeper architecture has small …, in the abstract.) \nb.\tContributions of the paper are not clear and concise. The authors mention different/new contributions in different sections.  \n5-\tEvaluation: To be honest, I am not quite convinced to just use default parameter (Table 2) of SVM and RF. We all know these methods need an exhaustive search of the parameter using a proper validation set to show their true potential.  \n6-\tMinor: page 3, to299x299, missing space.\n\nSome of the above shortcomings root in the fact that these types of papers are not suitable for just one conference paper with the page limit. If the authors wish to publish this valuable research as a conference paper, my suggestion for them is to be focused on just one of the contribution and evaluate that one from different aspects, act more incrementally, and don’t make a mix of contribution.\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		B1luMUUxgN	rkgEokrqGV	MIDL.io/2019/Conference/-/Paper107/Official_Review	[]	1		['everyone']	B1luMUUxgN	['MIDL.io/2019/Conference/Paper107/AnonReviewer1']	1547485084041		1548856705608	['MIDL.io/2019/Conference/Paper107/AnonReviewer1']
317	1547462333748	"{'pros': 'This paper uses unsupervised anomaly detection to create artificial labels . A spatial autoencoder is trained on healthy data to learn the appearance of normal data, using an optimization loss composed of L1-reconstruction + L2-reconstruction + Gradient-Difference-Loss. During inference time, the residual between the input and the reconstruction provides a anomaly detection mask on pixel-level. These artificial labels are then used as (additional) labels during training for segmentation models. Three main results are provided: (1) In the single domain-setting, when a segmentation model is trained on the artifical labels alone, the segmentation performance increases compared to the unsupervised anomaly detection model; (2) In the single domain-setting, when used as additional labels in a single-domain setting, performance decreases; (3) When used for domain-adaptation (manual annotations in the source domain, artificial labels from the unsupervised anomaly detection approach in the target domain), the segmentation in the target domain increases significantly.\n\nI accept the paper based on the assumption that in the revised version of the manuscript, my points stated below in the ""cons"" section will be addressed.\n\n\nPros:\n - The paper presents an interesting idea\n - The work tackles a highly relevant problem\n - Results look promising and are interesting. In particular the domain adaptation results, but also the improvement of unsupervised anomaly detection when supervised segmentation model is re-trained on the unsupervisedly created labels is worth to be published.\n - Introduction is written in a clear way and contains an extensive discussion of related work.\n - Method section is very clear and easy to follow.\n - The conducted experiments are reasonable and provide an appropriate insight regarding the presented method. (but structure/presentation of experiments needs to be improved and minor evaluations should be added, see cons)\n', 'cons': 'Cons:\n - In Section 1, the authors cite (Pawlowski et al., 2018) and state that they used model uncertainty in Variational Auto-Encoders to detect anomalies. This is imprecise, as Pawlowski et al. proposed to average multiple auto-encoder reconstruction outputs (generated by Monte-Carlo dropout sampling) and do not use uncertainty in their model.\n - In Section 2 (Methodology), in equation (1), the weighting-terms of l1/l2/gdl are missing (lambda_l1/lambda_l2/lambda_gdl). In the corresponding experimental setup description, only the value of lambda_gdl is mentioned. Please also provide weighting values for the other two terms.\n - In Section 2, the authors mention that many false positive residuals are avoided by optimizing for the gdl term, but this is a statement without proof. I kindly ask the authors to evaluate the performance of a comparison model where the auto-encoder is trained without the gdl-term.\n - I think Section 3 needs to be revised in order to present the results in a more clear and better structured way. In its current form, it is quite hard for the reader to follow the evaluation and compare different models. Furthermore, i some informations are missing. Some issues/suggestions (not necessarily complete):\n     - provide values for lambda_l1/lambda_l2 of equaiton (1)\n     - The datasets have different images: ""MSSEG2008"" datasets contain FLAIR + T1 + T2-weighted images; ""healthy""+""MS"" dataset contain only FLAIR and T1 weighted images. How is this handled in the semi-supervised domain-adaption experiment?\n     - Figure 2 is not described in the text (i would have expected that in Section 3.2). I guess it is a result on the MS-unlabeled dataset (or is it on the MS-validation set?), but please provide that information. the model is referred to as ""AE"" (also in 3.2), while in Table 2 it is referenced as ""UAE"" - be consistent.\n     - In Section 3.2, the authors state that ""all non-empty slices of the 19 unlabeled subjects have been processed with the AE to detect anomalies"". How are non-empty slices determined?\n     - The title of Section 3.3 does not mention supervised deep learning, though a fully supervised experiment is conducted as well. Maybe a different subsection-title would help?\n     - In Section 3.3, the term ""lower bound"" is confusing. A supervised trained model is compared against unsupervised and semi-supervised models and is the lower bound? As the results show, this is also not true. I would recommend to use completely different notation/naming of the compared models. For instance, the authors could use the used labels for notation (instead of ""lower"", ""SS"", etc. --> ""Y_MS"", ""Y_MS + S_MS"", etc.). This should then also be reflected in the subscript-notation of X/Y/S: (X_MS,Y_MS) denotes the pair image+ground-truth in the ""MS"" dataset; (X_MS,S_MS) denotes the pair image+artificial-label in the ""MS"" dataset; etc.. \n     - Change the name of the ""unlabeled"" subset in D_MS, since labels of this data are used in the experiments. Maybe ""Addtional Training Set""?\n     - Use a shorter subscript for D_MSSEG2008-CHB as long as there is no specific reason for this long version.\n     - It could be useful to introduce the X/Y/S notation in table 1, but once the authors revise this Section they will see if this is a useful suggestion or not.\n     - Introduce the experiment pipeline and its target directly at the beginning of the subsections. Especially in Section 3.4 this would help to clarify.\n     - Introduce an additional line in Table 2, showing the UAD-performance of an autoencoder trained without the gdl-term, as mentioned above. \n     - The UAD model is not explicitly described in the experimental setup. The reader can derive it from the context somehow, but this is unneccessarily difficult.\n     - Section 3.4: again the notation of the models is confusing. In consistency with the above suggestions, i would recommend to use labels/training-data info to describe the different models.\n     - Section 3.4: Abbreviation in Table 3 and 4 is not consistent with the abbreviations used everywhere else: ""MSSEG CHB"" vs. ""MSSEG2008-CHB""\n     - What is the difference between ""DICE"" and ""DICE(mean+standardDeviation)""? What is ""AUPR"" (i guess it is area under the precision recall curve, but this should be mentioned)?\n - In the discussion the authors mention ""that AU S is slightly inferior, which is due to FPs in segmentations (see Figure 2 C) provided by the UAD for training the segmentation network. The FP in S are learned and again reflected by the segmentation model."" The authors should weaken this statement, if they just observed many false positives in the output of the autoencoder in a qualitative assessment. At least they should mention which evaluation was performed that leaded to this conclusion.\n - The authors state ""We even outperform the upper bound model B_upper which has been trained from labeled data of both domains."" Be clear about which dataset and model are you referring to (i guess itsMSSEG2008-CHB, but please state it). In the next sentence the authors claim that ""The same effect is noticed in the experiments involving the D_MSSEG2008-UNC dataset,..."", which is not correct. The model B_SS does not outperform the upper model on the MSSEG2008-UNC dataset. Please revise this. If the authors mean something different here, please correct/clarify that in the text.\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		ryxNhZGlxV	ByeUT8k5fN	MIDL.io/2019/Conference/-/Paper71/Official_Review	[]	1		['everyone']	ryxNhZGlxV	['MIDL.io/2019/Conference/Paper71/AnonReviewer1']	1547462333748		1548856705350	['MIDL.io/2019/Conference/Paper71/AnonReviewer1']
318	1547448560260	{'pros': 'To investigate whether a conditional mapping can be learned by a generative adversarial network to map CTP inputs to generated MR DWI that more clearly delineates hyperintense regions due to ischemic stroke.\nTo perform image-to-image translation from multi-modal CT perfusion maps to di\x0busion weighted MR outputs\nTo make use of generated MR data inputs to perform ischemic stroke lesion segmentation.', 'cons': 'There is no detail on qualitatively visual comparison of generated MR to ground truth.\nThe authors had better compare segmentation result between CTP with orginal MRI and CTP with CGAN MRI.\nThe gain using CGAN MRI looks marginal, which would be better to apply ablation study.\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		HyemZQzExE	H1edxZ3FzE	MIDL.io/2019/Conference/-/Paper144/Official_Review	[]	1		['everyone']	HyemZQzExE	['MIDL.io/2019/Conference/Paper144/AnonReviewer3']	1547448560260		1548856705092	['MIDL.io/2019/Conference/Paper144/AnonReviewer3']
319	1547430368689	"{'pros': '1. New hybrid cascade model for deep-learning-based magnetic resonance (MR) imaging reconstruction techniques\n2. This architecture improvements were statistically signi\x0ccant (Wilcoxon signed-rank test, p < 0:05)\n3. Visual assessment of the images reconstructed con\x0crm that our method outputs images similar to the fully sampled reconstruction reference.', 'cons': ""There is no detail of evaluating pSNR. What's the PSNR of reference image?\nIt would be better to present magnified images in Figure 3 and 5.\n"", 'rating': '3: accept', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		HJeJx4XxlN	SkeF1qDYGV	MIDL.io/2019/Conference/-/Paper85/Official_Review	[]	2		['everyone']	HJeJx4XxlN	['MIDL.io/2019/Conference/Paper85/AnonReviewer1']	1547430368689		1548856704877	['MIDL.io/2019/Conference/Paper85/AnonReviewer1']
320	1547357595949	{'pros': '1. New architecture definition for a DNN. Integrates 3 different networks working in parallel. Each with a different receptor field using 1x1, 3x3 and two successive 3x3 spatial convolution operators, along with residual connections.\n2. Enables the network to capture the appearance variation across multi-scale in terms of different receptor fields.', 'cons': '1. Computational complexity notably higher than a since receptor field type network.\n2. Classical inception block mixes all receptor field responses per layer before passing over this mixed response to the next layer. The dis-/advantage if any in this architecture by not mixing multiple receptor fields before computing the next layer is not elucidated.\n3. Needs dataset specific training and is not demonstrated to be dataset agnostic.\n4. Some recent papers from 2018 which have a richer layer abstraction, similar philosophy in architecture but also make use of better cost functions are not compared: \na. Md Zahangir Alom, Mahmudul Hasan, Chris Yakopcic, Tarek M Taha, and Vijayan K Asari. Recurrent residual convolutional neural network based on u-net (r2u-net) for medical image segmentation. arXiv preprint arXiv:1802.06955, 2018.\nb. Kai Hu, Zhenzhen Zhang, Xiaorui Niu, Yuan Zhang, Chunhong Cao, Fen Xiao, and Xieping Gao. Retinal vessel segmentation of color fundus images using multiscale convolutional neural network with an improved cross-entropy loss function. Neurocomputing, 2018.\n\n5. Authors may consider including link to their project, and provide details on the hw/sw platform and versions including number system used for implementing this so as to make the experiments repeatable. ', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		Hyxj9HXrJV	B1eViTHdfV	MIDL.io/2019/Conference/-/Paper1/Official_Review	[]	1		['everyone']	Hyxj9HXrJV	['MIDL.io/2019/Conference/Paper1/AnonReviewer2']	1547357595949		1548856704664	['MIDL.io/2019/Conference/Paper1/AnonReviewer2']
321	1547324125868	{'pros': '- Well written, well referenced, very clearly written\n- Comprehensive comparison between many state-of-the-art algorithms\n- The paper contains many fruitful insights: firstly, it demonstrates -- via over three different architectures -- that the unrolled approach seems to outperform a single multi-scale architecture such as U-net. Secondly, the image domain reconstruction should be done before k-space reconstruction. Thirdly, the authors make the effort of understanding the unrolled architecture.', 'cons': '- Lack of novelty: the only difference from KIKI-net is the fact that it is now trained end-to-end and the order is improved. However, this is interesting because in KIKI-net paper, they showed that the proposed order is better than IKIK-net. This may be attributed to end-to-end training? Please add further details here.\n\n- The paper is missing the detail about the number of parameters of each network. Because of this, I cannot make a fair comparison between the methods. In particular, how many convolution layers are used in each subnet & how many cascades for (a) KIKI-net and (b) Deep Cascade? For example, Hybrid net has 5 conv. layers for subnet, 6 cascades. I wonder if the parameters are matched? Please report them and redo the experiment by matching them.\n\n- Please report the SSIM value, the number of parameters and the speed of each method.', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		HJeJx4XxlN	BJgIJs6PMN	MIDL.io/2019/Conference/-/Paper85/Official_Review	[]	1		['everyone']	HJeJx4XxlN	['MIDL.io/2019/Conference/Paper85/AnonReviewer2']	1547324125868		1548856704453	['MIDL.io/2019/Conference/Paper85/AnonReviewer2']
322	1547307352482	"{'pros': 'Summary: \nThis paper tries to solve the tracking and mitosis detection problem in microscopy for neural cell tracking. Methodologically the authors focus on the data association problem between frames and try to solve it by using a convolutional segmentation network complemented by an unsupervised optical flow network. Data association is optimised using the Hungarian algorithm. \n\nStrengths:\n- this work tackles a non-trivial problem with deep convolutional neural networks. \n- the paper is reasonably well structured \n- the results look promising ', 'cons': 'Weaknesses:\n- in my opinion the presented approach only replaces the computationally expensive parts of segmentation and optical flow with ConvNets for a conventional tracking algorithm  as shown in Listing 1. I would be more impressed if an efficient network would be able to find and predict cell trajectories directly. I cannot see why the networks should predict optical flow and blobs separately if they could predict positional trajectories directly for frame sequences of a given length.  BSNet and UnFlowNet are by themselves not methodologically novel since they essentially resemble a U-Net and the Wang CVPR 2018 approach for unsupervised flow.  \n- since the used BSNet and UnFlowNet have the same encoder stage, why not using a M-Head architecture to train end-to-end?\n- no data augmentation has been used. Given the small size of the training set it would be interesting to see which augmentation strategy would increase performance (or not). \n- it is unclear if the proposed method is meant for online tracking or if it is a retrospective analysis tool since no (forward) runtime analysis is provided.  \n- p. 2: ""(Tang and Bengtsson, 2005; Al-Kofahi et al., 2006; Pinidiyaarachchi and W¨ahlby, 2005; Magnusson et al., 2015)"". What do these methods do and why exactly do they fail? Reference chains aren\'t good practice and difficult to read. (also ""(Bewley et al., 2016; Son et al., 2017; Leal-Taix´e et al., 2016; Kim et al., 2015)"")\n- p. 2: ""This task aims to find the optimal set of trajectories for moving objects within a video"" -> I would assume that the task is just to find the correct trajectories; to what other criteria would \'optimal\' be evaluated against? Of course, \'correct\' is approximated by optimisation, but an optimal trajectory can also mean, e.g., most energy efficient, least intersections, etc. \n- p. 4: ""thus commonly performed on synthetic dataset"" - "" a dataset"" or ""datasets""\n- p.5: can you elaborate a bit on ""anchor assignments"" please?\n- p. 5: ""The head prediction of BSNet..."" -> ""The prediction head of BSNet...\n- p.5: first paragraph of 3.2. is redundant. \n- p.5: ""...blob segments of the current frame is solved by Hungarian algorithm using mask IoU."" missing article -> the Hungarian algorithm \n- p.6: ""run on 4 Nvidia K80 GPUs"": how is the approach parallelized across the GPUs?\n\nminor:\n- ""Existing neural cell tracking methods generally use the morphology cell features..."" -> ""Existing neural cell tracking methods generally use morphological cell features...""""\n- ""mitosis determination"" -> ""mitosis detection""\n- ""... the vision techniques could be applied for automatic analysis."" -> ""... computer vision techniques can be applied for automatic analysis.""\n\nOverall, I am not entirely convinced by the paper. Essentially this work combines two known ConvNets to solve a challenging problem in a conservative way. Last year, MIDL has defined itself as a venue where unconventional deep learning methods for medical image analysis can be discussed. The present paper is missing this touch and should be methodologically revised or submitted to other medical image analysis forums.    ', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		SJe92BmJlE	SJlxDFtPM4	MIDL.io/2019/Conference/-/Paper30/Official_Review	[]	1		['everyone']	SJe92BmJlE	['MIDL.io/2019/Conference/Paper30/AnonReviewer3']	1547307352482		1548856704239	['MIDL.io/2019/Conference/Paper30/AnonReviewer3']
323	1547318524747	"{'pros': 'Summary: This paper discusses an approach to automatically recognise surgical gestures from temporal kinematics data. The authors propose to extend an existing method (Lea et al. 2016) with skip connections and test on a newly available dataset (JIGSAWS).   \n\n- The authors use a new dataset to test a convolutional neural network approach for action recognition \n- This work extends a work by Lea et al. 2016 for action recognition by introducing skip connections\n- The presented results outperform previous work', 'cons': '- my understanding of the JIGSAWS dataset is that it also comes with video data. Why hasn\'t this data been used as additional source of rich information. Only kinematics data has been used for 1D segmentation. \n- what\'s the difference between a U-Net (Ronneberger 2015) and the proposed Lea et al. 2016 approach with skip connections? Wouldn\'t a 1D U-Net be better suited for this job or do the same? \n- While the method is very straight forward and easy to understand, the paper is difficult to read mainly because of language and grammar shortcomings.  \n- The authors describe the problem as a dictionary recognition problem in 1D. My feeling is that methods from the domain of natural language processing would be promising for the targeted problem (1D high dimensional features, dictionaries, grammars, etc.). Using a conventional convolutional segmentation approach method might not be ideal for this class of problems. \n- there is a lot of white space, especially around the figures that could have been used more efficiently.\n\nminors: \n- abstract: "" Automatic segmentation of continuous scene"" -> scenes\n- abstract: ""it is important that they understand the scene they are in, which imply the need to recognize action"". This sentence does not make any sense. understanding a scene does not imply understanding actions. Understanding actions usually requires understanding scenes...\n- abstract: ""specifically 1D Convolutional layer"" -> a layer? layers?\n- p1: ""it is crucial to be able to segment the scene in smaller segment"" ?? segments ?\n- at this point I gave up  to suggest detailed language improvements. It feels like every sentence is grammatically wrong in the abstract and large parts of the remaining paper. \n- p3: \'an high level representation\' -> \'a high level representation\'\n- p4: ""ski connections"" -> ""skip connections""\n- p7: ""This results could be because"" these results, this result...?\n- p7: ""which means their are not many"" -> their -> there\n- p.2: ""Unsupervised methods are what everyone is aiming for, however for now, the results are not as high as with supervised methods."" -- what does this sentence contribute to the paper?\n- p10: ""Hochschreiter S. and Schmidhuber S. ..."" reference format is inconsistent with other references. \n\nOverall, this paper describes a trivial extension of an existing approach. The paper seems to have been written in a rush and would need major revision, both regarding the presentation as well as methodologically. I would suggest to condense this paper and submit to ISBI as a 1-page abstract. ', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		B1g5dgfee4	rkgBbSnDfV	MIDL.io/2019/Conference/-/Paper69/Official_Review	[]	1		['everyone']	B1g5dgfee4	['MIDL.io/2019/Conference/Paper69/AnonReviewer1']	1547318524747		1548856703983	['MIDL.io/2019/Conference/Paper69/AnonReviewer1']
324	1547293705006	"{'pros': 'This paper combines an auto-encoder, an adversarial loss and a classification loss to localize structures that occur only in a diseased/anomalous class, but not in healthy images. While labels are only available on image-level, the output of the model provides predicitons on pixel-level. The idea is very interesting, the tackled problem is a relevant issue in medical imaging and the qualitative results look promising. On the other hand, I have serious concerns regarding the experimental setup and evaluation and the paper needs to be revised to improve structure and clarity.\n\n - The paper presents an interesting idea.\n - The work addresses a relevant problem in medical imaging.\n - Qualitative results look promising (but have serious limitations, see below)\n', 'cons': ' - In general, many explanations are confusing. One example is the description of the push and pull components mentioned below.\n - I think the name could be an appropriate description of the method, but the explanation in Section 2 (in its current form) does not support it. According to the definition in Section 2 (""the classifier is an external force which helps the encoder-decoder ‘pull’ biomarkers out from the original (abnormal) images."" vs. ""the discriminator ‘pushes’ the encoder-decoder to remove all possible biomarker signals from original images""), both components have the same role. I understand the idea of the adversarial discriminator, forcing the autoencoder to remove abnormal-specific features, and the role of the classifier, preventing the autoencoder of being to aggressive in terms of altering the input. But this must be clearly described in Section 2, since this is the core idea of the proposed method.\n - One of my main concerns is, that there is no clear separation of training and test set. In Section 3.1, the authors state that they used the whole dataset to train their architecture. Of course, this is not an appropriate setting to evaluate the generalization performance of the model. Therefore, the presented results loose validity and significance, since they could be the product of overfitting.\n - No quantitative values for ablation studies are provided (encoder-decoder without classifier, encoder-decoder without discriminator).\n - There are no qualitative results of CAM, Grad-CAM shown for the fundus dataset.\n - It would be helpful to provide class-specific accuracies in addition to Table 1.\n - It would be interesting to compare against other weakly-supervised lesion localization techniques (e.g. [1]), or unsupervised anomaly detection methods (e.g. [2]).\n\n[1]González-Gonzalo, Cristina, et al. ""Improving weakly-supervised lesion localization with iterative saliency map refinement."" (2018).\n[2] Schlegl, Thomas, et al. ""Unsupervised anomaly detection with generative adversarial networks to guide marker discovery."" International Conference on Information Processing in Medical Imaging. Springer, Cham, 2017.', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		rJlyzeEBgN	Bke-fE8DfN	MIDL.io/2019/Conference/-/Paper151/Official_Review	[]	1		['everyone']	rJlyzeEBgN	['MIDL.io/2019/Conference/Paper151/AnonReviewer1']	1547293705006		1548856703692	['MIDL.io/2019/Conference/Paper151/AnonReviewer1']
325	1547233378470	"{'pros': 'The work looks into the interesting problem of evaluating robustness of DL models in clinical settings, with a focus on scenarios where adversarial examples are used.\n\nThe research is novel as it explores the use of robustness to ‘physical world attacks’ as an approach to model evaluation, which has not previously been investigated in the medical imaging literature.\n\nOverall, the paper is written clearly and the methodology is well designed. Long term, high impact application of the work is feasible. The work also makes publicly available a new dataset (PADv1), which would make it easy to reproduce elements of this work by others. \n', 'cons': ""-Title:\n--------\nIf the reader is not familiar with the ML adversarial attacks literature, terms such as 'physical attacks in dermoscopy' may be confusing at the first instance. Perhaps the title can be rephrased to help convey the message of the paper.   \n\nIntroduction:\n------------------\n- “While medical systems empowered by Deep Learning (DL) are getting approved for clinical procedures ...”\nPlease support by referring to examples of such systems that have received approvals. \n\n- “physical world attacks are constrained to changing the appearance of the region under consideration in the real world … ”\nTo ensure a robust argument is made for motivating the paper, please comment on how realistic  such ‘attacks’ are in clinical settings. If they are not performed by the clinicians themselves, the attacker would need to go through a great deal of, perhaps unrealistic, effort to draw on a patient’s skin, taking dermatology as an example. \n\n- “Wherever there is money to be made, some people will exploit the opportunity and abuse ambiguities, which is shown by cyber threats ...” \nPlease rephrase. It doesn’t read well.\n\nMethods:\n-------------\n- It is unclear what the deep models were trained to classify. Were they initially trained to classify each image into one of the seven classes that were pathologically verified? If so, does the classification problem remain the same when using applying the models on the images from the new dataset? Please clarify.\n\n- “All lesions are non suspicious for melanoma ...”\nWhat is the significance of this? And also note that a large number of readers would not necessarily be familiar with terms common in dermatology.\n\nResults & Discussion:\n------------------------------\n- It appears that susceptibility is measured on a negative scale, i.e. the lower the number the more susceptible the system is. Please confirm and clarify in the text (not only figure caption) if this is true.\n\n-Accuracy on its own is generally not sufficient as an evaluation metric. It would be interesting to see how susceptibility and robustness metrics derived from, say, sensitivity and specificity of the models, compare to the currently reported observations. \n\n- Please elaborate on the limitations of this work."", 'rating': '4: strong accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'special_issue': ['Special Issue Recommendation'], 'oral_presentation': ['Consider for oral presentation']}"		Byl6W7WeeN	HJx9vuDUGV	MIDL.io/2019/Conference/-/Paper60/Official_Review	[]	1		['everyone']	Byl6W7WeeN	['MIDL.io/2019/Conference/Paper60/AnonReviewer2']	1547233378470		1548856703439	['MIDL.io/2019/Conference/Paper60/AnonReviewer2']
326	1547119385957	{'pros': 'The authors claim that it is possible to obtain equivalent information to the one given by fluorescence microscopy (images where nuclei and cytoplasms are labelled with different fluorochromes and therefore, it is possible to distinguish almost uniquely both parts of the cells) with bright-field microscopy data. \nThe problem at hands is very interesting for the improvement of biological results as fluorochromes are sometimes toxic and might change the metabolism and behaviour of cells. \nBesides, each of the trained convolutional neural networks (U-net, Tiramisu and Deeplabv3+) results in a high Dice coefficient (0.91, 0.93 and 0.94 respectively), showing the great potential of the employed methods. \n', 'cons': 'The methods used in this paper are already published and widely discussed convolutional neural network architectures (U-net, Tiramisu, Deeplabv+3), that show to work very well in this case. However, the writing style is so messy that it does not make clear the process followed by the authors to obtain the presented results. There is also a large number of format errors and the use of English should be reviewed:\n- Author names and institutions are missed!\n- Some references are missed along the text and appear as ‘#’(in the first paragraph for instance)\n- Format errors in the bibliography: “CVPR” is written as “Cvpr”, “Computer Vision and Pattern Recognition Workshops (CVPRW)”, “Proceedings of the IEEE conference on computer vision and pattern recognition“\n- “yielding different label images.” → yielding different images FOR EACH LABEL\n- “with some of blocks” → some of THE blocks\n- “The encoding phase consist… ” → consistS\n- “without resampling and to, patches” → ???\nand so on.\n\nIn terms of clarity, I would highlight the following points:\n- The abstract does not reflect clearly what is the main motivation of this work and what is exactly the concept the authors want to prove: The use of image processing methods for the prediction of cell nuclei and cytoplasms from different (less toxic, and less expensive in terms of work) microscopy modality images, such as bright field microscopy.\n- The process to build the ground truth is not explained.\n- The software used for the implementation of the networks is not specified. \n- The data for cross-validated training of Deeplabv+3 was split into 136 and 44 images, while authors only had 170 images. Therefore, some of the images must be included in both training and validation datasets. Might this be a reason why the reported accuracy measure in Table 1 is higher than the one for Deeplabv+3 without cross validation?\n- There are some questions that should be detailed along the manuscript: Why did you decide to use cross-validation only for Deeplabv+3? Equation 1: What is the value range of k?\n- Font size in Figure 1 is too small.\n\nThe results show that it might be possible to segment cell nuclei and cytoplasms from brightfield microscopy. In order to prove it, I would say that the data should be more heterogeneous: different cell lines and microscopy devices. \n\nAir bubbles are quite common in brightfield microscopy. Also, a previous step to remove this part of the images (or the whole image) might slow down the process or even introduce some bias in the cases in which discarding of air bubbles is not correct. Do you think that a machine learning method could learn to discard the pixels belonging to air bubbles and classify them as background for example? How would you evaluate it (in fluorescence microscopy bubbles are not a problem as the fluorescent signal is recorded in any case, and therefore in the ground truth these pixels will not appear as background)?\n\n\n\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		Sygt37F21E	rJgzmojVfE	MIDL.io/2019/Conference/-/Paper6/Official_Review	[]	1		['everyone']	Sygt37F21E	['MIDL.io/2019/Conference/Paper6/AnonReviewer2']	1547119385957		1548856703189	['MIDL.io/2019/Conference/Paper6/AnonReviewer2']
327	1547051462391	{'pros': 'The study looked into the utility of using of CNNs for semantic segmentation of histopathological images in colorectal cancer, with a focus on settings where training labels are available in the form of sparse manual annotations. \n\nOverall, the paper is well written, nicely structured, its methodology is well designed, and observations are clearly described.\n\nThe problem it aims to address is important, and has potential applications beyond histopathological images and colorectal cancer.', 'cons': 'Some areas for improvement are included below.\n\n*Abstract:\n----------------\n- “We propose to address this problem by modifying the loss function in order to balance the contribution of each pixel of the input data”...  This statement gives an initial impression to the reader that a major focus of the study is the modification of the loss function, where in fact this refers to an empirical evaluation of different strategies of assigning weights to pixel samples (which, in turn, contributes to the loss function).  Please edit.\n\n*Introduction :\n----------------------\n-The authors build a good case for why the use of sparse labels is interesting. Reference to previous work in the literature is, however, rather limited.\n\n*Materials :\n-----------------\n- It would be good to mention details on variation within the colorectral cancer patient cohort analysed here, e.g. Does the number of images reported corresponding to one image per patient? how many malignant vs. benign cases? Age and sex distributions? etc. \n- What is the experience of the annotators with this kind of data? How familiar are the non-pathologists with medical images?\n- Please clarify if there is any overlap between the images on which sparse and dense annotations were carried out? (i.e. are some of the sparsely annotated images essentially a sub-set of the densely annotated ones?)\n- Please confirm that there are actually two training sets, two validation sets, and two test sets, for sparse and dense data respectively?\n\n*Method :\n---------------\n- The subheadings are confusing in this section. Perhaps some use of numbering can help organise related subheadings together. \n\n*Results, Discussion and Conclusion:\n------------------------------------------------------\n- One needs to be careful here before interpreting and drawing solid conclusions from the reported results. There are no confidence intervals associated with the reported Dice scores and it is difficult to really evaluate the levels of overlap between different models’ performances. Since model robustness is not evaluated, and given that sampling effects may play a big role here, there might be different observations if the same analysis was carried out on a perturbed version of the dataset. These points need to be highlighted in the discussion.\n- Dice scores alone may not give a sufficient idea into how the models perform. Please comment on the need for additional metrics e.g. accuracy, sensitivity, specificity. \n- Please discuss limitations and what the next steps are for this research.\n\n', 'rating': '4: strong accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'oral_presentation': ['Consider for oral presentation']}		SkeBT7BxeV	BygRaZomfE	MIDL.io/2019/Conference/-/Paper100/Official_Review	[]	1		['everyone']	SkeBT7BxeV	['MIDL.io/2019/Conference/Paper100/AnonReviewer1']	1547051462391		1548856702936	['MIDL.io/2019/Conference/Paper100/AnonReviewer1']
328	1546978979604	{'pros': 'Training data for medical imaging tasks is not easy to obtain. Using synthetic data is helpful, yet transferring networks trained on synthetic data to real world applications is challenging. This paper tried to solve this problem by Domain Randomization. The authors proposed two kinds of Domain Randomization.  1. varying the intensity transfer function and 2) adding collimation to the 2D projections.\n\nBoth of the two kinds of randomization are designed based on physical model.\n\nThe robustness is improved after using the proposed method.\n\n', 'cons': 'Major:\nThere is only qualitative comparison. It would be more helpful if the paper contains another evaluation that is quantitative.\n\nMinor:\nIt would be helpful if the authors can provide evaluation for each of the two kinds of randomization. The paper only shows the improvement after applying both of them.\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		rkgHu5GglV	Sylnj8YGfV	MIDL.io/2019/Conference/-/Paper79/Official_Review	[]	1		['everyone']	rkgHu5GglV	['MIDL.io/2019/Conference/Paper79/AnonReviewer2']	1546978979604		1548856702680	['MIDL.io/2019/Conference/Paper79/AnonReviewer2']
329	1546975068189	"{'pros': 'The authors implement a modified U-net on surgical video segmentation which is more challenging than single image segmentation. The authors claim three contributions of this paper. 1. They adopt the idea of replacing the encoder of U-net with a pre-trained VGG-16 or VGG-11 (Iglovikov et al.). 2.  In decoder, they replace deconvolution layer with an up-sampling layer that uses nearest-neighbor interpolation followed by two convolution layers. 3. They use a data augmentation library ( Shvets et al.).\n\nIt is an application paper. It shows that by combing multiple engineering techniques they can improve U-net segmentation significantly.\n\nThe introduction section is well written.\n\n', 'cons': 'Minor: The second contribution --- replacing deconvolution layer with an up-sampling layer that uses nearest-neighbor interpolation followed by two convolution layers --- is not novel. For example, Zhao, Can, et al. ""Whole brain segmentation and labeling from CT using synthetic MR images."" International Workshop on Machine Learning in Medical Imaging. Springer, Cham, 2017. And actually some open source implementations like ""https://github.com/zhixuhao/unet/blob/master/model.py"" used this setting. But I understand it is difficult to do a complete literature review. \n\nMajor: \nNote that none of the three contributions is novel. That makes this paper an application paper. As an application paper, ""MIDL values application articles that present solid validation"". \n\nThe major concern about this paper is that the authors did not perform a paired significance test between the proposed method and the state-of-art method TernausNet. For both IoU and Dice, the standard deviation is larger than 10%, while the improvement of mean value is around 0.2%, way smaller than standard deviation. The authors only show a box plot between U-net and proposed method, but somehow omitted the other method TernausNet. \n\nSuggestions:\n1. Significance test is necessary when the improvement is small.\n2. An evaluation on the improvement from each of the three techniques is preferred. \n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		SkxQ6fjlx4	SklEwwuMGE	MIDL.io/2019/Conference/-/Paper128/Official_Review	[]	1		['everyone']	SkxQ6fjlx4	['MIDL.io/2019/Conference/Paper128/AnonReviewer2']	1546975068189		1548856702465	['MIDL.io/2019/Conference/Paper128/AnonReviewer2']
330	1548688353544	{'pros': '\nWell written\n\nInteresting application of an existing technique\n', 'cons': 'Little to no methodological contribution: this paper evaluates a technique already developed (in 2016) for natural images and its application to diffusion MRI. \n\nNot entirely clear but it seems that the technique is applied to each slice and each diffusion gradient separately. If it is the case then the potential to gain resolution by better taking into account all the information in q-space is lost.\n\nOnly FA/MD is evaluated (ie metrics that involve estimation of 6 unknown, a single diffusion tensor), while the dataset used has 134 diffusion directions with multiple non-zero b-values.\nWhile it is true that FA/MD is used in clinical applications, the impact of the work with typical clinical DW scans (for example a range of 12-30 directions and a single b-value) is unknown.\n\nThe usefulness of FA/MD with a multi b-value acquisition is unclear (cf non Gaussianity of the diffusion signal)\n\nThere is unfortunately no comparison with the upsampled scan (bicubic interpolation) used as a starting point of the algorithm. This would have provided a great understanding of the added value of DL.\n\nFig6 shows an orientation map of the super resolved DTI but there is no comparison with the original (or bicubic interpolated) orientation map. Can we see more details on the super-resolved orientation map? This cannot be appreciated.\n\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		B1gETswelE	rkgckhq2XE	MIDL.io/2019/Conference/-/Paper115/Official_Review	[]	3		['everyone']	B1gETswelE	['MIDL.io/2019/Conference/Paper115/AnonReviewer2']	1548688353544		1548856702257	['MIDL.io/2019/Conference/Paper115/AnonReviewer2']
331	1548689526101	"{'pros': 'Authors identify real-time imaging as one of major US advantages. To improve over conventional delay and sum reconstruction (DAS), they propose to train convolutional network to reproduce advanced time-conusming reconstructions (minimum variance beamforming) in real-time.\n\n1) Proposed reconstruction method allowed improvements in anatomy classification', 'cons': '1) The key idea to train using reconstructed ""ground truth"" (gold standard more appropriately) is not stated explicitly. E.g. learned reconstruction is limited by MA-BF.\n2) Speckle information can be useful to track motion or assess tissue characteristics, therefore speckle filtering is not always desired.\n3) Since real-time application is one of motivations for the method, reconstruction times of MV-BF, DAS and DeepFormer shall be reported.\n4) TV regularization is not employed in MV-BF, therefore improvements in classification accuracy or image similarity cannot be directly attributed to the DeepFormer. \n\nSome minor comments:\n* Eq (1) cs_j is not defined\n* Capitalisation of bibliographic entries is incorrect. E.g. cnn, 3d.', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		SygEcVLHl4	BJlROli274	MIDL.io/2019/Conference/-/Paper158/Official_Review	[]	3		['everyone']	SygEcVLHl4	['MIDL.io/2019/Conference/Paper158/AnonReviewer3']	1548689526101		1548856702043	['MIDL.io/2019/Conference/Paper158/AnonReviewer3']
332	1548689645548	"{'pros': 'They use deep learning in a new dataset and suggest a method to make use of unlabelled data.', 'cons': '\nI do not think the contribution is strong enough for the a paper to be accepted in the conference. They use an old method (classify the centre pixel of a patch in the image) with the idea that it will use less memory and expand the data available since it will be as many samples as available voxels. However, this is unlikely the case. More modern approach like a 3D U-net (Ö Çiçek et al. \u200e2016) should fit in a modern consumer grade GPU like 1080Ti. The ""extra"" data will be highly redundant since neighboring pixels are basically the same patch, there therefore a lot of the operations during training will be redundant. It would be better do data augmentation (e.g. random shifts and rotations). If they are concerned about sample imbalance they could just weight the loss for each pixel according to the sample weight. They never describe they model properly. Is it exactly a LeNet network? What backend did they use to train it? Why they didn\'t use batch normalization? A supplementary material with more information will be needed.\n\nThe ground truth confuse me. They compare with the BrainVISA model, but at the same time they use this model to extract candidate regions.  If that the case, they are rather using a sort of ensemble of models. It would be impossible for the deep learning model to do worse than BrainVISA since the pixels are first classified by the later.\n\nI find problems in the statistics they use to claim an improvement in their model. They mention a p-value of 2.15e − 26. With the numbers shown table 1 I would assume the only way to get those p-values is by having thousands of independent samples. Since they total labelled sample only consist in ~60 individuals I do not understand those statistics. Are they consider each individual sulci in each individual as a different sample? Are they doing multiple hypothesis correction? Since their model only considers positive or unknown labels (not the sulci identity) I think the correct approach will be to pool the results from all the voxels. If they want a standard deviation they should have use cross-validation.\n\nAdditionally, the english and presentation need to be polished. The text is at times informal and unclear. The black highlightings at times do not indicate relevant information. The figure 2 seems that it might be important, but it is not clearly stated why. There titles that are abbreviations and those abbreviations that are never explained (e.g. ESI).', 'rating': '1: strong reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BkeZySSxlE	S1gIl-i2mN	MIDL.io/2019/Conference/-/Paper101/Official_Review	[]	1		['everyone']	BkeZySSxlE	['MIDL.io/2019/Conference/Paper101/AnonReviewer2']	1548689645548		1548856701828	['MIDL.io/2019/Conference/Paper101/AnonReviewer2']
333	1548691176705	"{'pros': '- The problem is highly significant\n\n- The paper is well written\n\n- Great contribution to the field of multi task learning. Mathematically grounded an elegant', 'cons': '- As recognized by the authors, the Dice metric is sensitive to the size of the structures evaluated. It was maybe not the most appropriate\n\n- The authors should consider evaluating the number of detected lesions  (together with positive predictive/negative value). While this seems ""much easier"" than the full extend of lesions, this is already a very useful information for clinical applications.\n\n- Why using a different class for brain stem? In some pathologies physicians are looking for brain stem lesions. Can lesions be in two different ""tissue"" classes?', 'rating': '4: strong accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'special_issue': ['Special Issue Recommendation'], 'oral_presentation': ['Consider for oral presentation']}"		HJeZW_QxxN	H1ebewsnmV	MIDL.io/2019/Conference/-/Paper87/Official_Review	[]	2		['everyone']	HJeZW_QxxN	['MIDL.io/2019/Conference/Paper87/AnonReviewer2']	1548691176705		1548856701615	['MIDL.io/2019/Conference/Paper87/AnonReviewer2']
334	1548691446309	"{'pros': ""1. Work proposes using attention modules after various layers in a CNN to predict the severity of the knee osteoarthiritis (OA). \n\n2. The paper's text is well-written. Although the 'attention module' by itself is not a novel contribution (convolutions followed by activation has existed in literature before [1]), the combination of attention blocks from multiple resolutions using a multi-loss paradigm is novel.\n\n3. A test of the module's adaptability to various architectures is interesting  \n\n[1] Oktay et al. 'Attention U-Net:Learning Where to Look for the Pancreas'. In: MIDL 2018"", 'cons': '1. Conclusions look very heuristic and the authors do not try to explain them. Ex: In table2 (Early fusion), why does att2 not feature in ResNet and VGG, but features in Anthony et al.s\' version? Based on the architecture details, this might have something to do with the resolutions attended by these layers.\n\n2. Results and Table 2 are presented in an unclear fashion and a redesign is strongly suggested. Ex: in pg. 7, text below fig. 3: Text says ""Best performance achieved with attention branches att0 and att1..."". However, Multi-Loss section in table 2 lists the performance numbers separately. Again, text in pg. 8 states ""... the VGG-16 attention branch att0, achieved the best classification performance..."". Is there no optimal combination of attention branches in Multi-loss?\n\n3. Captions of figures and tables provide little information. Similarly, the take away from the captions of loss curves and activation maps in the appendices is not obvious.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		B1epyN8rlV	SJgAxOj3Q4	MIDL.io/2019/Conference/-/Paper157/Official_Review	[]	3		['everyone']	B1epyN8rlV	['MIDL.io/2019/Conference/Paper157/AnonReviewer2']	1548691446309		1548856701408	['MIDL.io/2019/Conference/Paper157/AnonReviewer2']
335	1548692313751	{'pros': 'This work introduces a new CNN architecture to perform joint learning of segmentation and registration tasks for 3D prostate MR images. The authors formulate both tasks using an architecture similar to that of U-Net, but incorporating an intermediate layer with 3 feature maps at multiple resolution levels which are interpreted as X,Y,Z components of the deformation field. These feature maps are concatenated with scaled versions of the moving image segmentation mask before entering the decoder part of the model. Since the method follows a supervised approach to train the registration network (i.e. ground truth deformation fields are required), synthetic deformations were used to generate the training samples. The method was trained/validated on 50 clinical scans, tested on 38 volunteer MR scans and compared with a registration-only network. Results suggest that incorporating the propagation network boosts performance by a significant margin.\n\nThe paper is well written, technically sound and the details provided in the manuscript seem to be enough to implement a similar architecture. Incorporating image registration into a segmentation network makes sense to improve the quality of the image segmentation task. Would it also help in the opposite direction, i.e. would segmentation help to improve the quality of the estimated deformation fields? I would like the authors to briefly discuss this point.\n\n', 'cons': '- The images used for training and validation seem to have a pretty large overlapping between the structures of interest even before registration. I would have liked to see if incorporating the registration module still helps when the initial overlapping is lower and the registration task becomes much harder. This would be something interesting to explore in future works.\n\n- The paper was trained and tested with an in-house dataset. Comparison with other segmentation methods on publicly available dataset (specially in multi-class anatomical segmentation scenarios like many of the challenges available at https://grand-challenge.org/challenges/ ) would better highlight the advantages of the proposed propagation network.\n\n- The idea of jointly learning segmentation and registration tasks using CNNs is not new, and has been explored in previous works like https://arxiv.org/pdf/1806.04066.pdf Differences with this approach should be discussed in the introduction.\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		B1evn4blx4	S1xGPoo3mV	MIDL.io/2019/Conference/-/Paper62/Official_Review	[]	2		['everyone']	B1evn4blx4	['MIDL.io/2019/Conference/Paper62/AnonReviewer1']	1548692313751		1548856701192	['MIDL.io/2019/Conference/Paper62/AnonReviewer1']
336	1548692550221	{'pros': '1. This paper addresses an important and challenging application: segmenting cortical lesions on MRI of multiple sclerosis patients. It addresses an important clinical application topic.\n\n2. The paper is clear.\n\n3. If extended with validation to prove impact and significant this can be a good fit for an application journal or conference.', 'cons': '1. There is no methodological contribution in terms of deep learning. It is focused on simple application of 3D Unet with drop-out and cross entropy loss to image data. Techniques are not original.\n\n2. The method has only been compared with 3D Unet without dropout. Dropout has consistently shown to be important and is routinely used with fully convolutional networks, so the comparison is not justified and does not carry any additional value. This simple comparison and the reported results do not make a solid validation that supports the impact os significance of the work. The claims of the paper remain largely unverified.\n\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		SkxkTcMex4	BJlABhj27V	MIDL.io/2019/Conference/-/Paper80/Official_Review	[]	1		['everyone']	SkxkTcMex4	['MIDL.io/2019/Conference/Paper80/AnonReviewer1']	1548692550221		1548856700976	['MIDL.io/2019/Conference/Paper80/AnonReviewer1']
337	1548692628329	{'pros': 'his paper addresses the issue of MR map reconstruction for magnetic resonance fingerprinting. The authors proposed a CNN based strategy to reduce the time required for reconstruction of such images. \n\nThe paper includes an extended state of the art review. One of the open issues highlighted is the limited amount of data available for this type of studies. In this study a dataset of 95 scans is included. \n \nThe proposed architecture is compared with two other deep learning architectures.', 'cons': 'Was experimentation performed before the proposed CNN architecture was defined?  Were other optimizers and optimizer hyperparameters evaluated?\n\nMeasures utilized for evaluation. It would be beneficial to see what are the effects of such methods in the texture of the reconstructed maps. For instance homogeneity.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		HyeuSq9ke4	Bke353snm4	MIDL.io/2019/Conference/-/Paper37/Official_Review	[]	3		['everyone']	HyeuSq9ke4	['MIDL.io/2019/Conference/Paper37/AnonReviewer2']	1548692628329		1548856700759	['MIDL.io/2019/Conference/Paper37/AnonReviewer2']
338	1548692733089	{'pros': 'The paper is proposing a segmentation method for eye tumor segmentation from MRI. The proposed approach leverages CNN based architectures to create activation maps that subsequently refined the the use of ASM and CRF in order to create training data that subsequently were used as input to a UNET architecture. \nThe paper includes an extended state of the art review. \n\nA weakly supervised approach is implemented.', 'cons': 'The dataset is limited. \n\nThe authors should quantify the effect of the ASM and CRF steps to the final segmentation outcome. \n\nThe False positive and True positive fractions should also be reported. Reporting Hausdorff distance should also be considered. \n\nLimited discussion/conclusion section. The authors should extend the section to compare the methodology proposed with ones existing in the literature and further analyze the technical innovations that make this approach superior to already proposed ones. \n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		ByldG19Ry4	BJerbpo3X4	MIDL.io/2019/Conference/-/Paper16/Official_Review	[]	3		['everyone']	ByldG19Ry4	['MIDL.io/2019/Conference/Paper16/AnonReviewer2']	1548692733089		1548856700549	['MIDL.io/2019/Conference/Paper16/AnonReviewer2']
339	1548692827733	"{'pros': 'A semi-supervised approach was proposed for training a convolutional neural network for automatic cortical sulci segmentation. The benefit of pretraining and regularization was shown. ', 'cons': 'The training and evaluation approach is unclear. First it is stated that a leave-one-subject-out cross-validation scheme is used. Then a propagation of the training ground-truth labels to 10 other brains via Voronoi-diagram is mentioned for measuring error rates. The propagation method itself is insufficiently described. What errors are introduced by this propagation? And why is this propagation needed for evaluation instead of evaluation the performance on the manual labels from the test data?\n\nThe method was only compared to the BrainVISA model, which is suboptimal, while stating in the conclusion ""shows the power of the CNNs compared to the methods developed so far"". The authors have previously proposed two other methods (Borne et al., 2018, Perrot et al., 2011), which performed better than BrainVISA. As the dataset was changed, performance cannot directly be compared with these previous methods. For the reader to know the benefit of the currently proposed method, authors should include a comparison on the same dataset with the state-of-the-art method (the better one of Borne et al., 2018 and Perrot et al., 2011).\n\nAll used parameters (e.g. number of neighbours, BrainVISA configuration) need to be included to allow reproduction of the method.\n\nThe evaluated method names should be stated in the text of 4.1 and in the methods section to ensure readers can follow what is compared. Why are the p-values when comparing BrainVISA to CNN+pretrain+reg (3% difference) larger or similar than when comparing BrainVISA to CNN (1% difference)? Also the p-values are very small for a difference of 1% and 62 test subjects.\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BkeZySSxlE	HyxEPpj2X4	MIDL.io/2019/Conference/-/Paper101/Official_Review	[]	2		['everyone']	BkeZySSxlE	['MIDL.io/2019/Conference/Paper101/AnonReviewer3']	1548692827733		1548856700334	['MIDL.io/2019/Conference/Paper101/AnonReviewer3']
340	1548693506630	{'pros': 'The paper is well written and describes an interesting and relatively novel approach to solving multi-class classification in a clinical domain where overlap between classes is frequently a possibility.   The approach is clearly explained and the results presented are sufficient to give merit to the idea. ', 'cons': 'The authors could spend a little more effort on explaining the intuition behind conditional versus unconditional labels and the advantages of each.\nOnly a single (large) dataset is used, while there are many publicly available datasets that could be included for additional experiments. \nNo public implementation of the method is provided, which would be a nice extra', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'oral_presentation': ['Consider for oral presentation']}		SJgNCUbke4	SyeiWe327E	MIDL.io/2019/Conference/-/Paper29/Official_Review	[]	3		['everyone']	SJgNCUbke4	['MIDL.io/2019/Conference/Paper29/AnonReviewer1']	1548693506630		1548856700122	['MIDL.io/2019/Conference/Paper29/AnonReviewer1']
341	1548693868412	"{'pros': '1. The use of two networks and triplet embedding loss to weight healthy samples is innovative and original and is well justified to address the challenge in segmenting tiny microaneurysms.\n\n2. The comparison of test metrics to IDRiD results imply that the technique could rank 4th, however, this is just an indirect unofficial inference and cannot be certainly stated as the challenge is closed.', 'cons': '1. The technique was only compared to ""vanilla fully convolutional network,"" presumably with cross entropy loss. For imbalanced data and segmentation of tiny microstructures like microaneurysms, a comparison to weighted cross entropy, focal loss, and other techniques for sample reweighting or loss functions was expected to make it a solid validation.\n\n2. The contribution of different parts of the network and the loss function could be compared and illustrated more clearly.\n\n3. The paper is overall clearly written but the presentation could be much improved. For example the details of the techniques and their contributions to results could be better explained in the text. Figure 3 could be improved for better visualization. The caption of Table 2 could explain the results. Figure 4 does not have a caption!\n\n4. The technique still seems to rank 4th in the challenge. Better validation could improve the impact and significance of the work.', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		BJlt-XMlgN	B1gNObnh74	MIDL.io/2019/Conference/-/Paper73/Official_Review	[]	3		['everyone']	BJlt-XMlgN	['MIDL.io/2019/Conference/Paper73/AnonReviewer1']	1548693868412		1548856699911	['MIDL.io/2019/Conference/Paper73/AnonReviewer1']
342	1548693910084	"{'pros': 'Summary: \n\nIn this work, a patch based approach to obtaining trees from image data is presented. Neural networks are used to train patch based predictors that predict nodes, which are then used to successively build trees of interest. Different neural networks are evaluated on synthetically generated data and U-net based patch predictor is finalised. This model is then evaluated on DRIVE data, comprising colour retinal images. Further, an updated U-net based regressor is used to predict vessel width. The preliminary evaluation presented is inconclusive as no relevant comparing methods are presented.\n\nPros:\n- The primary motivation of the work is interesting: to go directly from images to trees instead of binary mask based segmentation\n- Use of neural networks to predict possible nodes in trees\n- The visualisation related work in Section 4 can be interesting. Perhaps it warrants a stand-alone short paper submission, as it does not blend fluently with the rest of the paper.\n', 'cons': '- With the larger objective of going from image-to-tree, the presented evaluations appear incomplete. For instance, the evaluation metrics are computed on ""binary masks of tree generated"" (Sec 2.2). This would, in my opinion, contradict the primary objective of bypassing the binary segmentation step. While not straightforward, there are works on tree-space statistics that can be used to perform evaluations directly on trees (for example in [1]). This will considerably strengthen the work by aligning it with its primary objective. \n\n- In choosing the neural networks, it is mentioned that the sequence-less models work better than the sequence-based ones, without a discussion. One would hope that in a recurrent setting, there is more information for making improved node predictions. So, it is surprising.\n\n- This brings me to my next question: Instead of using sequence-less neural networks to predict individual nodes on small patches, why not train the networks like U-net to predict all possible nodes on the entire image? \n\n- Section 3.1 is ambiguous. It describes three levels of vascular tree construction in ""increasing order of difficulty"". Do the authors see each of these tasks as going from image-to-tree? Because the evaluations in Figure 6 seem to indicate this. Obtaining trees from retinal images is what is most interesting, and this seemed to be the motivation presented earlier. Given a segmentation map, obtaining a skeleton and then a tree from it is not as interesting. As a result, the comparisons presented in Figure 6 do not tell much. It is not surprising that the skeleton-to-tree is better, as the segmentation task is already solved.\n\n[1] https://di.ku.dk/forskning/Publikationer/tekniske_rapporter/2011/techrep_trees_Aasa_Feragen.pdf', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		B1geHYXgx4	BkeA9-22mV	MIDL.io/2019/Conference/-/Paper88/Official_Review	[]	3		['everyone']	B1geHYXgx4	['MIDL.io/2019/Conference/Paper88/AnonReviewer3']	1548693910084		1548856699690	['MIDL.io/2019/Conference/Paper88/AnonReviewer3']
343	1548693931247	{'pros': 'This paper is well-written and easy to follow. The objective of this paper is to segment seven cardiac structures in NCCT by using virtual non-contrast CT with reference segmentation from CCTA. \nThe authors developed automatic segmentation when manual reference is hardly feasible. This is clinically important as many patients undergo NCCT, but only some of them do further CCTA scanning. To bridge the gap between CCTA and NCCT, the authors trained the model on virtual non-contrast images which are similar to NCCT.\n', 'cons': '1. The authors adopted 3D FCN for sufficient contextual information within axial slices and smooth prediction across axial slices. However, it seems that this method is not robust to step-and-shoot artifact (or, possibly motion artifact) as shown in Figure 4. Have the authors compared with 2D FCN?\n2. There is a study of the variability of CT scanners in measurement of plaque volume (Radiology, Dec. 2016, Vol. 281:3). Maybe the difference of the volumetric measure is partly due to the different scanner, as well as the different modality as mentioned in Discussion. Also, the authors need to test with images acquired from the different scanners from other vendors for generalization.\n3. The DSC is relatively low for LV. This needs to be improved.\n3. There is no comparison results with other methods.  \n4. The authors mentioned six of the 15 cases in which the segmentation failed due to anatomical abnormality. What other reasons are for the remaining cases?\n\nSome minor comments:\n1. Please insert color legend in Figure 1.\n2. Please add some space between Table 1 and Figure 3.\n3, In Figure 4, the colors for LV cavity and AA are too similar. I would suggest to change the color. \n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		BJxgkz-xeV	Hye73W2h7N	MIDL.io/2019/Conference/-/Paper58/Official_Review	[]	3		['everyone']	BJxgkz-xeV	['MIDL.io/2019/Conference/Paper58/AnonReviewer3']	1548693931247		1548856699477	['MIDL.io/2019/Conference/Paper58/AnonReviewer3']
344	1548695869911	"{'pros': 'This work proposes an image-to-image translation approach for improving lesion segmentation, in the scenario when time and cost limitations allow for acquisition of only CT perfusion images. Overall, the article is well-motivated and clearly written.', 'cons': ""1. This work applies a known technique (image-to-image translation using paired training data) to a new problem (CT perfusion to MRI for lesion segmentation), but the experiments demonstrate only marginal improvement, unfortunately. Thus, despite being a promising start, I believe that this work is not ready for publication at this stage.\n\n2. As the mean values of all metric show only marginal improvements and qualitative results in Fig. 3 show some samples with much better results for the FCN-CGAN, it would seem that there are other cases for which segmentation with the FCN performs much better. Is this the case? If so, it would make sense to show some examples of this type and mention this as a limitation. Also, in this case, the sentence 'The results show that, in general, the FCN-CGAN model results in predictions that cover more of the ischemic core region...' might be misleading.\n\n3. A benchmark experiment, where CT perfusion and real MR images are used for segmentation, should be added.\n\nMinor:\n4. As the methods have been compared with several metrics, a discussion about how the different metrics compare with one another might be suitable.\n\n5. The related work section requires some restructuring, in my opinion. Perhaps, the authors could consider a higher level of abstraction such as 'image-to-image translation for downstream tasks', 'image-to-image translation for data augmentation', etc. Also, details about some of the mentioned works that are perhaps irrelevant for the proposed work (e.g. 'Heavier weighting of the L1 loss around the border...') could be skipped.\n\n6. A suggestion to the authors: perhaps optimizing the MR image generation such that it leads to good segmentation results might lead to improved segmentation results?\n\n7. Do the authors mean to number Sec 4.4 and Sec 4.5 as Sec 4.3.1 and Sec 4.3.2 respectively?"", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		HyemZQzExE	rJl8HF3nXN	MIDL.io/2019/Conference/-/Paper144/Official_Review	[]	2		['everyone']	HyemZQzExE	['MIDL.io/2019/Conference/Paper144/AnonReviewer1']	1548695869911		1548856699260	['MIDL.io/2019/Conference/Paper144/AnonReviewer1']
345	1548697633113	"{'pros': 'The article is well written and describes a complex idea with reasonable clarity. \nThe topic is very relevant (multi modal registration using deep neural networks) and the idea is novel.\n\nThe idea is interesting but requires quite some further development and improved experimentation to prove its merit.  I would suggest provisional acceptance in this case. ', 'cons': 'The authors do not mention from the outset that their method requires (in this case) organ segmentations to guide the registration process.  This is a key fact and should be mentioned from the abstract onwards. \nThe experiments carried out are minimal and constitute little more than a ""proof of concept"" as stated by the authors themselves.\nThe method is compared with an alternative which does not make any use of organ segmentations to achieve the registration results.  Any gain in accuracy should be considered in this context. ', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		SkeI0-QelE	ryet7gT2XN	MIDL.io/2019/Conference/-/Paper84/Official_Review	[]	2		['everyone']	SkeI0-QelE	['MIDL.io/2019/Conference/Paper84/AnonReviewer2']	1548697633113		1548856699045	['MIDL.io/2019/Conference/Paper84/AnonReviewer2']
346	1548698307672	{'pros': 'The paper proposed an attention or saliency map-based method to prevent the degradation of convolutional neural network (CNN) performance on base tasks, when it is incrementally trained further for new tasks. The degradation in performance is quantify as change in saliency map obtained before and after the incremental training. This change is represented in terms of 2 measures namely overall saliency quality (OSQ) and KL divergence. OSQ is the summed smallest sufficient region over a n by n grid over the input image and KL divergence quantifies the shift in the distribution of attention caused by new parameters in incremental training. The authors proposed, to explicitly retain some examples of base tasks in incremental training of new tasks so that the network can preserve the old knowledge. They proposed 2 strategies to choose these retaining examples, first is to choose examples which are closer to each class, average saliency map (averaged representation distance selection ARDS) and second is to choose examples based on class confidence probabilities and inter-sample saliency map distance (distributed exemplar distance selection DEDS). Finally they perform incremental training by optimizing a loss function which is sum of 3 losses 1) cross entropy classification loss, 2) distillation loss over the new and retained examples compared with only the retained examples and 3) is the regularization term which penalizes the shift in Frobenius norm over the parameters as compared to base model. They demonstrated their proposed strategy on fetal echocardiography dataset, where the base task was to classify 6 fetal cardiac structures namely ventricle walls, valves, left atrium, right ventricles, pulmonary artery, and aorta. They first trained a base model to classify these 6 structures and then incrementally train their base model, to classify 3 new structures namely Foramen ovale, right atrium, and superior vena cava. \n\n•\tThe proposed method combines the different continual learning strategies to present a method of incremental learning for medical images.\n\n', 'cons': '\n•\tThe authors failed to highlight the novelty of their approach.\n•\tThe proposed method’s success heavy depends on the quality of the saliency map produced. It is more challenging to produce good saliency maps for medical images as compared to natural images. An analysis of how well the input saliency maps are, is missing. \n•\t As mentioned in the discussion section, a class level analysis of the results would help the readers to better evaluate the proposed algorithm. In medical images, usually the different classification tasks vary a lot in terms of difficulty. A small structure like aorta or superior vena cava may be are much more difficult to classify as compared to right ventricle. It is difficult to judge the method from only 2 numbers calculated over all the classes (map quality and percentage accuracy)\n•\tThe degradation of the saliency map is a continuous process, spread over epochs of learning. A graph showing the retention of information over old classes and learning of new information over new classes over training epochs is missing.\n•\tAn analysis of how to choose number of examples to retain for each base class is missing. \n•\tAn analysis of how to choose the base and incremental task is missing. The authors used 6 structures for the base task, but the motivation behind choosing the specific structures as base or incremental task is missing. \n•\tIn Table 1, it’s not clear if the present and past accuracy is reported on the same set of images. The overall accuracy is not sufficient to understand the table. The past and present accuracy of each of the old classes is required to compare the before and after scenario.\n\nMinor\n•\tThe Table 1 is incorrectly labeled as Figure 3.\n•\tThere are formatting issues throughout the manuscript where adequate spacing is not provided between consecutive sentences. \n•\tThe method section sometimes contained the literature review, which is distracting for the readers.\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		S1gyUgN7eE	rJe36Ga3XV	MIDL.io/2019/Conference/-/Paper142/Official_Review	[]	3		['everyone']	S1gyUgN7eE	['MIDL.io/2019/Conference/Paper142/AnonReviewer3']	1548698307672		1548856698828	['MIDL.io/2019/Conference/Paper142/AnonReviewer3']
347	1548689508113	{'pros': 'The paper proposes to improve the Unet architecture for robotic instrument segmentation, which embeds the batch normalization technique in the network and uses Nearest-neighbor interpolation to increase the feature size. Extensive experiments have been conducted to verify the effectiveness of proposed framework.', 'cons': '- The main weakness of the paper is that there is a lack of novelty in the methodology. The proposed methods for increasing the result accuracy are quite common and well-established techniques, such as batch normalization strategies and Nearest-neighbor interpolation. The contribution is introducing and integrating these techniques in the network, instead of modifying according to the characteristic of tasks.\n- Figure 1 may be uploaded wrongly, where colors are not shown correctly as the caption described.\n- There are few related works about robotic instrument segmentation which are described and compared in the paper. It seems that only one paper of this direction is mentioned in the related work section.\n- The logicality and organization of this paper need to be further improved. For example, it is better to describe the Performance Metrics and Attention study in Result section. \n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		SkxQ6fjlx4	HJgnwxs37V	MIDL.io/2019/Conference/-/Paper128/Official_Review	[]	3		['everyone']	SkxQ6fjlx4	['MIDL.io/2019/Conference/Paper128/AnonReviewer1']	1548689508113		1548856698575	['MIDL.io/2019/Conference/Paper128/AnonReviewer1']
348	1548699710291	"{'pros': 'The paper proposes to solve data-imbalance problem which is one of the most fundamental problems in medical image analysis.\nIt demonstrates good results with seemingly easy-to-reproduce method.\n\nPatch-based synthesizing following with blending to make a whole-image is also a creative approach suitable for the problem and data described.', 'cons': 'Some key definitions are not very clear \n\n- What is ""translation""? It seems to be a key concept of the paper, though it relies on references in other field for the readers to fully understand. Table 1 shows datasets for ""classification"" and ""translation"" task, but it\'s not quite clear what each are, what are the findings from it.\n\n- It would be more beneficial to describe and study the details of data augmentation using synthetic images. \n-- What were the initial data distribution and how were the synthetic images created to make the dataset balanced? \n-- In what case does it work well and which case doesn\'t?\n\n- Transfer learning part is too short, not clear to understand how it\'s performed to help solve the problem.', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		rkxOKaj0yV	BygUBOp3Q4	MIDL.io/2019/Conference/-/Paper21/Official_Review	[]	3		['everyone']	rkxOKaj0yV	['MIDL.io/2019/Conference/Paper21/AnonReviewer2']	1548699710291		1548856698361	['MIDL.io/2019/Conference/Paper21/AnonReviewer2']
349	1548700316563	{'pros': '- This paper presents a GAN-based network which can transform surgical instruments while preserve the background information, for enlarging the training database and alleviating data imbalance problem. \n- Experiments have been conducted to verify the effectiveness of proposed framework.\n- The paper is well written and easy to follow. \n', 'cons': '- The main weakness of the paper is the lack of novelty in the methodology. The proposed methods for improving network are quite common and well-established techniques, such as domain adversarial loss, self attention and cycle loss. The novelty perhaps lies in this particular way of using the elements together and not on the elements themselves. \n- One concern of this paper is the question formulation: transforming the instruments while preserving the background to enlarge database. According to the regulation of surgery procedure, surgeons are requested to perform specified operations with corresponding sets of instruments for different surgery phases. Therefore, some information of background is important for the instrument recognition. While, this method may lead to some abnormal scenes which cannot happen in the real surgery and therefore may confuse the recognition network learning. For example, if the transformation of the cadiere to bipolar happens in the packaging stage, it is unreasonable since the bipolar hardly appears in that stage. It is interesting to see how to deal with this issue.\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		rJxMvRFxeN	BkgBscphmE	MIDL.io/2019/Conference/-/Paper124/Official_Review	[]	1		['everyone']	rJxMvRFxeN	['MIDL.io/2019/Conference/Paper124/AnonReviewer3']	1548700316563		1548856698148	['MIDL.io/2019/Conference/Paper124/AnonReviewer3']
350	1548700564260	{'pros': 'This paper studies two aspects of computer-aided detection (CAD) or patch classification based microcalcification detection in mammography: different network models/architectures with various depths; and transferring learning from imagenet to a bridging dataset, to the final breast mammography dataset.\n\nThis is a valid and important research topic in the CAD domain. The study is relatively comprehensive.', 'cons': '-, The main issue of this paper is that this is by far a very empirical study, not novel or principled sufficiently to be accepted. For example, from Figure 3 and Table 2, it is hardly to observe anything generalizable or beneficial for later studies. These experiments basically tell that you have to try different parameter settings to optimize/tune the performance on training and playing your luck on the validation data (more or less).\n\n-, the studied topics are very relevant issues for CAD on significant clinical findings, such as potential cancer sites, etc. However it is not clearly observable the larger difference from previous papers in this nature of study, such as authors have reviewed in this submission. Focal loss also has already been used for CAD problems quite actively.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		B1luMUUxgN	Bkl2qjT3Q4	MIDL.io/2019/Conference/-/Paper107/Official_Review	[]	2		['everyone']	B1luMUUxgN	['MIDL.io/2019/Conference/Paper107/AnonReviewer2']	1548700564260		1548856697934	['MIDL.io/2019/Conference/Paper107/AnonReviewer2']
351	1548701004927	"{'pros': 'A simulation model is proposed which generates ground truth data for the task of cell-nuclei detection in immunohistogloy.\n\nThe paper presents and interesting approach which is valuable for medical imaging applications where training data is scarce.\n\n', 'cons': 'The motivation, methods, and evaluation are not clear and difficult to follow.\n\nThe main methodological contribution is the transform consistency loss, however, the effect of this additional loss term was not properly evaluated.\n\nThe cell-nuclei detection model is not really fully unsupervised training. While the model is training using synthetic data, it is still trained with supervision of the synthetic labels. \n\nFigure 1 caption: should lower row be 4th column?\n\nPage 3 states a corresponding ground-truth is generated by placing a small disk at the exact nuke positions. Does the disk undergo the same random transform that was used for generating the image?\n\nPage 3 states Cell boundaries are denoted as the morphological gradient of the simulated cells. Why is this done? Isn\'t the cell boundary known since it was artificially generated?\n \nPage 4, 4th equation: should the first f_R2S be f_S2R?\n\nIt is not clear what the inverse one against-all validation is. Is only one sample used for training and evaluation on the remaining? If so, the effect of dataset sizes only evaluated on two sizes? \n\nFirst paragraph of results and Figure 2 discuss the ""unsupervised scenario"". How is this unsupervised? A detection model is still trained with the simulated training data labels which makes it supervised. Simulated vs real data is not anagulous to unsupervised vs supervised. The GAN-based data generation is unsupervised, but the detection model trained is supervised. It should be clarified that Figure 2 shows results of the detection model training on purely synthetic data.\nThere are many typos, see below for some examples:\n\nDifference between Figure 2 and Figure 3 results should be clarified. \n\nReasoning for the decrease in performance when using GAN on LOOCV scenario is not clear.\n\nIntroduction, first sentence: tasks should be task\n\nIntroduction, paragraph 1, sentence 2: remove hyphen after cell lineage-\n\nIntroduction, paragraph 1, last sentence: tsks should be tasks\n\nSpace after Table 1 reference in text on page 3', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		B1en70uU1V	S1erUTa2QE	MIDL.io/2019/Conference/-/Paper3/Official_Review	[]	3		['everyone']	B1en70uU1V	['MIDL.io/2019/Conference/Paper3/AnonReviewer1']	1548701004927		1548856697717	['MIDL.io/2019/Conference/Paper3/AnonReviewer1']
352	1548701132981	{'pros': '\nThe paper is well-written and easy to follow.\n\nThe experiment is well described.\n', 'cons': '\nThe LiTS challenge dataset and labels are mainly for the purpose of Liver Tumor Segmentation. The golden standard for liver labels are only rough segmentation and therefore not very accurate for this purpose.\n\nThe choices of WCE and Tversky are not very appealing when there are more recent loss functions with state-of-the-art results like Generalized Dice Loss (GDL) or Focal loss especially being applied to unbalanced datasets. Moreover, Tversky seems to balance between precision and recall already, thus not always getting the best dice scores and not a valid choice at all in this comparison.\n\nThe ratio (r = 0.5) seems to have worked for 2D liver slices, however is the ratio generalizable to much smaller or even larger organs where the data imbalance is different per slice? In other words, the authors have not taken into account the ratio of liver in each 2D slice and only considered whether there is part of the liver in each slice or not. This is even more important when using patches instead of whole images.\n\nThe meaning of random sampling is relative to the dataset. In datasets with equal number of positive/negative labels random sampling will be equal to 50-50 ratio, whereas in other dataset it may be different. Therefore, the comparison of ratio (r = 0.5) to random sampling should only be considered when you take into account the exact ratio of liver vs non-liver slices in your dataset.\n\nAuthors used a fixed number of positive (liver) slices, while determining the rest (background) based on the ratio (1 - r). Given that, whether the ratio (r) is 0.2 or 0.8, the number of background slices when 0.8 could be four times larger than 0.2. This makes that the comparison completely invalid since the total number of patches used for training would not have been even nearly the same.\n\nThe contribution of this paper is quite unclear. The network architecture have already been used in previous literature as well as the loss functions. And although the comparison of the ratios of training images have already been investigated before, the comparisons in this paper are invalid because of the reasons mentioned above. The results are not compared to other liver segmentation methods either, especially since the dataset belongs to a public challenge.\n', 'rating': '1: strong reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		BkeeIavee4	BkgBATpnXN	MIDL.io/2019/Conference/-/Paper117/Official_Review	[]	2		['everyone']	BkeeIavee4	['MIDL.io/2019/Conference/Paper117/AnonReviewer1']	1548701132981		1548856697503	['MIDL.io/2019/Conference/Paper117/AnonReviewer1']
353	1548701585216	"{'pros': '- Examination of multi-parametric MRI generation using GAN\n- Thorough evaluation of generated samples, including by human experts\n', 'cons': '- Lack of technical novelty\n-- It is usually easy to generate synthetic images of 64x64 size, even with basic GAN settings. It is harder to generate larger images and more so to generate images with diversity or heterogeneity. Those aspects were not considered in the paper.\n-- Medical images of 64x64 size may have limited utility in practice.\n-- We need to consider how to put the generated samples back to the whole image for a useful application, and evaluate.\n--  In conclusion, the authors mention ""The synthesized images are diverse and realistic to a degree that they may help for training a classifier in the future."" - the degree of diversity probably needs to be evaluated, or usefulness for training a classifier.\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		BJlpmkJxl4	B1ltcy0hmN	MIDL.io/2019/Conference/-/Paper48/Official_Review	[]	3		['everyone']	BJlpmkJxl4	['MIDL.io/2019/Conference/Paper48/AnonReviewer2']	1548701585216		1548856697287	['MIDL.io/2019/Conference/Paper48/AnonReviewer2']
354	1548701614818	"{'pros': 'In this paper, Zhang and colleagues proposed a deep learning network architecture able to localize abnormalities in medical images by means of an interesting adversarial approach: combining together an encoder-decoder, a convolutional classifier and a discriminator network as in GAN, they are able to enforce the training to extract as many features as possible.\nOverall, I think that the paper is well written, provides the appropriate review of previous approaches, and offers results from both real and synthetic data. I particularly appreciated the comparison with the CAM and Grad-CAM architectures.', 'cons': 'Although the overall idea is interesting and the architecture is more than reasonable, I believe this paper presents two major issues and several minor ones.\nThe major issues are related to the quantitative validation on the real and synthetic data. The authors have stressed several times that precise localization and segmentation of abnormalities in retinal images are almost impossible for humans. Nevertheless, they do not mention any other possible gold standard, and they proposed to use an additional classifier, trained on labels assigned by humans, to evaluate the performances of their architecture in terms of removing abnormalities and generating the related fake healthy images. This looks to me as circular reasoning and although it is reasonable to say that the architecture is identifying and removing abnormal regions, it does not give any reassurance on the fact that all (or as many as possible) the abnormalities are being detected. This is problematic since it is the most interesting goal of the paper together with the one of offering precise localization. I agree that it is hard to validate this on real data, but I do not think the current choice is appropriate.\nThe synthetic data are more suitable to try to reason about this aspect. However, in order to introduce abnormalities in their dataset, the authors used thumbnail images from ImageNet. Although the very small ones can look as just black spots, I do not think this approach is fair, since those thumbnails look so artificial (fig.2, far right) that they offer completely unrealistic data, misleading the validation results. I would suggest to either train a cycleGAN in order to generate abnormal pictures from a normal ones or to manually introduce distortions and/or smoothed dark spots able to resemble reasonable abnormalities for this specific dataset.\n\nOther minor issues:\n- The authors use the term ""biomarker"" with a very peculiar meaning of ""abnormal visual indicator"", while a biomarker is defined as a distinctive feature that can highlight pathological processes but not necessarily be abnormal; I would advise to either find a better word or rephrase where possible.\n- The authors stress a lot on these push and pull mechanisms, but it is not clear to me how are they different from the adversarial perspective used in GANs: at least in the current version of the paper, push-pull is just a way to indicate the generator-discriminator contrast.\n- The paragraph 3.2 reports qualitative results, it is worth to mention that in the current title of the paragraph.\n- The authors should provide some additional details on the retinal dataset.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		rJlyzeEBgN	rJxvh1C2mV	MIDL.io/2019/Conference/-/Paper151/Official_Review	[]	2		['everyone']	rJlyzeEBgN	['MIDL.io/2019/Conference/Paper151/AnonReviewer2']	1548701614818		1548856697070	['MIDL.io/2019/Conference/Paper151/AnonReviewer2']
355	1548701945232	"{'pros': ""(this was done as an emergency review, and won't be as detailed as it could be)\n\nThe paper is about cortical sulci segmentation, performed with CNNs. The problem and the specifics of this task are well explained and motivated.\n\nThe method is performed in several steps. First, a neural network is trained on annotated data (62 patients). Then, inference is run on 500 un-annotated patients. Those predictions are then used to train a new network, which is then fine-tuned on the original patients. Since BrainVISA is extensively used to either select the voxels to labels or to regularize the results, the process can not be called end-to-end.\n\nFor performances reasons, segmentation is not performed on the whole 3D volume, but on a list of voxels (with their neighbor patches) selected from BrainVISA. This divides the number of voxels to classify by 1000. The authors then used a modified LeNet for 3D to classify each voxel. "", 'cons': '«Despite the impressive results of deep learning models in computer vision, these techniques have difficulty achieving such high performance in medical imaging. »\nThis is a really bold statement to start a paper, one which is objectively wrong. This might indicate a lack of awareness of the state-of-the-art by the authors. If you refer only to this specific task, this should be updated to reflect that. \n\nI am concerned about the use of BrainVISA to select which voxels should be classified, as it introduces the bias of this imperfect tool into the training process. On top of that, even a trained network will need it as a pre-processing to perform inference, which is not ideal.\nI am not even convinced this is really needed, especially with such a lightweight network ; GPUs made great progresses in recent years in memory/parallel capabilities. Training time with only 62 patients is usually not really a concern, we are not dealing with the millions of images found in natural images datasets. I would like to see a baseline of an end-to-end trained 3D-CNN, and then compare your method to it.\n\nIt is mentioned that at each epoch, only 100 points are randomly selected per subject for training. Why ? Why not use all the data available ? Is this some weird kind of data augmentation ?\n\nThe cross-entropy is actually not a great loss function for unbalanced tasks, as least in his unweighted version. There is also some other works on specific losses for unbalanced tasks, such as:\n- Sudre, Carole H., et al. ""Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations."" Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. Springer, Cham, 2017. 240-248.\n- Milletari, Fausto, Nassir Navab, and Seyed-Ahmad Ahmadi. ""V-net: Fully convolutional neural networks for volumetric medical image segmentation."" 3D Vision (3DV), 2016 Fourth International Conference on. IEEE, 2016.\nV-Net could actually be a good baseline for this paper. \n\nThe strategy used for the semi-supervision is usually referred as proposals. In this case, the proposals are refined using BrainVISA. Some related works that might be interesting to acknowledge and maybe compare to:\n- Rajchl, Martin, et al. ""Deepcut: Object segmentation from bounding box annotations using convolutional neural networks."" IEEE transactions on medical imaging 36.2 (2017): 674-683.\n- Papandreou, George, et al. ""Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation."" Proceedings of the IEEE international conference on computer vision. 2015.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BkeZySSxlE	B1g-bZC27V	MIDL.io/2019/Conference/-/Paper101/Official_Review	[]	3		['everyone']	BkeZySSxlE	['MIDL.io/2019/Conference/Paper101/AnonReviewer4']	1548701945232		1548856696814	['MIDL.io/2019/Conference/Paper101/AnonReviewer4']
356	1548702920635	"{'pros': '- The paper tackles an importance and highly sought-after method of machine learning in medical image analysis - continual few-shot learning, that can improve over time.\n', 'cons': '- Overall, the study and writing seem to need more work\n-- Please check the layout of the paper for the conference proceedings.\n-- No ""Conclusion""\n\n- Lacking technical details\n-- The paper is hard to follow, and major part of the contribution is described only by explanations, which makes it hard to reproduce. Hard to reproduce results/methods has less impact on the field.\n\n- Not strong enough baseline\n-- Probably need to compare other stronger baseline for the situation of limited dataset, like transfer learning, data augmentation, etc.', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		BkeM-yMgeE	rylZA403mN	MIDL.io/2019/Conference/-/Paper68/Official_Review	[]	3		['everyone']	BkeM-yMgeE	['MIDL.io/2019/Conference/Paper68/AnonReviewer2']	1548702920635		1548856696563	['MIDL.io/2019/Conference/Paper68/AnonReviewer2']
357	1548703303051	{'pros': 'The authors propose to apply the 3D U-net for segmentation of cortical lesions in multiple sclerosis. The problem appears difficult, important and relatively unexplored in the literature. They propose therefore an interesting new application of u-net to a clinically relevant problem, and yield promising results. ', 'cons': '-\tMy main concern is the limited added value of this paper. The methodological contribution is very limited: a 3D U-net is engineered (using standard techniques) to optimise performance on the authors’ dataset. \n-\tExperimental comparisons are limited. Comparing the out of the box 3D U-net to a more engineered U-net offers limited interest. It would have been more interesting to offer a fair comparison to the state of the art method. Numbers are currently compared based on different databases and sample sizes, the conclusion claims (“These results show a clear improvement compared with a standard U-Net, and also with the only previously proposed method for an automatic cortical lesions detection”) are therefore not justified as direct comparison is not possible (this is mentioned in the results section then ignored in the conclusion). \nThe experimental validation would have also benefited from comparisons to SOTA WM detections, to provide context regarding the performance of the method (even though WM is not the main objective here).\nAuthors should also explain what LTPR/LFPR refers to.\n-\tThe results vs lesion size analysis doesn’t provide much added value, and essentially confirms that overlap based evaluation measures are sensitive to object size. \n-\tOverall, the main interest of the paper seems to be clinical, and I would therefore recommend to consider a more clinically oriented venue for this work.\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		SkxkTcMex4	HJxkULR3mE	MIDL.io/2019/Conference/-/Paper80/Official_Review	[]	2		['everyone']	SkxkTcMex4	['MIDL.io/2019/Conference/Paper80/AnonReviewer3']	1548703303051		1548856696351	['MIDL.io/2019/Conference/Paper80/AnonReviewer3']
358	1548703494626	{'pros': 'To classify MR vendor and MR field strength from brain images, the authors apply a pre-trained deep convolutional neural network (VGG-16 with the fully-connected layers removed) to generate feature maps of 2D MR. Three 2D orthogonal planes from each image volume were run through three VGG networks to produce three feature vectors that were concatenated together. These feature vectors were then fed into a classifier (either SVM or k-NN). The authors train their classifiers on a dataset consisting of images from 3 vendor and 2 magnet strengths, and then test their method through both cross-fold validation studies and on a test dataset (1 vendor and 1 field strength).\n\nThe paper addresses and interesting problem in medical imaging – the problem of inter-scanner variability, which has a tremendous impact on machine learning algorithm performance. While the results are intuitive (in retrospect), it is nice to see this problem being addressed.\n', 'cons': 'Enthusiasm for the paper is limited though in that a pre-trained deep neural network (VGG) was applied (the networks were not retrained at any point) and then classification was performed using standard classification methods (SVM and kNN). Considering MIDL’s focus on deep learning, I would have liked to see more methodological use of deep learning to address the classification.\n\nIn addition to comments below, my main criticism of the work is the motivation for using VGG as a feature extractor in the  first place. While the convolutional properties of the neural network might be beneficial, are they necessary? Does this task fail for simpler classification methods using standard image statistics? Or, why not just use a deep CNN classifier directly like VGG to classify the images? Why bother going through the expanded feature space (still 75,264 features large) and then performing an SVM or kNN? Some motivation along these lines would be helpful to fully appreciate the proposed methods. \n\n\n\nSec. 2.1: How was voxel spacing handled in the image pre-processing and resized output? Different imaging protocols may result in different voxel spacings for different sites/vendors, what were the original image spacings (in millimeters)? What was the spacing of the resulting output? Failure to account for image spacing could also lead to easy-to-classify results (your network would learn spacing characteristics from the anatomy).\n\n\nSec. 2.2: The method works by using 3 orthogonal 2D slices mid-brain. Are all the image volumes spatially normalized (registered) to some anatomical atlas space to ensure somewhat similar orientation of the head? If not, what happens in cases were the head is rotated and the orthogonal slices no longer align with the anatomical coordinate system? \n\nSec. 3: You performed classification in still a very large feature space (75,264 features). Were all those features really necessary? I would like to have seen some results/discussion as to why 3 2D planes were necessary. Maybe only a single plane would suffice?\n\n\nSec. 3: What value of k was eventually used for the kNN classifier?\n\n\nSec. 3.1: What was the result of SVM on the field strength data (how much worse than kNN)? And how did kNN perform on vendor?\n\n\nSec. 3: It is hard to claim full generalizability without really having testing data that spans the full range of categories. Testing was limited to a single vendor and field strength. While you mention abnormalities in the Conclusion, it is not hard to imagine that non-normal imaging would not work well with the classifier, so generalizability of the method can only be claimed for normal subjects at this point.\n\n\n\nMinor Comments:\n\nSec. 2.3: For the ten-fold cross validation, I’m guessing that you included 56 training examples and 6 testing samples from each category in the CC-359 in Table 1? Just like to clarify that training was equally sampled among the different categories.\n\n\nFig. 5: It might be more valuable to plot the training and testing results on the same plots, e.g. use circles for training and ‘+’ for testing while keeping colors for the different categories – move (c) into (a) and also (c) into (b). This way we could see how the GE 3T data fits with respect to the training clusters within the same plot.\n\nIt would also be interesting to see another set of plots showing the projection of the CC-359 test data onto the PCA space.\n\n\n\nGrammar/Typographical:\n\nSec. 1, p2: “at a different magnetic” -> “at different magnetic”\nSec. 1, p2: “must be consider” -> “must be considered”\nSec. 3.1, p6: “There results confirms” -> “These results confirm”\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		HylnnSQlgV	r1x1GvAhXE	MIDL.io/2019/Conference/-/Paper86/Official_Review	[]	3		['everyone']	HylnnSQlgV	['MIDL.io/2019/Conference/Paper86/AnonReviewer3']	1548703494626		1548856696135	['MIDL.io/2019/Conference/Paper86/AnonReviewer3']
359	1548703664041	"{'pros': 'Nice figures', 'cons': ""Not a scientific paper. No novelty claims. \nA DL algorithm was built to grade prostate cancer from RGB images and compared to other such algorithms. \nThe performance seems much better. The reason why it is so much better is unclear. There is no motivation for the proposed methodical changes. Why should it be better? There are no experiments that research the effect of such changes.  What is the effect of, for example, ''automatic semantic segmentation of cancer tissue images using DCGAN''. At least the authors could have done an experiment with and without this segmentation. Same for all the other so called ''contributions''. Suggest the authors read about https://en.wikipedia.org/wiki/Scientific_method.\n\nLot's of typos, old references. E.g. Lacks citing Deepmind https://arxiv.org/abs/1811.06497.\n"", 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		BJgXUQLBgV	SJxu3wRhXE	MIDL.io/2019/Conference/-/Paper156/Official_Review	[]	2		['everyone']	BJgXUQLBgV	['MIDL.io/2019/Conference/Paper156/AnonReviewer1']	1548703664041		1548856695875	['MIDL.io/2019/Conference/Paper156/AnonReviewer1']
360	1548703701239	{'pros': 'The authors propose a modification to Gated residual units named Picky Residual Units that explicitly chooses to execute one layer per residual block rather than making a decision at each layer. This simple modification is interesting and yields a strong performance on their test dataset that outperforms the state of the art.', 'cons': 'I have unfortunately several issues with the paper, my main concerns being the lack of clarity and lack of proper evaluation. \n-\tThe paper is overall poorly written, several explanations are unclear and required to check references. The abstract in particular reads as a few sentences have been patched together. I would suggest proof reading the paper more carefully.\n-\tWhile the proposed idea looks interesting, it is a relatively small modification and its advantage has not been proven. The paper would be a lot stronger with a thorough evaluation demonstrating the advantage of the PRU by comparison of performance to 1) a regular resnet, and 2) the gated residual units. The evaluation as it is doesn’t demonstrate the benefit of the novel methodology, and mainly suggests that a resnet backbone provides a good tool for segmentation.\n-\tThe conclusion claims of that the paper has demonstrated that the method reduces computational burden. Those claims are inaccurate as no experiments (e.g. run/training times) or complexity analysis have been provided to justify it.\n-\tThe  motivation is weak:  \nChoosing to use layer-wise adaptive method over spatially-adaptive methods because it has not been proven that spatially-adaptive methods are faster is hardly a justification.\nMotivation with respect to the application is non-existent. It is never mentioned why the topic is important, no state of the art review is provided and methods compared to are not described. This is particularly important considering this is the only evaluation to the method provided. \n-\tTable 1 is never referred in the main text, and results are not discussed. \n\nThe paper in its current state is incomplete, and it would be a lot stronger with an experimental evaluation that confirms the decisions made in the methods section. \n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		r1xJrU9llV	r1gp0PRn7V	MIDL.io/2019/Conference/-/Paper126/Official_Review	[]	3		['everyone']	r1xJrU9llV	['MIDL.io/2019/Conference/Paper126/AnonReviewer2']	1548703701239		1548856695654	['MIDL.io/2019/Conference/Paper126/AnonReviewer2']
361	1548703984272	"{'pros': 'This paper proposes a method to use a large amount of unlabeled dataset for the cortical sulci recognition.', 'cons': '1. It is not so clear whether the authors use the same architecture for the first pre-training and for the second fine-tuning network. If not, it should be clarified.\n2. In 2.2.3, the pre-training model is trained after only 15 epochs. Is it enough? Was this model already converged? Also, does “points” mean patches or voxels? It’s not clear.\n3. In 3.1., the authors mentioned that four additional sulci were used compared to the previous paper. In previous paper, 63 and 62 sulci were used for left and right, respectively. In this paper, 64 and 63 sulci were used for left and right, respectively. How does this become four additional sulci?\n4. In 4.1, the p values are strange. It seems that the final model shows the best results, but why is the p-value higher than the ones for the other methods?\n5. In figure 2, what does the blue bar mean? According to the caption, there should be only violet and pink bars.\n\nMinor comments:\n1. Voxel resolution should be mm^3, not mm.\n2. Please represent the measures Elocal and ESI with subscript such as E_{local} and E_{SI}.\n', 'rating': '2: reject', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		BkeZySSxlE	ryeugtChmE	MIDL.io/2019/Conference/-/Paper101/Official_Review	[]	4		['everyone']	BkeZySSxlE	['MIDL.io/2019/Conference/Paper101/AnonReviewer1']	1548703984272		1548856695435	['MIDL.io/2019/Conference/Paper101/AnonReviewer1']
362	1548650571133	"{'pros': ""This paper investigates the problem of artefact in MRI images. Instead of trying to reconstruct a un-corrupted image from the corrupted one (or its k-space), which is the common approach - and may destroy important image information, here, the authors' stance is that for a specific task, to make its corresponding CNN model robust to the presence of such artefacts.\nThe author investigate how introducing images with artefacts in training models can improve not image reconstruction, but a downstream task : segmentation quality. The hypothesis is that a good segmentation can be obtained even on artefacted images.\n\nPros :\nTackles a difficult problem, i.e. images with possibly big motion artifacts, which are typically excluded from medical imaging datasets.\nThe authors propose a new fully 3D motion model of MRI acquisitions, more realistic than standards methods consisting of simple k-space sampling and mixing in 2D.\nThe validation is extensive: the proposed method is tested on a variety of segmentations tasks (TIV, hippocampus, CGM), on both real and synthetic data, and on a test set containing both clean and artificially artefacted data, and with a panel of metrics (Dice score, positive predictive value, sensitivity and average distance metrics).  \nThe model provides a better uncertainty estimation for segmentation predictions of motion-corrupted data.\n\nThe paper is clear and easy to follow."", 'cons': ""It seems that for each new segmentation task, new adapted artefacts volumes have to be modeled. But in the paper, there is no discussion of how long it takes to generate these artefacted volumes. For example, how long did it take to generate the 15 artefacted volume per scan ?\n\nThe method isn't consistently better than other more standard augmentation  (rotations...) as measured by 4 segmentation metrics (although authors do provide an possible explanation, which is that for the hippocampus - for which the augmentation model doesn't outperform classical augmentation-, the motion artefact model may not be well adapted."", 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		ryxqlJzge4	S1gmLOW3XN	MIDL.io/2019/Conference/-/Paper67/Official_Review	[]	1		['everyone']	ryxqlJzge4	['MIDL.io/2019/Conference/Paper67/AnonReviewer1']	1548650571133		1548856695223	['MIDL.io/2019/Conference/Paper67/AnonReviewer1']
363	1548705385120	"{'pros': 'This work presents a method for selectively sampling and augmenting a dataset for training a deep neural network and was evaluated on classification tasks using the same skin lesion image dataset. The method indicates similar performance/accuracy as networks that have been trained on the full dataset while using only 50 % of the data. The authors claim that the main novelty for the sampling step is the independence of both informativeness and representativeness. For the augmentation they propose to stitch 4 image samples together. \nThe authors address an important problem: by determining the samples that are beneficial for training the network, the amount of necessary annotations is reduced (which are “expensive” due to the need of expert annotation). The pipeline is nicely explained, and the paper is for the most part clearly written. The proposed method is compared to other active learning methods. \n', 'cons': 'The major weakness is the evaluation and novelty of the method. The general direction is interesting and sounds promising, but unfortunately especially the comparison to random sampling does not indicate a significant improvement (shows very similar performance). \nComments:\n- It is not specified how the authors measure the prediction confidence. \n- How fast is the selective sampling?\n- some formatting issues (e.g. ""s-elect"", ""The images that the model presents low prediction confidence"", ""t-wo"")\n- The mathematical formulations should be improved (e.g. ""10% x N x y"") . Equation (1) only indicates, that you apply the current Model M on the subset of samples and Rank them. But it does not specify according to which measure!\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		rygLeM3C1N	HkgWd0Rn74	MIDL.io/2019/Conference/-/Paper23/Official_Review	[]	3		['everyone']	rygLeM3C1N	['MIDL.io/2019/Conference/Paper23/AnonReviewer2']	1548705385120		1548856694955	['MIDL.io/2019/Conference/Paper23/AnonReviewer2']
364	1548707906642	{'pros': 'This paper describes a study in which the authors experiment with various training strategies to train a U-Net for the task of locating metastatic lymph nodes on I-131 whole body planar scans. \nThe paper uses a training set of 292 scans and a test set of 95 scans. In the training set, 99 scans contain metastatic lymph nodes. It is unclear to me how many scans in the test set contain mLN. \nAccording to the table, the UNet with hard negative sampling has the best accuracy (although not the highest precision) and the authors recommend to use this training strategy for this task. I thnk this is an important clinical task and the authors have a reasonable size data set.', 'cons': 'In short, I think this paper lacks methodological novelty. In the original U-Net paper, the authors already introduced a weighted loss function (they weighed background voxels between touching cells higher) so I do not agree that baseline U-Net has no weight loss. In addition, there have been many other papers published using U-Net which tackle the class imbalance problem in many, many ways. A dice loss or weighted cross-entropy would also be a standard way to make sure the network focuses on the segmentation of the mLNs, and are not validated in this paper.\n\nSome detailed comments which may improve the paper:\n- Is the test set actually a real test set, so not used during training, or is it used as a validation set, so used in the training to stop training and do hyperparameter optimization?\n- Why is augmentation performed on test set? Are you using test time augmentation?\n- Why no dice or IoU results of the different U-Net results? That would be interesting to know as well next to the precision, recall, etc measures.', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		S1evJSAxgN	B1loBuJTXE	MIDL.io/2019/Conference/-/Paper136/Official_Review	[]	2		['everyone']	S1evJSAxgN	['MIDL.io/2019/Conference/Paper136/AnonReviewer3']	1548707906642		1548856694740	['MIDL.io/2019/Conference/Paper136/AnonReviewer3']
365	1548712637368	"{'pros': 'This work addresses the problem of transfer learning and proposes a method for improving the robustness of deep neural networks when trained only on synthetic data. Traditional transfer learning (from synthetic to clinical data) is limited by the quality of the simulations. To tackle this problem, the authors propose to use Domain Randomization in the context of cardiac image registration (X-Ray, DRR, CT). The technique has already been successfully applied for autonomous driving and robots. Here, the authors modify medical specific parameters, such as Hounsfield units, to realize Domain Randomization (and this should be mentioned in the introduction as a contribution instead of “only applying”). Furthermore, it is great that the authors not only the method quantitatively on a synthetic dataset, but also qualitatively on a clinical dataset. ', 'cons': 'I believe the work has great potential but needs some major revision. Especially the description of the method and the evaluation/experiments has room for improvement. For example, it is not clear how the CNN is defined. How does the reward come into play here? Is the ground truth transformation simply used for the computation of the loss function or in another way? Furthermore, is the definition of experiments and the evaluation the most meaningful choice?\n\nIn particular:\n-\tDefinition of CNN model. It is understandable that the network is not the central element of this paper. However, the is important for reproducibility and should be included\n-\tIn section 2.1 many arguments are repeated from the introduction and do not really contribute to the explanation of the method. \n-\tWhy is the training data order of importance? You write ""If the same weights are used for initialization, but the network sees the training data in a different order, the optimization can take different steps and can end up in different local minima,"". Yes, the optimization is not deterministic and will take different steps. But especially with a synthetic dataset a balanced distribution should be possible and with random subsampling and stochastic optimization it should not be a major problem… Also, in Table 1, I don’t see a major deviation. What mm accuracy is needed for the application?\n-\tA more interesting experiment would be how the range of parameters affect the transfer learning. Do you confuse the network at some point, if the examples become to unrealistic? The range for the HU etc. are not well explained / investigated.  What is the impact of the variation parameters?\n-\tHow would Domain Randomization compare to classical augmentation? E.g. creating the DRR and simply modifying the contrast? \n-\tThe registration accuracy measured “by the points of a 3D landmark at the center of the LV model” (in mm). If your “rewards for action” correspond to the transformation parameters, wouldn’t it be more interesting to look at the error in terms of translation, rotation etc?\n\nMinor comments:\n- ""data shuffing (D1) and weight initialization (W1)"" would be helpful in Table 1 description\n- what does ""synthetically generated data of 1711 CT volumes of 799 patients"" mean? Are the patients real and the CT is estimated from it? Or do you have virtual patients and for every patient you created at least 2 CT volumes?\n\nI highly encourage the authors to address upper points because I believe the work has great potential!\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		rkgHu5GglV	BkgBa9gaQE	MIDL.io/2019/Conference/-/Paper79/Official_Review	[]	2		['everyone']	rkgHu5GglV	['MIDL.io/2019/Conference/Paper79/AnonReviewer3']	1548712637368		1548856694517	['MIDL.io/2019/Conference/Paper79/AnonReviewer3']
366	1548712728625	"{'pros': 'The authors demonstrated three different networks and their performance on pneumothorax detection and localization problem using chest X-rays. Detection results are reported on a ~1000 hand labeled dataset among three networks and ensembled ones. \n+ The idea of evaluating different network architectures on the same problem is sound and useful, although it is not well justified (see below).\n+ The manuscript is overall well written. ', 'cons': '- Lack of contribution in terms of methodology. Applying three existing models on the pneumothorax classification problem is not attractive to the audience without further insights. What\'s hyperthesis? Why these three networks were chosen for the comparison is not clearly explained. More discussion about the pros and cons of these three models should also be given.\n- Many details are missing in the manuscript, e.g. How the Ensemble Learning was performed? and some points listed below.\n- do those 437 images that were labeled as positives of pneumothorax have other diseases as well? What\'s the detailed number and distribution of data used for the evaluation of each of the three networks?\n- By ""We generated pixel-level pneumothorax annotations for 305 of the positive cases"", do you mean that those images have masks of pneumothorax regions? Are they used for evaluation MIL alone? \n- I am curious about the localization performance of FCN. Will it be possible to show the IOU or DICE?\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		B1lpH4jkxN	Skxbmseam4	MIDL.io/2019/Conference/-/Paper41/Official_Review	[]	3		['everyone']	B1lpH4jkxN	['MIDL.io/2019/Conference/Paper41/AnonReviewer1']	1548712728625		1548856694270	['MIDL.io/2019/Conference/Paper41/AnonReviewer1']
367	1548713045351	"{'pros': 'Summary: \n\nIn this work, a CNN architecture that has both local and global rotation invariances in introduced. Furthering the recent advancements in group CNNs for rotational invariance, the proposed work uses steerable filters based on spherical harmonics to obtain efficient sampling of 3D rotations. The model is evaluated on synthetic data and on lung nodule detection tasks. The performance is shown to be superior with a substantial reduction in the number of parameters when compared to CNNs.\n\nPros: \n\n- Use of steerable filters to avoid approximating filter rotations and to introduce local rotation invariance is a solid contribution\n- Experiments clearly show the importance of introducing local rotation invariance for both the synthetic data and for the lung nodule detection task. That 3D CNN model outperforms in a couple of instances but with almost two orders of magnitude more parameters.\n- The paper is very well written; the discussion section is very insightful. Figure 1 is a great visual abstract of the work.\n\n', 'cons': 'Minor comments: \n\n- There appears to be a substantial increase in accuracy with increasing M for the synthetic data. A similar trend is also observed for the lung nodule classification data reported in Table 2. However, M = 96 is not reported here. A comment on why this is the case can be useful.\n\n- Literature survey could include one more closely related G-CNN work that also uses max  pooling over different rotations, in what the authors call the projection layer [1]\n[1] Bekkers, Erik J., et al. ""Roto-translation covariant convolutional networks for medical image analysis."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2018.  https://arxiv.org/pdf/1804.03393.pdf', 'rating': '4: strong accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'oral_presentation': ['Consider for oral presentation']}"		H1gXZLzxeE	r1l68nx6XE	MIDL.io/2019/Conference/-/Paper77/Official_Review	[]	2		['everyone']	H1gXZLzxeE	['MIDL.io/2019/Conference/Paper77/AnonReviewer2']	1548713045351		1548856694057	['MIDL.io/2019/Conference/Paper77/AnonReviewer2']
368	1548713682652	"{'pros': '- 3D ultrasound image segmentation is an important application, and there might be cases in the real world where the access to the larger data set is not possible.\n', 'cons': 'The authors claim that spatial and intensity transformation layers are needed for a better transfer learning, but the theoretical justification and experimental verification is lacking in several aspects:\nTheoretical justification:\n- I do not see, why translation and rotation should be different between adult and child images. Especially given the dominant cone shape of the image.\n- why do the author apply a scaling along each axis? The axes change during rotation.\n- the 3-layer network to predict the transformation parameters is not described in the paper. I doubt that a 3 layer network is powerful enough to predict useful transformation parameters.\n- the ""intensity"" scaling layer in the middle of the network does not make much sense. I doubt that such features are correlated with the image intensity -- if they are the data augmentation was not sufficient. If the authors want to compensate image intensity changes, this layer should be directly placed at the beginning of the network.\n- The author should have optimised the the baseline architecture (only 3 downscaling levels seems too few to me), number of channels is not provided. They could have tried to add more convolutional layers in the bottom layer, instead of the ""intensity"" scaling layer\n- the training procedure is not described. Have the authors checked for overfitting?\n- The authors do not report data augmentation during training. \n- The authors did only cross validation experiments with random splits on the same dataset. An independent test set is missing. It is very likely that they have overfitted their hyper parameters to this data set.\n- it would have been interesting to see, what transformations the network selects, and if they make sense\n- the authors describe many failure cases. I would epect\n\nAll in all this paper leaves more questions open, and contains some  than it answers. It seems that all experiments were done in a quite suboptimal setting, and so the relative changes in performance might not transfer to a setting with a state-of-the art baseline. In my opinion not ready for publication at MIDL.', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		HyxhwW_6k4	HyeoAAlp7E	MIDL.io/2019/Conference/-/Paper9/Official_Review	[]	2		['everyone']	HyxhwW_6k4	['MIDL.io/2019/Conference/Paper9/AnonReviewer3']	1548713682652		1548856693843	['MIDL.io/2019/Conference/Paper9/AnonReviewer3']
369	1548713864937	{'pros': 'Overall this paper addresses and potentially solves  a useful task for CT multi-phase/phase contrast data curation and preparation, e.g. for retrospective studies. \n\nThis is a nice application of CNN for an CT contrast phase identification task, trained on a very large database of abdominal and chest scans (60k) from multiple institutions, dealing with noisy, free-text data from the DICOM, and phase labels only available in <8% of cases.\n\nResults are very promising, achieving >93% accuracy over radiological assessment. Transfer learning from multi-phase abdominal images to a binary classification of contrast/no-contrast chest images is interesting, needing only 100 chest samples for a very good performance.\n\nThere is a helpful demonstration of saliency/attention maps to reassure the reader that the network is looking at relevant areas (portal vein etc). \n\n', 'cons': 'The actual method employed uses a fairly standard CNN architecture that offers no novelty per se, making this mainly an application paper - this is of course welcome at MIDL, however there are some aspects on the experimental setup and evaluation that could be improved upon:\n\nThis includes some missing details on preparing the training data, ie are volumes of interest cropped out automatically or by hand? The cropped volumes are treated as multiple channels for a 2D CNN, rather than used  as 3D inputs for the CNN - it might be interesting to check what difference this makes. The use of MIPs is interesting, but could difference images instead be used across phases (see also points below)?\n\nGiven this is time-dynamic data (at discrete intervals), some reasoning on why multi-phase volumes are used separately rather than with a time component (e.g. recurrent networks) could be provided - or are for the same patient not all phases available - and if they are, are they treated independently?  Are time stamps in the DICOM header (as opposed to the noisy free text) available and if so, could these be used directly rather than predicting them?\n\nCT is a quantitative imaging technique, with HU providing some fairly objective possibility of extracting presence of contrast directly, or at least via segmenting the relevant organ (say, kidney or liver) and assessing the intensity distribution therein. \nI would like to suggest some form of baseline comparison.\n\nMinor issues: Figures need better referencing; Fig 4b is not clear to me; the confusion matrix in Fig shows only 178 test cases - but 10% test data should be 6000 images - even if 60k referred only to images slices and not individual cases, I would expect a much larger number of test cases. \n\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		SylDbkUlxE	rygbqJ-TQN	MIDL.io/2019/Conference/-/Paper104/Official_Review	[]	3		['everyone']	SylDbkUlxE	['MIDL.io/2019/Conference/Paper104/AnonReviewer2']	1548713864937		1548856693623	['MIDL.io/2019/Conference/Paper104/AnonReviewer2']
370	1548711782793	"{'pros': 'The authors address an important problem in medical image segmentation - the paucity of labeled training data. They address this problem by presenting a data augmentation method which combines background tissue and tool labels from different images to create new labeled images.', 'cons': ""Contribution of the paper is limited. The overlay of pixels belonging to an object of interest over different backgrounds is a fairly basic operation. Although interesting, this in combination with other contributions (like a new loss function, more interesting architecture, etc.) could have made this paper a stronger contender, but in and of itself this contribution is not sufficient for a conference paper.\n\nSmall issue with motivation: authors claim that one important requirement during surgery is to recognize objects within the endoscope's view. I would encourage the authors to think critically about this claim. You would expect a surgeon to recognize objects within the endoscope's view. Perhaps what is more important is knowing the extent of different organs, where one ends and the other begins, if there are critical structures nearby (possibly not in the endoscope's view but inferred from identified organs). Tools are segmented for tracking, for applications like training a resident or fellow by observing their motion and comparing to an expert. Perhaps motivation like this would seem stronger.\n\nLarger issue with experiment: authors mention that they used 12 out of 16 sequences for their experiments with 9 used for training and 3 for validation. Is validation the same as test here? If so, was a validation set used to fine tune the hyperparameters. The learning rate, momentum, etc. are set to very specific values. If these were fine tuned on the test set, then results are misleading.\n\nThere are several other classification methods that have been published on this/similar datasets. How does this method compare to those?\n\nFinally, the paper needs thorough proof-reading. There are several typos that made it hard to understand what sentences were trying to convey. Also, please define all acronyms at their first usage, not later on in the paper."", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		HJl-FXD4eN	BylydvgpXN	MIDL.io/2019/Conference/-/Paper146/Official_Review	[]	3		['everyone']	HJl-FXD4eN	['MIDL.io/2019/Conference/Paper146/AnonReviewer2']	1548711782793		1548856693346	['MIDL.io/2019/Conference/Paper146/AnonReviewer2']
371	1548714193354	{'pros': 'This paper deals with brain tumor segmentation on MR images with missing modalities. The paper presents a comparative study of different U-Net based architectures to deal with missing data, comprising a standard U-Net, a U-Net with dropout in the input layer, an ensemble approach and a late fusion approach.\n\nThe paper is well written, easy to follow and validated in a well known dataset (BRATS). The authors tackle a challenging task that is still an open problem for the MIC community. The methodology is fairly simple and seems to have a significant impact in the results.', 'cons': '- My main concern with this work is in terms of lack of mention and comparison with really similar approaches in the literature, like that of https://arxiv.org/pdf/1607.05194.pdf This paper tackles exactly the same problem and proposes a fairly similar strategy, but is not even mentioned in the manuscript. Authors should at least discuss the differences with this work.\n\n- The authors only validated the proposed approach for a single missing modality and for a binary segmentation scenario. Given that BRATS provides 4 modalities and mutli-label annotations, why not validating with more missing modalities and in the context of multi-label segmentation? This would make a more solid validation of the proposed architectures. Moreover, it would be interesting to see how the proposed methods perform in the absence of multiple modalities.\n\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		S1lXP-nJxE	rJxY0gbpX4	MIDL.io/2019/Conference/-/Paper43/Official_Review	[]	3		['everyone']	S1lXP-nJxE	['MIDL.io/2019/Conference/Paper43/AnonReviewer1']	1548714193354		1548856692982	['MIDL.io/2019/Conference/Paper43/AnonReviewer1']
372	1548714489206	"{'pros': 'The paper describes a method employing conditional generative adversarial networks to aid the stroke lesion segmentation on CT perfusion images. The paper is well-written and clearly structured. The clinical application is well-motivated. ', 'cons': 'The major problem is the presented results. With 94 pairs of data from 63 subjects, the statistical significance of the claimed improvement from Table 1 seems questionable. For instance, a difference of 0.06 in Hausdorff Distance, which is known for being with high variance, is unlikely to be significant given reported standard deviation being around ~20. This continues with other metrics, which suggested that the visually superior results in Figure 3 can be highly selective and therefore misleading.  \n\nA few minor comments include: 1) fairly limited technical contribution to improve the results, e.g. why 3D network was not adopted and tested while 2D formulation maybe efficient but loses 3D ""convolutional constraint""; 2) no effort has been made to network adaptation, e.g. hyper-parameters from other unrelated applications, to the application - which itself may not be a problem and may be problematic within cross-validation. However, given the presented results, this became relevant and needs a better experiment strategy for future work.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		HyemZQzExE	BylbWfbpm4	MIDL.io/2019/Conference/-/Paper144/Official_Review	[]	3		['everyone']	HyemZQzExE	['MIDL.io/2019/Conference/Paper144/AnonReviewer2']	1548714489206		1548856692768	['MIDL.io/2019/Conference/Paper144/AnonReviewer2']
373	1548715099771	{'pros': 'In this paper the author proposed a postprocessing refinement solution for semantic segmentation problem.  After several layers of code/decode convolutional layers, the final pixel label prediction is estimated based on pixel-wise softmax. There are also many region-based segmentation methods too. In this paper author further utilize pixel-wise probability on each slice of the resulting segmentation to propose a novel uncertainty measure. This uncertainty measure can be used to detect and eliminate oversegmentation, and some errors in segmentation by training fast classifiers such as random forest regression. The proposed method is tested on T1 and T2 myocardium images, and shows the feasibility of proposed method.\n\nThe paper addresses a difficult problem in segmentation.  Cardiac MRI is a difficult modality for image segmentation because of low resolution and motion and other artifacts.  Precise boundaries can be difficult to determine.  I like the approach to segmentation that considers a sort of confidence estimation.  This is very practical for clinical scenarios and helpful for clinicians to understand the outputs.\n\nThe paper is reasonably well written and clearly argued.', 'cons': 'However, the whole proposed segmentation process is basically two stages. Both segmentation and postprocessing classifiers need to separate training. Also, considering that the authors used the pixel wise softmax probability as an uncertainty measure, it is totally feasible to incorporate region information with pixel-wise softmax, and thus design an end-to-end architecture instead.\n\nThe data presented on the 40 image datasets was compelling, however, the presentation would benefit from a more in-depth analysis of the segmentation accuracy.  Some example images showing typical results would be helpful to understand both the problem under consideration and the results.  ', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		rJe3YQqgxE	H1xEvNZTX4	MIDL.io/2019/Conference/-/Paper125/Official_Review	[]	2		['everyone']	rJe3YQqgxE	['MIDL.io/2019/Conference/Paper125/AnonReviewer2']	1548715099771		1548856692556	['MIDL.io/2019/Conference/Paper125/AnonReviewer2']
374	1548716152567	"{'pros': 'This work presents an original approach of using weak label (=slice number) annotations for classifying lung CT volume slices into containing lung nodules or not, in a slice detection task facilitated by the use of a CNN for feature encoding, followed by LSTM for coupling detection in adjacent slices (given that nodules will be present in neighbouring slices).\n\nWeak annotations, for identifying slices that contain nodules, may be beneficial to direct radiologists to specific slices, rather having them scroll through entire volumes; more importantly (though not mentioned by the authors) this could be helpful to prepare training data for other researchers, were only even weaker annotations exist (e.g. ""patient has nodule in upper left lobe of the right lung""), with no slice number or nodule mask provided. \n\nThe method is individually tested on two large datasets, the well-known LIDC data (normally used for nodule detection, segmentation or classification), as well as a dataset from a large Chinese hospital.', 'cons': 'While the idea of weak slice annotations for training is very interesting, normally radiologists would provide more detailed information such as mentioned above (""patient has nodule in upper left lobe of the right lung"") which could provide useful extra information that could be integrated into this method for training. Could the proposed method provide the first step in that direction? It would in fact be quite interesting to see if saliency (attention) maps would reveal a (probabilistic) location of the nodule within the detected slices, and, as a next step, if the nodule could be detected, segmented, or classified into the different relevant types (solid, sub-solid, ground glass,...)\n\nI\'m wondering whether the proposed method could be trained on one dataset (say LIDC), and applied to the other - ie does it generalise well? As many researchers have worked on the LIDC, it would be interesting to test whether their methods can classify the slices indirectly to a similar accuracy (after detecting/segmenting the nodules using their deep learning architectures), or if they could benefit from using the proposed method as a first step, leading to lower FPs in their nodule localisation.\n\nResults could be more clearly presented, and a more critical comparison to at least the existing LIDC work be made. It is otherwise very difficult to see how well the achieved accuracy would compare other works that directly extract nodules (and indirectly the slices containing the nodules).', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		B1gQDNA3JE	SJxZYOb67V	MIDL.io/2019/Conference/-/Paper7/Official_Review	[]	3		['everyone']	B1gQDNA3JE	['MIDL.io/2019/Conference/Paper7/AnonReviewer3']	1548716152567		1548856692340	['MIDL.io/2019/Conference/Paper7/AnonReviewer3']
375	1548716512341	"{'pros': 'The author proposed an unsupervised method to give images labels. Convolutional adversarial autoencoder was used to compute the features for each image/patch and Gaussian Mixture Model was adopted to cluster the image into groups using the afore-computed features. Evaluation results show that features extracted from both images and ROI patches achieve better clustering performance than using images or ROI patches alone. A dataset of 503 slices of images and 6314 ROI patches were used for the experiments. \n\n+ The idea of clustering image features for labeling image in an unsupervised and group-wise manner is valid and practically useful, although similar ideas have been proposed in previous works (missed reference), e.g. \n\nWang, Xiaosong, et al. ""Unsupervised joint mining of deep features and image labels for large-scale radiology image categorization and scene recognition."" Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on. IEEE, 2017.', 'cons': 'things to improve and where details are missing:\n- how were the ROI images cropped from the original images, by sliding window or equally dividing? different for training and testing?\n- what\'s the filter size of Block1 in the network shown in Figure 4? As small as 32*32 images, I wonder the size features after 4 convolution layers. \n- why the samples from the Generator to the Discriminator are produced from mid feature layers (after Encoder) in Figure 4? is the input to the Discriminator features?\n- How the positive samples are drawn from the ""Gaussian distribution""? Where does this distribution come from?\n- How the training data, validation, and testing ones are divided? How many normal patches are used?\n- What method was used to generate the detection results shown in Figure 7? Were the patches extracted by a sliding window method? ', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		HylqNO4RkV	HkxO15Z6mE	MIDL.io/2019/Conference/-/Paper12/Official_Review	[]	3		['everyone']	HylqNO4RkV	['MIDL.io/2019/Conference/Paper12/AnonReviewer1']	1548716512341		1548856692127	['MIDL.io/2019/Conference/Paper12/AnonReviewer1']
376	1548716813367	{'pros': 'This paper addresses the problem of scarcely available expert annotated ground truth for medical image segmentation. The contribution of this paper is reliable segmentation even when trained on inexpert labels. They are able to do this by combining a regular deep segmentation network with an autoencoder that estimates labels from a geodesic map of training labels. This geodesic constraint is an interesting addition to a regular segmentation network.', 'cons': 'Although the addition of a geodesic constraint is very interesting, the authors need to spend more time explaining their intuition on why a geodesic map derived from, say inexpert labels, would be meaningful. Or why does it make sense to minimize mean square error between the geodesic distance map and segmentation probability map. Simply saying we did this and it worked does not make a strong paper - the strength is in trying to explain why certain steps make sense and would improve results and then showing that results actually improved.\n\nThe synthetic inexpert labels do not seem realistic. Perhaps a better way to acquire inexpert labels is to crowd source labels from general population.\n\nAlthough results show improved segmentation with the presented method, the improvement seems small. Authors should do significance tests to evaluate their contribution. Further, the Hausdorff distance on even the improved segmentation seems large - the authors need to discuss what expectations are. How are accurate are the segmentations required to be in order for them to be useful to the medical community? How close are these results? How good or bad is it to be off by over 1 cm? It is hard to gauge without introducing the reader to these expectations.  Also, compare to other segmentation methods for this particular task, if possible. This does not have to be a deep net. Many traditional segmentation methods are very sound - how does this compare to those? \n\nFinally, the writing needs a little bit more work to iron out typos, simplify sentences, and define acronyms/terms when they are first used. Mostly importantly though, the technical section needs to be cleaned up a little. Currently, the equations are sort of all over the place and it is hard to know what the different terms are. There needs to be better organization and description of terms.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		r1lkO0u1lV	rkxSfiWamV	MIDL.io/2019/Conference/-/Paper33/Official_Review	[]	3		['everyone']	r1lkO0u1lV	['MIDL.io/2019/Conference/Paper33/AnonReviewer3']	1548716813367		1548856691910	['MIDL.io/2019/Conference/Paper33/AnonReviewer3']
377	1548621157392	"{'pros': 'The authors have developed a collaborative learning framework for segmenting medical images when the data is multimodal, and potentially acquired at different resolutions. The specific application is White Matter Segmentation from T1 and FLAIR images, with annotations available for the Low Resolution FLAIR images. \n\nThe two main concepts in the paper, i.e shared representations , and knowledge distillation for transfer learning are clearly presented and well explained in terms of the objective formulation. They also provide an intuitive justification to the benefits of the two techniques over a simple resampling of one of the domains. They compare two baselines, one, where the T1 is downsampled, and the second, where the FLAIR is upsampled, and fed into two U Nets, for segmentation. The shared representation case is shown to converge faster than the baselines, to nearly similar dice overlap.\n\nThe concepts of collaborative shared representations, and knowledge distillation to Deep Learning for multimodal image segmentation is interesting in terms of its potential for application to similar tasks from other domains. There is an additional flexibility in terms of the mapping function which relates the two domains. Though not exploited here, this could potentially be used to capture more complex domain interactions.', 'cons': ""Major Concerns: \n\n1. The arguments for quantitative improvements in terms of the results are not entirely convincing. Table 1 indicates that baseline 1 has an average dice coefficient that is higher than the two modifications the authors propose. However, this phenomenon is not explained in the results section. Is this an artifact of the annotations themselves, or of the criterion used to classify lesions based on the network outputs?\n\n2. While an improvement in terms of FP and FN is relation to Baseline 1 is indicated, there is no comparison made in terms of the FP and FN rates in relation to Baseline 2, which achieves nearly the same average dice performance. The conclusions to be drawn from this performance are not clear.\n\n3. The merits of using distillation for the chosen task over SRC is not evident from the experimental results.\n\nMinor Concerns:\n\n1. In the learning objectives section, the authors use an exponential temperature decay for the softmax. The effect of this scheme on the overall convergence for the DL task isn't explained or justified.\n\n2. The learning objectives section uses a softmax temperature of 1. This is either a typo or needs further justification.\n\n3. The authors do not provide the comparison between the number of tunable hyperparameters, and space complexity for each of the baselines as compared to the modifications proposed in the paper."", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		SJgw_QIlx4	HkgpPBci7N	MIDL.io/2019/Conference/-/Paper106/Official_Review	[]	2		['everyone']	SJgw_QIlx4	['MIDL.io/2019/Conference/Paper106/AnonReviewer2']	1548621157392		1548856691695	['MIDL.io/2019/Conference/Paper106/AnonReviewer2']
378	1548717073959	{'pros': 'A domain randomization method is proposed to improve the robustness and transfer of synthetic data to a target domain. The method is applied to 3D/2D cardiac model-to-xray-registration. The results show that domain randomization resulted in more consistent transfer to the target domain. The method does not require any data from the target domain which is interesting. The method could be applicable to other medical imaging applications where training data is not available. The paper is well written and fairly easy to follow.', 'cons': 'The major limitation is no quantitative evaluation of the method was performed.\n\nAbstract states the model was trained fully on synthetic data from 1711 CT volumes. This statement is not clear, the 1711 CT volumes are not synthetic, but the X-ray images generated from the real CT volumes are synthetic, correct?\n\nWhat rewards is the agent learning for registration? Is it just translation in two directions? Rotation? This information should be included.\n\nThe evaluation metrics reported in all tables/figures should be defined. What is Deviation e_f (mm) reported in figure 3, 4, and 5? What is being reported in Table 1,  is it also deviation e_f? \n\nHow was distance measured for evaluation? Was just the center distance measured? The distance should be measured for all points on the surface to account cases the have incorrect rotation.\n\nIt would be interesting to evaluate the effects of each domain randomization on its own, i.e., just intensity mapping and just collimation. \n\nAcronyms should only be defined upon first use and then the acronym should be used throughout remainder of paper, e.g.,  first use: digitally reconstructed radiograph (DRR), all subsequent uses: DRR', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		rkgHu5GglV	BJg5Mn-pQN	MIDL.io/2019/Conference/-/Paper79/Official_Review	[]	3		['everyone']	rkgHu5GglV	['MIDL.io/2019/Conference/Paper79/AnonReviewer1']	1548717073959		1548856691478	['MIDL.io/2019/Conference/Paper79/AnonReviewer1']
379	1548717670589	"{'pros': 'The paper presents a novel approach based on Knowledge Distillation and Collaborative Learning. The paper is well-written and the method sounds and well-validated on a multimodal Image segmentation. ', 'cons': 'While the method is quite interesting, the results shown in Table 1 are not rather conclusive. For instance, I was expecting to see a statistical significance test, and further discussion on the results and the performance of SCR, and DL. \n\ntypos: \n- Sec. 5 line 1: redundant ""information""\n- Sec.6 line 3: ""for a give threshold"" --> ""for a given threshold""\n- Sec.7 line 10: ""that"" --> ""than""', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		SJgw_QIlx4	Skg1uAZ6Q4	MIDL.io/2019/Conference/-/Paper106/Official_Review	[]	3		['everyone']	SJgw_QIlx4	['MIDL.io/2019/Conference/Paper106/AnonReviewer1']	1548717670589		1548856691267	['MIDL.io/2019/Conference/Paper106/AnonReviewer1']
380	1548718464535	"{'pros': 'The authors propose a simple clever idea to improve segmentation performance for MRI images: simulating movements during MRI acquisition. They demonstrate significant improvements in on simulated and real-world images with movement artefacts. Furthermore they show that this kind of augmentation improves drop-out based uncertainty estimation.\nthe paper is clearly written, the experimental setup is convincing.', 'cons': '- the presentation of the results could be improved. E.g., I found the  axis labels ""benchmark"" and ""clean"" and ""Against"" quite confusing\n- A reference for the ""benchmark"" method is missing.\n- Several references are incomplete (e.g., Arxiv identifier missing, or no source at all for Pawar et al.)\n- Figure 6 is very hard to interpret. ', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		ryxqlJzge4	rygFYWMam4	MIDL.io/2019/Conference/-/Paper67/Official_Review	[]	2		['everyone']	ryxqlJzge4	['MIDL.io/2019/Conference/Paper67/AnonReviewer2']	1548718464535		1548856691039	['MIDL.io/2019/Conference/Paper67/AnonReviewer2']
381	1548718696528	"{'pros': 'Topic itself could be an interesting application of deep learning.', 'cons': 'The title is very misleading, this work does neither ""Deep Learning Based Image Reconstruction"", nor ""Tumor Detection"", nor ""in Multimodal Microwave-Ultrasound Breast Images"". The real work this paper performs is: using U-net to learn a mapping between a pair of simulated 2D images. After removing all the fancy-looking terms, the method is very simple, straightforward, and lack of novelty. A more accurate title would be ""U-net based enhancement for simulated Microwave-Ultrasound breast image""\n\nThere is no real data used, not even ""phantom images"", which are supposed to be acquired by real machine. Only ""simulated phantom images"" are used, and from Fig.2, they are quite rough and hard to estimated how well the resulting images reflect the reality.\n\nComplex data is handled simply by treating them separately, there may be better options, at least authors can compare with another option.\n\nExamples may be need to show readers how well previous reconstruction methods work, and the ""artifacts"". \n\nRotation and flipping may not be relevant unless there is an issue in real life, and data augmentation fails to solve the problem.\n\n', 'rating': '1: strong reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		H1lkgwfeg4	SylbuMzTXN	MIDL.io/2019/Conference/-/Paper78/Official_Review	[]	2		['everyone']	H1lkgwfeg4	['MIDL.io/2019/Conference/Paper78/AnonReviewer1']	1548718696528		1548856690820	['MIDL.io/2019/Conference/Paper78/AnonReviewer1']
382	1548719218236	"{'pros': 'The paper proposes a 2--stage fine-tuning strategy to utilise both silver- and gold-standard annotations to skull-stripping task.\n\nThe paper is well-written and easy-to-follow. The experiment is well-designed, given constraints on data and ground-truth availability. The reported results seem comprehensive, with comparison to several well-known methods and test statistics.', 'cons': 'The manuscript lacks experimental details. For examples: In cross-validation, ""For all networks, 70% of the data was used for training and 30% was left for validation."" is confusing, as not all the data have manual ""gold-standard"";  In the fine-tuning stage, training details are not given, e.g. convergence criterion may be of importance in fine-tuning stage. \n\nThere isn\'t any clinical interpretation of the results - not sure what the 1% dice improvement means for any patients. Even with the said improvement, I would argue that how much cost-saving can be made is not as clear as the authors hypothesised.\n\nThis raises the questions in contribution of this work, which is also not identified clearly, short both in technical area and clinical applications.', 'rating': '2: reject', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		Syen8HNxeN	Bkg5ONGa7N	MIDL.io/2019/Conference/-/Paper95/Official_Review	[]	2		['everyone']	Syen8HNxeN	['MIDL.io/2019/Conference/Paper95/AnonReviewer1']	1548719218236		1548856690603	['MIDL.io/2019/Conference/Paper95/AnonReviewer1']
383	1548720347275	"{'pros': 'Application-specific transfer learning method by adding simple learnable transformation layers into existing U-net architecture that has been pre-trained on adult images for the segmentation of the kidney from ultrasound images for both adult and pediatric images.\n\nI like that the authors have compared their method with different alternatives, and appreciate that they give examples also of bad results. The segmentation problem at hand is certainly not an easy one given the image quality.\n\nIt is also positive that the authors acknowledge that their technique does not add a benefit when one has access to both data sets for training, and that ""the quantity of available data is more important than its quality"".', 'cons': 'From a practical standpoint, I have to ask: What is the motivation for a transfer learning method that performs well on both adult and pediatric images? In clinical practice, I assume the age of the patient will be known and an age group-specific model can be applied. In this case, the noted fine-tuning should be sufficient.\n\nThe authors achieve slightly better results with their approach than with fine-tuning, but I would attribute this rather to the substantially lower number of parameters that are learned as part of their adapted U-net model with mostly fixed weights (except for the newly introduced layers) from a rather small pediatric data set. How would this compare to a simple fine-tuning with only a few convolutional layers of the U-net whose weights are not fixed, e.g., convolution layers at the lowest level instead of adding a separate intensity transformation layer?\n\nThe authors state the goal of a transfer learning method that cannot utilize the source data set. Further, they say their transformation layers may apply the identity transformation to not hurt the performance on adult data. However, as these layers are never learned on adult data, but pediatric images only, these layers do indeed not learn such identity transformation. This is especially noticeable for the geometric transformation layer, which indeed worsens the results on adult data due to the non-identity transformation applied right to the input as seen in Table 1.\n\nIn order to really achieve their goal of extending a U-net pre-trained on adult data to both adult and pediatric images, their introduced transformation layers would need to be trained on both adult and pediatric images. But then again, the authors noted already that in this case joint training of the original U-net does best and there is no need for transfer learning.\n\nSimilarly, adding the intensity transformation layer alone also results in lower Dice overlap when compared to a U-net trained with the few available pediatric images alone.\n\nNeither transformation layer alone actually improves the results as intended. Only a combination of these manages to be perform better on the pediatric images, but worsens results on adult images. Also, this turns out to be just slightly (possibly insignificantly) be better than fine tuning for either age group. The minor difference might be rather explained by a more ill-posed optimization problem in case of fine tuning given the significantly larger number of parameters compared to the small pediatric training set.\n\nGiven above criticism of the overall approach, I would encourage the authors to better motivate it. As it stands, I cannot see a true benefit of the suggested methodology.', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		HyxhwW_6k4	Hyem1Kfam4	MIDL.io/2019/Conference/-/Paper9/Official_Review	[]	3		['everyone']	HyxhwW_6k4	['MIDL.io/2019/Conference/Paper9/AnonReviewer2']	1548720347275		1548856690388	['MIDL.io/2019/Conference/Paper9/AnonReviewer2']
384	1548676774944	{'pros': 'This paper presents a deep learning based approach to unbiased atlas construction based on LDDMM, which directly learns a diffeomorphic atlas deformation predictor from a set of images (alongside the deformable template) instead of regressing pre-computed momenta fields as in Yang et al. (2017). For this, the authors present a deep learning approach whose training replicates the unbiased atlas construction approach of Joshi et al. (2004). The maths are sound and the article is written well. The provided open source implementations are a good reference for other researchers.', 'cons': 'My main criticism is regarding the motivation of the approach. Despite the expectation set by title and abstract, the integration of atlas-building with deep learning does not actually produce a machine learning model for atlas formation. At the face of it, the authors utilise deep learning methodology to minimize an objective function for atlas creation during the training procedure. The learned model cannot be applied to create an (unbiased!) atlas from new images, though this is suggested by at least the title of this paper. What is learned, however, is a predictor of the initial momenta that maps an image to the deformable template derived during training, but the authors do not motivate such use and do not evaluate the performance of this template registration for images not used during training. With focus on the latter, the learned model should be directly compared to Quicksilver from Yang et al. (2017). In contrast to Yang et al. (2017), the proposed method does not require momenta that have been pre-computed by another algorithm (e.g., conventional LDDMM). In the discussion of closely related work, the authors do not discuss the method of Yang et al. (2017), but only in the conclusion draw a direct comparison to that method.\n\nInstead, the authors could motivate their approach through computational anatomy, where after training their method provides a deformable template that is representable for a given population, and a model that can predict the momenta of the diffeomorphisms that deform this template to new study images. In fact, the authors point this out in the conclusion: “Integration of deep learning into the atlas creation methodology promises to enable creative new approaches to statistical shape analysis in neuroimaging and other fields“. I would recommend reformulating abstract and introduction to motivate their approach in this context right from the beginning.\n\nBesides this critique, this paper is in my opinion of interest to the conference attendees and may spark some useful discussions. Following are minor remarks.\n\nThe authors write in the abstract that “the encoder network maps an image to a transformation” and “the decoder interpolates a deformable template“. I disagree with these statements. Both encoder and decoder together map an image to a transformation, not only the encoder. The authors themselves write later in the method description “notice that a diffeomorphic autoencoder amounts to a regular image encoder along with a decoder that maps from the latent space to a momentum vector field that is integrated via EPDiff to produce a  diffeomorphism”. This contradicts the statements made in the abstract.\n\nWhy have the authors only used 25 out of 990 available brain images for evaluation?', 'rating': '4: strong accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		Hkg0j9sA1V	Syek2RP3QE	MIDL.io/2019/Conference/-/Paper19/Official_Review	[]	3		['everyone']	Hkg0j9sA1V	['MIDL.io/2019/Conference/Paper19/AnonReviewer2']	1548676774944		1548856690135	['MIDL.io/2019/Conference/Paper19/AnonReviewer2']
385	1548649035918	"{'pros': 'This work builds on Automap-GAN, a framework previously proposed by the authors to directly reconstruct good quality images from corrupted k-space acquisitions. They had proven this framework to be able to remove motion artifacts in cardiac magnetic resonance (CMR) imaging. They had measured this improvement both qualitatively and quantitatively using MSE loss in image space, using artificially corrupted data. They noted that the MSE loss may not be the best loss to train with and evaluate results. For training, they introduce here an additional SSIM loss. This loss may be able to reduce the blurring effect of reconstruction using only the MSE loss. \n\nTo evaluate results, the MSE of the reconstruction is replaced by the evalutaion of the improvement for an important downstream task, i.e. semantic segmentation quality, measured by classical metrics (Dice, Hausdorff distance…). This consists the main originality of the paper.\n\nPros :\nTackles a difficult problem, i.e. images with possibly big motion artifacts, which are typically excluded from medical imaging datasets.\nThe paper investigates the interesting influence of artefact correction on segmentation quality.\nThe proposed method is compared against with a variety other standard reconstruction methods (4 in total). \nThe method is consistently better than all the others as measured by 3 segmentation metrics.\n\nThe paper is clear and easy to follow.', 'cons': ""\nThe SSIM loss was introduced after “smoothed-out” and blurred looking reconstruction images were observed in previous work by authors. Here though, there is no analysis of how the additional SSIM loss improves this matter:\n-No presentation of improvement in reconstruction metrics (i.e. MSE… which could be calculated for the artificially corrupted data), if there was some ?\n-In fact, the reconstructed images of the proposed method shown in Fig 1 and Fig. 3 still seem blurry (more so than with WIN5). The comment “the proposed method corrects the artefact but loses some structural information” from previous paper still seems to hold. No qualitative comparison of reconstructed images with and without SSIM loss is provided.\n-Improved segmentation with no improvement of image quality might not be well accepted in practice by clinicians, so it may be better that both be demonstrated. This is actually what is anticipated for future work by others.\nThe SSIM calculation is not clear. What are the “regions” x and y in this case ? Are they parts of the images around a pixel location p ? Or the whole images (consistently with the notations a few lines above) ? Is Lssim the same for all pixel, otherwise should it be averaged on the whole image - like Lmse  ? Could this be better explained ? \nA limitation of the method is the memory burden for motion correction, as acknowledged by authors.\n\nNote: it might be judicious to drop the few lines defining Dice and Hausdorff distance, which are well-known, and use this space to spend a few more lines explaining the adversarial setting, which isn't so obvious to understand.\n"", 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BkgjbQ30yN	SyxN8fWn74	MIDL.io/2019/Conference/-/Paper24/Official_Review	[]	2		['everyone']	BkgjbQ30yN	['MIDL.io/2019/Conference/Paper24/AnonReviewer1']	1548649035918		1548856689869	['MIDL.io/2019/Conference/Paper24/AnonReviewer1']
386	1548722472647	"{'pros': 'This is a very interesting and engaging paper that is a worthy contribution to MIDL. The introduction of a boundary loss is highly relevant, and it nicely ties up an intuitive sense (that errors should be weighted by a distance map) with theory. I appreciate the mathematical rigour, the clear writing, the nice motivations, and of course, tying DL together with some important theoretical insights that increasingly seem to be lost in the DL era. \n\n\n--Motivation\n\n\nA thought that the authors might find useful as an intuitive motivation: volume grows as N^3 whereas surface grows as N^2. Thus, the boundary loss helps mitigate effects of unbalanced segmentations by reducing the order of magnitude of the effect of changes in pixel values for small segmentations.  ', 'cons': ""Minor\n\n-abstract could be tightened up; getting faster to the point would make it more engaging\n\nClarity\n\n--readers would probably appreciate how you got to (4)\n\nEvaluation\n--It would have been nice to see experiments with only boundary loss. Why was this not done? Were there stability or convergence issues, or did it just not work as well? It's a curious omission, and I think readers would liek to know if the loss can operate on its own or if it only works as an auxiliary loss. At the very least, I think the authors need to address this within the text with an explanation. \n--Obviously an ablation study would be welcome, but I think for MIDL, the evaluation is sufficient. One thing I'm curious about: depending on how the distance map is calculated, e.g., with pixel distance, the boundary loss can add significant weights to each softmax, effectively increasing the learning rate. A hyper-parameter sweep on a validation set for both experiment settings would assuage any worries that the extra performance was due in part to the increased effective learning rate. Or perhaps there is a more principled way to do this. \n-Also, it would have been nice to have seen experiments with other losses, e.g., CE. "", 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'oral_presentation': ['Consider for oral presentation']}"		S1gTA5VggE	Byx-VbQp7V	MIDL.io/2019/Conference/-/Paper97/Official_Review	[]	3		['everyone']	S1gTA5VggE	['MIDL.io/2019/Conference/Paper97/AnonReviewer2']	1548722472647		1548856689615	['MIDL.io/2019/Conference/Paper97/AnonReviewer2']
387	1548723393873	"{'pros': 'This paper addresses a very interesting and relevant topic for the community; uncertainty estimation in medical image segmentation. To be able to know when to trust deep learning based image segmentation results and when to do manual inspection is extremely relevant in clinical practice and research. Doing this well is expected to improve speed, reproducibility and accuracy.\nThe authors apply this work also on a very relevant application, namely cardiac MRI image segmentation. Segmentation of cardiac MRI is done routinely in clinical practice and improvement of this workflow can therefore directly impact clinical practice.', 'cons': 'Unfortunately the paper was difficult to read at times. The English style and grammar is lacking a bit and the structure of the paper can also be improved. Also, I think the mathematical notation is excessively casual.\nThe authors suggest to segment the cardiac MRI structures with a standard UNet and then use the output of the softmax layer as a measure of uncertainty. The authors suggest that they are the first to use the output of the softmax function as a measure of uncertainty and present it as a novel idea. In my opinion, this is not a novel idea. Even more, I believe that using this method is questionable if you don\'t adapt the training specifically for this. It is well-known that large networks or often not well calibrated (i.e. the predicted probabilities don\'t actually represent true correct likelihoods) and often get ""squashed"" to 0 and 1 if no special care is taken (e.g. ""On Calibration of Modern Neural Networks\n"", Guo et al. https://arxiv.org/pdf/1706.04599.pdf). The authors don\'t say anything about this. In general, very little details are provided about the method.\nAlso, while this paper is about uncertainty estimation, the authors don\'t mention any of the literature on uncertainty estimation or segmentation accuracy estimation in medical imaging (e.g. ""Evaluating Segmentation Error Without Ground Truth"", Kohlberger et al. https://pdfs.semanticscholar.org/f470/d0a22585a221aec09c668b7fbd061fc9dc20.pdf).\nThe evaluation is difficult to follow and the results are hard to interpret. The method is not compared to any other method and no result images are shown.\n\n\n\n', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		rJe3YQqgxE	B1xcaEQpQN	MIDL.io/2019/Conference/-/Paper125/Official_Review	[]	3		['everyone']	rJe3YQqgxE	['MIDL.io/2019/Conference/Paper125/AnonReviewer1']	1548723393873		1548856689394	['MIDL.io/2019/Conference/Paper125/AnonReviewer1']
388	1548724746249	"{'pros': '- Introduction of two approaches for binary segmentation of neurosurgical instruments, one based on hand-crafted features and the other based on CNNs\n- Evaluation on both publicly available dataset (MICCAI 2017 Endoscopic Vision Challenge Dataset) and in-house neurosurgical instrument dataset', 'cons': '- The work lack of novelty and a clear presentation of two approaches, especially the baseline approach\n- The baseline approach is not clearly presented. At its current form, I cannot see the learning part. Thus, what do you mean using ""hand-crafted features""?\n- The CNNs-based approaches are based on U-net-like architectures (vanilla U-net and a replacement of encoder with VGG 16).  For a surgical application, speed and GPU memory consumption would be very important. Thus, why not thinking about using SqeezeNet-like architectures which require much smaller number of parameters? \n\n', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		BylPGrhVxN	BJxGG9maQN	MIDL.io/2019/Conference/-/Paper147/Official_Review	[]	3		['everyone']	BylPGrhVxN	['MIDL.io/2019/Conference/Paper147/AnonReviewer2']	1548724746249		1548856689180	['MIDL.io/2019/Conference/Paper147/AnonReviewer2']
389	1548725427481	"{'pros': '- This paper presents a method for the instrument recognition task from laparoscopic images, using two generators and two discriminators to generate images which are then presented to the network to classify surgical gestures. \n- The method introduces a self attention mechanism using weakly supervised labels, thereby avoiding the need to use more exhaustive annotations such as segmentations. This is an important advantage for leveraging hundreds of recorded cases without having available segmentations.\n- Overall a clearly written paper, with nice visual results.', 'cons': ""- Mainly an incremental paper, proposing a combination of well established GAN-based networks to accomplish a classification task. The different loss functions are all based on previously proposed approaches and exploited in this case for this dual background/foreground problem.\n- The presented evaluation is limited, with training done on only 8 datasets, which in this particular case is a limitation due to the importance of presenting the networks with different backgrounds from various surgical sites and perspectives during surgery. Indeed the critical factor is not to capture the instrument's appearance but rather model how variable the anatomical environment is. A more complete evaluation with different surgical scenarios would be needed to demonstrate this feature.\n- Quantitative assessment is fairly limited, and yielding underwhelming results compared to individual networks (ex. CycleGAN). It would be interesting to have the author's point of you on the less than optimal results, and how they plan to improve it."", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		rJxMvRFxeN	SJxjhnQ6Q4	MIDL.io/2019/Conference/-/Paper124/Official_Review	[]	2		['everyone']	rJxMvRFxeN	['MIDL.io/2019/Conference/Paper124/AnonReviewer2']	1548725427481		1548856688965	['MIDL.io/2019/Conference/Paper124/AnonReviewer2']
390	1548725922935	"{'pros': 'The paper presents an interesting idea of using cycle-consistent GANs for stain transfer in histopathology.\n\nthere are several minor contributions presented by the authors to the histopathology image analysis (mostly applications):\n+ applying cycleGAN for stain transfer in histopathology for segmentation task;\n+ sliding through whole image to reduce tiling artifacts (which are apparent when performing this task in tile-by-tile approach);\n+ a limited but promising cross-center evaluation. \n\nThe paper is also clearly written, the structure and the content is easy to follow.', 'cons': '-- it is not clear why Wasserstein distance is chosen as a quality assessement to measure differences between image histograms? Elaborate on this, or provide reference for this choice? Why not any other distance between histograms?\n\n-- it is also not clear why the full results of the segmentation network are not provided, only average Dice overlap between all classes is provided.\n\n-- the presented validation is probably sufficient for conference paper, but it would be very interesting to see comparison to the papers cited by the authors for stain transfer - (Shaban et al., 2018; Rivenson et al., 2018).\n\n-- the authors did only one way cross validation: AMC to RUMC transformation due to limited training segmentation, so the conclusions should be some-how scaled back to the claims that are sufficiently supported in the paper.\n\n-- no quantitative results on comparison between tile-by-tile approach and the proposed sliding through whole image approach. \n\nminor:\n - provide full form of WSI in the text (not only in abstract)\n- section 3.1: a gap between ""twenty four"" is missing\n- SSIM is not a metric in the mathematical sense, please clarify. \n- SSIM has also two parameters, provide them.\n- the authors use CycleGAN, cycleGAN - make it consistent though the paper\n\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BkxJkgSlx4	BJlioA76mV	MIDL.io/2019/Conference/-/Paper99/Official_Review	[]	3		['everyone']	BkxJkgSlx4	['MIDL.io/2019/Conference/Paper99/AnonReviewer1']	1548725922935		1548856688748	['MIDL.io/2019/Conference/Paper99/AnonReviewer1']
391	1548726440568	{'pros': 'The work proposes a deep normative modeling framework based on Neural Processes to model variation of neuroimaging measures across individuals, with the goal of deriving biomarkers of psychiatric disorders. The proposal is an alternative to the use of Gaussian processes, that are very computationally expensive and rely on parametric kernels.\n\n- The work seems very interesting and with potential for clinical data.\n- The network architecture is explained well and related to the mathematical modelling (section 2.3).\n- Different diseases are tested for the novelty detection.\n- Results seem to be much better when dealing with ADHD data, and they even localize the region responsible for it in Figure 4. It would be interesting to see the results of figure 4 with the sMT-GPTR method too.\n- The data and the codes are publicly available.', 'cons': '- The paper could benefit from a clearer writing and lighter explanations. \n- The paper presents a complex framework with a lot of detail. Although this could be an advantage, in this case it adds a level of complexity that does not help in the understanding of the paper. Sections 2.1 and 2.2 are difficult to follow.\n- One of the advantages mentioned for the use of NPs over GPs is the computational tractability. How do they compare in terms of computational cost? How does this cost change when you change the M?\n- How the data analysis is done is not clear to me.  The number of subjects used for the training is very low compared to the amount you have. Why not using more? You could cross-validate in different ways: a simple leave-one-out, leave a subset out, or if you had more data a stratified shuffle split. This is my main concern for this paper. \n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		BJxTPziyeE	rJeZ3x4TQV	MIDL.io/2019/Conference/-/Paper40/Official_Review	[]	2		['everyone']	BJxTPziyeE	['MIDL.io/2019/Conference/Paper40/AnonReviewer3']	1548726440568		1548856688529	['MIDL.io/2019/Conference/Paper40/AnonReviewer3']
392	1548727241120	"{'pros': '- This paper presents a data mining procedure that will seek to oversample structures of interest, for example for a segmentation task, while on the hand will under-sample the background in order to generate balanced datasets for training purposes.\n\n- Overall a well written paper, with clear illustrations and concise explanations of the framework.', 'cons': ""- Several sampling strategies have already been presented in the literature to address the problem of class imbalance, using a wide range of approaches from augmentation schemes, to generators and \n\n- Which liver segmentations were actually used from the LiTS for this particular problem? The provided ground-truth from experienced radiologists was mostly done for the liver lesions, so it's unclear what data was used for for liver labels and if these were indeed reliable.\n\n- Very little novelty in the approach. The method is mostly based on a U-net architecture, and proposing to implement a fairly straightforward approach for sample mining.\n\n- Class imbalance between liver and background is not as important as for example lesion vs liver parenchyma, which depending on the lesion size, can be completely overwhelmed by the background class. It's not clearly exactly what is the notation of introducing a class imbalance problems with data mining for liver segmentation."", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BkeeIavee4	S1g-R746mE	MIDL.io/2019/Conference/-/Paper117/Official_Review	[]	3		['everyone']	BkeeIavee4	['MIDL.io/2019/Conference/Paper117/AnonReviewer2']	1548727241120		1548856688316	['MIDL.io/2019/Conference/Paper117/AnonReviewer2']
393	1548728516005	{'pros': 'To classify prostate histopathology images, the authors propose a hybrid deep learning network that combines a variety of network architectures: (i) CNNs, (ii) an LSTM, and another CNN,  (iii) a pre-trained VGG network. Each network produces a feature map, and all three maps are concatenated together and then used to perform the final Gleason score classification.\n\nThe authors propose an interesting combination of diverse networks (CNN, LSTM, and VGG) to form a hybrid network to classify histopathology images.\n\nThe authors demonstrate (Table 2) how well some of the individual parts of their hybrid network contribute to the network’s performance.\n', 'cons': 'My main concern with the paper is the lack of detail provided about the experimental setup. It is unclear from the paper what data was used for training and what was used for validation? Was a cross-fold validation performed? With all the augmentation, you must ensure that some original data is set aside for testing to avoid contaminating your training data. Data contamination leads to inflated performance accuracy as neural networks readily fit to the training data.\n\nMy second main concern with the paper is that is seems like a bunch of different network types/strategies pieced together in an ad hoc manner. It is not clear why some network architecture choices were made, for instance why use an RNN to process the Shearlet Coefficients compared to using a CNN? Design justifications such as this are missing throughout the paper.\n\nSec. 2.4.4 & Fig. 5: For the CNN, do you have one network and pass two types images to train the same parameters? From the text it sounds like you have two separate CNNS, but in the figure is looks like you concatenate the two types of images (RGB and saliency maps).\n\n\nMinor Comments:\n\nSec. 2.4: Some details of CNN architecture could be clarified. You use 4x4 convolution kernels, which is a little non-standard (typically 3x3, 5x5, or another odd number is used). What is the stride size for your convolution operation? Same with pooling operations, do you stride by 2x2 to reduce the resolution by a factor of 2? Where and how many dropout layers did you use (show on Fig. 3)?\n\nSec. 2.3: Should probably cite the original GAN work:\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014). Generative Adversarial Networks. Advances in Neural Information Processing Systems 27, 2672–2680. \n\n\nGrammar/Typographical:\n\nThe paper contains numerous grammatical and typographical errors, a few of which I display below. Please carefully proofread you manuscript.\n\nAbstact: “architecture is combination” -> “architecture is a combination”\nSec 1, p3: “that employees deep” -> “that employs deep”\nFig 1: “subbdand” -> “subband”\nSec. 2, p3: “along wit” -> “along with”\nSec. 2.3, p5: “has been proved” -> proved seems too strong here, maybe “has empirically shown” is a better word choice\nSec. 2.4.3, p7: “utilized66” -> “utilized”\nSec. 3.1, p9: You should not have “&” in the text, please use “and”\n', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		BJgXUQLBgV	HJg3TuE6mN	MIDL.io/2019/Conference/-/Paper156/Official_Review	[]	3		['everyone']	BJgXUQLBgV	['MIDL.io/2019/Conference/Paper156/AnonReviewer3']	1548728516005		1548856688098	['MIDL.io/2019/Conference/Paper156/AnonReviewer3']
394	1548730729238	{'pros': 'Authors tackled with tracking of neural cells in time-lapse microscopy video. They addressed this problem as the pair of two problems: segmentation and motion estimation, and solve each problem by two different deep-neural-network approaches: BSNet and UnFlowNet. After segmentation and motion estimation, the Hungarian algorithm achieves data association. The topic about analysis of micro biomedical images are suitable for this conference.\n\nManuscript is well structured with high readability.\nThe results in Table 1 looks promising\n', 'cons': 'Proposed method does not sound technically novel.  The proposed method just replaces the basic two-step tracking approach of segmentation and motion estimation with two different deep-learning architectures: BSNet (segmentation) and UnFlowNet (motion estimation). Moreover, motivation of these replacing is weak. I didn’t understand why deep learning approach is better than non-deep-learning approach. For example, there is no fair validation between deep learning and non-deep learning computation of optical flow (with edge-based constraint , total variation and so on). In my opinion, usual optical flow computation can work for this images since the edges and blobs on images are well observed. As mentioned in computer vision filed with differential geometry, these are trackable structures on images. \n\nI think that U-Net like architecture gives state-of-the-art results for segmentation of bio and medical imaging. However, I didn’t catch why authors adopt optical flow computation on original images instead of using segmented images. At first step, they already done extraction of region of interest for tracking. Why they didn’t use directly this extracted information for motion estimation? This can be interpreted as image registration technique. Furthermore, there are already so many registration techniques for bio and medical images. \n\nThere are many unclear points about assumption of tracking. \nHow is the frame rate per second?\nHow large motion of cells between successive frames?\nHow good this data size is for deep-learning-based proposed method?\nIs there optimal preprocessing that enhance specific structure of images for tracking? \nIf the motion is too large, optical flow approach looks invalid.\nUndefined technical terms also exist. \n\nFor the evaluation of proposed method in experiments, authors adopted watershed segmentation and Kalman filter. This combination is already legacy. If authors want to show advantage of deep-leaning approach, they compare the proposed method with state-of-the-art registration methods for fair comparison.  \n', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		SJe92BmJlE	BylZu-H674	MIDL.io/2019/Conference/-/Paper30/Official_Review	[]	2		['everyone']	SJe92BmJlE	['MIDL.io/2019/Conference/Paper30/AnonReviewer2']	1548730729238		1548856687846	['MIDL.io/2019/Conference/Paper30/AnonReviewer2']
395	1548731106645	{'pros': 'This manuscript introduces a novel application of U-net. Compared to Regularized Gauss-Newton method, U-net performs much faster and has superior image quality for SCT reconstruction. The major advantage is that the deep learning approach does not need to know the prior knowledge of the scanner energy response.  ', 'cons': 'The photon counting data, the phantom are simulated and augmented for training and testing. U-net is able to learn the inverse function of the forward model, but realistic experiments need to be used for demonstration. \n\nminor comments: section 3, which is done projection (delete) by projection. ', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		HkgoYYqJlV	SkxjyXSaX4	MIDL.io/2019/Conference/-/Paper35/Official_Review	[]	2		['everyone']	HkgoYYqJlV	['MIDL.io/2019/Conference/Paper35/AnonReviewer2']	1548731106645		1548856687627	['MIDL.io/2019/Conference/Paper35/AnonReviewer2']
396	1548733228063	{'pros': 'The author(s) have proposed using 2 networks to solve the two essential components of this problem. One is purely based on the photometric appearance and the other is purely on the motion model using 2 time adjacent frames. Quite an end to end deep learning approach disentangling the problem into 2 essential parts into which it can be broken down to.', 'cons': '1. The forceful use of the 1st conv layer in BSNet to convert a grayscale image to a 3 channel image, while these images being very different from colored natural images, the network could have been trained end-to-end using single channel grayscale images only, thereby reducing the compute complexity even at inference.\n2. The reliability of motion predictions using only 2 time adjacent frames in UnFlowNet while more than 2 frames would have made it better. An exploration in this regards would have made the work better.\n3. Since the network anyways uses only a sequence of grayscale images to process, and the nature of convolutions are quite similar, what would have been the challenge if feature sharing would have existed between the networks, which would have made it computationally faster.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		SJe92BmJlE	BJx4NoBp7E	MIDL.io/2019/Conference/-/Paper30/Official_Review	[]	3		['everyone']	SJe92BmJlE	['MIDL.io/2019/Conference/Paper30/AnonReviewer1']	1548733228063		1548856687411	['MIDL.io/2019/Conference/Paper30/AnonReviewer1']
397	1548733494881	"{'pros': 'The paper presents a model for the joint learning of two segmentation tasks (brain tissue and lesion) from different datasets. The model uses an average operation to deal with the different number of input modalities. An upper bound on the expected loss for segmenting tissue is derived, which allows transferring information across tasks (i.e., from lesion to tissue segmentation), during training. Experiments on three datasets show the proposed model to offer comparable performance for both tasks, compared to task-specific models. \n\npros:\n\n- Original and principled approach to deal with tasks for which input modalities may differ.\n\n- The proposed approach is motivated by a sound mathematical framework.\n\n- Experimental evaluation on three separate datasets.   ', 'cons': 'cons:\n\n- Results are not so convincing. The multi-task network performs significantly worse than single-task models, for both tissue and lesion segmentation. Table 2 shows improvements, however these are misleading since models were trained using different datasets (and MRBrains has only 7 training subjects). Given these results, it would be beneficial to clarify the benefits of the proposed model, compared to running single-tasks models separately. If the main advantage is runtime, than experimental results should be added to support this.\n\nOther comments: \n\n\n- While interesting the derivation of the upper-bound on R^t and its estimation is a bit long. In particular, going from Eq (5) to (7) is rather straightforward and may not deserve such length in the paper. I would have preferred this space used for a deeper experimental validation.  \n\n- The average operation in the network allows dealing with a variable number of input modalities. However, it is unclear how this affects the information from different inputs. More specifically, I wonder if this forces the network to learn a ""common representation"" for T1 and FLAIR, which would make it less sensitive to when either one of these modalities is missing. How would the model perform if trained for a single task (lesion), with instances which can have missing modalities? Perhaps authors could comment on this in their paper. \n\n- Subsection ""Joint model versus fully-supervised model"" and Table 2 are hard to understand. It should be made clearer that the FS model is trained on MRBrainS18, whereas the proposed model is trained on WMH and Neuromorphics (ideally, this should be mentioned in the caption of Table 2).   \n\n- p.9 : ""shwown"" --> ""shown""; Figure 3-a --> Figure 4-a ?', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'oral_presentation': ['Consider for oral presentation']}"		HJeZW_QxxN	BkeJrnHaQV	MIDL.io/2019/Conference/-/Paper87/Official_Review	[]	3		['everyone']	HJeZW_QxxN	['MIDL.io/2019/Conference/Paper87/AnonReviewer3']	1548733494881		1548856687197	['MIDL.io/2019/Conference/Paper87/AnonReviewer3']
398	1548735273818	{'pros': 'This paper proposes to improve the performance of CNN classifiers by reducing the effects of adversarial noises through auto-encoders. The idea sounds quite interesting, and the experimental results evaluated on a variety of datasets, including both natural images and medical images, have demonstrated a better classification rate compared to original CNN classifiers. ', 'cons': 'While I think this is an interesting idea, the presented work is very similar to [1]. Besides that, I have several other major concerns based on the experimental results. \n\nFirst, the proposed method has surprisingly worse performance than the original CNN in clean image case. This also suggests that the proposed model becomes unnecessary if CNN already can achieve better results on the original image data (before artificially being perturbed). \n\nSecond, why does the classification accuracy increase at the same time when the noise perturbation is also increasing, e.g., Tab. 3 (\\epsilon = 0.05 with 32.43% accuracy vs. \\epsilon = 0.1 with 42.32% accuracy)? The author shall explain. \n\n[1] Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser, 2018.', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		rygMhHISx4	HJgMEQIaQN	MIDL.io/2019/Conference/-/Paper159/Official_Review	[]	1		['everyone']	rygMhHISx4	['MIDL.io/2019/Conference/Paper159/AnonReviewer3']	1548735273818		1548856686981	['MIDL.io/2019/Conference/Paper159/AnonReviewer3']
399	1548736569585	"{'pros': 'The paper addresses an important problem for biomedical image segmentation: lack of high quality manual labeled data. Manual labeling (segmenting) a medical image requires experts who are very expensive to collect any reasonable amount of data for machine learning models. The paper considers generation of training data using 2 unsupervised and 1 supervised classical segmentation approaches as well as a U-Net trained on randomly perturbed (augmented) dataset of baseline segmentations as an input training dataset to U-Net. The choice of competing models is definitely interesting and the models are relevant to the proposed work.', 'cons': 'Although the paper is in general well structured, the diagram in Figure 1b together with the language in the introductory sections is confused. It is not clear from the first reading whether the paper re-uses the U-Net, trained on the data produced by the algorithms of the first layer to again produce training data for itself until convergence in a circular fashion. The mention of circular approach without explaining it until page 3 makes it even more confusing as it made me think that the paper proposes some sort of a bootstrapping method.\n\nIt is not totally clear if the final model trained on the data produced by the methods being compared is indeed a U-Net. Please be more explicit about it. If this as I suspect the case, it would be interesting to discuss the fact that preparing data with U-Net the final U-Net undergoes a proxy pre-training. The results produced by U-Net are already easier for another U-Net to approximate. May be the model trained on other datasets needs more time to train?\n\nWith 10-fold cross validation and DICE obtained from each run it is possible to test whether the differences between the methods are statistically significant. Please do so.\n\nSection 5.1 What\'s described in the first paragraph is the standard 10-fold cross validation. It would be nice to named it so.\nPage 6 ""AAM performes worse"" -> ""AAM performs worse""\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		BklpVE1eg4	BklfSuL6Q4	MIDL.io/2019/Conference/-/Paper49/Official_Review	[]	3		['everyone']	BklpVE1eg4	['MIDL.io/2019/Conference/Paper49/AnonReviewer3']	1548736569585		1548856686767	['MIDL.io/2019/Conference/Paper49/AnonReviewer3']
400	1548737239915	{'pros': 'The paper applies point set classification on intra-oral scan of teeth. The proposed method includes a non-uniform resampling mechanism, and also a point-wise classification loss with an adversarial loss. \n\nThe paper is basically well written. The challenges are discussed, and the contributions are summarized. The whole paper is easy to follow. \n\nThe proposed method out-performed three state-of-the-art algorithms. ', 'cons': 'Minor comments: the two losses are not shown very well in Figure 1', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		ByxLSoblgV	HklgkoI6X4	MIDL.io/2019/Conference/-/Paper65/Official_Review	[]	2		['everyone']	ByxLSoblgV	['MIDL.io/2019/Conference/Paper65/AnonReviewer3']	1548737239915		1548856686544	['MIDL.io/2019/Conference/Paper65/AnonReviewer3']
401	1548739381803	{'pros': '- a nicely formulated system and well described approach for improving robustness of CNN classification systems\n- Multiple adversarial mechanisms have been considered', 'cons': '- The results could have been better presented - show the actual images and how different operations changed them, and the final classification output\n- Minor spelling errors\n- Space permitting, can you include results for MRI denoising (since MRI would be a better example than skin images)', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		rygMhHISx4	BJg0EmDamV	MIDL.io/2019/Conference/-/Paper159/Official_Review	[]	2		['everyone']	rygMhHISx4	['MIDL.io/2019/Conference/Paper159/AnonReviewer2']	1548739381803		1548856686329	['MIDL.io/2019/Conference/Paper159/AnonReviewer2']
402	1548741810402	{'pros': '- novel idea to localize biomarkers using a weakly supervised framework\n- fairly good experimental validations\n', 'cons': '- quantitative results of contribution of different architecture components should have been reported\n- can you explain how you ensure that in the push-pull mechanism artefacts are not introduced that could alter the classification result?', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		rJlyzeEBgN	BJlq3hvTQV	MIDL.io/2019/Conference/-/Paper151/Official_Review	[]	3		['everyone']	rJlyzeEBgN	['MIDL.io/2019/Conference/Paper151/AnonReviewer3']	1548741810402		1548856686117	['MIDL.io/2019/Conference/Paper151/AnonReviewer3']
403	1548741875048	{'pros': 'This study applies deep learning to retinal images acquired with ultra-widefield scanning-laser ophthalmoscope (UWFoV-SLO), with the aim of determining laterality and localizing both the optic disc (OD) and fovea.\n--  It appears that much prior work has been done for fundus camera images, and that one novelty of this work lies in its application to SLO data. \n-- The clinical importance of this question is well-motivated.\n-- Introductory sections provide a thorough review of previous literature on OD and fovea localization based on fundus camera images.\n\n', 'cons': '-- This paper provides relatively little context by which to evaluate the results and advances of the proposed approach, specifically with regard to OD and fovea detection:\n* There is no direct comparison of their approach against any other automated technique.\n* Observers’ ratings were provided only for CP images; none were provided for ES data. Yet, given that ES images were more challenging, it would be very useful to have some notion of observers’ accuracy on ES datasets.\n* There is no mention of the range of accuracies reported in similar studies in the literature (e.g. on fundus camera data). Currently, the Conclusion section only mentions that “similar works … on fundus camera images have achieved even better results”.\n* While the paper mentions that this is the first application of *deep learning* to SLO data for OD/fovea detection, it is unclear if other automated detection approaches (e.g., based on techniques other than deep learning) have been applied to SLO data - and if so, what levels of accuracy were attained.\n\n-- The only image pre-processing steps included downsampling, selecting the green channel, and scaling images to zero-mean and unit standard deviation. However, based on the example images shown in Figure 5, it appears that some fairly simple image processing strategies (e.g., masking out regions outside the eye) could lead to considerably better performance.\n\n--  Including more discussion of the unique challenges/properties of UWFoV-SLO data would better motivate the use of the proposed method, along with its differences from methods previously applied to fundus camera images.\n\n\n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		rke8t4Gxg4	Bklig6Pa7N	MIDL.io/2019/Conference/-/Paper74/Official_Review	[]	3		['everyone']	rke8t4Gxg4	['MIDL.io/2019/Conference/Paper74/AnonReviewer3']	1548741875048		1548856685897	['MIDL.io/2019/Conference/Paper74/AnonReviewer3']
404	1548743911910	{'pros': 'The paper proposed an interesting approach to handle the label conflicts between different brain datasets, via using a new loss function modified from the cross-entropy.\n\nThe paper is well-written and well-organized.', 'cons': 'The novelty of the paper is limited. The major contribution is a loss function, and the rest of paper adapted established works. And the overall performance is not convincing.\n\nIn the definition of the proposed loss function, it is not clear that the sum is inside of log function. Why not placing the sum outside the log function? Please further explain the loss function theoretically.\n\nThere might be issues with multi-class cross entropy function. Because the objects like tumor are very small, then the loss might be biased during training with unbalanced sampling. It is better to use weights for either cross-entropy or the proposed loss function.\n \nThe experimental results did not show significant improvement using the proposed loss function over the multi-UNet. Also, it would be nice to compare with the state-of-the-art brain tumor segmentation methods (e.g. Myronenko, A., 2018. 3D MRI brain tumor segmentation using autoencoder regularization. arXiv preprint arXiv:1810.11654) in the experiments.\n\nWhat would be the outcome when considering all the datasets (brain tissue, WMH, and brain tumor) as one scenario?\n\nPlease compare with the performance using other segmentation networks to justify the advantage introduced by the new loss function.', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'special_issue': ['Special Issue Recommendation']}		Syest0rxlN	BkeeerOTmV	MIDL.io/2019/Conference/-/Paper103/Official_Review	[]	3		['everyone']	Syest0rxlN	['MIDL.io/2019/Conference/Paper103/AnonReviewer3']	1548743911910		1548856685669	['MIDL.io/2019/Conference/Paper103/AnonReviewer3']
405	1548745125166	"{'pros': ""\t- The paper introduces some interesting ideas of combining deep models (often using in medical image analysis nowadays) with GPs in an application area that can use more focus.\n        - The clinical applicability and joint modelling between different domains (even just tackling this area is a plus) is nice\n\t- The mathematical development, although very dense (see below), is mostly well written and well defined\n\t- I think that addressing the comments below (and perhaps others reviews'), the paper can be presented at a future conference.\n\t- the figures and architecture are fairly clear, and most of the prose text is well written.\n"", 'cons': ""\t- The paper essentially builds on two frameworks - Normative models (the authors' previous work) and Neural Processes (2018). The authors do not really spend time giving an overview of these models, while neither of them are widely known that they should assume the reader is familiar with them. It makes for a very difficult read.  I tried to learn more by looking at the previous papers for these two frameworks for purposes of this review, but these should be summarized in the current paper\n\t- The mathematical development is dense and perhaps unnecessarily generalized (e.g. 2.2 development) -- while generality is certainly nice, in terms of a fit with MIDL it feels like some more intuition could have been developed alongside the technical development, and a more clear focus on a finite neuroimaging dataset. \n\t- It seems like the authors build on NPs that seem to have overlap with latent variable models (VAEs, etc), but there are not described or cited. It seems like an entire field is omitted, at least from discussion. \n\t- Building on the previous models, there are several works on deep latent models with GP priors that seem relevant, eg. Tran ICLR 2016, Casale NeurIPS 2018 (earlier on arxiv), and several others. Some of these are recent, so they shouldn't preclude the authors presenting this work, but some citations and discussion should be included, especially because the technical contribution seems important to the authors (rather than extreme/novel results)\n\t- There are several concepts introduced without clear explanation. Novelty detection (in this setting), GEVD, etc are all introduced and important in the results but not really well described.\n\t- The results are unfortunately not sufficiently convincing, with comparable behaviour to the authors' previous work. This is okay if we gain some new insight through a new method, but due to the aspects mentioned above, this is hard to obtain in this particular paper.\n"", 'rating': '2: reject', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		BJxTPziyeE	HygasK_pQ4	MIDL.io/2019/Conference/-/Paper40/Official_Review	[]	3		['everyone']	BJxTPziyeE	['MIDL.io/2019/Conference/Paper40/AnonReviewer2']	1548745125166		1548856685453	['MIDL.io/2019/Conference/Paper40/AnonReviewer2']
406	1548746314335	"{'pros': ""\t- The paper is overall well written, with fairly clear figures (although see comment below). \n\t- The authors provide pretty good motivation and citations.\n\t- The intuition is interesting -- it sounds reasonable that registration and segmentation should help each other (it has been shown in classical literature), and it's interesting to study in CNNs.\n\t- The experiments include several metrics and variance measures (a rare commodity nowadays), as well as a train/test/validation split. \n\t- Despite my evaluation (see main issues below), I think the general direction is quite promising, and encourage the authors to continue the work\n"", 'cons': '\t- The biggest issue here, and the main reason for my rejection recommendation, is that the authors seem to not evaluate the method properly, and their experimental task is not the original goal. Please correct (during rebuttal) if I have misunderstood something. It seems that the authors motivate general segmentation, but their method is in fact performing segmentation *propagation* between scans of the same subject. They compare a method that is trained to propagate a segmentation (theirs) to a method that is trained to do segmentation de novo (baseline) -- which are significantly different tasks (with the former being much easier). In effect, the first method, the propagation network, *sees* the given (test) subject\'s segmentation (of a different scan), and its task is simply to shift it -- while the segmentation network has only seen training segmentations . A more fair evaluation might be to include the original test segmentations (the ones you propagate) in the training of the seg network, and then evaluate this network on a new scan from the same subject. Alternatively, experiments on propagating segmentations with a trained registration network (of even classical methods) could be done. Unfortunately, the authors do none of these, and they seem to compare incompatible tasks.\n\t- The proposed network is a bit confusing to me -- in the middle of the propagation network, are the deformation and moving image concatenated? The text says that the network ""applies the registration result to a segmentation"", but is this really applying a transform (this doesn\'t seem like it from the figure), or applying another network (a \'decoder\')? If it\'s the latter, it\'s a bit peculiar -- why is this needed, since we know what the transform should do - so the authors could simply apply a transform layer (a la STNs, or some of the registration networks the authors cite). Perhaps I am misunderstanding something, but currently it seems counterintuitive.\n\t- Section 3.2 is a bit confusing as well, why do two images need to be synthesized, why not just one? Is this to add more variability?\nThe authors describe the need to generate ground truth deformations -- but these are ill defined and may limit the types of transforms the final network can deal with. Would it not be possible to work in an \'unsupervised\' or \'end-to-end\' fashion as many of the recent registration networks do? This can easily be combined with some synthesized data as well, of course. ', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		B1evn4blx4	H1lGLAdT7E	MIDL.io/2019/Conference/-/Paper62/Official_Review	[]	3		['everyone']	B1evn4blx4	['MIDL.io/2019/Conference/Paper62/AnonReviewer3']	1548746314335		1548856685236	['MIDL.io/2019/Conference/Paper62/AnonReviewer3']
407	1548746968291	{'pros': '1. Very good database\n2. The proposed group-attention mechanism is not only novel but also make sense. The authors successfully build an attention-based detection model in the pulmonary nodule detection.\n3. The proposed model works well in the proposed large dataset.\n4. The authors indicate the proposed group-attention  mechanism works!', 'cons': 'There may need more comparison experiments.\nThere are also some minor writing issues.', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		H1x-pmF0JE	H1xgJZt6Q4	MIDL.io/2019/Conference/-/Paper15/Official_Review	[]	3		['everyone']	H1x-pmF0JE	['MIDL.io/2019/Conference/Paper15/AnonReviewer1']	1548746968291		1548856684976	['MIDL.io/2019/Conference/Paper15/AnonReviewer1']
408	1548748297042	"{'pros': ""\t- Discrete registration is a promising classical registration method that has not been explored much in neural nets, so this seemed very exciting to me.\n\t- The paper is in general clear, although with some caveats (see below)\n\t- The technical developments do seem to improve on the author's previous methods in this space, some by quite a bit (in terms of Dice for several structures), which is promising.\n\t- The authors have some interesting development in section 2.3. Unfortunately, really understanding the insight is hard with the limited provided details (understandable given the paper length)\nThe integration of several relevant methods is interesting and warrants further investigation"", 'cons': '\t- My main concern is with the approximations made in the method, and the lack of results to convince me that that my worry is unfounded. Essentially, one advantage of discrete methods is that they can search a space of large deformations (e.g. without for example getting stuck in local minima or being too slow). However, to make the method work in a neural network, the authors approximate the discrete method using the concept in 2.3, which makes it continuous, which in a sense is needed by GD/backprop. However, by making this continuous, the method essentially does not live on just the discrete set anymore -- and enables any deformation in practice by aggregating the resulting probabilities. It seems that, although clearly different than other continuous methods (in some way that\'s hard to judge at a deep level), this is essentially continuous now -- making it unclear how much of an advantage it can have over other discrete registration methods (e.g. Hu et al, and Balakrishnan et al, which the author refers to). \n\t- Joint with the comment above, unfortunately, the authors do not compare with those other methods in the results, but instead design a (fairly unspecified)network to compare to continuous registration networks in general -- but this seems difficult to judge, since there are quite a few powerful networks out there that have shown promising results, some with available code, but are not tested here. Is the authors\' ""Regressor EDE"" comparable? Does it, for example, have sufficient conv+down-sampling layer in the registration network part to capture large fields to allow for large deformations? The encoder to regress displacments seems a bit vague. A careful specification of this model, or running one of the other cited methods seems crucial to the claims of this paper.\n\t- Figure 2 Right shows that the trained models are not really converged (they are still going up in terms of Dice -- this could mean that the author is still selling their own method short, but it also does not allow for a conclusion like which model is better. Also, the fact that one method performs better at the beginning is not quite that meaningful if in the long run one still need the same amount of epochs to converge, as is shown in Fig 2.\n\t- The technical development is a bit patchy by joining several concepts together without very good flow/intuition (at least in the text). This makes it hard to evaluate the contribution clearly. Similarly, there are several choices, eg. Hyper-parameter choices like Laplacian weights, which are set but not explained\n\t- There are several typos throughout the paper\n\t- One of the interesting aspects of the current method is that it is probabilistic, but the author omits recent probabilistic registration work -- see probabilistic registration at MICCAI 2018  and MICCAI-DLMIA 2018 for example. These are different developments completely, but should be discussed if this is an important aspect of the proposed method. Simiarly, graph Laplacians have been used to regularize registration in the models mentioned above (e.g. miccai 2018), as well as several classical models (e.g. in diffeomorphisms) but this is not mentioned or cited. I think it would strengthen the paper to discuss these methods and cite them, since the current method essentially builds on these concepts but in a different use case.\n\t- Xu et al, which is used to justify several motivating statements and results, is from 2016 and does not cover recent CNN registration developments.\nPerhaps I misunderstood, but the promise in the abstract and intro are for a general registration method, whereas the developed method requires segmentation maps of some form in practice?', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		SygkpKmglN	rylWGIKaXV	MIDL.io/2019/Conference/-/Paper89/Official_Review	[]	1		['everyone']	SygkpKmglN	['MIDL.io/2019/Conference/Paper89/AnonReviewer3']	1548748297042		1548856684758	['MIDL.io/2019/Conference/Paper89/AnonReviewer3']
409	1548749334798	{'pros': '- Introduction of a new GAN-based approach, named DavinciGAN, to address imbalance problem in surgical instrument recognition\n\n- Incorporation of background consistency loss using self-attention mechanism to encourage the transformation of only the candidate tool to the target tool while maintaining background\n\n- A comparison of the proposed approach with other STOA approaches\n\n- A discussion of self attention via weekly supervised learning and the effectiveness of background consistence loss', 'cons': '- The presentation of the methodology can be further improved.\n\n- Figure 2 is difficult to understand and a better illustration is required\n\n- {x_i}i=1, 2, 3, 4 and {y_i}i=1, 2, 3, 4 are only illustrated in Figure 2 but never used in any equation of the main text, which makes the description of the methodology difficult to read.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		rJxMvRFxeN	ryxJXcYaQE	MIDL.io/2019/Conference/-/Paper124/Official_Review	[]	3		['everyone']	rJxMvRFxeN	['MIDL.io/2019/Conference/Paper124/AnonReviewer1']	1548749334798		1548856684501	['MIDL.io/2019/Conference/Paper124/AnonReviewer1']
410	1548750228816	{'pros': 'This paper develops a new model for cortical lesions detection, with the 3D U-Net and dense cross-entropy loss.  The results seem good and the paper is easy to follow.', 'cons': 'However, there are some concerns related to this paper.\n1) The novelty of this paper is relative low, just the combination of the existing methods: 3D-Unet and dense cross entropy.\n2) The comparison in this method is quite limited. The author just listed the comparison results with 3D Unet. Although the author also mentioned the method in the literature (Fartaria et al., 2016), the experimental detail is not clear. Are you just cite the number in this paper or you implemented this method by yourself? I suggest the author provide more comprehensive comparisons.\n3) The data in this paper is quite limited. Is there any overfitting? \n4) The author should show more visual results to demonstrate the effectiveness of this method. \n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		SkxkTcMex4	rkxaqpKpmN	MIDL.io/2019/Conference/-/Paper80/Official_Review	[]	3		['everyone']	SkxkTcMex4	['MIDL.io/2019/Conference/Paper80/AnonReviewer2']	1548750228816		1548856684288	['MIDL.io/2019/Conference/Paper80/AnonReviewer2']
411	1548752874597	"{'pros': 'This paper proposed a deep learning segmentation model with the shape information. The shape information may come from the contour map or the distance map. Although the novelty of this paper is not very high, it provides some interesting ideas for integrating the distance map to the segmentation.', 'cons': ""1. Table 3 presents results for different kinds of distance maps. What's the differences of results for using the contour map and distance maps and why? More explanations should be provided to make the paper clear.\n2. The author should also prepare more comparison results with state-of-the-art methods on the polyp segmentation or fundus data. "", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		SJxWBKR1gN	rJeQl_5pQ4	MIDL.io/2019/Conference/-/Paper47/Official_Review	[]	3		['everyone']	SJxWBKR1gN	['MIDL.io/2019/Conference/Paper47/AnonReviewer1']	1548752874597		1548856684076	['MIDL.io/2019/Conference/Paper47/AnonReviewer1']
412	1548754292676	{'pros': 'Authors tried locally rotation-invariant feature extraction for 3D-texture classification.  This locally rotation-invariant feature extraction has essential role for classification of soft and non-rigid medical volumetric data.  In the proposed method, authors designed locally rotation-invariant feature extraction by 3D steerable filter convolution and max pooling. This filter convolution is integrated to constitutional neural network (CNN). Using pre-designed kernel, that is, steerable filter, we can dramatically reduce the number of parameters of CNN, which should be learned in training procedure. So, this is computationally efficient even as data driven method. The proposed method sounds technically novel.\n\nIn experiments, authors evaluated their proposed method by using phantom data and real clinical data by comparing with usual 3D CNN architecture. In the evaluation with phantom data, authors clarified the relation among the number of filters, the number of direction and classification results. In the evaluation with real clinical data, they demonstrated the superiority of the proposed method. \n\nManuscript is well structured, and experimental results are convincing.     \n', 'cons': 'Just my comments. \nA figure describing which filters have strong response to input 3D texture, as an example, is welcome for visual interpretation.  \n', 'rating': '4: strong accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'special_issue': ['Special Issue Recommendation'], 'oral_presentation': ['Consider for oral presentation']}		H1gXZLzxeE	rJgpuaqTXV	MIDL.io/2019/Conference/-/Paper77/Official_Review	[]	3		['everyone']	H1gXZLzxeE	['MIDL.io/2019/Conference/Paper77/AnonReviewer1']	1548754292676		1548856683859	['MIDL.io/2019/Conference/Paper77/AnonReviewer1']
413	1548343250569	"{'pros': 'This paper proposes a model compression method for U-net by quantizing network weights and activations.\nIn general, this paper tackles an interesting problem -- quantizing U-net for image segmentation. It provides a good review of the literature on network quantization and some empirical studies of the proposed method in terms of spinal cord gray matter segmentation. However, the description of the proposed method needs to be improved for better clarity. \n\n', 'cons': ""Section 3 the proposed quantization which is the core part of this paper is rather unclear to me:\n- a ')' is missing somewhere in equation 5.\n- according to equation 7, x_f is a value in [0, 1), how is x_f quantised by equation 5?\n- it's interesting to see that the distributions of Q4.4 and Q0.4 are very different in figure S2, even though the weights are in [-1, 1] and same precisions are used for the fractional part. Any insight?\n- How are the weights initialized during training and which optimization method is used?\n\nSection 3.2.3 it's unclear whether the batch norm is used in the proposed model.\n\nSection 2.4 mentioned that the biases are not quantised. So in which layers are the biases used, and what are the treatments for these variables?\n"", 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		rketmBuel4	ryxoADLDXE	MIDL.io/2019/Conference/-/Paper118/Official_Review	[]	1		['everyone']	rketmBuel4	['MIDL.io/2019/Conference/Paper118/AnonReviewer2']	1548343250569		1548856683636	['MIDL.io/2019/Conference/Paper118/AnonReviewer2']
414	1548762363306	"{'pros': 'The paper presents a novel MR image reconstruction method that can exploit temporal redundancy in an elegant way. The authors propose to extract optical flow between consecutive time frames to gather complimentary information in reconstruction. The flow information is later used in compressed sensing to obtain the final result.\n\nThe paper is overall well written and the approach is clearly motivated. The experimental results obtained on the clinical data demonstrate the benefits of temporal analysis. ', 'cons': '- Some literature on MR image reconstruction is missing:\n1) Adversarial loss based DL-Recon approaches. Even if they are not used in benchmarking the proposed algorithm, it would be good to mention those in the introduction. \n2) Similarly, dictionary learning & sparsity based techniques can be added as well. There were examples of such approaches to exploit spatio-temporal information in k-space data: \n""Dictionary learning and time sparsity for dynamic MR data reconstruction"", TMI 2014. \n\n- Reference is required (page 2)\n""Different from traditional motion estimation algorithms which may fail in low resolution and weak contrast""\n\n- I am not sure that the model illustrated in Figure 2 actually optimises the equation (2). In other words, the connection between the equation (2) and (6-7) is not well defined. \n\n- MC(z_t, v_t) does actually depend on z_1 and z_T, this is not captured in the presented formulation. \n\n- Fig.3, why does the DC-CNN model performs significantly worse in the ES phase? Given that the DRN-wo-GRU model has no recurrent unit, I would expect DC-CNN (3D kernels and DC components) to display similar performance. How many consecutive frames are used as input in the DC-CNN model?\n\nMinor comments\n--\n\nCould you specify/display what the y-axis correspond to in Figure 3? (RMSE)? .\n\n""The number of iterations is set to be N = 3"" -> the number of recurrent (or RNN) iterations is set to be N=3.\n\nPlease include the reference for structural similarity index measure (SSIM).', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		Bke-CJtel4	H1eXb6h6XN	MIDL.io/2019/Conference/-/Paper122/Official_Review	[]	3		['everyone']	Bke-CJtel4	['MIDL.io/2019/Conference/Paper122/AnonReviewer1']	1548762363306		1548856683420	['MIDL.io/2019/Conference/Paper122/AnonReviewer1']
415	1548767135650	{'pros': 'This paper proposes a two stage architecture for the task of optic cup and disc segmentation with the final endgoal to diagnose glaucoma. The paper uses state-of-the-art architectures and the presented approach is interesting. Using CycleGAN for data augmentation is popular at the moment and the community is still learning how to apply this best.', 'cons': 'I think there are too many essentials details missing in the paper to really understand whether this paper presents something novel and teaches me something. The presented two-stage approach is promising, but with all the missing details, I cannot really judge whether the experiments are conducted correctly, and what the impact of this paper is.\n\nFor me, it is unclear how the experiments are performed. After looking up the REFUGE challenge website, I know understand that there were 1200 images available, 400 for training, 400 for off-site validation, and 400 for on-site testing. It is unclear to me what dataset was exactly used here, only the first 800, or all scans? \n\nFurthermore, there is a comparison with a standard U-Net and the Tiramisu network. How are these results obtained? Did the authors train these themselves, or is this a result on the challenge website? I assume the former.\n\nDetailed comments:\n- Reference to darknet53 is missing. Many people will know it, but the authors should add a reference.\n- How are lambda1 and lambda2 optimized? On the full set, on the validation set? IF this is done on the full dataset, that would create a bias.\n- No details on the training of the CycleGan are added to the paper. How many images from one domain, how many from the other, etc?\n', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		B1gV2yZ4g4	HygOoyR6QE	MIDL.io/2019/Conference/-/Paper143/Official_Review	[]	3		['everyone']	B1gV2yZ4g4	['MIDL.io/2019/Conference/Paper143/AnonReviewer3']	1548767135650		1548856683207	['MIDL.io/2019/Conference/Paper143/AnonReviewer3']
416	1548777103400	"{'pros': 'The paper is very well written and uses extensive experiments to back up the claims made in the paper. The main contribution of this work is a deep learning multi-organ segmentation approach for segmenting abnormal chest x-ray images. The authors address the problem of multi-organ segmentation in the scenario where expert segmented datasets for abnormal cases are generally not available. \nThe authors combine criss-cross attention networks (that provide computational speedup benefits compared with the standard attention methods) with multi-model unsupervised translation method to generate virtual abnormal C-xray datasets using the expert-segmented normal C-xray images. \nThe authors provide comparison of their method to Unet. Ablation tests showing the benefits of the criss-cross attention and the data augmentation are also provided. ', 'cons': ""While the results are very promising, the approach itself is somewhat incremental, making use of existing methods, with the exception of application of image translation using MUNIT in a new way to generate abnormal images. Also, MUNIT is an approach to model multi-modes in the data arising from different classes. Its unclear why this is the most suited approach for this work -- is this not a bit of an overkill? There aren't really that many stylistic variations when translating from normal to abnormal Chest X-rays. \n\nPerhaps including a very brief discussion on why such an approach was chosen would be helpful. "", 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		B1lpb10Ry4	Hkgvc8x0QE	MIDL.io/2019/Conference/-/Paper26/Official_Review	[]	2		['everyone']	B1lpb10Ry4	['MIDL.io/2019/Conference/Paper26/AnonReviewer2']	1548777103400		1548856682989	['MIDL.io/2019/Conference/Paper26/AnonReviewer2']
417	1548777658821	{'pros': 'The paper proposed a neural network based framework for analysis of ductal carcinoma in situ (DCIS) in breast pathological images. Multiple current approaches are utilized and compared in the paper.\n\nThe paper is well-written and well-organized.', 'cons': 'The novelty of the paper is limited. All of approaches mentioned in the paper are from established works. The main contribution of the paper is to apply these approaches in the specific application. The paper might be more suitable for clinical conferences or journals.\n\nIt is not clear how to generate segmentation masks from the results of faster R-CNN or SSD. Is it related with the generated region-of-interest? Please add necessary explanation for these two approaches.\n\nWhat is the input size for U-Net or Micro-Net? Is it critical for the final results?\n\nThe networks with multi-level output has been developed in the others’ studies. Please add the related works into the reference (e.g. Yang, D., Xiong, T., Xu, D., Huang, Q., Liu, D., Zhou, S.K., Xu, Z., Park, J., Chen, M., Tran, T.D. and Chin, S.P., 2017, June. Automatic vertebra labeling in large-scale 3D CT using deep image-to-image network with message passing and sparsity regularization. In International Conference on Information Processing in Medical Imaging (pp. 633-644). Springer, Cham.).\n\nTypo:\n“Figure 1: AAn …” => “Figure 1: An …”', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'special_issue': ['Special Issue Recommendation']}		BJlBEUUHlV	rJxm6dg0QE	MIDL.io/2019/Conference/-/Paper160/Official_Review	[]	2		['everyone']	BJlBEUUHlV	['MIDL.io/2019/Conference/Paper160/AnonReviewer2']	1548777658821		1548856682769	['MIDL.io/2019/Conference/Paper160/AnonReviewer2']
418	1548778459343	"{'pros': 'The paper presents a deep learning approach to detect and segment DCIS from histopathology images. Four different convolutional network architectures were evaluated and compared.', 'cons': 'While this is an important problem, there is nothing new in the approach. Existing methods have been reused for this application.\n\nMethods lack in detail: For instance, ""In our application, we set bounding boxes to cover different aspect ratio and size depending on the location of DCIS in the feature map."" :This suggests that there is a separate network for detecting location of the DCIS based on which an aspect ratio and size is chosen, implying these are parameters that are learned. That does not seem to be the case. Its unclear what the authors mean here.  \n\nThere are similar gaps when explaining RCNN, micro-nets etc.\n\nExperiments could be improved by adding a bit more rigour such as investigating parameter tuning, etc. ', 'rating': '1: strong reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		BJlBEUUHlV	SkgQJ2gRXV	MIDL.io/2019/Conference/-/Paper160/Official_Review	[]	3		['everyone']	BJlBEUUHlV	['MIDL.io/2019/Conference/Paper160/AnonReviewer3']	1548778459343		1548856682557	['MIDL.io/2019/Conference/Paper160/AnonReviewer3']
419	1548779109559	{'pros': '- Authors proposed clustering analysis for approximating shapes of aortic valve prostheses.\n\n- Authors proposed a representation learning method in latent space based on autoencoder for shape clustering of aortic valves, instead of pixel-based training.\n\n- It seems that the authors proposed a somewhat novel idea for a pragmatic application.\n\n- In Results and Discussion sections, the authors provide sufficient validation and discussions via observing the performance change according to the number of clusters or the structure of learning frameworks.', 'cons': '- There is an insignificant difference between the performances of comparative methods, which is probably due to the small dataset. In spite of difficulties of acquiring the additional data, it can be argued that the minimum amount of data required for training the suggested learning model should be larger than the current dataset.\n\n- In experiments, the comparison was conducted mainly on similar deep learning models. Considering the small dataset mentioned above, it could be enough to construct the conventional feature-based clustering model via extracting the classic shape features, e.g. curvature and convexity, etc. Additional comparison with this conventional feature-based model would be better.\n\n- It would be also better to provide the further visual analysis of whether the shapes of same cluster data are actually similar and the shapes of different cluster data are actually different.\n', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		BJesxFsA1N	HyeRDAlRm4	MIDL.io/2019/Conference/-/Paper18/Official_Review	[]	2		['everyone']	BJesxFsA1N	['MIDL.io/2019/Conference/Paper18/AnonReviewer1']	1548779109559		1548856682341	['MIDL.io/2019/Conference/Paper18/AnonReviewer1']
420	1548782032155	{'pros': 'This paper is well written and easy to understand. The authors apply neural network for SCT decomposition and compared with the traditional RGN methods. The experiments show that both the image quality and the reconstruction speed are improved.', 'cons': 'The author did not explain clearly why validating the proposed method with experiments on Phantom, not on real data. Also, it should also try cross-validation in the experiments. \n', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		HkgoYYqJlV	r1xuCF-07E	MIDL.io/2019/Conference/-/Paper35/Official_Review	[]	3		['everyone']	HkgoYYqJlV	['MIDL.io/2019/Conference/Paper35/AnonReviewer1']	1548782032155		1548856682128	['MIDL.io/2019/Conference/Paper35/AnonReviewer1']
421	1548782635598	"{'pros': 'The paper talks about an interesting topic; how do we use state of the art deep learning methods if we have limited manual annotations? The authors try to tackle this by fusing (with STAPLE) existing automatic segmentation algorithms and training a pseudo 3D Unet CNN with that. They subsequently fine tune the results using a small amount of manually annotated data.\nThe authors evaluate on three publicly available datasets and compare against eight published segmentation algorithms. They show improved performance against virtually all of the existing methods on all datasets. I think the paper is generally well written.', 'cons': 'The structure of the paper is a bit confusing at points. For example, why include the paragraph about the U-net segmentation in the ""Materials"" section? Also, a lot of details of the method are missing. What was the architecture of the Unet and specifically, how was the fine tuning performed? The authors also didn\'t mention a paper of last year\'s MIDL where a method for a very similar application was proposed: ""NeuroNet: Fast and Robust Reproduction of Multiple Brain Image Segmentation Pipelines"", Martin Rajchl et al. (https://arxiv.org/pdf/1806.04224.pdf). The authors should at least discuss this paper, but preferably compare with it.\nI also doubt about the validity of the validation strategy. If you fine tune on a specific dataset, you expect that results will be better on that dataset. I\'m not sure how I should read the results because of that.\n\nBecause of the lack of novelty in the method, the lack of theoretical motivation and the questionable evaluation, I don\'t think there is enough for the community to learn from this paper.', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		Syen8HNxeN	H1eN43ZAXE	MIDL.io/2019/Conference/-/Paper95/Official_Review	[]	3		['everyone']	Syen8HNxeN	['MIDL.io/2019/Conference/Paper95/AnonReviewer3']	1548782635598		1548856681907	['MIDL.io/2019/Conference/Paper95/AnonReviewer3']
422	1548784199478	"{'pros': 'The authors proposed  the use of sparse annotation data to train a U-net architecture for semantic segmentation of histopathology images.   This is an interesting problem, since the training of deep neural networks usually requires a large amount of labeled images and the process of labelling is very time consuming. ', 'cons': ""The main problem of this paper is the weakness of the experiments. The method is validated only on 5 test images. In my opinion, this is also the reason of the instability of the results of Table 2 when the two different types of balancing strategies are compared.  \nIt follows my detailed comments:\n-- In the introduction, there should be more references to other segmentation approaches that use sparsely annotated data.\n-- Why the percentage of pixels for each class change between the sparse and the dense annotations in Table 1? Why for some classes it increases and for some decreases? I think for a fair comparison between dense and sparse annotations the authors should keep these ratios more similar. \n-- Why the authors didn't apply a cross-validation analysis? This would have validated their method in a more robust way.\n-- It is not clear if the authors use the standard or modified U-net.  What does it mean a 5 deep layer U-net? A figure of the network architecture could help the reader\n-- The authors should also show the results obtained with the dense annotated data and the two unbalancing strategies, at least for the mini-batch balancing. Although it’s true that using dense annotations could solve the instance balancing problem, I don't understand why also the problem of mini-batch unbalancing is solved.   \n-- A comparison with at least the method presented in [Xu et al., 2014] should be performed. \n"", 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		SkeBT7BxeV	H1e1LGMRX4	MIDL.io/2019/Conference/-/Paper100/Official_Review	[]	2		['everyone']	SkeBT7BxeV	['MIDL.io/2019/Conference/Paper100/AnonReviewer3']	1548784199478		1548856681689	['MIDL.io/2019/Conference/Paper100/AnonReviewer3']
423	1548786118064	"{'pros': 'Automatic segmentation model for a difficult problem (MRI of the wrist) and this seems to be a new application area for this kind of methods.\n\nThe technical approach seems to achieve, an albiet, small improvement to just applying a UNet directly.\n\nPotential for new data / open data.\n\n', 'cons': 'Novelty is limited because spatial information (manual priors) has been used in the past. \n\nIt is unclear why the angular image just uses the centre of gravity. You could bias it to incorporate more priors about the non circular/ellipsoidal nature for the wrist, e.g. principle axes of an ellipse being a simplistic case of it.\n\nTreating slices individually can obviously be improved on.\n\n', 'rating': '2: reject', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		r1lVVk3RyE	rygA6YM0m4	MIDL.io/2019/Conference/-/Paper22/Official_Review	[]	2		['everyone']	r1lVVk3RyE	['MIDL.io/2019/Conference/Paper22/AnonReviewer1']	1548786118064		1548856681469	['MIDL.io/2019/Conference/Paper22/AnonReviewer1']
424	1548787065642	"{'pros': 'Method is clearly explained and the paper is easy to follow. Overall, it seems interesting although difficult to position wrt to the state of the art in general abnormality detection in terms of novelty.\n\nPotentially interesting to use this kind of approach to overcome problems in medical imaging where labels are tricky to obtain at scale.', 'cons': 'Clinically, the results are not compelling. This makes it difficult to argue whether this approach is a ""success"" or a valuable lesson not to pursue this path.\n\nMore effort and thought is needed to consider the real physiological domain of tissue for this specific problem and to condition any generators to conform to something physically meaningful. In general, somehow biasing the GAN (or newer) approach to be compliant to the real world more accurate would be good.', 'rating': '3: accept', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		Ske2oyiye4	BklztaMRXN	MIDL.io/2019/Conference/-/Paper38/Official_Review	[]	3		['everyone']	Ske2oyiye4	['MIDL.io/2019/Conference/Paper38/AnonReviewer2']	1548787065642		1548856681254	['MIDL.io/2019/Conference/Paper38/AnonReviewer2']
425	1548796733554	"{'pros': 'The approach presents an automatic segmentation of the wrist surrounding tendons, in order to automate the manual scoring system used to assess inflammation of these tendons in rheumatoid arthritis patients.\n\n- The problem to solve is interesting and has a direct and practical application.\n- The paper is well written and easy to follow.\n- Data used is public and has ground truth.\n- The network architecture seems interesting.\n- The proposed architecture provides a higher DSC compared to using a Unet-16 (Figures 4, 5, 6), especially for certain labels.', 'cons': '- It is not clear to me why would you need to use two UNets, with the first one as a feature extraction step. Maybe alternative architectures could provide the same results with a single network.', 'rating': '3: accept', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		r1lVVk3RyE	HkxIBmr074	MIDL.io/2019/Conference/-/Paper22/Official_Review	[]	3		['everyone']	r1lVVk3RyE	['MIDL.io/2019/Conference/Paper22/AnonReviewer3']	1548796733554		1548856681038	['MIDL.io/2019/Conference/Paper22/AnonReviewer3']
426	1548802511698	{'pros': 'This paper proposed to use super pixel/voxel to do data augmentation. Authors conducted comprehensive experiments and evaluation to show the effectiveness of the method. ', 'cons': 'Technical novelty not very significant, but still it is a good study overall.', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		H1gLLOgxeE	SkgdRYICXN	MIDL.io/2019/Conference/-/Paper55/Official_Review	[]	2		['everyone']	H1gLLOgxeE	['MIDL.io/2019/Conference/Paper55/AnonReviewer2']	1548802511698		1548856680824	['MIDL.io/2019/Conference/Paper55/AnonReviewer2']
427	1548803895029	"{'pros': 'The author proposes a reasonable end-to-end trainable registration algorithm that works better that the tested baseline methods on 10 CT scans of the VISCERAL3 training dataset. I think it is a fair MIDL paper, but the presentation needs improvement.\n\nPro:\n- the final approach (compute similarity map at each key point, extend local minima with a minimum filter and smoothing, smooth over neighbouring key points for global smoothness) makes sense and seems to work quite well. \n- The whole pipeline allows end-to-end training.\n', 'cons': ""Contra:\n- The derivation for the final simple algorithm is overly complicated and I got the impression that the author tried to impress the reader with many advanced mathematical concepts, that in the end collapse to very simple operations.\n- The author claims that a min-convolution can be approximated by a minimum filter with subsequent smoothing. This only for a certain distance between the local minima. For the desired effect (give displacements close to a sharp local minimum also good scores) the min-convolution is not necessary and can be completely removed from the paper -- making it much more readable and easier to understand\n- The same critic applies for the message passing, which collapses to a simple smoothing operation (averaging over the 15 neighbouring key points) on the graph. Again the smoothing makes sense, but can be explained much simpler.\n- the experiments are somehow limited (only 10 images, cross validation, same images used in training and test set -- only specific pairs were left out in training)  and the performance metric (evaluation only at the key points) is heavily biased.\n- The algorithm should be applied to an independent test set of images. \n- I don't see the motivation for the irregular spaced key points. The costs for a thin-plate-spline interpolation to obtain a dense displacement field are much higher than using regular spaced key points on a regular grid. A regular grid would also make the smoothing of the displacement field faster and easier.\n- in Figure 2 the comparison to CNN + CNN is missing. \n- in paragraph 2.3 the definition of R(di, dj) seems to contain a typo. Guess this should have been ||di - dj|| \n\n\n"", 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		SygkpKmglN	rye1BkwAmV	MIDL.io/2019/Conference/-/Paper89/Official_Review	[]	2		['everyone']	SygkpKmglN	['MIDL.io/2019/Conference/Paper89/AnonReviewer1']	1548803895029		1548856680608	['MIDL.io/2019/Conference/Paper89/AnonReviewer1']
428	1548814446962	"{'pros': 'This paper addresses a class of difficult problems MRI neuro image analysis, which is the detection and localization of small anomalies (such as parivascular spaces and lacunes) in MRI.   The features are important in studies/analysis of, e.g., cognitive decline with aging (e.g. ""vascular dementia"").    The paper is moderately well written (with a few grammar problems), clear.  The evaluation is only moderately strong.   ', 'cons': 'This is an application of a well-known neural-net architecture (RCNN).  The authors extended the NN to 3D (but little is discussed on that topic) and there were some challenges in messaging the training data (e.g. dealing with asymmetries in numbers of examples), multirater ground truth, and a few other details.   Overall this is a straightforward application with a moderate evaluation, suffering from a small sample size, and a lack of evaluation of different design choices. ', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		BJlXpDh1gV	ByewdOKAQE	MIDL.io/2019/Conference/-/Paper44/Official_Review	[]	3		['everyone']	BJlXpDh1gV	['MIDL.io/2019/Conference/Paper44/AnonReviewer3']	1548814446962		1548856680398	['MIDL.io/2019/Conference/Paper44/AnonReviewer3']
429	1548814574740	"{'pros': 'The paper reports the first exploration of the network quantization of the the now classic U-net.\n\nNetwork quantization is an interesting topic and could prove useful for embedded devices with limited devices.\n\nExploration of such ideas may be of great value.\n', 'cons': 'The reported ""fullprecision"" U-net has very poor accuracy and it is not entirely clear how much value there is in being able to replicate such results. Moreover, how well would a quantized U-net be able to replicate a good/excellent ""fullprecision"" U-net.\n(By poor results I am referring to the DIce scores below 0.6.)\n\nThe case for inclusion at MIDL is not compelling.\n\n\nThe paper requires many improvements before it should be considered for inclusion. Such as the right hand side of Fig. 1, showing the same slice (on the same subject) for each of the various quantization levels. The images should also be bigger it is currently impossible to discern the general quality of any of the results.\n\nThe notation could be improved Q_{fp} could simply be Q, the fp is superfluous.\n\nThere are numerous spelling and grammar errors: ""stat-of-the-art"" for example.\n\nThere are several citations of papers from arXiv that are two plus years old, which do not have a peer-reviewed version. Such citations should be avoided, as peer-review is the best mark of research accuracy/quality/scientific merit/et cetera.\n\n\nAn important question that is not mentioned and should be considered: Can segmentation based U-nets be quantized (effectively) because the problem being solved really has a low dimensional embedding? With more complicated tasks being unable to benefit from such computational trickery?', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		rketmBuel4	HJxDeFY0X4	MIDL.io/2019/Conference/-/Paper118/Official_Review	[]	3		['everyone']	rketmBuel4	['MIDL.io/2019/Conference/Paper118/AnonReviewer4']	1548814574740		1548856680180	['MIDL.io/2019/Conference/Paper118/AnonReviewer4']
430	1548815170428	"{'pros': 'The paper is moderately well written (mostly comprehensible) with a few grammatical errors (e.g. ""much lesser"").  The problem is interesting/challenging and important.     The evaluation included some results on interpreting the network processing, which is rare in this field and especially interesting.  ', 'cons': 'The authors refer to ""our model"", but I could not understand what ""their model"" was or how it was any different from a typical image classification NN.   They talk about ""their model"" vs ""Alexnet"", but aren\'t they really just comparing ResNet-34 against alternatives.    I could not figure out the logic or contribution (other than the evaluation), but authors seem to be claiming some novelty.   If this is just an evaluation of alternative, conventional NN architectures to this problem, then why not say so?\n\nI was also bothered by the use of the ImageNet pretrained weights for the feature extraction layers of this network.   Sure, low-level feature descriptors make sense when moving between similar images (e.g. photographs or photographs of different animals), but the image status of the PapS\'s are completely different from those of cat photographs.   This decision should have been evaluated.   Why not train the feature layers on PapS images from scratch.   I think we would have learned something from that experiment.  ', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		S1lrzR0gg4	Hkx5HotCmE	MIDL.io/2019/Conference/-/Paper137/Official_Review	[]	3		['everyone']	S1lrzR0gg4	['MIDL.io/2019/Conference/Paper137/AnonReviewer3']	1548815170428		1548856679954	['MIDL.io/2019/Conference/Paper137/AnonReviewer3']
431	1548815914828	"{'pros': 'Authors propose an image resampling based approach to do data augmentation (motion artifacts) for MRI images, based on which, the trained deep segmentation network shows better performance on artifacted data. The paper is well written. The experiments are well done.', 'cons': '1.\tSection 3.1. How and why to overfit the model in training? Usually overfitting means low generalization ability.\n2.\tBesides ‘classic’ augmentation, authors may want to compare the motion-based augmentation with some other basic techniques, like median filter to simulate the blurring artifact.\n3.\tThere may exist some previous approaches about generating MRI motion artifacts, authors need to discuss the novelty/difference of the proposed method against them. In addition, it looks like there is a similar approach of motion-based augmentation for deep learning based MRI segmentation:\n-\tAndersson, Erik, and Robin Berglund. ""Evaluation of Data Augmentation of MR Images for Deep Learning."" (2018).\nIt’s better to compare with it or discuss about it somewhere in the paper.\n', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		ryxqlJzge4	SkeX4RKC7E	MIDL.io/2019/Conference/-/Paper67/Official_Review	[]	3		['everyone']	ryxqlJzge4	['MIDL.io/2019/Conference/Paper67/AnonReviewer3']	1548815914828		1548856679735	['MIDL.io/2019/Conference/Paper67/AnonReviewer3']
432	1548817435186	"{'pros': ""The paper describes a deep learning (DL) framework to segment potentially cancerous areas in prostate biopsies using semi-automatically generated data. To minimize manual effort from pathologists and reduce inter/intra observers' variety, glandular tissue WithOut Basal cells (WOB) class is applied in the semi-automated data generation process. An evaluation on 63 biopsies was conducted and demonstrated the effectiveness generated data for training a DL model.\n\n-The paper is well written and easy to understand.\n-The topic is relevant and interesting. \n-The proposed method was thoroughly evaluated on clinical biopsy data. \n"", 'cons': '- The work utilizes data annotated using different strategies, acquired from different scanners and has different characteristics (e.g. different Gleason scores). I would suggest authors provide a table to clearly summarize all the information about the datasets.\n\n- What are the Gleason scores (GS) of the training and testing data? What would be the performance for data with different GS? I would suggest the authors to also provide an analysis on data of different GS individually.\n\n- Despite an interesting topic and practical solution, the technical novelty of this work is somewhat limited.  ', 'rating': '3: accept', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		SJxVA7xleE	r1lXX4qA7V	MIDL.io/2019/Conference/-/Paper53/Official_Review	[]	3		['everyone']	SJxVA7xleE	['MIDL.io/2019/Conference/Paper53/AnonReviewer2']	1548817435186		1548856679511	['MIDL.io/2019/Conference/Paper53/AnonReviewer2']
433	1548849368853	{'pros': '- A clustering method based on features from a convolutional autoencoder is proposed to define clusters of similar aortic valve prosthesis shapes. \n- Interesting application and well described methodology.', 'cons': '- It took me a while to realise what kind of imaging was used. This could be more clear from the abstract and/or title.\n- It would be good to have a more visual representation of the quality of the results, now only figures of the Jaccard coefficient and Hausdorff distance are shown.\n- Listing the resolution in mm/pixel instead of pixel/mm would be more intuitive.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		BJesxFsA1N	rylWybGJEV	MIDL.io/2019/Conference/-/Paper18/Official_Review	[]	3		['everyone']	BJesxFsA1N	['MIDL.io/2019/Conference/Paper18/AnonReviewer3']	1548849368853		1548856679292	['MIDL.io/2019/Conference/Paper18/AnonReviewer3']
434	1548850027623	"{'pros': 'The authors present an approach to classify patches of WSI into six different tissue classes using pretrained VGG networks. The paper proposes to use a multi-scale approach that classifies each patch by evaluating it on multiple magnifications with different context regions. The authors conclude that including the multiple scales improves the performance of the model and show experiments to support those conclusions.\n\nThe paper is well written and easy to read. The training details are easy to find and the algorithmic setup seems easily reproducible. The approach of integrating multiple scales seems like a reasonable and memory efficient approach to integrate more context into the model. The range of parameter tuning for the proposed architecture. The proposed method achieves convincing results.', 'cons': 'The novelty of this method is relatively limited and this approach is very close to [1]. Further, I am not convinced that the experiments provide enough evidence that this multi-scale approach is better than the single-scale approach or whether the benefits in performance stem from integrating more visual context. It would be beneficial to include experiments that train only on 25x and 100x or test the classification of bigger patches at 400x to integrate more context. Without those experiments it is not clear where the improvement is coming from.\n\n- Specifics about data normalisation are missing.\n\n- If I understand Table 1 correctly, the test set might only contain 3 whole-slide images. Given that number it would have been nice to see an evaluation that is rather based on cross-validation that tests multiple splits of the data.\n\n- The experimental section mentions, that the presented results show the best configuration for the specific methods. Have those been selected on the test set or according to validation set performance? If simply the best test performances are reported, there might be a selection bias based on a lucky optimisation. It would have been valuable to run multiple seeds of those experiments - this should not be too expensive, as the global features of each patch can be precomputed because of the fixed weight VGG making this a learning problem of a relatively shallow fully-connected network.\n\n- Minor: Did you check that your model had converged after 50 epochs? Adding learning curves to an appendix could have clarified that.\n\n- There are minor grammatical errors.\n\n[1] Sirinukunwattana, Korsuk, et al. ""Improving Whole Slide Segmentation Through Visual Context-A Systematic Study."", MICCAI 2018', 'rating': '2: reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		rJlDYnoJlN	HJgE_mM1NN	MIDL.io/2019/Conference/-/Paper42/Official_Review	[]	3		['everyone']	rJlDYnoJlN	['MIDL.io/2019/Conference/Paper42/AnonReviewer4']	1548850027623		1548856679062	['MIDL.io/2019/Conference/Paper42/AnonReviewer4']
435	1548850588336	{'pros': '- The authors propose dedicated loss functions to train CNNs for segmentation tasks based on sparsely annotated data.\n- Clearly described methods which could be applied in a wide range of tasks as manual annotations are always difficult to obtain in medical imaging.\n-  Comparison with densely annotated images shows comparable results.', 'cons': '- Overall there seems to be a slight improvement over just using a mask of the annotated pixels, but this improvement is not clear for all segmentation classes. In figure 3 it can also be clearly observed that the improvements are very different for different tissues, which makes it difficult to evaluate the approach.', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}		SkeBT7BxeV	BkgNoSMyN4	MIDL.io/2019/Conference/-/Paper100/Official_Review	[]	3		['everyone']	SkeBT7BxeV	['MIDL.io/2019/Conference/Paper100/AnonReviewer2']	1548850588336		1548856678815	['MIDL.io/2019/Conference/Paper100/AnonReviewer2']
436	1548852717395	{'pros': 'This paper proposed a method for labeling bright-field images using three types of convolutional neural networks, i.e., U-Net, Tiramisu model, and Deeplabv+3 model. The experiments were performed on 170 2D images.\n', 'cons': '1. The main concern is the novelty of the proposed method, since the authors simply tested three types of CNN for segmentation and no new methodology is proposed here. Why using these three networks and not the others?\n\n2. Besides, the paper is hard to follow. For instance, in the experimental setting, there are two similar sentences: ”The data was split into 153 training images and 17 validation images” and “The data was randomly split into 136 training images and 44 for validation.” From the perspective of a reader, it is not clear at all. \n\n3. The same issue occurs in Table 1 and Fig. 2, where no clear explanation to present the difference between the method “Deeplabv3+” and “Deeplabv3+ (cross validation)”.', 'rating': '2: reject', 'confidence': '3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		Sygt37F21E	SJgrlAf1N4	MIDL.io/2019/Conference/-/Paper6/Official_Review	[]	3		['everyone']	Sygt37F21E	['MIDL.io/2019/Conference/Paper6/AnonReviewer3']	1548852717395		1548856678590	['MIDL.io/2019/Conference/Paper6/AnonReviewer3']
437	1548852796924	{'pros': 'This paper presented propose a lung segmentation framework for chest X-rays, including a criss-cross attention based segmentation network and a radiorealistic chest X-ray image synthesis process for data augmentation. Experiments were performed on multiples datasets. The proposed method sounds reasonable, and the manuscript is easy to follows.', 'cons': '1) The main concern is that the experimental results seem to be not strong enough. For instance, the authors simply compared their method with U-Net, while there are many other deep learning methods for segmentation. \n\n2) Besides, there is no comparison between the proposed method and the method that does not use the attention module. \n\n3) In addition, as shown in Table 2, the U-Net_A4 achieves better results than U-Net_R and U-Net_R+A3, which suggesting that using only the constructed images is better than those real images. No explanation is given in the manuscript.  ', 'rating': '3: accept', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct', 'special_issue': ['Special Issue Recommendation']}		B1lpb10Ry4	HJgBS0GkEV	MIDL.io/2019/Conference/-/Paper26/Official_Review	[]	3		['everyone']	B1lpb10Ry4	['MIDL.io/2019/Conference/Paper26/AnonReviewer1']	1548852796924		1548856678371	['MIDL.io/2019/Conference/Paper26/AnonReviewer1']
438	1548855903733	"{'pros': '- The authors shown a new method to deal with class imbalance by adding a new loss that will force the network activation into a previously labelled ROI.\n\n- They make a clever use application of a visualization technique ( Grad-CAM) into the learning process.\n\n- They make a pretty good validation on their technique and comparison with other approaches to deal with class imbalance.\n\n- It is a well written and well presented paper.', 'cons': '- I am not sure the technique should be called attention since it is fully supervised.\n\n- The authors claim that their selection of bbox or alpha parameters do not change the final result. This is clearly not the case on the Recall of the pneumonia dataset.  I think this is likely because in the skin cancer dataset their CARE method does not make a huge improvement with respect to the other augmentation methods (table 1) since in most of the images are centre around the area of interest anyway (as shown in Fig 2). However, in the pneumonia dataset selecting relevant ROI makes a lot of difference because the lesions are multiple and interspaced around the image. Therefore they will be more affected with by the ROI selected or the weight in the attention loss.', 'rating': '3: accept', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		BJl2cMBHlN	HyxOP5mJ4N	MIDL.io/2019/Conference/-/Paper153/Official_Review	[]	3		['everyone']	BJl2cMBHlN	['MIDL.io/2019/Conference/Paper153/AnonReviewer2']	1548855903733		1548856678139	['MIDL.io/2019/Conference/Paper153/AnonReviewer2']
439	1548856458735	"{'pros': '- Cross-modality registration is a relevant and challenging application.\n\n- Learning a shared modality-agnostic feature space and using segmentations to derive a weak supervisory signal for registration seems like an interesting and promising approach.\n\n- Although I cannot recommend acceptance at this stage, I feel the central idea has merit and should be pursued further.', 'cons': 'This work feels very preliminary and I believe the manuscript can be made much stronger by addressing the following issues for a future submission:\n\n==== Method ====\n- Fundamental assumption for the approximation in Eq. (1) is that displacements are small, e.g. sub-pixel scale. While it may be reasonable for computing (potentially multi-scale) optical flow between consecutive video frames, this condition seems unjustified for the sorts of deformations expected in intra-subject registration (e.g. Fig. 3)---even assuming feature brightness consistency. If this linearisation approach is in fact ""widely used"" in this context, please cite the relevant references backing the claim.\n\n- Otherwise, is the approximation applied iteratively as the moving image is deformed and resampled with small incremental displacements? If this is the case instead, I strongly suggest the authors clarify Section 2.1.\n\n- What is Delta(u,v)? Although it is a crucial element of the proposed pipeline, the actual output of the B-Spline Descent module is never properly defined. The paper could greatly benefit from a clear algorithmic description of all the steps involved.\n\n- What is the dimensionality of the feature maps fed into B-Spline Descent (M and F, and also the corresponding SDMs)? If it is greater than one, the equations as they are written are incorrect (see next point). The exposition can be made much clearer by defining the dimensions of all the variables.\n\n- Improper mathematical notation: x is undefined; some terms are missing x as an argument; unconventional partial derivative notation; missing energy summation over the coordinates (and feature dimensions?). I also suggest the authors switch to matrix notation for a clearer and more general formulation.\n\n- Careful with claims of ""disentanglement"", as it can lead to mischaracterisation of the present contribution. The authors can say the pipeline *decouples* a feature learning step from a deformation estimation step, but this is not a representation learning method (nothing wrong with that), so I\'d also be wary of relating it to Shu et al. (2018), for example.\n\n==== Evaluation ====\n- The dataset description needs more information. How many distinct subjects are there and how many scans of each? Are the scans paired across modalities? Are they healthy or pathological cases?\n\n- Unclear why the authors used only 10 scans per modality, when the dataset seems to provide many more annotated scans for the chosen structures (http://www.visceral.eu/assets/Uploads/Anatomy-3-Segmentations.pdf).\n\n- Please clarify the 3D pre-registration step with deeds-SSC. Is it rigid/affine or deformable? What is used as the alignment target?\n\n- Especially with such small sample size, the averages in Table 1 mean very little without error estimates. I suggest the authors tone down the claims of ""significant improvements"" until more rigorous experimental analysis can be performed.\n\n- Missing baselines: How does the full method compare to the purely unsupervised B-Spline Descent? This experimental comparison would make the argument for external supervision much stronger, and would be a fairer competitor to the MIND descriptor. Furthermore, comparison to a traditional pairwise iterative registration method with multi-modal cost function (e.g. mutual information-based) would be greatly informative.', 'rating': '1: strong reject', 'confidence': '2: The reviewer is fairly confident that the evaluation is correct'}"		SkeI0-QelE	rJx792m1EN	MIDL.io/2019/Conference/-/Paper84/Official_Review	[]	3		['everyone']	SkeI0-QelE	['MIDL.io/2019/Conference/Paper84/AnonReviewer4']	1548856458735		1548856677899	['MIDL.io/2019/Conference/Paper84/AnonReviewer4']
440	1548852078404	{'title': 'Relevant contribution for humans-in-the-loop in KB construction', 'review': 'The paper presents a novel solution to an interesting problem - when KBs are automatically expanded user feedback is crucial to identify incorrect and missing entity attributes and relations. More specifically, in the case of entity identity uncertainty, enabling the user feedback as part of the entity resolution mentions, appears novel and important. \nThe paper is well written and organized. \n\nPoints for improvement:\n- It would be interesting to see more in-depth analysis on examples where the proposed approach fails, and based on this to also outline open issues and future work. \n- The human computation aspects of the paper are lacking sufficient explanation in terms of implementation in real settings, as well as positioning with related work in human computation research\n- it would be interesting to know, what is the experimental design that authors would consider for an evaluation with actual user feedback vs. the simulated one', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		SygLHbcapm	BkeLdizyEN	AKBC.ws/2019/Conference/-/Paper51/Official_Review	['AKBC.ws/2019/Conference/Paper51/Reviewers/Unsubmitted']	3		['everyone']	SygLHbcapm	['AKBC.ws/2019/Conference/Paper51/AnonReviewer3']	1548852078404		1548852405913	['AKBC.ws/2019/Conference/Paper51/AnonReviewer3', 'AKBC.ws/2019/Conference']
441	1548849177708	{'title': 'Quantum Annealing as an Optimized Simulated Annealing: A Case Study', 'authors': ['Aasish Kumar Sharma', 'Pradip Maharjan'], 'authorids': ['aasish.sharma@ncit.edu.np', 'pradip.maharjan@gmail.com'], 'pdf': '/pdf/092c5ef6c6eecefbac42a47cd0c0b858fdca82b0.pdf', 'paperhash': 'sharma|quantum_annealing_as_an_optimized_simulated_annealing_a_case_study'}		SJlzQxGk44	SJlzQxGk44	OpenReview.net/Archive/-/Direct_Upload	[]	207		['everyone']		['~Aasish_Kumar_Sharma1']	1548849177708		1548849177708	['~Aasish_Kumar_Sharma1']
442	1538087793118	{'title': 'Double Viterbi: Weight Encoding for High Compression Ratio and Fast On-Chip Reconstruction for Deep Neural Network', 'abstract': 'Weight pruning has been introduced as an efficient model compression technique. Even though pruning removes significant amount of weights in a network, memory requirement reduction was limited since conventional sparse matrix formats require significant amount of memory to store index-related information. Moreover, computations associated with such sparse matrix formats are slow because sequential sparse matrix decoding process does not utilize highly parallel computing systems efficiently. As an attempt to compress index information while keeping the decoding process parallelizable, Viterbi-based pruning was suggested. Decoding non-zero weights, however, is still sequential in Viterbi-based pruning. In this paper, we propose a new sparse matrix format in order to enable a highly parallel decoding process of the entire sparse matrix. The proposed sparse matrix is constructed by combining pruning and weight quantization. For the latest RNN models on PTB and WikiText-2 corpus, LSTM parameter storage requirement is compressed 19x using the proposed sparse matrix format compared to the baseline model. Compressed weight and indices can be reconstructed into a dense matrix fast using Viterbi encoders. Simulation results show that the proposed scheme can feed parameters to processing elements 20 % to 106 % faster than the case where the dense matrix values directly come from DRAM.', 'keywords': ['quantization', 'pruning', 'memory footprint', 'model compression', 'sparse matrix'], 'authorids': ['daehyun.ahn@postech.ac.kr', 'dslee3@gmail.com', 'taesukim@postech.ac.kr', 'jaejoon@postech.ac.kr'], 'authors': ['Daehyun Ahn', 'Dongsoo Lee', 'Taesu Kim', 'Jae-Joon Kim'], 'TL;DR': 'We present a new weight encoding scheme which enables high compression ratio and fast sparse-to-dense matrix conversion.', 'pdf': '/pdf/a8a7bfd3d4b3f94f71c2da2d846d7bfdc04dfa6c.pdf', 'paperhash': 'ahn|double_viterbi_weight_encoding_for_high_compression_ratio_and_fast_onchip_reconstruction_for_deep_neural_network', '_bibtex': '@inproceedings{\nahn2018double,\ntitle={Double Viterbi: Weight Encoding for High Compression Ratio and Fast On-Chip Reconstruction for Deep Neural Network},\nauthor={Daehyun Ahn and Dongsoo Lee and Taesu Kim and Jae-Joon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkfYOoCcYX},\n}'}		HkfYOoCcYX	HkfYOoCcYX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	374	S1ldXiDcYQ	['everyone']		['ICLR.cc/2019/Conference']	1538087793118		1548844081577	['ICLR.cc/2019/Conference']
443	1546504119546	"{'title': 'Interesting task. The sentence simplification method and dataset look useful, but lack comparison with other methods.', 'review': '\n== Summary ==\nThe paper proposes a system that takes a screenplay script and generates animation based on the actions and descriptions in the script. The process includes (1) extraction of relevant sentences, (2) simplification/decomposition of complex sentences into simpler ones, (3) semantic role detection (""ARFs""), and (4) conversion to animated scenes. The most involved step is the simplification process, which uses several syntax-based rules tailored toward screenplay sentences. The paper also presents a dataset of ~ 1000 sentences annotated with multiple simplication and semantic roles.\n\n== Pros ==\n- The task of generating animation from text is an interesting one. While this is not a new task, it extends the capabiity of the original system (CARDINAL) to handle complex sentences.\n- The simplification method is described in detail and looks useful for information extraction in other domains.\n- The evaluation set will be useful for other researchers on information extraction and semantic role detection of complex sentences.\n\n== Cons ==\n- The major weakness of this paper is that it lacks quantitative and qualitative comparison with previous methods. The paper claims that previous sentence simplification methods do not work well, but it would be much more convincing to compare those systems with the one proposed in the paper. Otherwise it is unclear if the proposed method works better and should be adopted. The same goes for other modules such as the script parser.\n- If my understanding is correct, the simplification method drops timing information (e.g., ""X before Y"" and ""X after Y"" are both simplified as X + Y), which can be crucial for generating animations.\n- The evaluation on semantic roles (ARFs) is the most trustable one and should be more standardized. Currently the scores are affected by issues such as annotation inconsistencies and ambiguous answers (e.g., annotators write short answers for the object field, which penalizes systems that produce the more complete mentions).\n\n== Neutral ==\n- The BLEU score evaluation is not that great, but it is probably the best we have for evaluating sentence simplification.\n- The extrinsic evaluation is greatly affected by the incompleteness of the animation module, but can be fixed by adding more features to the animation module.', 'rating': '6: Marginally above acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		Hkg5zW96p7	rkgl6DrsWN	AKBC.ws/2019/Conference/-/Paper36/Official_Review	['AKBC.ws/2019/Conference/Paper36/Reviewers/Unsubmitted']	1		['everyone']	Hkg5zW96p7	['AKBC.ws/2019/Conference/Paper36/AnonReviewer3']	1546504119546		1548832397217	['AKBC.ws/2019/Conference']
444	1546653289008	"{'title': 'OK work, but the framing makes it sound trivial', 'review': '\n== Summary ==\nThe paper proposes a way to embed the relation between a given pair of words (e.g., (aquarium, fish)).\n- Assume a dataset of |R| relations (e.g., hypernym, meronym, cause-effect) and many word pairs for each relation.\n- In each of the 10 test scenarios, 5 relations are randomly chosen. The test procedure is: for each pair (a, b) with embedding r(a, b), rank (c, d) based on cosine(r(a, b), r(c, d)). Evaluate how well we can retrieve pairs from the same relation as (a, b) (top-1 accuracy and mean average precision).\n- The proposed method is to train a multiclass classifier on the |R| - 5 relations. The model is a feed-forward network, and the output of the last hidden layer is used as r(a, b).\n- The method is compared with unsupervised baselines (e.g., PairDiff: subtracting pre-trained embeddings of a and b) as well as similar supervised methods trained on a different objective (margin rank loss).\n\n== Pros ==\n- The evaluation is done on relations that are not in the training data. It is not trivial that a particular relation embedding would generalize to unseen and possibly unrelated relations. The proposed method generalizes relatively well. Compare this to the proposed margin rank loss objective, which performs well on the classification task (Table 7) but is worse than PairDiff on test data.\n\n== Cons ==\n- The paper is framed in such a way that the method is trivial. My initial thought from the abstract was: ""Of course, supervised training is better than unsupervised ones"", and the introduction does not help either. The fact that the method generalizes to unseen test relations, while a different supervised method does to a lesser extent, should be emphasized earlier.\n- The reason why the embedding method generalizes well might have something to do with the loss function used rather than how the word vectors are combined (difference, concatenation, bilinear, etc.). This could be investigated more. Maybe one loss is better at controlling the sizes of the vectors, which is an issue discussed in Section 2.2.\n- The result on the DiffVec dataset is pretty low, considering that a random baseline would get an accuracy of ~20% (1 in 5 classes). The results also seem to be only a bit better than the unsupervised baseline PairDiff on the BATS dataset.\n- The writing is a bit confusing at times. For instance, on page 12, for the ""lexical-overlap"" category, it should not be possible for any two test pairs to have exactly 1-word overlap, or maybe I am missing something.', 'rating': '7: Good paper, accept', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		r1e3WW5aTX	HJl-uAtpZ4	AKBC.ws/2019/Conference/-/Paper30/Official_Review	['AKBC.ws/2019/Conference/Paper30/Reviewers/Unsubmitted']	1		['everyone']	r1e3WW5aTX	['AKBC.ws/2019/Conference/Paper30/AnonReviewer3']	1546653289008		1548831636767	['AKBC.ws/2019/Conference']
445	1542459658865	{'title': 'GraphIE: A Graph-Based Framework for Information Extraction', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper33/Authors'], 'keywords': ['information extraction', 'graph convolutional network'], 'TL;DR': 'We propose a graph-based approach to model the non-local and non-sequential dependencies for information extraction.', 'abstract': 'Most modern Information Extraction (IE) systems are implemented as sequential taggers and only model local dependencies. Non-local and non-sequential context is, however, a valuable source of information to improve predictions. In this paper, we introduce GraphIE, a framework that operates over a graph representing a broad set of dependencies between textual units (i.e. words or sentences). The algorithm propagates information between connected nodes through graph convolutions, generating a richer representation that can be exploited to improve word-level predictions. Evaluation on three different tasks -- namely social media, textual and visual information extraction -- shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin.', 'pdf': '/pdf/b54d8cf88712dcfbab368b17d1e8d512af4822f4.pdf', 'archival status': 'Archival', 'subject areas': ['Natural Language Processing', 'Information Extraction'], 'paperhash': 'anonymous|graphie_a_graphbased_framework_for_information_extraction', '_bibtex': '@inproceedings{    \nanonymous2019graphie:,    \ntitle={GraphIE: A Graph-Based Framework for Information Extraction},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=Sye7fZcTTm},    \nnote={under review}    \n}'}		Sye7fZcTTm	Sye7fZcTTm	AKBC.ws/2019/Conference/-/Blind_Submission	[]	33	S1gEAJnnTQ	['everyone']		['AKBC.ws/2019/Conference']	1542459658865		1548825141085	['AKBC.ws/2019/Conference']
446	1548824496792	{'title': 'Response to Reviewer 3', 'comment': 'We thank the reviewer for the comments. \n\nAs the reviewer pointed out, incorporating non-local context is important in information extraction. We propose a framework to encode the graph structure within the data. In some applications where the graph structure is evident, such as social network and visual layout, it can be easily modeled within our framework. In traditional textual IE tasks such as NER, we suggest that many correlations could help, for instance coreference. In our paper, we use a very simple graph structure, that is we simply connect identical words to support tagging consistency. The experiment results show that even such a simple structure helps to achieve better results. We are currently investigating a more general setting, in which the model starts from a fully-connected graph and learns the importance of the edges. The framework described in this paper should be seen as the infrastructure for future work.  Finally, although coreference resolution is a hard task, it does not depend on NER and it has already achieved good performance [1,2]. If such information is available, we believe it is beneficial to incorporate it and our framework is a principled way to do that.\n\nIn footnote 10, we agree that the claim is not very accurate and we have clarified it in the revised version. \n\nIn footnote 5, feature engineering method precomputes the features beforehand. The computation of the features is also time-consuming, but only needs to be done once. On the other hand, our neural model learns the text representation through LSTM. Such representation cannot be precomputed as the LSTM parameters are updating during training. Since some users in the dataset might have thousands of tweets, which cannot fit in the GPU memory, we decide to conduct the experiments after sampling the data.\n\n[1] Lee, K., He, L., Lewis, M., & Zettlemoyer, L. (2017). End-to-end neural coreference resolution. arXiv preprint arXiv:1707.07045.\n[2] Lee, K., He, L., & Zettlemoyer, L. (2018). Higher-order Coreference Resolution with Coarse-to-fine Inference. arXiv preprint arXiv:1804.05392.'}		Sye7fZcTTm	Hyltny30XE	AKBC.ws/2019/Conference/-/Paper33/Official_Comment	['AKBC.ws/2019/Conference/Paper33/Reviewers/Unsubmitted']	1		['everyone']	ryeLSiz7fN	['AKBC.ws/2019/Conference/Paper33/Authors']	1548824496792		1548824541507	['AKBC.ws/2019/Conference/Paper33/Authors', 'AKBC.ws/2019/Conference']
447	1548818937483	"{'title': 'Rebuttal', 'comment': ""We appreciate your review. Thank you.\n\nWe are planning to pre-initialize our model with contextual word embeddings (BERT/ELMo), and compare them with the same model but using Glove vectors.\n\nYes, the placeholders and the position embeddings mentioned in 'Conclusion and Future Work' (Section 6) refer to the explicit information about the relational arguments. We are running experiments and will share the results soon.""}"		SkxE1b56TQ	BkgZbq9CXV	AKBC.ws/2019/Conference/-/Paper14/Official_Comment	['AKBC.ws/2019/Conference/Paper14/Reviewers/Unsubmitted']	3		['everyone']	HkxLTQxrMN	['AKBC.ws/2019/Conference/Paper14/Authors']	1548818937483		1548821875848	['AKBC.ws/2019/Conference/Paper14/Authors', 'AKBC.ws/2019/Conference']
448	1548821751132	"{'title': 'Rebuttal', 'comment': ""Thank you for the detailed review. We appreciate the opportunity to respond to your remarks.\n\nRegarding the difference between points (1) and (2), in the first point, we refer to the distribution of the instances for each relation type in the KGs, such as Wikidata and DBpedia. In fact, the distantly supervised datasets built using those KGs are usually unbalanced. The second point refers to domains where the training sets for RE have to be manually curated. In this case, collecting enough relation examples to train a standard neural-based RE classifier requires a considerable effort. Those points motivate our work.\n\nThe comparison of our one-shot technique with the zero-shot RE through querification by (Levy et al., 2017) as well as with USchema is a very interesting idea which deserves more investigations. We agree on this point.\xa0\n\nCurrently, the HSN does not differentiate between entity and non-entity tokens. As we pointed out in Section 6, we are trying to differentiate them using placeholders for the entities.\n\nWe added the work by (Liu, 2017) as related work in the new version. Thank you for this suggestion.\n\nWe think of the context vector as a way to help the attention mechanism give higher weights to those words in the mentions which better express a specific relation. For this reason, we use it at the word level. We already tried it at the relation level as you suggested, but without substantial improvements. Anyway, we understand your remark and we would like to deeply investigate this aspect.\n\nThank you for raising the interesting question about the possible overlaps of the relation instances across the three datasets. For the one-shot experiment, as we point out in the 'One-shot trials' paragraph of Section 5.3, we randomly choose 20 entity-pairs from the long-tailed relation types for each dataset in order to prevent the overlap. Our inspection did not reveal overlaps. We did not perform this analysis when we used the pre-trained analogy embeddings for the standard RE task. However, there are 3 reasons we believe this might not impact our results: (1) During training of the analogy model, we choose only a few entity-pairs for each relation type in T-REX, and then we compute all combinations amongst them. Thus, the probability of overlapping with the test sets of NYT-FB and CC-DBP is very low. (2) Even if some entity-pairs during training might occur in the two test sets, they are highly likely to have different textual mentions since the corpora are different among the datasets. So, the inputs would not overlap in this case. (3) The analogy embeddings are trained differently than standard neural-based RE embeddings, such as PCNN-KI. Despite these points, we plan to perform a detailed analysis w.r.t. this point and, eventually, re-run the evaluation after filtering any duplicate entity-pairs to address your concern.\xa0""}"		SkxE1b56TQ	Hkey-SjC7V	AKBC.ws/2019/Conference/-/Paper14/Official_Comment	['AKBC.ws/2019/Conference/Paper14/Reviewers/Unsubmitted']	4		['everyone']	BkxdvJNmzE	['AKBC.ws/2019/Conference/Paper14/Authors']	1548821751132		1548821751132	['AKBC.ws/2019/Conference/Paper14/Authors', 'AKBC.ws/2019/Conference']
449	1538087924174	"{'title': 'Pay Less Attention with Lightweight and Dynamic Convolutions', 'abstract': ""Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU."", 'keywords': ['Deep learning', 'sequence to sequence learning', 'convolutional neural networks', 'generative models'], 'authorids': ['fw245@cornell.edu', 'angelfan@fb.com', 'alexei.b@gmail.com', 'yann@dauphin.io', 'michael.auli@gmail.com'], 'authors': ['Felix Wu', 'Angela Fan', 'Alexei Baevski', 'Yann Dauphin', 'Michael Auli'], 'TL;DR': 'Dynamic lightweight convolutions are competitive to self-attention on language tasks.', 'pdf': '/pdf/89c0b2081ffa0fa0fe453389c86b208815202a94.pdf', 'paperhash': 'wu|pay_less_attention_with_lightweight_and_dynamic_convolutions', '_bibtex': '@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}'}"		SkVhlh09tX	SkVhlh09tX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1115	BkxV9H65tm	['everyone']		['ICLR.cc/2019/Conference']	1538087924174		1548819962804	['ICLR.cc/2019/Conference']
450	1548818436533	"{'title': 'Rebuttal', 'comment': ""Thank you for your feedback. Your comments are well received.\n\nThe related work section has been updated by following your suggestion regarding the USchema. We also added the references about the column-less and row-less variants. Thank you for underlining this important point.\n\nThe model was trained solely on T-REX (Wikidata/Wikipedia) in order to evaluate its transfer capability across different textual corpora. In the paragraph 'Results and Discussion' of Section 5.3, we discuss this aspect. However, we would like to repeat the experiments,\xa0 re-training the model on each dataset independently, to see how it performs.\xa0 This is a good direction for future analysis.\n\nThe reference (Jameel et al., 2017) has been added to the new version.""}"		SkxE1b56TQ	S1e6-OcA7N	AKBC.ws/2019/Conference/-/Paper14/Official_Comment	['AKBC.ws/2019/Conference/Paper14/Reviewers/Unsubmitted']	2		['everyone']	r1g1OqZ5fV	['AKBC.ws/2019/Conference/Paper14/Authors']	1548818436533		1548818436533	['AKBC.ws/2019/Conference/Paper14/Authors', 'AKBC.ws/2019/Conference']
451	1538087968189	{'title': 'Learning to Learn with Conditional Class Dependencies', 'abstract': 'Neural networks can learn to extract statistical properties from data, but they seldom make use of structured information from the label space to help representation learning. Although some label structure can implicitly be obtained when training on huge amounts of data, in a few-shot learning context where little data is available, making explicit use of the label structure can inform the model to reshape the representation space to reflect a global sense of class dependencies.  We propose a meta-learning framework, Conditional class-Aware Meta-Learning (CAML), that conditionally transforms feature representations based on a metric space that is trained to capture inter-class dependencies. This enables a conditional modulation of the feature representations of the base-learner to impose regularities informed by the label space. Experiments show that the conditional transformation in CAML leads to more disentangled representations and achieves competitive results on the miniImageNet benchmark.', 'keywords': ['meta-learning', 'learning to learn', 'few-shot learning'], 'authorids': ['xiang.jiang@dal.ca', 'mohammad@imagia.com', 'f.varno@dal.ca', 'gabriel@imagia.com', 'nic@imagia.com', 'stan@cs.dal.ca'], 'authors': ['Xiang Jiang', 'Mohammad Havaei', 'Farshid Varno', 'Gabriel Chartrand', 'Nicolas Chapados', 'Stan Matwin'], 'TL;DR': 'CAML is an instance of MAML with conditional class dependencies.', 'pdf': '/pdf/e99ac633ab9056a8e643a4a7d896aa861a1353a0.pdf', 'paperhash': 'jiang|learning_to_learn_with_conditional_class_dependencies', '_bibtex': '@inproceedings{\njiang2018learning,\ntitle={Learning to Learn with Conditional Class Dependencies},\nauthor={Xiang Jiang and Mohammad Havaei and Farshid Varno and Gabriel Chartrand and Nicolas Chapados and Stan Matwin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJfOXnActQ},\n}'}		BJfOXnActQ	BJfOXnActQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1374	SJgrzW05FX	['everyone']		['ICLR.cc/2019/Conference']	1538087968189		1548797897757	['ICLR.cc/2019/Conference']
452	1538087822856	{'title': 'Multi-Domain Adversarial Learning', 'abstract': 'Multi-domain learning (MDL) aims at obtaining a model with minimal average risk across multiple domains. Our empirical motivation is automated microscopy data, where cultured cells are imaged after being exposed to known and unknown chemical perturbations, and each dataset displays significant experimental bias. This paper presents a multi-domain adversarial learning approach, MuLANN, to leverage multiple datasets with overlapping but distinct class sets, in a semi-supervised setting. Our contributions include: i) a bound on the average- and worst-domain risk in MDL, obtained using the H-divergence; ii) a new loss to accommodate semi-supervised multi-domain learning and domain adaptation; iii) the experimental validation of the approach, improving on the state of the art on two standard image benchmarks, and a novel bioimage dataset, Cell.', 'keywords': ['multi-domain learning', 'domain adaptation', 'adversarial learning', 'H-divergence', 'deep representation learning', 'high-content microscopy'], 'authorids': ['alice.schoenauer@polytechnique.org', 'louise.heinrich@ucsf.edu', 'marc.schoenauer@inria.fr', 'sebag@lri.fr', 'lani.wu@ucsf.edu', 'steven.altschuler@ucsf.edu'], 'authors': ['Alice Schoenauer-Sebag', 'Louise Heinrich', 'Marc Schoenauer', 'Michele Sebag', 'Lani F. Wu', 'Steve J. Altschuler'], 'TL;DR': 'Adversarial Domain adaptation and Multi-domain learning: a new loss to handle multi- and single-domain classes in the semi-supervised setting.', 'pdf': '/pdf/df404be71e138da4225d39479d514b05576dad01.pdf', 'paperhash': 'schoenauersebag|multidomain_adversarial_learning', '_bibtex': '@inproceedings{\nschoenauer-sebag2018multidomain,\ntitle={Multi-Domain Adversarial Learning},\nauthor={Alice Schoenauer-Sebag and Louise Heinrich and Marc Schoenauer and Michele Sebag and Lani Wu and Steve Altschuler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Sklv5iRqYX},\n}'}		Sklv5iRqYX	Sklv5iRqYX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	541	S1l5otVOKX	['everyone']		['ICLR.cc/2019/Conference']	1538087822856		1548796986333	['ICLR.cc/2019/Conference']
453	1538087936196	{'title': 'Zero-training Sentence Embedding via Orthogonal Basis', 'abstract': 'We propose a simple and robust training-free approach for building sentence representations. Inspired by the Gram-Schmidt Process in geometric theory, we build an orthogonal basis of the subspace spanned by a word and its surrounding context in a sentence. We model the semantic meaning of a word in a sentence based on two aspects. One is its relatedness to the word vector subspace already spanned by its contextual words. The other is its novel semantic meaning which shall be introduced as a new basis vector perpendicular to this existing subspace.  Following this motivation, we develop an innovative method based on orthogonal basis to combine pre-trained word embeddings into sentence representation. This approach requires zero training and zero parameters, along with efficient inference performance. We evaluate our approach on 11 downstream NLP tasks. Experimental results show that our model outperforms all existing zero-training alternatives in all the tasks and it is competitive to other approaches relying on either large amounts of labelled data or prolonged training time.', 'keywords': ['Natural Language Processing', 'Sentence Embeddings'], 'authorids': ['ziyi.yang@stanford.edu', 'chezhu@microsoft.com', 'wzchen@microsoft.com'], 'authors': ['Ziyi Yang', 'Chenguang Zhu', 'Weizhu Chen'], 'TL;DR': 'A simple and training-free approach for sentence embeddings with competitive performance compared with sophisticated models requiring either large amount of training data or prolonged training time.', 'pdf': '/pdf/314fca9c6ffaeb179665758d42c410074ad84d25.pdf', 'paperhash': 'yang|zerotraining_sentence_embedding_via_orthogonal_basis', '_bibtex': '@misc{\nyang2019zerotraining,\ntitle={Zero-training Sentence Embedding via Orthogonal Basis},\nauthor={Ziyi Yang and Chenguang Zhu and Weizhu Chen},\nyear={2019},\nurl={https://openreview.net/forum?id=rJedbn0ctQ},\n}'}		rJedbn0ctQ	rJedbn0ctQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1187	S1es565tY7	['everyone']		['ICLR.cc/2019/Conference']	1538087936196		1548791490643	['ICLR.cc/2019/Conference']
454	1538087981292	{'title': 'Guiding Policies with Language via Meta-Learning', 'abstract': 'Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning. However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration. Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines. However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task. In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill. Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly. In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.', 'keywords': ['meta-learning', 'language grounding', 'interactive'], 'authorids': ['jcoreyes@eecs.berkeley.edu', 'abhigupta@berkeley.edu', 'suvansh@berkeley.edu', 'naltieri@berkeley.edu', 'j.d.andreas@gmail.com', 'denero@berkeley.edu', 'pabbeel@cs.berkeley.edu', 'svlevine@eecs.berkeley.edu'], 'authors': ['John D. Co-Reyes', 'Abhishek Gupta', 'Suvansh Sanjeev', 'Nick Altieri', 'Jacob Andreas', 'John DeNero', 'Pieter Abbeel', 'Sergey Levine'], 'TL;DR': 'We propose a meta-learning method for interactively correcting policies with natural language.', 'pdf': '/pdf/0d2fa487022c6bef09fe6c4993c83b261997cbe4.pdf', 'paperhash': 'coreyes|guiding_policies_with_language_via_metalearning', '_bibtex': '@inproceedings{\nco-reyes2018metalearning,\ntitle={Meta-Learning Language-Guided Policy Learning},\nauthor={John D Co-Reyes and Abhishek Gupta and Suvansh Sanjeev and Nick Altieri and John DeNero and Pieter Abbeel and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgSEnA5KQ},\n}'}		HkgSEnA5KQ	HkgSEnA5KQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1449	rJeh69n5FX	['everyone']		['ICLR.cc/2019/Conference']	1538087981292		1548788268285	['ICLR.cc/2019/Conference']
455	1548782749698	"{'title': 'explanations continued', 'comment': '""- (1) methodology: I do not see why entities should be useful for extracting context words that indicate a relation. ""\n\nWe followed the intuition that a relation is defined by the two entities taking part in it and additional context with it. We wanted to model that as literally as possible and therefore used the words as queries to pick up additional information. \n\n""- That would mean that relation-specific information is encoded in the entity names already. In fact, this might be the case for ELMo embeddings since they are context dependent but would not work with other embeddings, especially when an entity is rare and has not been mentioned in the context of a specific relation in the training dataset for the embeddings. This is basically the result the authors get when they compare ELMo embeddings + attention only with GloVe embeddings + attention only on the SemEval dataset. However, entities in SemEval are nominal phrases which might actually be very indicative of their possible relations already.""\n\nThis is all correct (and we included an ablation study for both datasets where we just concatenated the two entities and directly classified on them)\n\n\n""I would expect an even more severe drop in performance for the TACRED dataset which includes persons, organizations and locations as entities. Just the name of a person, for example, does not indicate all possible relations it could participate in. Thus, it would be good if the authors evaluated GloVe + attention only on TACRED as well. In this setting, I would expect that the impact of the position encodings and the named entity embeddings increases a lot and would be much higher than the impact of the actual entity embeddings.""\n\nThis is correct too. We ommited the ""GloVe + TACRED"" because we have a rather large OOV ratio while looking up the entities and then we would  often just query with the UNKNOWN token + NE types and this does not work well.\n\n""- Another side note on interpretability: ELMo is a bidirectional RNN model, thus the authors actually use a bidirectional RNN + attention, which is not easier interpretable than other neural networks with attention (since the authors only analyze the attention weights but not the embeddings from the bidirectional RNN model) or even CNNs without attention (from which it is also possible to extract the pooling results).""\n\nWe agree and have withdrawn the claim about interpretability.\n\n""- (2) engineering: The paper does not include any technical contribution. Instead, it seems to be the result of a lot of engineering effort, such as reducing the transformer architecture to something that worked for them, adding and tuning a lot of different regularization techniques, adding named entity information to one of their datasets (without an ablation study), adding class weights also to only one of their datasets (without an ablation study), etc.""\n\nWe agree partly and have let go of some regularization techniques in the resubmission and reran the experiments. We think we give a good intuition why we reduced the transformer architecture to what we use eventually in section 3.3. We have included some more ablation studies for the NE information and class weights and tried to make more clear why we introduced them.\n\n""- Moreover, it is not clear how the authors tune the hyperparameters (listed in the appendix) on the SemEval dataset since they mention that they do not use a development set.""\n\nWe evaluated the model in the very end 5 times on the testset and report the median of these 5 runs. We tuned hyperparameters on a dev set (which we sampled from the training set)\n\n""- Are the relative positions in Equation 2 fed into an embedding layer? If not, this means that these values can get very large for long contexts which might affect the neural network negatively.""\n\nThey get fed into an embedding layer which returns the parameter-free positional embeddings (equation 1). Because they are sine and cosine frequencies, their values are capped between -1 and 1.'}"		S1xlgbcT6X	rkxLsh-0QV	AKBC.ws/2019/Conference/-/Paper19/Official_Comment	['AKBC.ws/2019/Conference/Paper19/Reviewers/Unsubmitted']	3		['everyone']	S1xlgbcT6X	['AKBC.ws/2019/Conference/Paper19/Authors']	1548782749698		1548782749698	['AKBC.ws/2019/Conference/Paper19/Authors', 'AKBC.ws/2019/Conference']
456	1548782689360	"{'title': 'Resubmission uploaded', 'comment': 'Thanks a lot for the helpful comments.\n\nWe worked on a resubmission where we tried to be more concise in writing in general and fixed several small things (typos, table sizes too big, unclear writing).\n\nWe also tried to improve on some of the more severe issues, namely:\n\n""- In particular, the hypothesis of attention mass falling on dependency paths is not quantitatively tested at all. ""\n\nWe included a quantiative analysis of how much attention mass falls on tokens lying on the depdendency path and find that this works surprisingly well for the semeval-10 dataset but not that great for the tacred dataset. We also reasoned about why this might be the case.\n\n""- Table 3: Improvement in F1 may be due to better balance of precision and recall. It is speculated that this has to do with the way of setting class weights. Removing this would then be an interesting side experiment, and would help understand if this is where the F1 improvement comes from, or from the other model components.""\n\nincluded ablation studies on the extensions for the TACRED models\n\n\n""""- The ""evidence"" for what the model is paying attention to consists in 2 (randomly picked?) examples. This is far from being substantial enough to support the claims made in the paper about the focus on shortest dependency path. I do however believe this to be an interesting hypothesis. Can this be quantified? Can this then be computed for more than 2 examples and put to the test?""\n\nWe quantified this (and also list some more examples in the appendix)\n\n""- ELMo embeddings seem to have a large impact on the final results, and it\'s great that authors performed an ablation on it using multiple embedding mechanisms. May it be the case that there is an overlap between the ELMo training data and this paper\'s test data?""\n\nELMo is pretrained on 1 Billion Word Benchmark (source data from WMT 2011 News Crawl data), TACRED source data consists of newswire, broadcast material, and web text collected by LDC. Therefore we don\'t see an overlap'}"		S1xlgbcT6X	rketvhbAXN	AKBC.ws/2019/Conference/-/Paper19/Official_Comment	['AKBC.ws/2019/Conference/Paper19/Reviewers/Unsubmitted']	2		['everyone']	S1xlgbcT6X	['AKBC.ws/2019/Conference/Paper19/Authors']	1548782689360		1548782689360	['AKBC.ws/2019/Conference/Paper19/Authors', 'AKBC.ws/2019/Conference']
457	1542459624248	{'title': 'Learning Relation Classification using Attention only', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper19/Authors'], 'keywords': ['relation classification', 'attention', 'ELMo embeddings'], 'TL;DR': 'We perform relation classification using attention only by letting the two entities of a relation pay attention to additional context.', 'abstract': 'We present a novel approach for relation classification using attention mechanisms only. Our proposed approach consists of separating the entities taking part in a relation from its context and use the entities to pay attention to the context. We add the retrieved information on top of the entities and use that enriched abstraction to classify the relations. Apart from reducing computational complexity and achieving state of the art results on two established benchmarks, our method also makes results more interpretable because we can highlight what part of the context the queries pay attention to.', 'pdf': '/pdf/185bf3c4de1440279e68d0b3fbc675a0c5c94d30.pdf', 'archival status': 'Non-Archival', 'subject areas': ['Machine Learning', 'Information Extraction', 'Knowledge Representation'], 'paperhash': 'anonymous|learning_relation_classification_using_attention_only', '_bibtex': '@inproceedings{    \nanonymous2019learning,    \ntitle={Learning Relation Classification using Attention only},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=S1xlgbcT6X},    \nnote={under review}    \n}'}		S1xlgbcT6X	S1xlgbcT6X	AKBC.ws/2019/Conference/-/Blind_Submission	[]	19	SJgGBNPh6Q	['everyone']		['AKBC.ws/2019/Conference']	1542459624248		1548780199817	['AKBC.ws/2019/Conference']
458	1546613041674	"{'title': 'A Practical Research Exercise towards Shallow Sematic Parsing', 'review': 'Summary:\nThis paper investigates the problem of shallow sematic parsing in inorganic materials synthesis from text under weak supervision. It makes a step to construction of knowledge bases of chemical reactions for inorganic chemistry, serving for automated synthesis planning. The authors proposed models for entity extraction and unlabeled edge placement, and the solution was tested on a dataset consisting of 230 annotated synthesis procedures.\n\nStrong points:\n1. A new test dataset is created.\n2. Valuable attempt to extraction of shallow semantic structure from un-annotated data.\n3. Relative better performance compared with a strong heuristic baseline.\n4. The paper is easy to read and follow.\n\nWeak points:\n1. Limited contributions in terms of model or method.\n2. Empirical evaluation and analysis in experiments are insufficient.\n3. Some mistakes are observed in the current version.\n\nDetailed comments:\nIn this paper, the authors addressed the bottleneck in extraction of structured synthesis representations from text by introducing a dataset of 230 annotated synthesis procedures. A matrix-completion-based model was proposed for unlabeled edge placement, which was trained on a weakly supervised dataset. Experiment results showed that the proposed model outperforms a heuristic baseline in precision and F1. The paper is well-organized and generally easy to follow.\n\nThis paper, however, also has some disadvantages:\n1) Terminology needs to be consistent throughout the whole paper; for instance, inorganic materials synthesis vs. inorganic material synthesis, operation $r$ vs. trigger $r$. \n\n2) Experiments could be improved; in particular, the proposed model needs to be compared with state-of-the-art methods if other works on this task exist. What’s more, it would better if the exhaustive search for edge placements was compared with other alternatives through experiments; for example, argument candidates can be all arguments between one triger and the triger which has two steps from it.\n\n3) It is better to provide more detailed experimental analysis. It is of interest to see why the additional entity type information for the arguments does not boost the performance in edge placement? It would better to clarify why additional entity type information does no help for the task in the experimental analysis because it is not easy to find the reason for readers.\n\n4) In experiments, the proposed method was only compared with mere one baseline. Is it possible to compare with the work of Peng et al. [2017] to which the proposed is closed? If not, it would be apprecaited if the authors can explain the reason why the closest work is not stuitable for this task. \n\n5) The positioning of the paper is somewhat unclear to me. Is this paper among the first efforts to extract unlabeled graph structures from synthesis sentences? Great to clarify in the paper.\n\n6) A few minor mistakes:\n- ""Fig 1"" --> ""Fig.1"".\n- ""we presented related work"" --> ""we present related work"".\n- ""lesser"".\n- ""has primarily been on on materials"" --> ""has primarily been on materials"".', 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		S1eg7-9pp7	Byxq4WgTWN	AKBC.ws/2019/Conference/-/Paper37/Official_Review	['AKBC.ws/2019/Conference/Paper37/Reviewers/Unsubmitted']	1		['everyone']	S1eg7-9pp7	['AKBC.ws/2019/Conference/Paper37/AnonReviewer2']	1546613041674		1548771424207	['AKBC.ws/2019/Conference']
459	1538087789291	{'title': 'Towards the first adversarially robust neural network model on MNIST', 'abstract': 'Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L-inf defense by Madry et~al. (1) has lower L0 robustness than undefended networks and still highly susceptible to L2 perturbations, (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-inf perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.', 'keywords': ['adversarial examples', 'MNIST', 'robustness', 'deep learning', 'security'], 'authorids': ['lukas.schott@bethgelab.org', 'jonas.rauber@bethgelab.org', 'matthias.bethge@bethgelab.org', 'wieland.brendel@bethgelab.org'], 'authors': ['Lukas Schott', 'Jonas Rauber', 'Matthias Bethge', 'Wieland Brendel'], 'pdf': '/pdf/ba645fe0aec0af7d9a401c8eb0c7462a25da3dba.pdf', 'paperhash': 'schott|towards_the_first_adversarially_robust_neural_network_model_on_mnist', '_bibtex': '@inproceedings{\nschott2018towards,\ntitle={Towards the first adversarially robust neural network model on {MNIST}},\nauthor={Lukas Schott and Jonas Rauber and Matthias Bethge and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1EHOsC9tX},\n}'}		S1EHOsC9tX	S1EHOsC9tX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	353	rylP2pxKYQ	['everyone']		['ICLR.cc/2019/Conference']	1538087789291		1548767423056	['ICLR.cc/2019/Conference']
460	1548759761500	{'title': 'ICLR 2018 Conference Acceptance Decision', 'comment': '\nWe have taken the feedback seriously and improved the paper substantially; see  https://arxiv.org/pdf/1901.09109.pdf\n\nThe employed data sets and software code are available at:  https://github.com/Tarzanagh/DADAM'}		SJeUAj05tQ	r1gtCf2pQV	ICLR.cc/2019/Conference/-/Paper896/Official_Comment	['ICLR.cc/2019/Conference/Paper896/Reviewers/Unsubmitted']	15		['everyone']	SJeUAj05tQ	['ICLR.cc/2019/Conference/Paper896/Authors']	1548759761500		1548765978116	['ICLR.cc/2019/Conference/Paper896/Authors', 'ICLR.cc/2019/Conference']
461	1548760772914	{'comment': '... like tears...  in rain', 'title': '....'}		rJlWOj0qF7	SJgTaLhpQE	ICLR.cc/2019/Conference/-/Paper331/Public_Comment	[]	15		['everyone']	ByeAhA9hmV	['(anonymous)']	1548760772914		1548760772914	['(anonymous)', 'ICLR.cc/2019/Conference']
462	1548759842883	{'title': 'ICLR 2018 Conference Acceptance Decision', 'comment': 'We have taken the feedback seriously and improved the paper substantially; see  https://arxiv.org/pdf/1901.09109.pdf\n\nFurther, the employed data sets and software code are available at:  https://github.com/Tarzanagh/DADAM'}		SJeUAj05tQ	S1ljm73TQV	ICLR.cc/2019/Conference/-/Paper896/Official_Comment	['ICLR.cc/2019/Conference/Paper896/Reviewers/Unsubmitted']	16		['everyone']	BkxHPk-Ig4	['ICLR.cc/2019/Conference/Paper896/Authors']	1548759842883		1548759842883	['ICLR.cc/2019/Conference/Paper896/Authors', 'ICLR.cc/2019/Conference']
463	1538087846628	"{'title': 'Neural network gradient-based learning of black-box function interfaces', 'abstract': ""Deep neural networks work well at approximating complicated functions when provided with data and trained by gradient descent methods. At the same time, there is a vast amount of existing functions that programmatically solve different tasks in a precise manner eliminating the need for training. In many cases, it is possible to decompose a task to a series of functions, of which for some we may prefer to use a neural network to learn the functionality, while for others the preferred method would be to use existing black-box functions. We propose a method for end-to-end training of a base neural network that integrates calls to existing black-box functions. We do so by approximating the black-box functionality with a differentiable neural network in a way that drives the base network to comply with the black-box function interface during the end-to-end optimization process. At inference time, we replace the differentiable estimator with its external black-box non-differentiable counterpart such that the base network output matches the input arguments of the black-box function. Using this ``Estimate and Replace'' paradigm, we train a neural network, end to end, to compute the input to black-box functionality while eliminating the need for intermediate labels. We show that by leveraging the existing precise black-box function during inference, the integrated model generalizes better than a fully differentiable model, and learns more efficiently compared to RL-based methods."", 'keywords': ['neural networks', 'black box functions', 'gradient descent'], 'authorids': ['alon.jacovi@il.ibm.com', 'guyh@il.ibm.com', 'einatke@il.ibm.com', 'boazc@il.ibm.com', 'oferl@il.ibm.com', 'gkour@ibm.com', 'joberant@cs.tau.ac.il'], 'authors': ['Alon Jacovi', 'Guy Hadash', 'Einat Kermany', 'Boaz Carmeli', 'Ofer Lavi', 'George Kour', 'Jonathan Berant'], 'TL;DR': 'Training DNNs to interface w\\ black box functions w\\o intermediate labels by using an estimator sub-network that can be replaced with the black box after training', 'pdf': '/pdf/32f89b33cd2283a8bdbd1da127f1142f36a834e1.pdf', 'paperhash': 'jacovi|neural_network_gradientbased_learning_of_blackbox_function_interfaces', '_bibtex': '@inproceedings{\njacovi2018neural,\ntitle={Neural network gradient-based learning of black-box function interfaces},\nauthor={Alon Jacovi and Guy Hadash and Einat Kermany and Boaz Carmeli and Ofer Lavi and George Kour and Jonathan Berant},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1e13s05YX},\n}'}"		r1e13s05YX	r1e13s05YX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	672	HyxApcj5Ym	['everyone']		['ICLR.cc/2019/Conference']	1538087846628		1548749493486	['ICLR.cc/2019/Conference']
464	1542459674310	"{'title': 'SHINRA: Structuring Wikipedia by Collaborative Contribution', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper38/Authors'], 'keywords': ['Resource construction', 'Structured Wikipedia'], 'abstract': 'We are reporting the SHINRA project, a project for structuring Wikipedia with collaborative construction scheme. The goal of the project is to create a huge and well-structured knowledge base to be used in NLP applications, such as QA, Dialogue systems and explainable NLP systems. It is created based on a scheme of ”Resource by Collaborative Contribution (RbCC)”. We conducted a shared task of structuring Wikipedia, and at the same, submitted results are used to construct a knowledge base.\nThere are machine readable knowledge bases such as CYC, DBpedia, YAGO, Freebase Wikidata and so on, but each of them has problems to be solved. CYC has a coverage problem, and others have a coherence problem due to the fact that these are based on Wikipedia and/or created by many but inherently incoherent crowd workers. In order to solve the later problem, we started a project for structuring Wikipedia using automatic knowledge base construction shared-task.\nThe automatic knowledge base construction shared-tasks have been popular and well studied for decades. However, these tasks are designed only to compare the performances of different systems, and to find which system ranks the best on limited test data. The results of the participated systems are not shared and the systems may be abandoned once the task is over.\nWe believe this situation can be improved by the following changes:\n1. designing the shared-task to construct knowledge base rather than evaluating only limited test data\n2. making the outputs of all the systems open to public so that we can run ensemble learning to create the better results than the best systems\n3. repeating the task so that we can run the task with the larger and better training data from the output of the previous task (bootstrapping and active learning)\nWe conducted “SHINRA2018” with the above mentioned scheme and in this paper\nwe report the results and the future directions of the project. The task is to extract the values of the pre-defined attributes from Wikipedia pages. We have categorized most of the entities in Japanese Wikipedia (namely 730 thousand entities) into the 200 ENE categories. Based on this data, the shared-task is to extract the values of the attributes from Wikipedia pages. We gave out the 600 training data and the participants are required to submit the attribute-values for all remaining entities of the same category type. Then 100 data out of them for each category are used to evaluate the system output in the shared-task.\nWe conducted a preliminary ensemble learning on the outputs and found 15 F1 score improvement on a category and the average of 8 F1 score improvements on all 5 categories we tested over a strong baseline. Based on this promising results, we decided to conduct three tasks in 2019; multi-lingual categorization task (ML), extraction for the same 5 categories in Japanese with a larger training data (JP-5) and extraction for 34 new categories in Japanese (JP-34).\n', 'archival status': 'Non-Archival', 'subject areas': ['Natural Language Processing', 'Information Extraction', 'Information Integration', 'Crowd-sourcing', 'Other'], 'pdf': '/pdf/74ce96da13d51a0e9df1df1b17131b8b9444fd90.pdf', 'paperhash': 'anonymous|shinra_structuring_wikipedia_by_collaborative_contribution', 'TL;DR': 'We introduce a ""Resource by Collaborative Construction"" scheme to create KB, structured Wikipedia ', '_bibtex': '@inproceedings{    \nanonymous2019shinra:,    \ntitle={SHINRA: Structuring Wikipedia by Collaborative Contribution},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=HygfXWqTpm},    \nnote={under review}    \n}'}"		HygfXWqTpm	HygfXWqTpm	AKBC.ws/2019/Conference/-/Blind_Submission	[]	38	rkxJXtVpa7	['everyone']		['AKBC.ws/2019/Conference']	1542459674310		1548732058547	['AKBC.ws/2019/Conference']
465	1538087878834	{'title': 'Learning Multimodal Graph-to-Graph Translation for Molecule Optimization', 'abstract': 'We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n', 'keywords': ['graph-to-graph translation', 'graph generation', 'molecular optimization'], 'authorids': ['wengong@csail.mit.edu', 'yangk@mit.edu', 'regina@csail.mit.edu', 'tommi@csail.mit.edu'], 'authors': ['Wengong Jin', 'Kevin Yang', 'Regina Barzilay', 'Tommi Jaakkola'], 'TL;DR': 'We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations.', 'pdf': '/pdf/dee24460691863813212530280fc3d4335b0caac.pdf', 'paperhash': 'jin|learning_multimodal_graphtograph_translation_for_molecule_optimization', '_bibtex': '@inproceedings{\njin2018learning,\ntitle={Learning Multimodal Graph-to-Graph Translation for Molecule Optimization},\nauthor={Wengong Jin and Kevin Yang and Regina Barzilay and Tommi Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xJAsA5F7},\n}'}		B1xJAsA5F7	B1xJAsA5F7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	856	rJlM0fFYKX	['everyone']		['ICLR.cc/2019/Conference']	1538087878834		1548717628730	['ICLR.cc/2019/Conference']
466	1548711637757	{'title': 'Submission Withdrawn by the Authors', 'withdrawal confirmation': 'I have read and agree with the withdrawal statement on behalf of myself and my co-authors.'}		rkeFyWcTTm	BylRCUlpXV	AKBC.ws/2019/Conference/-/Paper16/Withdraw_Submission	[]	1		['everyone']	rkeFyWcTTm	['AKBC.ws/2019/Conference/Paper16/Authors']	1548711637757		1548711637757	[]
467	1546937445094	{'title': 'Nice paper with experimentation falling a bit short, but mostly not a good fit for AKBC.', 'review': 'The paper describes a new method for unsupervised domain adaptation based on self-training and learned projections to decrease the distance between source and target feature distributions. This combination has not been explored before and shows promising results on cross domain sentiment analysis. The paper is well written and presented, however, the methods section should be expanded with more details for an easier read.\n\nOverall I like the idea and this is a nice paper but I do not see how the explored task in this work (sentiment analysis) fits well to the scope of the AKBC conference. My suggestion is to expand this paper with other, maybe more interesting experiments than simple binary sentiment analysis and submit to another venue as AKBC is in my opinion not the right fit for this paper. ', 'rating': '4: Ok but not good enough - rejection', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		H1xtpgqTaQ	H1laDEkMfN	AKBC.ws/2019/Conference/-/Paper4/Official_Review	['AKBC.ws/2019/Conference/Paper4/Reviewers/Unsubmitted']	2		['everyone']	H1xtpgqTaQ	['AKBC.ws/2019/Conference/Paper4/AnonReviewer2']	1546937445094		1548709565783	['AKBC.ws/2019/Conference']
468	1548709511616	{'title': 'RE', 'comment': 'Just to be clear, it is obvious that domain adaptation is not limited to sentiment analysis, but the paper unfortunately is. Since sentiment analysis is by comparison very different to typical AKBC tasks, both in difficulty and modeling, I do not see how this paper can directly be used in the context of AKBC but even if it was clear there are no guarantees that the described methods can be helpful. Sentiment analysis is just not a good proxy task for more complex tasks that go beyond binary document classification.'}		H1xtpgqTaQ	rJggqCyTX4	AKBC.ws/2019/Conference/-/Paper4/Official_Comment	['AKBC.ws/2019/Conference/Paper4/Reviewers/Unsubmitted']	5		['everyone']	SJlg-FNtQN	['AKBC.ws/2019/Conference/Paper4/AnonReviewer2']	1548709511616		1548709511616	['AKBC.ws/2019/Conference/Paper4/AnonReviewer2', 'AKBC.ws/2019/Conference']
469	1548709049161	"{'title': 'RE', 'comment': ""Well, the biggest issues still remain though. Considering everything this work added images from a web search to an existing KG and showed that traditional approaches do not work well when using **a pretrained VGG16 model**, in fact they do not work at all. Don't get me wrong, the idea of exploring visual information for AKBC and using it for zero-shot learning is very interesting but the experiments are just too limited. End-to-end training should be considered instead of taking a pretrained feature extractor that doesn't work for this task, or at the very least other models for visual feature extraction should have been tried. The problem is that in its current form the experiments tell us only that the pretrained VGG16 model is a bad feature extractor but nothing conclusive about existing models for link prediction. Because of that unfortunately also the zero shot experiments add little value.""}"		BylEpe9ppX	SyeZ6nyp7N	AKBC.ws/2019/Conference/-/Paper2/Official_Comment	['AKBC.ws/2019/Conference/Paper2/Reviewers/Unsubmitted']	4		['everyone']	S1lSE5O4Q4	['AKBC.ws/2019/Conference/Paper2/AnonReviewer2']	1548709049161		1548709049161	['AKBC.ws/2019/Conference/Paper2/AnonReviewer2', 'AKBC.ws/2019/Conference']
470	1548707947460	{'title': 'Paper and Code Now Available', 'comment': 'We have put up a de-anonymized version of the paper. Unlike the draft from the reviewing cycle, this draft shows OE can also work on large-scale images (Places365). Code for most of the experiments, including the NLP experiments, has been made available: https://github.com/hendrycks/outlier-exposure'}		HyxCxhRcY7	r1gmudJa7E	ICLR.cc/2019/Conference/-/Paper1125/Official_Comment	['ICLR.cc/2019/Conference/Paper1125/Reviewers/Unsubmitted']	15		['everyone']	HyxCxhRcY7	['ICLR.cc/2019/Conference/Paper1125/Authors']	1548707947460		1548707947460	['ICLR.cc/2019/Conference/Paper1125/Authors', 'ICLR.cc/2019/Conference']
471	1538087925806	{'title': 'Deep Anomaly Detection with Outlier Exposure', 'abstract': 'It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.', 'keywords': ['confidence', 'uncertainty', 'anomaly', 'robustness'], 'authorids': ['hendrycks@berkeley.edu', 'mantas@ttic.edu', 'tgd@oregonstate.edu'], 'authors': ['Dan Hendrycks', 'Mantas Mazeika', 'Thomas Dietterich'], 'TL;DR': 'OE teaches anomaly detectors to learn heuristics for detecting unseen anomalies; experiments are in classification, density estimation, and calibration in NLP and vision settings; we do not tune on test distribution samples, unlike previous work', 'pdf': '/pdf/0013057d549e73af3bd2c00485579c3171e90391.pdf', 'paperhash': 'hendrycks|deep_anomaly_detection_with_outlier_exposure', '_bibtex': '@inproceedings{\nhendrycks2018deep,\ntitle={Deep Anomaly Detection with Outlier Exposure},\nauthor={Dan Hendrycks and Mantas Mazeika and Thomas Dietterich},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxCxhRcY7},\n}'}		HyxCxhRcY7	HyxCxhRcY7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1125	H1eYYyAcKX	['everyone']		['ICLR.cc/2019/Conference']	1538087925806		1548707724464	['ICLR.cc/2019/Conference']
472	1538087813282	"{'title': 'Benchmarking Neural Network Robustness to Common Corruptions and Perturbations', 'abstract': ""In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize."", 'keywords': ['robustness', 'benchmark', 'convnets', 'perturbations'], 'authorids': ['hendrycks@berkeley.edu', 'tgd@oregonstate.edu'], 'authors': ['Dan Hendrycks', 'Thomas Dietterich'], 'TL;DR': 'We propose ImageNet-C to measure classifier corruption robustness and ImageNet-P to measure perturbation robustness', 'pdf': '/pdf/ff36332a1ef24aa2619312f7649b8f4ba5870ec6.pdf', 'paperhash': 'hendrycks|benchmarking_neural_network_robustness_to_common_corruptions_and_perturbations', '_bibtex': '@inproceedings{\nhendrycks2018benchmarking,\ntitle={Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},\nauthor={Dan Hendrycks and Thomas Dietterich},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJz6tiCqYm},\n}'}"		HJz6tiCqYm	HJz6tiCqYm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	489	S1xnna59tm	['everyone']		['ICLR.cc/2019/Conference']	1538087813282		1548707482836	['ICLR.cc/2019/Conference']
473	1548689077784	{'comment': 'The saddest thing to me is that, because of all this drama, this unworthy paper is getting a lot of attention and will likely reach everyone in the community and get cited like crazy in the next few years. On the other hand, other papers, accepted by merit and hard work, are likely to get lost in the archives of the proceedings...', 'title': 'Sadness'}		rJlWOj0qF7	ByeAhA9hmV	ICLR.cc/2019/Conference/-/Paper331/Public_Comment	[]	14		['everyone']	H1gmsmaPX4	['(anonymous)']	1548689077784		1548689077784	['(anonymous)', 'ICLR.cc/2019/Conference']
474	1548671410098	{'title': 'additional evaluation', 'comment': 'we have finished the additional evaluation with 500 top scored triples and asked doctors. The acceptance rate is about 75%. A new draft has been uploaded, new results are in the last paragraph of section 6.2'}		HJxmxbq66Q	Syl52t83QV	AKBC.ws/2019/Conference/-/Paper20/Official_Comment	['AKBC.ws/2019/Conference/Paper20/Reviewers/Unsubmitted']	6		['everyone']	r1gyZE8UXE	['AKBC.ws/2019/Conference/Paper20/Authors']	1548671410098		1548671410098	['AKBC.ws/2019/Conference/Paper20/Authors', 'AKBC.ws/2019/Conference']
475	1538087863220	{'title': 'Unsupervised Learning via Meta-Learning', 'abstract': 'A central goal of unsupervised learning is to acquire representations from unlabeled data or experience that can be used for more effective learning of downstream tasks from modest amounts of labeled data. Many prior unsupervised learning works aim to do so by developing proxy objectives based on reconstruction, disentanglement, prediction, and other metrics. Instead, we develop an unsupervised meta-learning method that explicitly optimizes for the ability to learn a variety of tasks from small amounts of data. To do so, we construct tasks from unlabeled data in an automatic way and run meta-learning over the constructed tasks. Surprisingly, we find that, when integrated with meta-learning, relatively simple task construction mechanisms, such as clustering embeddings, lead to good performance on a variety of downstream, human-specified tasks. Our experiments across four image datasets indicate that our unsupervised meta-learning approach acquires a learning algorithm without any labeled data that is applicable to a wide range of downstream classification tasks, improving upon the embedding learned by four prior unsupervised learning methods.', 'keywords': ['unsupervised learning', 'meta-learning'], 'authorids': ['kyle.hsu@mail.utoronto.ca', 'svlevine@eecs.berkeley.edu', 'cbfinn@eecs.berkeley.edu'], 'authors': ['Kyle Hsu', 'Sergey Levine', 'Chelsea Finn'], 'TL;DR': 'An unsupervised learning method that uses meta-learning to enable efficient learning of downstream image classification tasks, outperforming state-of-the-art methods.', 'pdf': '/pdf/0ad8350b7d37fdbac2cf97652cea40589ed03c3d.pdf', 'paperhash': 'hsu|unsupervised_learning_via_metalearning', '_bibtex': '@inproceedings{\nhsu2018unsupervised,\ntitle={Unsupervised Learning via Meta-Learning},\nauthor={Kyle Hsu and Sergey Levine and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1My6sR9tX},\n}'}		r1My6sR9tX	r1My6sR9tX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	766	HJgpW-wYFm	['everyone']		['ICLR.cc/2019/Conference']	1538087863220		1548669839923	['ICLR.cc/2019/Conference']
476	1542459626564	{'title': 'Extracting Medical Information Using Machine Reading', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper20/Authors'], 'keywords': ['Semantic Role Labelling', 'Information Extraction', 'Triple Extraction', 'Entity Linking', 'Triple Scoring'], 'abstract': 'A wealth of medical knowledge has been encoded using semantically rich languages like RDF and OWL leading to medical Knowledge Bases (KBs) like SNOMED CT, NCI, FMA, and more. Nevertheless, medical information like new treatments, drug-disease interactions, or relations between diseases, symptoms, and risk factors is initially published in the form of unstructured text requiring a considerable amount of time and resources until these are incorporated in existing KBs. In this paper we present techniques we developed for extracting medical facts (triples) from unstructured sources. Our approach follows the Machine Reading paradigm and more precisely Semantic Role Labelling (SRL) based extraction which is fully unsupervised. We show how we dealt with several deficiencies of SRL-based information extraction (IE), like entity linking with large arguments, copula verbs that are treated as first-class relations, inability to identify relations expressed through nouns, and the lack of scoring of extracted triples. Regarding scoring, we evaluate several methods which are based on existing off-the-self KB learning algorithms but also develop our custom classifier after some facts were validated by medical professional as accepted/rejected. We have applied our approach on unstructured sources and extracted about 120k triples. A random list of 5k were carefully validated by medical professional showing encouraging acceptance rate for an unsupervised approach. Our set of triples was also compared with a manually constructed network of diseases, symptoms and risk factors that is intended to be used for symptom-checking. The comparison showed that a large part of this network was extracted highlighting the usefulness of approach and the possibilities for using our set of triples for further extending and revising this network.', 'archival status': 'Archival', 'subject areas': ['Natural Language Processing', 'Information Extraction', 'Knowledge Representation', 'Semantic Web'], 'pdf': '/pdf/ba25b76e548a8ae7c60d0d3723f3745af1e2cc35.pdf', 'paperhash': 'anonymous|extracting_medical_information_using_machine_reading', '_bibtex': '@inproceedings{    \nanonymous2019extracting,    \ntitle={Extracting Medical Information Using Machine Reading},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=HJxmxbq66Q},    \nnote={under review}    \n}'}		HJxmxbq66Q	HJxmxbq66Q	AKBC.ws/2019/Conference/-/Blind_Submission	[]	20	SyeYpVO2aX	['everyone']		['AKBC.ws/2019/Conference']	1542459626564		1548669547114	['AKBC.ws/2019/Conference']
477	1548666445094	"{'title': 'Response to Reviewer1', 'comment': 'We would like to thank the reviewer for their helpful comments. \n\n- “Future research directions”\n\nAs the future work, we aim to investigate practical uses of \\AALP in improving existing link prediction models through two scenarios, 1) by identifying the error links in the training data and removing them from the KG, and 2) by incorporating the intuition gained through interpretability to improve existing representations (e.g. rule extracting experiment). We are also interested extending our approach to adversarial modifications that are more than just a single edge, i.e. what is the minimal set of facts to be changed to change the prediction of the model.\n\n- “Figure 1 clarity”\n\nWe address this issue in the revised version.\n\n- “Expandability to additive models”\n\nThe approach that we consider for approximating the effect of adversarial modification can potentially generalized to additive models as well. As an example, we drive a first-order approximation of the change for TransE model in the appendix. We focus on the multiplicative models due to their recent success.\n\n- ""Uncertain Test data""\n\nYes, the evaluation set in this case is model-dependent. For each model, from the test triples that the model predicts correctly, we pick 100 triples with the minimum difference between their scores and the negative sample with the highest score. The distribution of relations differs because the different models are confident on different relations (we can provide further details if needed).\n\n- “The alignment of the numbers in the Table 3”\n\nIn the revised version we improved the alignment of the numbers.\n'}"		Hkg7rbcp67	rklHU8HhXN	AKBC.ws/2019/Conference/-/Paper50/Official_Comment	['AKBC.ws/2019/Conference/Paper50/Reviewers/Unsubmitted']	6		['everyone']	BylFwOqgGV	['AKBC.ws/2019/Conference/Paper50/Authors']	1548666445094		1548666445094	['AKBC.ws/2019/Conference/Paper50/Authors', 'AKBC.ws/2019/Conference']
478	1548666344069	"{'title': 'Response to Reviewer2 (part 1)', 'comment': ""We would like to thank the reviewer for their helpful comments, and have attempted to address the concerns. \n\n- D1. “The definition of the loss function”\n\nWe follow [1] to define the loss function, and thus omit the details. The summation is over the observed triples (<s, r, o>) in the training data. As a result, based on the definition of the Y_o^{s,r}, the zero values of Y_o^{s,r} represent the negative samples. In the revised version of the paper, we add more details to loss function to provide a more precise definition.\n\n- D2. “The optimization problems”\n\nWe agree that we could define the optimization problem as  $\\argmin \\bar \\psi(s,r,o)$, but that is identical to $\\argmax \\psi(s,r,o) - \\bar \\psi(s,r,o)$, since \\psi(s,r,o) is a constant for the search. We use the latter for notational convenience for the final approximation. \n\n- D3. “The focus on fixed-object attacks and easily expandability claim”\n\nWe address this issue in the revised version and provide more justifications on the expandability of the method in the appendix.\n\n- D4. “The clarity of Sec. 4.1”.\n\nWe provided a polished version of this section in the revised version.\n\n- D5. “having the same object argument”\n\nWe add more clarification to the new version of the paper.\n\n- D6. “Clarifying the process of finding z_{s',r'}”\n\nThe explanation of our method to find z_{s’, r’} is provided in section 4.2. We further add more clarification in the revised version of the work.\n\n- D7. “Accuracy of the inverter network and maximum inner-product search”\n\nWe add a new study on the accuracy of the inverter networks to the paper. As a result, our networks achieve more than 90% accuracy demonstrating their capability in correctly inverting the vector z_{s,r} to {s,r}. Furthermore, although we could use maximum inner-product search for DistMult, we were looking for a general algorithm which would work across multiple models. We elaborate this issue in the revised version.   \n\n- D8. “The effect of normalization”\n\nWe did not consider the effect of normalization in our approximations, because a recent implementation [1] found it unnecessary. However, normalization can be easily incorporated into our formulation as an additional term when deriving the approximation. We can include the details in the appendix if the reviewer feels that will be valuable.\n\n- D9. “The description of the experimental setup”\n\nFor training the link prediction task, we adopt the implementation and hyperparameters from [1]. To retrain the models, we simply alter the training data and do the training process from scratch with the same hyperparameters. In section 4.2, to find the optimum z_{s’, r’} we use a gradient-based method which we found its optimum step size through a grid search. As we mentioned, we adopt the same configuration as the [1] that uses the filtered scenario. We have added more clarification on this part to the revised version.\n\n- D10. “The natural baseline for AALP-Remove”\n\nWe consider this new baseline in our revised version and provide its performance in Table 3. As for AALP-Add, the reasonable manifestation of this baseline is $-f(s,r)$ which we have already considered in the paper.\n""}"		Hkg7rbcp67	BkgelUH27N	AKBC.ws/2019/Conference/-/Paper50/Official_Comment	['AKBC.ws/2019/Conference/Paper50/Reviewers/Unsubmitted']	5		['everyone']	H1enoaZGfV	['AKBC.ws/2019/Conference/Paper50/Authors']	1548666344069		1548666344069	['AKBC.ws/2019/Conference/Paper50/Authors', 'AKBC.ws/2019/Conference']
479	1548666276318	"{'title': 'Response to Reviewer2 (part 2)', 'comment': '- D11. “The influence function as well as the relationship to Koh and Liang [2017]”\n\nWe provide more clarification on influence function in the revised version.\n\n- D12. “AALP vs influence function experiment”\n\nThe scores are based on the correlation of the **true** ranking (which is calculated by literally removing each triple one by one and observing the effect of retraining) and the ranking of the triples’ effect based on AALP and the other baselines. Since the approaches make different approximations, the rankings are different. The targets are triples randomly selected from the data and we average the result by choosing 10 random target samples. It is intractable to evaluate all triples as potential adversaries even for these small KBs, since we would have to retrain the model for every triple. We added more details for this section in the revised version.\n\n- D13. “The discussion in the experimental study and the reason behind better performance of attacks on some relations”\n\nIt is difficult to conjecture precisely why some relations are more robust, and we hope these results will seed future research in this direction. We have revised the text for the experiments in the current version of the paper to be more clear.\n\n- D14. “How much interpretability we can get from attacks”\n\nInterpretability comes primarily from the removal, which aids our understanding by identifying the observed fact that has the **most influence** on the model for predicting a specific target. Note that the adversaries that are uninsightful/unintuitive does not mean the approach is not useful for interpretability, but might indicate a problem with the model or the dataset.\n  \n- D15. “How rules have been extracted”\n\nWe provide a more detailed explanation of our rule extraction method and compare the extracted rules with an existing method for the rule extraction [2] in the revised version.\n\n- D16. “Error detection experiment”\n\nIn our setting, we assume that the target triples that have an error in their neighborhood are given. We can make the error detection fully automatic by setting a threshold (or even learn this threshold) on the value of $\\Delta_{(s\',r\')}(s,r,o)$ (defined in the revised version) and choose the triples whose removal causes a change less than this threshold as our errors.\n\n- D17. “The relationship to [Minervini et al., 2017, Cai and Wang, 2017]”\n\nThe authors in [Minervini et al., 2017] consider an adversarial training using some predefined rules to provide a more accurate representation of KBs. Furthermore, [Cai and Wang, 2017] utilizes the GANs network to provide more meaningful negative samples during training. As a result, none of these works can be considered a suitable baseline for our method. We added more details about them in the related work section.\n\n[1] Dettmers, Tim, et al. ""Convolutional 2d knowledge graph embeddings."" Thirty-Second AAAI 2018.\n[2] Yang, Bishan, et al. ""Embedding entities and relations for learning and inference in knowledge bases."" International Conference on Learning Representations (ICLR 2015).'}"		Hkg7rbcp67	HJg3jBS37V	AKBC.ws/2019/Conference/-/Paper50/Official_Comment	['AKBC.ws/2019/Conference/Paper50/Reviewers/Unsubmitted']	4		['everyone']	H1enoaZGfV	['AKBC.ws/2019/Conference/Paper50/Authors']	1548666276318		1548666276318	['AKBC.ws/2019/Conference/Paper50/Authors', 'AKBC.ws/2019/Conference']
480	1548665626949	{'title': 'Response to Reviewer3', 'comment': 'We would like to thank the reviewer for their helpful comments. \n\n- “Being clear and reproducible”.\n\nWe added more explanation on the experiments and implementations in the revised version of the work. Further, we will make the code publicly available with the final version of our paper.'}		Hkg7rbcp67	Byg7QXBh74	AKBC.ws/2019/Conference/-/Paper50/Official_Comment	['AKBC.ws/2019/Conference/Paper50/Reviewers/Unsubmitted']	2		['everyone']	rye-yPICGN	['AKBC.ws/2019/Conference/Paper50/Authors']	1548665626949		1548665626949	['AKBC.ws/2019/Conference/Paper50/Authors', 'AKBC.ws/2019/Conference']
481	1548665514024	"{'title': 'General Response', 'comment': 'We sincerely appreciate all the reviews, they provided useful and constructive feedback. In the revised paper, we address the concerns and suggestions to strengthen our paper. We hope reviewers revisit the rating in light of our revision and response. The following summarizes our changes. \n\n1) Inverter function accuracy: To study the accuracy of our inverter functions we evaluate the performance of our networks on the test set of our benchmarks. Our networks achieve more than 90% accuracy demonstrating their capability in correctly inverting the vector z_{s,r} to {s,r}. Details are provided in the revised paper (see Table 1).\n\n2) Scalability test: To better evaluate the performance of AALP (in the revised version we rename the method to AMLP) with influence function (IF), we compare the time to compute a single adversary by IF to AALP, as we steadily grow the number of entities (randomly chosen subgraphs), averaged over 10 random triples. Based on this experiment, we showed that AALP is mostly unaffected by the number of entities while IF increases quadratically. Considering that real-world KGs have tens of thousands of times more entities than our setting, we demonstrate that IF is infeasible for them (see Figure 4).\n\n3) New alternative for our method as AALP-flip: Using  AALP method, we introduce a new alternative that increases the score of a fake fact, i.e., we identify the adversary that would increase the prediction for a fake fact. We study the effect of this new adversarial attack in Table 3.\n\n4) New baseline for AALP-Remove: We consider a new baseline for AALP-Remove by removing the neighbor where f(s\',r\') is closest to f(s,r) and demonstrate its behavior in Table 3.\n\n5) More on rule extraction: We provide a more detailed explanation of our rule extraction method and compare the extracted rules with an existing method for the rule extraction [1] in the revised version.\n\n\nMore details: We add more detailed explanations for our problem setup and experiments and provide a more detailed discussion on the behavior of the models in each experiment. We include more details on the expandability of our methods to other settings in the appendix.\n\n[1] Yang, Bishan, et al. ""Embedding entities and relations for learning and inference in knowledge bases."" International Conference on Learning Representations (ICLR 2015).\n'}"		Hkg7rbcp67	B1ez2MBnXN	AKBC.ws/2019/Conference/-/Paper50/Official_Comment	['AKBC.ws/2019/Conference/Paper50/Reviewers/Unsubmitted']	1		['everyone']	Hkg7rbcp67	['AKBC.ws/2019/Conference/Paper50/Authors']	1548665514024		1548665514024	['AKBC.ws/2019/Conference/Paper50/Authors', 'AKBC.ws/2019/Conference']
482	1542459707115	{'title': 'Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper50/Authors'], 'keywords': ['Adversarial Attack', 'Knowledge Base Completion'], 'TL;DR': 'In this work, we consider the task of adversarial attack on knowledge base completion to study the robustness and interpretability of representation models.', 'abstract': 'Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data. Existing approaches, however, primarily focus on improving ranking metrics and ignore other aspects of knowledge base representations, such as robustness, interpretability, and ability to detect errors. In this paper, we propose adversarial attacks on link prediction models (AALP): identifying the fact to add into or remove from the knowledge graph that changes the prediction of a target fact. Using these attacks, we are able to identify the most influential related fact for a predicted link and investigate the sensitivity of the model to additional made-up facts. We introduce an efficient approach to estimate the effect of making a change by approximating the change in the embeddings upon altering the knowledge graph. In order to avoid the combinatorial search over all possible facts, we introduce an inverter function and gradient-based search to identify the adversary in a continuous space. We demonstrate that our models effectively attack the link prediction models by reducing their accuracy between 6-45% for different metrics. Further, we study patterns in the most influential neighboring facts, as identified by the adversarial attacks. Finally, we use the proposed approach to detect incorrect facts in the knowledge base, achieving up to 55% accuracy in identifying errors.', 'pdf': '/pdf/1c3277e0292d37092667991c375453b8ec8b0e84.pdf', 'archival status': 'Non-Archival', 'subject areas': ['Machine Learning', 'Reasoning', 'Knowledge Representation'], 'paperhash': 'anonymous|investigating_robustness_and_interpretability_of_link_prediction_via_adversarial_modifications', '_bibtex': '@inproceedings{    \nanonymous2019investigating,    \ntitle={Investigating Robustness and Interpretability of Link Prediction via Adversarial Attacks},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=Hkg7rbcp67},    \nnote={under review}    \n}'}		Hkg7rbcp67	Hkg7rbcp67	AKBC.ws/2019/Conference/-/Blind_Submission	[]	50	H1x80q5sTm	['everyone']		['AKBC.ws/2019/Conference']	1542459707115		1548665291961	['AKBC.ws/2019/Conference']
483	1538088004145	"{'title': 'Rethinking the Value of Network Pruning', 'abstract': 'Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all the six state-of-the-art pruning algorithms we examined, fine-tuning a pruned model only gives comparable or even worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for a wide variety of pruning algorithms with multiple network architectures, datasets, and tasks. Our results have several implications: 1) training a large, over-parameterized model is not necessary to obtain an efficient final model, 2) learned ""important"" weights of the large model are not necessarily useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited ""important"" weights, is what leads to the efficiency benefit in the final model, which suggests that some pruning algorithms could be seen as performing network architecture search; we further show that it is possible to use the sparsity patterns from the pruned network to design efficient models.', 'paperhash': 'liu|rethinking_the_value_of_network_pruning', 'TL;DR': 'In network pruning, fine-tuning a pruned model only gives comparable performance than training it from scratch.', 'authorids': ['zhuangl@berkeley.edu', 'sunmj15@gmail.com', 'tinghuiz@eecs.berkeley.edu', 'gaohuang.thu@gmail.com', 'trevor@eecs.berkeley.edu'], 'authors': ['Zhuang Liu', 'Mingjie Sun', 'Tinghui Zhou', 'Gao Huang', 'Trevor Darrell'], 'keywords': ['network pruning', 'network compression', 'architecture search', 'train from scratch'], 'pdf': '/pdf/03f3f0026332ab402a010c8078ac4eb5b361ab7a.pdf', '_bibtex': '@inproceedings{\nliu2018rethinking,\ntitle={Rethinking the Value of Network Pruning},\nauthor={Zhuang Liu and Mingjie Sun and Tinghui Zhou and Gao Huang and Trevor Darrell},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJlnB3C5Ym},\n}'}"		rJlnB3C5Ym	rJlnB3C5Ym	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1580	SygFAapcYX	['everyone']		['ICLR.cc/2019/Conference']	1538088004145		1548655786969	['ICLR.cc/2019/Conference']
484	1538087850417	{'title': 'Residual Non-local Attention Networks for Image Restoration', 'abstract': 'In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk branch and (non-)local mask branch in each (non-)local attention block. The trunk branch is used to extract hierarchical features. Local and non-local mask branches aim to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutional operations, while non-local attention considers more about long-range dependencies in the whole feature map. Furthermore, we propose residual local and non-local attention learning to train the very deep network, which further enhance the representation ability of the network. Our proposed method can be generalized for various image restoration applications, such as image denoising, demosaicing, compression artifacts reduction, and super-resolution. Experiments demonstrate that our method obtains comparable or better results compared with recently leading methods quantitatively and visually.', 'keywords': ['Non-local network', 'attention network', 'image restoration', 'residual learning'], 'authorids': ['yulun100@gmail.com', 'kunpengli@ece.neu.edu', 'li.kai.gml@gmail.com', 'bnzhong@hqu.edu.cn', 'yunfu@ece.neu.edu'], 'authors': ['Yulun Zhang', 'Kunpeng Li', 'Kai Li', 'Bineng Zhong', 'Yun Fu'], 'TL;DR': 'New state-of-the-art framework for image restoration', 'pdf': '/pdf/86a13f9d28a81b723347a59017de0559bfe3dca7.pdf', 'paperhash': 'zhang|residual_nonlocal_attention_networks_for_image_restoration', '_bibtex': '@inproceedings{\nzhang2018residual,\ntitle={Residual Non-local Attention Networks for Image Restoration},\nauthor={Yulun Zhang and Kunpeng Li and Kai Li and Bineng Zhong and Yun Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeGhoA5FX},\n}'}		HkeGhoA5FX	HkeGhoA5FX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	693	r1eMkfaqt7	['everyone']		['ICLR.cc/2019/Conference']	1538087850417		1548653677509	['ICLR.cc/2019/Conference']
485	1538087860091	{'title': 'Dynamic Channel Pruning: Feature Boosting and Suppression', 'abstract': 'Making deep convolutional neural networks more accurate typically comes at the cost of increased computational and memory resources. In this paper, we reduce this cost by exploiting the fact that the importance of features computed by convolutional layers is highly input-dependent, and propose feature boosting and suppression (FBS), a new method to predictively amplify salient convolutional channels and skip unimportant ones at run-time. FBS introduces small auxiliary connections to existing convolutional layers. In contrast to channel pruning methods which permanently remove channels, it preserves the full network structures and accelerates convolution by dynamically skipping unimportant input and output channels. FBS-augmented networks are trained with conventional stochastic gradient descent, making it readily available for many state-of-the-art CNNs. We compare FBS to a range of existing channel pruning and dynamic execution schemes and demonstrate large improvements on ImageNet classification. Experiments show that FBS can respectively provide 5× and 2× savings in compute on VGG-16 and ResNet-18, both with less than 0.6% top-5 accuracy loss.', 'keywords': ['dynamic network', 'faster CNNs', 'channel pruning'], 'authorids': ['xt.gao@siat.ac.cn', 'yaz21@cam.ac.uk', 'lukaszd.mail@gmail.com', 'robert.mullins@cl.cam.ac.uk', 'cz.xu@siat.ac.cn'], 'authors': ['Xitong Gao', 'Yiren Zhao', 'Łukasz Dudziak', 'Robert Mullins', 'Cheng-zhong Xu'], 'TL;DR': 'We make convolutional layers run faster by dynamically boosting and suppressing channels in feature computation.', 'pdf': '/pdf/305c0e4e8d26ee4200babe028ceb423be60ec102.pdf', 'paperhash': 'gao|dynamic_channel_pruning_feature_boosting_and_suppression', '_bibtex': '@inproceedings{\ngao2018dynamic,\ntitle={Dynamic Channel Pruning: Feature Boosting and Suppression},\nauthor={Xitong Gao and Yiren Zhao and Łukasz Dudziak and Robert Mullins and Cheng-zhong Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxh2j0qYm},\n}'}		BJxh2j0qYm	BJxh2j0qYm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	748	HJgCOaw9t7	['everyone']		['ICLR.cc/2019/Conference']	1538087860091		1548651349935	['ICLR.cc/2019/Conference']
486	1538087988778	{'title': 'A Variational Inequality Perspective on Generative Adversarial Networks', 'abstract': 'Generative adversarial networks (GANs) form a generative modeling approach known for producing appealing samples, but they are notably difficult to train. One common way to tackle this issue has been to propose new formulations of the GAN objective. Yet, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend methods designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a novel computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam.', 'keywords': ['optimization', 'variational inequality', 'games', 'saddle point', 'extrapolation', 'averaging', 'extragradient', 'generative modeling', 'generative adversarial network'], 'authorids': ['gauthier.gidel@umontreal.ca', 'hugo.berard@gmail.com', 'gaetan.vignoud@gmail.com', 'vincentp@iro.umontreal.ca', 'slacoste@iro.umontreal.ca'], 'authors': ['Gauthier Gidel', 'Hugo Berard', 'Gaëtan Vignoud', 'Pascal Vincent', 'Simon Lacoste-Julien'], 'TL;DR': 'We cast GANs in the variational inequality framework and import techniques from this literature to optimize GANs better; we give algorithmic extensions and empirically test their performance for training GANs.', 'pdf': '/pdf/4a763815ea6b5ec591836c935240062524afd3a4.pdf', 'paperhash': 'gidel|a_variational_inequality_perspective_on_generative_adversarial_networks', '_bibtex': '@inproceedings{\ngidel2018a,\ntitle={A Variational Inequality Perspective on Generative Adversarial Networks},\nauthor={Gauthier Gidel and Hugo Berard and Gaëtan Vignoud and Pascal Vincent and Simon Lacoste-Julien},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1laEnA5Ym},\n}'}		r1laEnA5Ym	r1laEnA5Ym	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1491	rJlqs7_vFX	['everyone']		['ICLR.cc/2019/Conference']	1538087988778		1548646270226	['ICLR.cc/2019/Conference']
487	1538087841160	"{'title': 'Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning', 'abstract': ""Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach uses meta-learning to train a dynamics model prior such that, when combined with recent data, this prior can be rapidly adapted to the local context. Our experiments demonstrate online adaptation for continuous control tasks on both simulated and real-world agents. We first show simulated agents adapting their behavior online to novel terrains, crippled body parts, and highly-dynamic environments. We also illustrate the importance of incorporating online adaptation into autonomous agents that operate in the real world by applying our method to a real dynamic legged millirobot: We demonstrate the agent's learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads."", 'keywords': ['meta-learning', 'reinforcement learning', 'meta reinforcement learning', 'online adaptation'], 'authorids': ['iclavera@berkeley.edu', 'nagaban2@berkeley.edu', 'simin.liu@berkeley.edu', 'ronf@berkeley.edu', 'pabbeel@berkeley.edu', 'svlevine@eecs.berkeley.edu', 'cbfinn@eecs.berkeley.edu'], 'authors': ['Ignasi Clavera', 'Anusha Nagabandi', 'Simin Liu', 'Ronald S. Fearing', 'Pieter Abbeel', 'Sergey Levine', 'Chelsea Finn'], 'TL;DR': 'A model-based meta-RL algorithm that enables a real robot to adapt online in dynamic environments', 'pdf': '/pdf/231f0a8ca2b1dbf81e529dbde7026283f7c53f2c.pdf', 'paperhash': 'clavera|learning_to_adapt_in_dynamic_realworld_environments_through_metareinforcement_learning', '_bibtex': '@inproceedings{\nclavera2018learning,\ntitle={Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning},\nauthor={Ignasi Clavera and Anusha Nagabandi and Simin Liu and Ronald S. Fearing and Pieter Abbeel and Sergey Levine and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyztsoC5Y7},\n}'}"		HyztsoC5Y7	HyztsoC5Y7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	642	SygH8F55Y7	['everyone']		['ICLR.cc/2019/Conference']	1538087841160		1548645491647	['ICLR.cc/2019/Conference']
488	1548643333405	{'title': 'Response', 'comment': 'We thank the reviewer for the feedback. We’ve uploaded a revised version based on the suggestions. \n\n1. We don’t claim major contribution on the model. The main contribution of this work is the large-scale dataset created from ClueWeb, and we show the textual relation embeddings learned from the data can help downstream relational understanding tasks. The Transformer is one possible architecture to model the textual relations. The technical novelty of NRE model is not the major focus of this work.  \n\n2. On relation extraction, the models augmented with the learned embedding get an improvement over GloRE on manual evaluations, which is considered more convincing and precise than automatic evaluation under distant supervision setting. '}		SklKr-qaT7	rJlTWhk37E	AKBC.ws/2019/Conference/-/Paper52/Official_Comment	['AKBC.ws/2019/Conference/Paper52/Reviewers/Unsubmitted']	3		['everyone']	H1eadgY3J4	['AKBC.ws/2019/Conference/Paper52/Authors']	1548643333405		1548643333405	['AKBC.ws/2019/Conference/Paper52/Authors', 'AKBC.ws/2019/Conference']
489	1548643193662	{'title': 'Response', 'comment': 'We thank the reviewer for the detailed comments and suggestions. We’ve uploaded a revised version based on the feedbacks. \n\nThe usefulness of the pre-trained embedding is two-folded: 1. Serving as an additional feature to provide complementary information on downstream tasks (like relation extraction on NYT). We get comparable performance in the automatic evaluation and an improvement in the manual evaluation, which is considered more precise under distant supervision setting. 2. Alleviating the need of designing task-specific textual relation models. On FB15k-237, we replace the original neural network with the learned embedding followed by one simple feed-forward layer. Thus a comparable performance can demonstrate the usefulness of the embedding. \n\n“I also think there is a missing experiment in 4.2 to support…”\nWe agree with carrying the experiment of using the textual relation embedding to augment GloRE. However, in the implementation of GloRE (https://github.com/ppuliu/GloRE), the merging model calculates loss for each individual target relation. Their result is unnormalized scores excluding “NA”. Therefore it’s hard to get a probability distribution over all targets serving as a base score, like PCNN+ATT. \n\n“I believe the authors are unaware of the work…”\nIn the revised version we add some discussion of the mentioned related works. https://arxiv.org/pdf/1809.03401.pdf and https://arxiv.org/pdf/1810.08854.pdf learn the embedding of word pairs based on their context words, which is not the textual relation (shortest dependency path) between them. \n\n“I also found the loss function a bit weird….why not use it as the target?”\nThis is due to the wrong labeling problem of distant supervision, the reason why we employ global co-occurrence statistics as supervision signal. The true relation each instance refers to cannot be always told by distant supervision, because there may be multiple relations between an entity pair, while at most one relation is truly expressed in an instance. Also sometimes the entity pair just appears together and refers to no relation. Distant supervision maps all the KB relations to each instance, thus results some wrong labels. See figure 2 for an example of wrong labeling problem and how to combat with global co-occurrence statistics. \n\n“- What is the difference from Su et al., 2017?...”\nTo show a clear ablation, in the revised version we denote the embedding trained with RNN as GloRE+, standing for new data; the embedding trained with Transformer as GloRE++, standing for both new data and different model. '}		SklKr-qaT7	ByxzKiy2XE	AKBC.ws/2019/Conference/-/Paper52/Official_Comment	['AKBC.ws/2019/Conference/Paper52/Reviewers/Unsubmitted']	2		['everyone']	Bkgxrh9zfN	['AKBC.ws/2019/Conference/Paper52/Authors']	1548643193662		1548643193662	['AKBC.ws/2019/Conference/Paper52/Authors', 'AKBC.ws/2019/Conference']
490	1548642355726	{'title': 'Response', 'comment': 'We thank the reviewer for the detailed feedback. We’ve uploaded a revised version based on the feedbacks, and will answer the questions and respond to each comment. \n\n“(-) The main contribution of the paper is applying the work of Su et al., 2017...“\nWe don’t claim major contribution on the method or the model. The main contribution of this work is the aligned large-scale dataset and the textual relation embedding learned through global co-occurrence, which we believe can facilitate the research on relational understanding tasks. \n\n“(-) Even after training on the large corpus, the model only performs comparably…”\nOn the NYT dataset, models augmented with textual relation embedding show an improvement on manual evaluation results, the precision of predictions with top confidence, which is considered more precise than automatic evaluation under distant supervision setting. The usage of the learned textual relation embedding is two-folded: 1. Serving as an additional feature to provide complementary information on downstream tasks, like relation extraction on NYT. 2. Alleviating the need of designing task-specific textual relation models. On FB15k-237, we replace the original neural network with the learned embedding followed by one simple feed-forward layer. Thus a comparable performance can demonstrate the usefulness of the embedding. \n\n“(-) I encountered problems understanding Sec 4.2….” \nAveraging is a simple baseline approach. There could be other better ways here, which we leave for future work. \n\n“Also do you fine tune the textual embeddings on the NYT…”\nNo, we do not fine tune the embedding. In the updated version, on the start of sec 4, we state that for both tasks we do not fine tune the embedding. \n\n“Would there be any effort to make the gathered aligned dataset public”\nYes, we will release the gathered aligned dataset, together with the relation graph. This is a major contribution of the work and we hope the large-scale data can facilitate future research in this area. '}		SklKr-qaT7	BJxhVOk3XV	AKBC.ws/2019/Conference/-/Paper52/Official_Comment	['AKBC.ws/2019/Conference/Paper52/Reviewers/Unsubmitted']	1		['everyone']	SyeT5Xz7zV	['AKBC.ws/2019/Conference/Paper52/Authors']	1548642355726		1548642355726	['AKBC.ws/2019/Conference/Paper52/Authors', 'AKBC.ws/2019/Conference']
491	1542459712724	{'title': 'Global Textual Relation Embedding for Relational Understanding', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper52/Authors'], 'keywords': ['Textual relation embedding', 'Relation extraction', 'Pre-trained embedding', 'Knowledge base completion'], 'abstract': 'Pre-trained embeddings such as word embeddings and sentence embeddings are fundamental tools facilitating a wide range of downstream NLP tasks. In this work, we investigate how to learn a general-purpose embedding of textual relations, defined as the shortest dependency path between entities. Textual relation embedding provides a level of knowledge between word/phrase level and sentence level. We show that it can facilitate downstream tasks requiring relational understanding of text. To learn such an embedding, we create the largest distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase. Using the global co-occurrence statistics between textual and knowledge base relations as supervision signal, we learn the embedding of textual relations with the Transformer model. We conduct intrinsic and extrinsic evaluation on two representative downstream tasks requiring relational understanding, and demonstrate that the learned textual relation embedding serves as a good prior for these tasks and boosts their performance. Our code and pre-trained model can be found at https://github.com/anonymous-repo.', 'pdf': '/pdf/68b4a8852a34e7deb96e515add12860bf2db4a2f.pdf', 'archival status': 'Non-Archival', 'subject areas': ['Information Extraction'], 'paperhash': 'anonymous|global_textual_relation_embedding_for_relational_understanding', '_bibtex': '@inproceedings{    \nanonymous2019global,    \ntitle={Global Textual Relation Embedding for Relational Understanding},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=SklKr-qaT7},    \nnote={under review}    \n}'}		SklKr-qaT7	SklKr-qaT7	AKBC.ws/2019/Conference/-/Blind_Submission	[]	52	HJlA_KbTam	['everyone']		['AKBC.ws/2019/Conference']	1542459712724		1548642179349	['AKBC.ws/2019/Conference']
492	1548635657364	"{'title': 'Response', 'comment': '\n>>> I believe there are a few other issues not adequately highlighted in the reviews that prevent this work from being accepted:\n\nResponse: We would like to thank the AC for taking the time to post the meta-review.  Unfortunately, the major concerns are simply not correct.  Below we address the points raised:\n\n>>> [premises] It has not been adequately established that ""large batch training often times leads to degradation in accuracy"" inherently which is an important premise of this work. \n\nResponse: That is incorrect. Multiple (if not tens) of works have shown that large batch size leads to significant degradation of accuracy (arxiv: 1609.04836, 1802.08241, 1706.02677,1811.12941, 1811.03600, etc). This is a well established phenomena and it is surprising that the area chair brings this point. Even the papers that the AC cites do clearly mention this.\n\n>>>Even the framing of this issue has become confused since, although it may be possible to achieve the same accuracy at any batch size with careful tuning, this might require using (at worst) the same number of steps as the smaller batch size in some cases and thus result in little to no speedup. For example see https://arxiv.org/abs/1705.08741 and recent work in https://arxiv.org/abs/1811.03600 for more information.\n\nResponse: This is incorrect. Large batch does not “at worst” need the same number of steps as smaller batch to achieve the same accuracy. It is not clear to us why the AC would even claim this. Large batch training may not be able to recover baseline if a vanilla SGD is used even with longer training (arxiv:1811.03600 Figure 23).\n\n>>> Even Keskar et al. reported that data augmentation eliminated the solution quality difference between their larger batch size and their smaller batch size experiments which indicates that even if noisiness from small batches serving to regularize training other regularization techniques can serve just as well.\n\nResponse: This is completely incorrect and a misunderstanding of Keskar’s paper by the AC. Keskar’s paper (arxiv: 1609.04836) showed that data augmentation can partially alleviate the generation gap between small and large batch training, and it is not possible to completely close this gap with data-augmentation. \n\n>>> [baseline strength] The appropriate baseline is standard minibatch SGD w/momentum (or ADAM or whatever) algorithm with extremely careful tuning of *all* of the hyperparameters. None of the popular learning rate heuristics will always work and other optimization parameters need to be tuned as well. If learning rate decay is used, it should also be tuned especially if one is trying to measure a speedup. The submission does not provide a sufficiently convincing baseline.\n\nResponse: This is not applicable to our work. We do NOT do any hyper-parameter tuning, and we emphasized this in multiple places in the paper. This was one of the main points of the paper, and it is very disappointing that it has been missed. Moreover, we did not change any hyper-parameters for the baseline neural networks that we used.\n\n>>> [measurement protocol] The protocol for measuring a speedup is not convincing without more information on how the baselines were tuned to achieve the same accuracy in the fewest steps. Approximating the protocols in https://arxiv.org/abs/1811.03600 would be one alternative.\n\nResponse: Please see above, we did not tune any hyper-parameters.\n\n\n\nAlso we should mention that Reviewer 1 is pointing to our own code in his review as a reason that our Hessian backpropogation is not novel.  That is, due to the double blind aspect of this publication venue, a reviewer incorrectly thought that our novel Hessian backpropogation procedure was due to someone else and thus not novel.   It was very frustrating for us that we could not clear this due to double blind review policy. Moreover, the reviewer completely missed the supplementary material (thus mentioning non-existent figures). We had hoped that he would at least read our response but unfortunately that did not happen. Combined with AC’s meta review it seems our paper’s points and efforts to clear up confusions were not helpful.\n\nAt the end, we would like to specially thank Reviewer 2 and Reviewer 3 for taking the time to follow up with our response, and providing their valuable feedback.\n'}"		H1lnJ2Rqt7	Hyg-zATimE	ICLR.cc/2019/Conference/-/Paper1021/Official_Comment	['ICLR.cc/2019/Conference/Paper1021/Reviewers/Unsubmitted']	13		['everyone']	HyxnRTvyx4	['ICLR.cc/2019/Conference/Paper1021/Authors']	1548635657364		1548635692391	['ICLR.cc/2019/Conference/Paper1021/Authors', 'ICLR.cc/2019/Conference']
493	1548623089178	{'title': 'DiaBLa: A Corpus of Bilingual Spontaneous Written Dialogues for Machine Translation', 'abstract': 'We present a new English-French test set for the evaluation of Machine Translation (MT) for informal, written bilingual dialogue. The test set contains 144 spontaneous dialogues (5,700+ sentences) between native English and native French speakers, mediated by one of two neural MT systems in a range of role-play settings. The dialogues are accompanied by fine-grained sentence-level judgments of MT quality, produced by the dialogue participants themselves, as well as by manually normalised versions and human reference translations produced a posteriori. The motivation for the corpus is two-fold: (i) providing a unique resource for the evaluation of MT models, and (ii) providing a corpus for future analysis of MT-mediated communciation. We provide a preliminary analysis of the corpus to confirm that the participants’ judgements reveal perceptible differences in MT quality between the two MT systems used.', 'authors': ['Anonymous'], 'keywords': ['machine translation', 'dialogue', 'corpus'], 'authorids': ['OpenReview.net/Anonymous_Preprint/Paper49/Authors'], 'pdf': '/pdf/44a2cda2273c716d5f1acc7fff7a9148889e7657.pdf', 'paperhash': 'anonymous|diabla_a_corpus_of_bilingual_spontaneous_written_dialogues_for_machine_translation', '_bibtex': '@unpublished{          \nanonymous2018diabla:,          \ntitle={DiaBLa: A Corpus of Bilingual Spontaneous Written Dialogues for Machine Translation},          \nauthor={Anonymous},          \njournal={OpenReview Preprint},          \nyear={2018},          \nnote={anonymous preprint under review}      \n}'}		B1xKgaqomN	B1xKgaqomN	OpenReview.net/Anonymous_Preprint/-/Blind_Submission	[]	49	rklKla9s7V	['everyone']		['OpenReview.net/Anonymous_Preprint']	1548623089178		1548623089294	['OpenReview.net/Anonymous_Preprint']
494	1548622037579	{'title': 'Integrating and Evaluating Extra-linguistic Context in Neural Machine Translation', 'abstract': 'In Machine Translation (MT), taking into account information related to the setting in which a text is produced can be crucial. We investigate the impact of different extra-linguistic factors (speaker gender, speaker age, film genre and film year) on the MT of subtitles. Our starting point is the pseudo-token approach (Sennrich, 2016). We explore the simultaneous addition of multiple factors in various orders in order to assess the limits of treating these factors as a sequence of words. We compare this approach to the encoding of the same factors using an additional, separate encoder. We evaluate both using BLEU and a targeted evaluation of how well the context is used. Our results show that both strategies are well adapted to exploiting such context.  Contrary to our intuitions, the pseudo-token approach appears unperturbed by the use of multiple values in various orders, and also results in significant improvements in BLEU score (p<0.01).  The multi-encoder approach proves more effective at integrating context but results in lower overall BLEU scores.', 'authors': ['Anonymous'], 'keywords': ['machine translation', 'context'], 'authorids': ['OpenReview.net/Anonymous_Preprint/Paper48/Authors'], 'pdf': '/pdf/80d7948be0309a36f23cd4c0d03465fc4e1bb10c.pdf', 'paperhash': 'anonymous|integrating_and_evaluating_extralinguistic_context_in_neural_machine_translation', '_bibtex': '@unpublished{          \nanonymous2018integrating,          \ntitle={Integrating and Evaluating Extra-linguistic Context in Neural Machine Translation},          \nauthor={Anonymous},          \njournal={OpenReview Preprint},          \nyear={2018},          \nnote={anonymous preprint under review}      \n}'}		SJeARdqoXV	SJeARdqoXV	OpenReview.net/Anonymous_Preprint/-/Blind_Submission	[]	48	HkxpC_9iX4	['everyone']		['OpenReview.net/Anonymous_Preprint']	1548622037579		1548622037701	['OpenReview.net/Anonymous_Preprint']
495	1538087995849	{'title': 'Learning to Infer and Execute 3D Shape Programs', 'abstract': 'Human perception of 3D shapes goes beyond reconstructing them as a set of points or a composition of geometric primitives: we also effortlessly understand higher-level shape structure such as the repetition and reflective symmetry of object parts. In contrast, recent advances in 3D shape sensing focus more on low-level geometry but less on these higher-level relationships. In this paper, we propose 3D shape programs, integrating bottom-up recognition systems with top-down, symbolic program structure to capture both low-level geometry and high-level structural priors for 3D shapes. Because there are no annotations of shape programs for real shapes, we develop neural modules that not only learn to infer 3D shape programs from raw, unannotated shapes, but also to execute these programs for shape reconstruction. After initial bootstrapping, our end-to-end differentiable model learns 3D shape programs by reconstructing shapes in a self-supervised manner. Experiments demonstrate that our model accurately infers and executes 3D shape programs for highly complex shapes from various categories. It can also be integrated with an image-to-shape module to infer 3D shape programs directly from an RGB image, leading to 3D shape reconstructions that are both more accurate and more physically plausible.', 'keywords': ['Program Synthesis', '3D Shape Modeling', 'Self-supervised Learning'], 'authorids': ['yonglong@mit.edu', 'aluo@mit.edu', 'xs5@princeton.edu', 'ellisk@mit.edu', 'billf@mit.edu', 'jbt@mit.edu', 'jiajunwu@mit.edu'], 'authors': ['Yonglong Tian', 'Andrew Luo', 'Xingyuan Sun', 'Kevin Ellis', 'William T. Freeman', 'Joshua B. Tenenbaum', 'Jiajun Wu'], 'TL;DR': 'We propose 3D shape programs, a structured, compositional shape representation. Our model learns to infer and execute shape programs to explain 3D shapes.', 'pdf': '/pdf/202578a96980d559719f24512d11b420b5489676.pdf', 'paperhash': 'tian|learning_to_infer_and_execute_3d_shape_programs', '_bibtex': '@inproceedings{\ntian2018learning,\ntitle={Learning to Infer and Execute 3D Shape Programs},\nauthor={Yonglong Tian and Andrew Luo and Xingyuan Sun and Kevin Ellis and William T. Freeman and Joshua B. Tenenbaum and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylNH20qFQ},\n}'}		rylNH20qFQ	rylNH20qFQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1532	H1xqKCTcYX	['everyone']		['ICLR.cc/2019/Conference']	1538087995849		1548619125059	['ICLR.cc/2019/Conference']
496	1548616592682	{'title': 'Reply', 'comment': 'Thanks for pointing out this related work, we have cited it appropriately.'}		rylDfnCqF7	HylFc7YoX4	ICLR.cc/2019/Conference/-/Paper1274/Official_Comment	['ICLR.cc/2019/Conference/Paper1274/Reviewers/Unsubmitted']	16		['everyone']	H1esYBzqxN	['ICLR.cc/2019/Conference/Paper1274/Authors']	1548616592682		1548616592682	['ICLR.cc/2019/Conference/Paper1274/Authors', 'ICLR.cc/2019/Conference']
497	1538087951133	"{'title': 'Lagging Inference Networks and Posterior Collapse in Variational Autoencoders', 'abstract': 'The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique. By using a neural inference network to approximate the model\'s posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods. In practice, however, VAE training often results in a degenerate local optimum known as ""posterior collapse"" where the model learns to ignore the latent variable and the approximate posterior mimics the prior. In this paper, we investigate posterior collapse from the perspective of training dynamics. We find that during the initial stages of training the inference network fails to approximate the model\'s true posterior, which is a moving target. As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs. Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model\'s current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update. Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work. Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster.', 'keywords': ['variational autoencoders', 'posterior collapse', 'generative models'], 'authorids': ['junxianh@cs.cmu.edu', 'dspokoyn@cs.cmu.edu', 'gneubig@cs.cmu.edu', 'tberg@cs.cmu.edu'], 'authors': ['Junxian He', 'Daniel Spokoyny', 'Graham Neubig', 'Taylor Berg-Kirkpatrick'], 'TL;DR': 'To address posterior collapse in VAEs, we propose a novel yet simple training procedure that aggressively optimizes inference network with more updates. This new training procedure mitigates posterior collapse and leads to a better VAE model. ', 'pdf': '/pdf/47f79f4015dbabc7f2eab6e432cddf975cf1c486.pdf', 'paperhash': 'he|lagging_inference_networks_and_posterior_collapse_in_variational_autoencoders', '_bibtex': '@inproceedings{\nhe2018lagging,\ntitle={Lagging Inference Networks and Posterior Collapse in Variational Autoencoders},\nauthor={Junxian He and Daniel Spokoyny and Graham Neubig and Taylor Berg-Kirkpatrick},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylDfnCqF7},\n}'}"		rylDfnCqF7	rylDfnCqF7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1274	HyxuP0h9F7	['everyone']		['ICLR.cc/2019/Conference']	1538087951133		1548616470435	['ICLR.cc/2019/Conference']
498	1547149971417	"{'title': 'Although the paper has a lot of promise, it is not suitable for publication in an academic conference in its present form, in my view. I advise the authors to review all the comments, and prepare a significantly revised version for future publication.', 'review': ""\nThe authors present Alexandria, a system for unsupervised, high-precision knowledge base construction. Alexandria uses a probabilistic program to define a process of converting knowledge base facts into unstructured text. The authors evaluate Alexandria by constructing a high precision (typically 97%+) knowledge base for people from a single seed fact. \n\nStrengths:\n\n--although it is unusual to combine and intro and related work into a single section, I enjoyed the authors' succinct statement (which I agree with) about the 'holy grail of KB construction'. Overall, I think the writing of the paper started off on a strong note.\n\n--the paper has a lot of promise in considering a unified view of KB construction. Although I am not recommending an accept, I hope the authors will continue this work and submit a revised version for future consideration (whether in the next iteration of this conference, or another)\n\nWeaknesses:\n\n--It is not necessary to be putting 'actual programs' (Figure 1). It does not serve much of a purpose and is inappropriate; the authors should either use pseudocode, or just describe it in the text. \n\n--In Web-scale fact extraction, the domain-specific insight graphs (DIG) system should also be mentioned and cited, since it is an end-to-end system that has been used for KGC in unusual domains like human trafficking. \n\n--Starting from Section 2, the whole paper starts reading a bit like a technical manual/specification document. This kind of paper is not appropriate for an academic audience; the paper should have been written in a much more conceptual way, with actual implementations/programs/code details relegated to a github repo/wiki, and with a link to the same in the paper."", 'rating': '6: Marginally above acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		rJgHCgc6pX	rygo5GmrM4	AKBC.ws/2019/Conference/-/Paper9/Official_Review	['AKBC.ws/2019/Conference/Paper9/Reviewers/Unsubmitted']	3		['everyone']	rJgHCgc6pX	['AKBC.ws/2019/Conference/Paper9/AnonReviewer2']	1547149971417		1548614117535	['AKBC.ws/2019/Conference/Paper9/AnonReviewer2', 'AKBC.ws/2019/Conference']
499	1538087899228	{'title': 'Unsupervised Speech Recognition via Segmental Empirical Output Distribution Matching', 'abstract': 'We consider the problem of training speech recognition systems without using any labeled data, under the assumption that the learner can only access to the input utterances and a phoneme language model estimated from a non-overlapping corpus. We propose a fully unsupervised learning algorithm that alternates between solving two sub-problems: (i) learn a phoneme classifier for a given set of phoneme segmentation boundaries, and (ii) refining the phoneme boundaries based on a given classifier. To solve the first sub-problem, we introduce a novel unsupervised cost function named Segmental Empirical Output Distribution Matching, which generalizes the work in (Liu et al., 2017) to segmental structures. For the second sub-problem, we develop an approximate MAP approach to refining the boundaries obtained from Wang et al. (2017). Experimental results on TIMIT dataset demonstrate the success of this fully unsupervised phoneme recognition system, which achieves a phone error rate (PER) of 41.6%. Although it is still far away from the state-of-the-art supervised systems, we show that with oracle boundaries and matching language model, the PER could be improved to 32.5%. This performance approaches the supervised system of the same model architecture, demonstrating the great potential of the proposed method. ', 'keywords': ['Unsupervised speech recognition', 'unsupervised learning', 'phoneme classification'], 'authorids': ['cjyeh@cs.cmu.edu', 'chenjianshu@gmail.com', 'czyu@tencent.com', 'dyu@tencent.com'], 'authors': ['Chih-Kuan Yeh', 'Jianshu Chen', 'Chengzhu Yu', 'Dong Yu'], 'pdf': '/pdf/b8062a74dd29e9171df591c59f61ea73b7bc5cd2.pdf', 'paperhash': 'yeh|unsupervised_speech_recognition_via_segmental_empirical_output_distribution_matching', '_bibtex': '@inproceedings{\nyeh2018unsupervised,\ntitle={Unsupervised Speech Recognition via Segmental Empirical Output Distribution Matching},\nauthor={Chih-Kuan Yeh and Jianshu Chen and Chengzhu Yu and Dong Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bylmkh05KX},\n}'}		Bylmkh05KX	Bylmkh05KX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	973	SyxCUa_cK7	['everyone']		['ICLR.cc/2019/Conference']	1538087899228		1548604105456	['ICLR.cc/2019/Conference']
500	1538087842923	{'title': 'Stable Opponent Shaping in Differentiable Games', 'abstract': 'A growing number of learning methods are actually differentiable games whose players optimise multiple, interdependent objectives in parallel – from GANs and intrinsic curiosity to multi-agent RL. Opponent shaping is a powerful approach to improve learning dynamics in these games, accounting for player influence on others’ updates. Learning with Opponent-Learning Awareness (LOLA) is a recent algorithm that exploits this response and leads to cooperation in settings like the Iterated Prisoner’s Dilemma. Although experimentally successful, we show that LOLA agents can exhibit ‘arrogant’ behaviour directly at odds with convergence. In fact, remarkably few algorithms have theoretical guarantees applying across all (n-player, non-convex) games. In this paper we present Stable Opponent Shaping (SOS), a new method that interpolates between LOLA and a stable variant named LookAhead. We prove that LookAhead converges locally to equilibria and avoids strict saddles in all differentiable games. SOS inherits these essential guarantees, while also shaping the learning of opponents and consistently either matching or outperforming LOLA experimentally.', 'keywords': ['multi-agent learning', 'multiple interacting losses', 'opponent shaping', 'exploitation', 'convergence'], 'authorids': ['ahp.letcher@gmail.com', 'jakobfoerster@gmail.com', 'dbalduzzi@google.com', 'tim.rocktaeschel@gmail.com', 'shimon.whiteson@cs.ox.ac.uk'], 'authors': ['Alistair Letcher', 'Jakob Foerster', 'David Balduzzi', 'Tim Rocktäschel', 'Shimon Whiteson'], 'TL;DR': 'Opponent shaping is a powerful approach to multi-agent learning but can prevent convergence; our SOS algorithm fixes this with strong guarantees in all differentiable games.', 'pdf': '/pdf/70c1f902a0584a62f83691cdcb6cd15b62433c8b.pdf', 'paperhash': 'letcher|stable_opponent_shaping_in_differentiable_games', '_bibtex': '@inproceedings{\nletcher2018stable,\ntitle={Stable Opponent Shaping in Differentiable Games},\nauthor={Alistair Letcher and Jakob Foerster and David Balduzzi and Tim Rocktäschel and Shimon Whiteson},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyGjjsC5tQ},\n}'}		SyGjjsC5tQ	SyGjjsC5tQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	652	Hygo2bK9F7	['everyone']		['ICLR.cc/2019/Conference']	1538087842923		1548602992185	['ICLR.cc/2019/Conference']
501	1538087955054	{'title': 'Fixup Initialization: Residual Learning Without Normalization', 'abstract': 'Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic. In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization. Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization. We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers. Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation.', 'keywords': ['deep learning', 'residual networks', 'initialization', 'batch normalization', 'layer normalization'], 'authorids': ['hongyiz@mit.edu', 'yann@dauphin.io', 'tengyuma@stanford.edu'], 'authors': ['Hongyi Zhang', 'Yann N. Dauphin', 'Tengyu Ma'], 'TL;DR': 'All you need to train deep residual networks is a good initialization; normalization layers are not necessary.', 'pdf': '/pdf/f077798016d6857fa8f466cf7cec45412cbfd164.pdf', 'paperhash': 'zhang|fixup_initialization_residual_learning_without_normalization', '_bibtex': '@inproceedings{\nzhang2018residual,\ntitle={Residual Learning Without Normalization via Better Initialization},\nauthor={Hongyi Zhang and Yann N. Dauphin and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gsz30cKX},\n}'}		H1gsz30cKX	H1gsz30cKX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1297	HJxQcfC9tX	['everyone']		['ICLR.cc/2019/Conference']	1538087955054		1548567670794	['ICLR.cc/2019/Conference']
502	1542459687384	{'title': 'Commonsense Knowledge Types Identification andReasoning for the Winograd Schema Challenge', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper43/Authors'], 'keywords': ['Commonsense Knowledge Types Identification', 'Winograd Schema Challenge', 'Commonsense Reasoning'], 'TL;DR': 'In this paper we describe various new commonsense knowledge types by exploiting a Winograd Schema Challenge dataset which will become helpful in crowd-sourcing or automating knowledge extraction.', 'abstract': 'The Winograd Schema Challenge was proposed as an alternative to the Turing test in 2011.  It is a text based question answering challenge which, according to its creators,“requires world knowledge and default reasoning abilities”. In this work, we performed a comprehensive study of the problems in the challenge from the perspective of identifying the types of knowledge which are needed to solve them.  We identified 12 knowledge types to address the entire challenge corpus.  We also defined a logical reasoning algorithm to tackle 10 out of 12 knowledge types.  The algorithm covers 82.47% of all the problems in the dataset.  Furthermore, we show how the overall approach generalizes on a well known pronoun resolution corpus which is inspired from the Winograd Schema Challenge.', 'pdf': '/pdf/6e1483dc58e2a0002536ee933305a50b084e9257.pdf', 'archival status': 'Non-Archival', 'subject areas': ['Question Answering', 'Reasoning', 'Knowledge Representation'], 'paperhash': 'anonymous|commonsense_knowledge_types_identification_andreasoning_for_the_winograd_schema_challenge', '_bibtex': '@inproceedings{    \nanonymous2019commonsense,    \ntitle={Commonsense Knowledge Types Identification andReasoning for the Winograd Schema Challenge},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=r1gyNZ9Tam},    \nnote={under review}    \n}'}		r1gyNZ9Tam	r1gyNZ9Tam	AKBC.ws/2019/Conference/-/Blind_Submission	[]	43	S1gqeEHpa7	['everyone']		['AKBC.ws/2019/Conference']	1542459687384		1548558483303	['AKBC.ws/2019/Conference']
503	1548558429768	"{'title': 'Author Reply', 'comment': 'Dear Reviewer, Thank you so much for your valuable feedback. Please find our reply to your concern about the manual knowledge extraction and the future extension of contributions of our work.\n\n1. The knowledge types identified in this work are useful in developing a large knowledge base. There are two prominent approaches for developing such a knowledge base. First is the automatic extraction of the knowledge from text. An example of such an extraction is as mentioned in the section 4.3.1 of the paper. Second and currently more popular approach is the use of crowd-sourcing. A recent example of such an effort is as shown in the ATOMIC knowledge base (https://homes.cs.washington.edu/~msap/atomic/). One of the common requirement of both of these approaches is the ""kind"" of knowledge that is being extracted. It helps in the development of knowledge specific extraction algorithm in case of automatic extraction from text and in the development of the knowledge type specific setups provided to crowd workers in case of crowd-sourcing.\n\n2. The current analysis on identifying the knowledge types can be extended to create a comprehensive list of knowledge types by analyzing other problems to identify the knowledge types. We believe such a categorization will be very useful in extracting/crowd-sourcing knowledge as mentioned above.\n\n3. We have taken care of the minor comments in the revised version of the paper.'}"		r1gyNZ9Tam	SkeUPgsqQN	AKBC.ws/2019/Conference/-/Paper43/Official_Comment	['AKBC.ws/2019/Conference/Paper43/Reviewers/Unsubmitted']	3		['everyone']	r1lRdvJ4M4	['AKBC.ws/2019/Conference/Paper43/Authors']	1548558429768		1548558429768	['AKBC.ws/2019/Conference/Paper43/Authors', 'AKBC.ws/2019/Conference']
504	1548558350391	"{'title': 'Author Reply', 'comment': 'Dear Reviewer, Thank you so much for your comments. Please find below our reply to your concerns about the paper.\n\n1. The main idea behind the merging operation in the reasoning algorithm is to generate a new representation such that it contains the information present in the representation of an input text and it also contains the additional information present in the representation of the input knowledge. Though the idea of merging is general and applies to any given text and knowledge, let us take a look at an example with respect to the Winograd Schema Challenge. \n\nLet S be a WSC sentence such that S = ""Jim yelled at Kevin because he was so upset."", and K be a knowledge such that K = ""x is upset causes x yells"". It can be observed from the knowledge that the person who ""yells"" is also the person who is upset. Using this new information in the knowledge, we can update the sentence and create a new sentence S\' as:\nS\' =  ""Jim,he yelled at Kevin because Jim,he were upset""\nHere, all the information that is present in the input sentence is preserved as is and the information from the knowledge is also added.\n\nThe merging operation in the reasoning algorithm performs the above mentioned update on the input sentence and generates a new merged representation corresponding to the new sentence as mentioned above. A set of logical operations were performed with respect to the input representations to get the merged representation. These operations are as explained in the description of the algorithm in the paper. \n\nNow, one would think that just replacing ""he"" with ""Jim"" in the original sentence above would be enough to solve the problem which would generate the new sentence S\' = ""Jim yelled at Kevin because Jim were upset."". Although that is true with respect to the problems in the WSC challenge corpus, in this work we developed a general algorithm which handles more kinds of question answering problems than just pronoun resolution problems. An example is as shown below.\n\nLet S be a sentence such that S = ""Donald Trump yelled at Rudy Giuliani because the president was upset with Rudy."". Given the sentence S and the knowledge ""x is upset causes x yells"", one could ask questions such as Q1 = ""Who was the president upset with?"", Q2 = ""Who was Donald Trump upset with?"", Q3 = ""Who was upset?"". Now, if we go with our approach of creating a new sentence by merging the knowledge and the sentence, the new sentence S\' becomes ""Donald Trump,the president yelled at Rudy Giuliani because Donald Trump,the president was upset with Rudy."". All the above given questions can be easily answered by using the new sentence S\'. On the other hand, if the new sentence is created by replacing one of the entities with the other then following two cases emerge:\n\nCase1: S\' = ""Donald Trump yelled at Rudy Giuliani because Donald Trump was upset with Rudy.""\nCase2: S\' = ""the president yelled at Rudy Giuliani because the president was upset with Rudy.""\n\nEach of the cases mentioned above are insufficient (just by itself) in answering all the questions mentioned above. Case 1 can not be used to answer Q1 and Q3 (in its entirety) and Case 2 can not be used to answer Q2 and Q3 (in its entirety).\n\nWe have also updated the paper to include the above mentioned basic idea behind merging so that the algorithm is more comprehensible.\n\n2. In section 4.2.2, we mean that each problem in the original WSC dataset of 291 problems falls into one of the 12 categories of knowledge. In other words, each of the problems in 291 WSC problems can be solved by using a knowledge of either one of the types in 1 through 12.'}"		r1gyNZ9Tam	rkeLMgj5Q4	AKBC.ws/2019/Conference/-/Paper43/Official_Comment	['AKBC.ws/2019/Conference/Paper43/Reviewers/Unsubmitted']	2		['everyone']	ryxFxMlEGE	['AKBC.ws/2019/Conference/Paper43/Authors']	1548558350391		1548558350391	['AKBC.ws/2019/Conference/Paper43/Authors', 'AKBC.ws/2019/Conference']
505	1548558046872	"{'title': 'Author Reply', 'comment': 'Dear Reviewer, Thank you for your valuable comments. Please find our reply to your concerns about the paper below.\n\n1. We presented a reasoning algorithm to handle 10 kinds out of 12 knowledge types identified in this work. One of the motivations behind the identification of the categories of knowledge was that different knowledge kinds may require different kinds of reasoning. Continuing on the motivation, in this work we progress by focusing on one reasoning approach which covers the knowledge kinds that cover a majority of examples in the WSC challenge and a similar corpora i.e., 82.5% in the WSC corpus and 69% in the DPR corpus. It is because of the ‘sentence1 predicate sentence2’ structure that a single reasoning algorithm was able to handle 10 kinds of knowledge. We realized that the remaining two kinds can not be handled in a similar reasoning approach. We are currently working on defining the reasoning paradigms for the remaining two categories of knowledge. Since the submission of this paper we are working on an idea to reason with the types 11 and 12 knowledge i.e., more likely statement and multiple knowledge pieces. We are currently in the process of formally defining the ideas and testing them on the real WSC dataset. The gist of the idea for handling the ""more likely"" knowledge is to use a Natural Language Inference model for ""more likely"" knowledge. There the statement that is more likely is assumed to be entailed (with greater entailment score as compared to the other statement) by the original WSC sentence when the pronoun to be resolved in it is replaced by the correct answer to the problem. Whereas the idea to handle multiple knowledge pieced is to process one knowledge piece at a time until all the knowledge pieces are processed.\n\nTo evaluate if the identified kinds are actually meaningful, we examined if the identified knowledge types can be used in another similar dataset i.e., DPR. The results show that they are actually useful. We also agree that it will be very useful to preform a comprehensive categorization of the required knowledge to cover many different datasets (of same or different kinds). To that end, in this work we started by analyzing one such corpus. We believe that this work would promote research in the direction of identifying different knowledge types and in the future we or someone else could analyze more corpora to identify new categories if needed.\n\n2. In our work we defined a representation for a sentence (Definition 1 in the paper), a question (Definition 2 in the paper) and a piece of knowledge (Definition 3 in the paper). The representations are used as inputs to the logical reasoning algorithm which outputs the answers to a winograd schema question through a series of logical operations. To evaluate the overall reasoning algorithm we used K-Parser just as a tool for generating the representations of a sentence, a question and a piece of knowledge. As mentioned in the section 4.3 of the paper we transformed the output of K-Parser according to the definitions 1, 2 and 3 so that they can be utilized by our reasoning algorithm. We used K-Parser because it was an easily available semantic parser and its representation was very similar to the representation defined in our work. That being said, any semantic parser which produces a semantic representation that can be automatically transformed into the representation defined in our work can be used in place of K-Parser. Although the K-Parser is not error-free we found it advantageous because as described in the K-Parser description (http://bioai8core.fulton.asu.edu/kparser/about.jsp), it is a pipeline of various NLP tools and hence it is possible to improve parts of it to remove/reduce the overall parsing errors. For example, the 18.1% parsing errors found in the DPR corpus were caused because of the incorrect part-of-speech tagging (7.2%) and the incorrect dependency parsing (10.9%). So, improving those systems in the K-Parser could be helpful in removing the errors caused by them. We made a note of such errors and notified the authors of K-Parser for further improvement of the parser.'}"		r1gyNZ9Tam	B1gPJki5XV	AKBC.ws/2019/Conference/-/Paper43/Official_Comment	['AKBC.ws/2019/Conference/Paper43/Reviewers/Unsubmitted']	1		['everyone']	rJeKV9DNMV	['AKBC.ws/2019/Conference/Paper43/Authors']	1548558046872		1548558046872	['AKBC.ws/2019/Conference/Paper43/Authors', 'AKBC.ws/2019/Conference']
506	1542459698672	{'title': 'End-to-End Entity and Event Extraction with Generative Adversarial Imitation Learning', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper47/Authors'], 'keywords': ['event extraction', 'inverse reinforcement learning'], 'TL;DR': 'We use dynamic rewards to train event extractors.', 'abstract': 'We propose a new framework for entity and event extraction based on generative adversarial imitation learning -- an inverse reinforcement learning method using generative adversarial network (GAN). We assume that instances and labels yield to various extents of difficulty and the gains and penalties (rewards) are expected to be diverse. We utilize discriminators to estimate proper rewards according to the difference between the labels committed by ground-truth (expert) and the extractor (agent).  Experiments also demonstrate that the proposed framework outperforms state-of-the-art methods.', 'pdf': '/pdf/814eef4b3923917102dd1761d4a19a0f1d22928e.pdf', 'archival status': 'Archival', 'subject areas': ['Natural Language Processing', 'Information Extraction'], 'paperhash': 'anonymous|endtoend_entity_and_event_extraction_with_generative_adversarial_imitation_learning', '_bibtex': '@inproceedings{    \nanonymous2019end-to-end,    \ntitle={End-to-End Entity and Event Extraction with Generative Adversarial Imitation Learning},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=BJlsVZ966m},    \nnote={under review}    \n}'}		BJlsVZ966m	BJlsVZ966m	AKBC.ws/2019/Conference/-/Withdrawn_Submission	[]	47	HygHu8OnTQ	['everyone']		['AKBC.ws/2019/Conference']	1542459698672		1548536964942	['AKBC.ws/2019/Conference']
507	1542459647055	{'title': 'How to measure the consistency of the tagging of scientific papers?', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper28/Authors'], 'keywords': ['keyword extraction', 'citation graphs'], 'TL;DR': 'A good tagger gives similar tags to a given paper and the papers it cites', 'abstract': ' A collection of scientific papers is often accompanied by tags:\n  keywords, topics, concepts etc., associated with each paper.\n  Sometimes these tags are human-generated, sometimes they are\n  machine-generated.  We propose a simple measure of the consistency\n  of the tagging of scientific papers: whether these tags are\n  predictive for the citation graph links.  Since the authors tend to\n  cite papers about the topics close to those of their publications, a\n  consistent tagging system could predict citations.  We present an\n  algorithm to calculate consistency, and experiments with human- and\n  machine-generated tags.  We show that augmentation, i.e. the combination\n  of the manual tags with the machine-generated ones, can enhance the\n  consistency of the tags.  We further introduce cross-consistency,\n  the ability to predict citation links between papers tagged by\n  different taggers, e.g. manually and by a machine.\n  Cross-consistency can be used to evaluate the tagging quality when\n  the amount of labeled data is limited.', 'pdf': '/pdf/6f052bbd1a20483892bb717bd45a94ebacc6c713.pdf', 'archival status': 'Archival', 'subject areas': ['Information Extraction', 'Knowledge Representation', 'Applications: Science'], 'paperhash': 'anonymous|how_to_measure_the_consistency_of_the_tagging_of_scientific_papers', '_bibtex': '@inproceedings{    \nanonymous2019how,    \ntitle={How to measure the consistency of the tagging of scientific papers?},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=SyeD-b9T6m},    \nnote={under review}    \n}'}		SyeD-b9T6m	SyeD-b9T6m	AKBC.ws/2019/Conference/-/Withdrawn_Submission	[]	28	BJggSYanaQ	['everyone']		['AKBC.ws/2019/Conference']	1542459647055		1548536964317	['AKBC.ws/2019/Conference']
508	1548528287610	"{'comment': 'There are some local domain adaptation based approaches that can be cited -\n\n[1] Courty, Nicolas, et al. ""Optimal transport for domain adaptation."" IEEE transactions on pattern analysis and machine intelligence 39.9 (2017): 1853-1865.\n\n[2] Das, Debasmit, and CS George Lee. ""Sample-to-sample correspondence for unsupervised domain adaptation."" Engineering Applications of Artificial Intelligence 73 (2018): 80-91.\n\n[3] Debasmit Das and C.S. George Lee, “Unsupervised Domain Adaptation Using Regularized Hyper-Graph Matching,” Proceedings of 2018 IEEE International Conference on Image Processing (ICIP), Athens, Greece, pp. 3758-3762, October 7-10, 2018. \n\n[4] Debasmit Das and CS George Lee. “Graph Matching and Pseudo-Label Guided Deep Unsupervised Domain Adaptation,” Proceedings of 2018 International Conference on Artificial Neural Networks (ICANN), Rhodes, Greece, pp. 342-352, October 4-7, 2018.', 'title': 'Related work on local domain adaptation approaches'}"		B1xFhiC9Y7	S1x_jcmcXN	ICLR.cc/2019/Conference/-/Paper728/Public_Comment	[]	4		['everyone']	B1xFhiC9Y7	['(anonymous)']	1548528287610		1548528287610	['(anonymous)', 'ICLR.cc/2019/Conference']
509	1538087768829	{'title': 'DELTA: DEEP LEARNING TRANSFER USING FEATURE MAP WITH ATTENTION FOR CONVOLUTIONAL NETWORKS', 'abstract': 'Transfer learning through fine-tuning a pre-trained neural network with an extremely large dataset, such as ImageNet, can significantly accelerate training while the accuracy is frequently bottlenecked by the limited dataset size of the new target task. To solve the problem, some regularization methods, constraining the outer layer weights of the target network using the starting point as references (SPAR), have been studied. In this paper, we propose a novel regularized transfer learning framework DELTA, namely DEep Learning Transfer using Feature Map with Attention. Instead of constraining the weights of neural network, DELTA aims to preserve the outer layer outputs of the target network. Specifically, in addition to minimizing the empirical loss, DELTA intends to align the outer layer outputs of two networks, through constraining a subset of feature maps that are precisely selected by attention that has been learned in an supervised learning manner. We evaluate DELTA with the state-of-the-art algorithms, including L2 and L2-SP. The experiment results show that our proposed method outperforms these baselines with higher accuracy for new tasks.', 'keywords': ['transfer learning', 'deep learning', 'regularization', 'attention', 'cnn'], 'authorids': ['1762778193@qq.com', 'xhyccc@gmail.com', 'wanghanchao01@baidu.com', 'yrao4@illinois.edu', 'liuliping@baidu.com', 'huanjun@baidu.com'], 'authors': ['Xingjian Li', 'Haoyi Xiong', 'Hanchao Wang', 'Yuxuan Rao', 'Liping Liu', 'Jun Huan'], 'TL;DR': 'improving deep transfer learning with regularization using attention based feature maps', 'pdf': '/pdf/f0e306fa55677ee7f9e0f111d80a5827a1e904a8.pdf', 'paperhash': 'li|delta_deep_learning_transfer_using_feature_map_with_attention_for_convolutional_networks', '_bibtex': '@inproceedings{\nli2018delta,\ntitle={{DELTA}: {DEEP} {LEARNING} {TRANSFER} {USING} {FEATURE} {MAP} {WITH} {ATTENTION} {FOR} {CONVOLUTIONAL} {NETWORKS}},\nauthor={Xingjian Li and Haoyi Xiong and Hanchao Wang and Yuxuan Rao and Liping Liu and Jun Huan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkgbwsAcYm},\n}'}		rkgbwsAcYm	rkgbwsAcYm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	237	SyxHmL_tFX	['everyone']		['ICLR.cc/2019/Conference']	1538087768829		1548510064171	['ICLR.cc/2019/Conference']
510	1548507033578	"{'title': 'Issue with replicating the results?', 'comment': 'When replicating the results of the CoNLL 2017 shared task from the models here https://nlp.stanford.edu/data/CoNLL17_Parser_Models/ , I ran into an issue as I could not get the same performance as is reported in the paper. My steps in reproducing, I showcase English LinES treebank:\n\n    download ud-treebanks-conll2017 that was distributed among participants of CoNLL 2017 shared task\n    download respective model from https://nlp.stanford.edu/data/CoNLL17_Parser_Models/\n    download test sets ud-test-v2.0-conll2017\n    tag test set file ""en_lines.conllu"" in ud-test-v2.0-conll2017 by the downloaded model ""English-LinES-Tagger""\n    parse tagged in step 4 test set file ""en_lines.conllu"" located in ""English-LinES-Tagger"" by downloaded model ""English-LinES-Parser""\n    evaluate by the conll17_ud_eval.py script with the gold data and parsed result.\n\nThe result is different than the one reported in the paper. So what could have gone wrong?'}"		Hk95PK9le	S1gMiPAK74	ICLR.cc/2017/conference/-/paper504/public/comment	[]	11		['everyone']	Hk95PK9le	['(anonymous)']	1548507033578		1548507063556	['(anonymous)']
511	1548499368822	"{'title': 'Study the Longitudinal in vivo and Cross-Sectional ex vivo Brain Volume Difference for Disease Progression and Treatment Effect on Mouse Model of Tauopathy Using Automated MRI Structural Parcellation', 'authors': ['Da Ma', 'Holly Elizabeth Holmes', 'Manuel Jorge Cardoso', 'Marc Modat', 'Ian Francis Harrison', 'Nick M Powell', ""James O'Callaghan"", 'Ozama Ismail', 'Ross A Johnson', 'Michael J O’Neill', 'Emily Catherine Collins', 'Mirza Faisal Beg', 'Karteek Popuri', 'Mark Lythgoe', 'Sebastien Ourselin'], 'authorids': ['da_ma@sfu.ca', 'h.holmes.11@ucl.ac.uk', 'm.jorge.cardoso@kcl.ac.uk', 'marc.modat@kcl.ac.uk', 'ian.harrison@ucl.ac.uk', 'nicholas.powell.11@ucl.ac.uk', 'james.ocallaghan@ucl.ac.uk', 'o.ismail@ucl.ac.uk', 'johnson_ross_a@lilly.com', 'oneill_michael_j@lilly.com', 'emilycollins@lilly.com', 'mfbeg@sfu.ca', 'kpopuri@sfu.ca', 'm.lythgoe@ucl.ac.uk', 'sebastien.ourselin@kcl.ac.uk'], 'pdf': '/pdf/bdf6a169e0341446182bf9ddd0786263f1d223d8.pdf', 'paperhash': 'ma|study_the_longitudinal_in_vivo_and_crosssectional_ex_vivo_brain_volume_difference_for_disease_progression_and_treatment_effect_on_mouse_model_of_tauopathy_using_automated_mri_structural_parcellation'}"		rJgbnK3YXE	rJgbnK3YXE	OpenReview.net/Archive/-/Direct_Upload	[]	206		['everyone']		['~Da_Ma1']	1548499368822		1548499368822	['~Da_Ma1']
512	1548466707634	{'title': 'Response to Reviewer 3', 'comment': 'Unsupervised Domain Adaptation is a challenging setting without labelled data in the target domain. The proposed projection learning is based on the separation between an instance and its opposite-labelled nearest neighbors. We need to assign the labels for the instances before the projection, so we use self-training [Yarowsky 1995, Abney 2007] as the solution. \n\nResponse to Nits:\n1. Domain Adaptation methods are not limited to the tasks and can be applied to other tasks. We use the task in this paper is only for evaluation.\n\n2. Multiple iterations took longer time on running and a half of the domain pairs produced worse or equal accuracy on the test data. The other half improves less than 1%. \n\n3. In Sec 4.1, we want to show the fair comparison on the steps, it can be fixed to other number.\n\n4. We will (a) either add an experiment or (b) drop this claim from the current version of the paper.\n\n Thanks for your review.\n'}		H1xtpgqTaQ	HyxnM5VYmN	AKBC.ws/2019/Conference/-/Paper4/Official_Comment	['AKBC.ws/2019/Conference/Paper4/Reviewers/Unsubmitted']	4		['everyone']	r1eM6-l-ME	['AKBC.ws/2019/Conference/Paper4/Authors']	1548466707634		1548466707634	['AKBC.ws/2019/Conference/Paper4/Authors', 'AKBC.ws/2019/Conference']
513	1548466423521	{'title': 'Response to Reviewer 2', 'comment': 'Domain adaptation is not limited to sentiment analysis and it is also beneficial to machine reading and automatic knowledge-base completion tasks. For example, a relation extraction system that is trained on one domain must be able to adapt and extract unseen novel relations. This problem is known as relation adaptation [Fu et al. IJCNLP 2017 http://www.aclweb.org/anthology/I17-2072, Bollegala et al. IJCAI 2015 https://www.ijcai.org/Proceedings/11/Papers/368.pdf, Jiang et al. ACL 2009 http://www.aclweb.org/anthology/P09-1059]. In our paper, we use sentiment analysis as evaluation task.\n\nThanks for your review.'}		H1xtpgqTaQ	SJlg-FNtQN	AKBC.ws/2019/Conference/-/Paper4/Official_Comment	['AKBC.ws/2019/Conference/Paper4/Reviewers/Unsubmitted']	3		['everyone']	H1laDEkMfN	['AKBC.ws/2019/Conference/Paper4/Authors']	1548466423521		1548466423521	['AKBC.ws/2019/Conference/Paper4/Authors', 'AKBC.ws/2019/Conference']
514	1548466356587	{'title': 'Response to Reviewer 1', 'comment': '1. We can use any document embedding method to represent reviews. We tried summation as it is simple and was sufficient to model the short reviews in the dataset used.\n\n2. We believe having the pseudo code helps readers to understand the overall method and prior work [Ruder and Plank ACL 2018 http://aclweb.org/anthology/P18-1096] also have used pseudo code to explain the proposed method.\n\n3. We use separate domains, not included in the test/train data for validation purposes. We will make this clear in the revision.\n\n4. A good question. We will explore the generalisability to other problems in follow up work.\n\nThanks for your review.'}		H1xtpgqTaQ	H1e6nd4KQN	AKBC.ws/2019/Conference/-/Paper4/Official_Comment	['AKBC.ws/2019/Conference/Paper4/Reviewers/Unsubmitted']	2		['everyone']	ryxGwrMSG4	['AKBC.ws/2019/Conference/Paper4/Authors']	1548466356587		1548466356587	['AKBC.ws/2019/Conference/Paper4/Authors', 'AKBC.ws/2019/Conference']
515	1538087915793	{'title': 'CAMOU: Learning Physical Vehicle Camouflages to Adversarially Attack Detectors in the Wild', 'abstract': 'In this paper, we conduct an intriguing experimental study about the physical adversarial attack on object detectors in the wild. In particular, we learn a camouflage pattern to hide vehicles from being detected by state-of-the-art convolutional neural network based detectors. Our approach alternates between two threads. In the first, we train a neural approximation function to imitate how a simulator applies a camouflage to vehicles and how a vehicle detector performs given images of the camouflaged vehicles. In the second, we minimize the approximated detection score by searching for the optimal camouflage. Experiments show that the learned camouflage can not only hide a vehicle from the image-based detectors under many test cases but also generalizes to different environments, vehicles, and object detectors.', 'keywords': ['Adversarial Attack', 'Object Detection', 'Synthetic Simulation'], 'authorids': ['yangzhang4065@gmail.com', 'foroosh@cs.ucf.edu', 'philip.j.david4.civ@mail.mil', 'boqinggo@outlook.com'], 'authors': ['Yang Zhang', 'Hassan Foroosh', 'Philip David', 'Boqing Gong'], 'TL;DR': 'We propose a method to learn physical vehicle camouflage to adversarially attack object detectors in the wild. We find our camouflage effective and transferable.', 'pdf': '/pdf/b2cf9169997bdf2dcb9f0096b30c5b8718b49b06.pdf', 'paperhash': 'zhang|camou_learning_physical_vehicle_camouflages_to_adversarially_attack_detectors_in_the_wild', '_bibtex': '@inproceedings{\nzhang2018camou,\ntitle={{CAMOU}: Learning Physical Vehicle Camouflages to Adversarially Attack Detectors in the Wild},\nauthor={Yang Zhang and Hassan Foroosh and Philip David and Boqing Gong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJgEl3A5tm},\n}'}		SJgEl3A5tm	SJgEl3A5tm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1065	HylNL_sqtm	['everyone']		['ICLR.cc/2019/Conference']	1538087915793		1548454548184	['ICLR.cc/2019/Conference']
516	1548453431330	{'title': 'Comment on third reviewer.', 'comment': 'As we mentioned on the two previous reviews we were not able to acquire the implementations of similar/state of the art methods or the datasets that those methods used. We are willingly to work further to make such comparisons either by finding such sources or by implementing the methods, use them to process the All The News dataset and compare their results with ours. We are unsure if this work can be completed by the 10th February deadline for paper revisions, but expect to have completed it in time for presentation at the conference, if accepted. Our software-vs-human evaluation used 1800 manually annotated articles where our method was able to detect the major peaking events as well as minor events that were described by a small number of articles (less than 5), proving that our method is sensitive enough to detect such events. \n\nThank you for your feedback.'}		HylqxZqppX	BJg1B8WKX4	AKBC.ws/2019/Conference/-/Paper23/Official_Comment	['AKBC.ws/2019/Conference/Paper23/Reviewers/Unsubmitted']	4		['everyone']	SJx22zvdgE	['AKBC.ws/2019/Conference/Paper23/Authors']	1548453431330		1548453431330	['AKBC.ws/2019/Conference/Paper23/Authors', 'AKBC.ws/2019/Conference']
517	1548453395101	{'title': 'Comment on second reviewer.', 'comment': 'Evaluation is key to demonstrating the utility of a new method. One of our long term goals is to compare our method against similar and state of the art methods. However, this is not straightforward and for this deadline we were not able to acquire either the code implementations or the datasets used for relevant published methods. Re-implementing established methods was the only possible choice but we did not have enough time to do so; this also creates a large amount of extra work. Our method is focused on news articles and other long-form text documents, so we did not want to compare against event detection methods (where we might have been able to get access to the datasets) focused on tweets. Twitter data and news media are sufficiently different that this comparison would be inappropriate. \n  \nFor these reasons we focused on evaluation against human analysts, as reported in the manuscript. Our method was able to detect all the major peaking events as well as events that were described by a very small number of articles. Therefore we are confident in the good performance of our approach. \n\nThe color of the nodes in figure 4 corresponds to the color of the entities in figures 2 and 3. The color of the edges is just a mixture of the connected node colors. We will indicate that on the caption.\n\ntf-idf is indeed the correct capitalization.\n\nThe KeyGraph generated from our method contains the entities and noun phrases that were detected from the filtered articles. The clusters of those keywords are generated using the co-appearance of those keywords on the articles. So the percentage of each cluster is the ratio of the cluster keywords over the total number of keywords in the graph. \n\nWe corrected section 4.1 to be more understandable by the user. \n\nThank you for your feed back we will try our best to apply your corrections.'}		HylqxZqppX	rJesfU-YmN	AKBC.ws/2019/Conference/-/Paper23/Official_Comment	['AKBC.ws/2019/Conference/Paper23/Reviewers/Unsubmitted']	3		['everyone']	Skxzxc1gGN	['AKBC.ws/2019/Conference/Paper23/Authors']	1548453395101		1548453395101	['AKBC.ws/2019/Conference/Paper23/Authors', 'AKBC.ws/2019/Conference']
518	1548453295096	{'title': 'Comment on first reviewer.', 'comment': 'In our paper we create and process knowledge (articles) about named entities and their relations. We use natural language processing and machine learning methods to extract useful knowledge while our approach is efficient enough to be applied on a large-scale dataset. For that reason we believe that our paper is relevant to this conference.\n\nRegarding the event detection our method tracks the weighted node degree over time and detects change-points on them which correspond to an event. The detected communities are formed not only from entities but from the most informative noun phrases (Fig. 10) and they are used to describe the detected event. We should also point out that each keyword has a weight which further helps the user to understand what the event is about. Our approach is a Feature-pivot method where sets of terms that describe the occurring events are extracted from a corpus of documents. Such method is described in [Sayyadi et al., 2009] and a good description of event detection methods is given in [Aiello et al., 2013].\n\nWe chose the specific date (24/6/2016) to make it easier to the reader to understand our evaluation. Our method does not only detect and summarize the well known events, but also detects real events that are described by a very small number of articles (Table 1). For example the Hillary Clinton email scandal is mentioned only in 5 articles out of 300 in total for the current day, the west Virginia floods in 4, the EgyprAir plane crash is mentioned in 2 and so on. Our method is sensitive enough to detect such events and give an easy to use summary to the user.\n\nOur pipeline does not only combine out of the box tools, but further improve their results and introduces new ideas to process the input data. For example the weighting and the use of the nodes weighted degree further in our method, provides a very fast way to identify important entities without calculating the computationally expensive page rank metric. By using time series analysis we introduce a way to filter out “noisy” articles which can contribute misleading keywords to the new network (KeyGraph) and can also cause the merging of communities that describe different events. In the case of the NER classifier we improve its results (which are only single words classified as Persons, Organizations, Locations and Other) by introducing a disambiguation algorithm that replaces single names with the fool name of the entity. A lot of effort was also given in disambiguating Organization and Location entities such as USA, U.S.A, EU, European Union and so on.\n\nAs mentioned before we address the challenge of event detection in news streams, introducing a pipeline that can detect  peaking events and not just a summary of the topics discussed on the news. For the evaluation we manually annotated 1800 articles and our method was able to detect all of the major events, as well as the majority of the minor events. (described in less than 5 articles). \n\nWe understand that comparing our method against other state of the art methods would produce a good evaluation but that was not an easy task on the available time, for a number of reasons:\n    • First there is no benchmark dataset of news articles which can be used to compare our methods with others. \n    • Second we were not able to find implementations of other methods and on the given time we were not able to implement them by ourselves.\n    • Third our method detects peaking events on news articles, for that reason it is hard to compare it with methods that only summarise articles without detecting peaking events. \nLast although there are other methods for event detection their target input is tweets and comparing them with news articles as input would be misleading. We plan to make adjustments to our method to be able to process tweets as well, but this is for future work. \n\nThank you for you feedback.'}		HylqxZqppX	HklD2H-FQE	AKBC.ws/2019/Conference/-/Paper23/Official_Comment	['AKBC.ws/2019/Conference/Paper23/Reviewers/Unsubmitted']	2		['everyone']	SygIQQxmfN	['AKBC.ws/2019/Conference/Paper23/Authors']	1548453295096		1548453295096	['AKBC.ws/2019/Conference/Paper23/Authors', 'AKBC.ws/2019/Conference']
519	1548453214710	{'title': 'Submitted an updated version', 'comment': 'We thank the reviewers for their comments. A new version of the paper has been submitted addressing most of the reviewers comments.'}		HylqxZqppX	SyxvPrbF7V	AKBC.ws/2019/Conference/-/Paper23/Official_Comment	['AKBC.ws/2019/Conference/Paper23/Reviewers/Unsubmitted']	1		['everyone']	HylqxZqppX	['AKBC.ws/2019/Conference/Paper23/Authors']	1548453214710		1548453214710	['AKBC.ws/2019/Conference/Paper23/Authors', 'AKBC.ws/2019/Conference']
520	1542459633747	{'title': 'Towards a complex network approach to detecting events in high-volume news streams', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper23/Authors'], 'keywords': ['social network analysis', 'natural language processing', 'time series analysis', 'information retrieval'], 'TL;DR': 'Detection of important events in news streams.', 'abstract': 'Detecting important events in high volume news streams is an important task for a\nvariety of purposes. The volume and rate of online news increases the need for automated\nevent detection methods that can operate in real time. In this paper we develop a network-\nbased approach that makes the working assumption that important news events always\ninvolve named entities (such as persons, locations and organizations) that are linked in\nnews articles. Our approach uses natural language processing techniques to detect these\nentities in a stream of news articles and then creates a time-stamped series of networks in\nwhich the detected entities are linked by co-occurrence in articles and sentences. In this\nprototype, weighted node degree is tracked over time and change-point detection used to\nlocate important events. Potential events are characterized and distinguished using commu-\nnity detection on KeyGraphs that relate named entities and informative noun-phrases from\nrelated articles. We performed an evaluation against human annotation and against simple\ntext-based clustering, finding that our system detects more events than simple clustering\nand compares well to human annotations.This methodology already produces promising re-\nsults and will be extended in future to include a wider variety of complex network analysis\ntechniques.', 'archival status': 'Archival', 'subject areas': ['Natural Language Processing', 'Information Extraction'], 'pdf': '/pdf/f2f00608f979e09a5821a85b64bbc4b393db288f.pdf', 'paperhash': 'anonymous|towards_a_complex_network_approach_to_detecting_events_in_highvolume_news_streams', '_bibtex': '@inproceedings{    \nanonymous2019towards,    \ntitle={Towards a complex network approach to detecting events in high-volume news streams},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=HylqxZqppX},    \nnote={under review}    \n}'}		HylqxZqppX	HylqxZqppX	AKBC.ws/2019/Conference/-/Blind_Submission	[]	23	HkxF3dt3aQ	['everyone']		['AKBC.ws/2019/Conference']	1542459633747		1548453057138	['AKBC.ws/2019/Conference']
521	1538087809063	{'title': 'code2seq: Generating Sequences from Structured Representations of Code', 'abstract': 'The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present code2seq: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding.\nWe demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models. An interactive online demo of our model is available at http://code2seq.org. Our code, data and trained models are available at http://github.com/tech-srl/code2seq.', 'keywords': ['source code', 'programs', 'code2seq'], 'authorids': ['urialon1@gmail.com', 'shakedbr@cs.technion.ac.il', 'omerlevy@gmail.com', 'yahave@cs.technion.ac.il'], 'authors': ['Uri Alon', 'Shaked Brody', 'Omer Levy', 'Eran Yahav'], 'TL;DR': 'We leverage the syntactic structure of source code to generate natural language sequences.', 'pdf': '/pdf/b0da14b8f966173a5542439a6e2f993f71a73f3c.pdf', 'paperhash': 'alon|code2seq_generating_sequences_from_structured_representations_of_code', '_bibtex': '@inproceedings{\nalon2018codeseq,\ntitle={code2seq: Generating Sequences from Structured Representations of Code},\nauthor={Uri Alon and Omer Levy and Eran Yahav},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gKYo09tX},\n}'}		H1gKYo09tX	H1gKYo09tX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	465	SkldYKNrK7	['everyone']		['ICLR.cc/2019/Conference']	1538087809063		1548452197610	['ICLR.cc/2019/Conference']
522	1548447238341	{'comment': 'Hi, any update on the source code?', 'title': 'Opensourcing?'}		HkfPSh05K7	SyxRWCJK7N	ICLR.cc/2019/Conference/-/Paper1551/Public_Comment	[]	4		['everyone']	HkfPSh05K7	['(anonymous)']	1548447238341		1548447238341	['(anonymous)', 'ICLR.cc/2019/Conference']
523	1538087780906	{'title': 'Visual Reasoning by Progressive Module Networks', 'abstract': 'Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn – most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.\n', 'keywords': [], 'authorids': ['seung@cs.toronto.edu', 'makarand@cs.toronto.edu', 'fidler@cs.toronto.edu'], 'authors': ['Seung Wook Kim', 'Makarand Tapaswi', 'Sanja Fidler'], 'pdf': '/pdf/7ba81b43cb43e5d88e3829f767d714f8de03f675.pdf', 'paperhash': 'kim|visual_reasoning_by_progressive_module_networks', '_bibtex': '@inproceedings{\nkim2018visual,\ntitle={Visual Reasoning by Progressive Module Networks},\nauthor={Seung Wook Kim and Makarand Tapaswi and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1fpDsAqt7},\n}'}		B1fpDsAqt7	B1fpDsAqt7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	305	B1gJqb3FK7	['everyone']		['ICLR.cc/2019/Conference']	1538087780906		1548440441160	['ICLR.cc/2019/Conference']
524	1542459587007	{'title': 'Applying Citizen Science to Gene, Drug, Disease Relationship Extraction from Biomedical Abstracts', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper5/Authors'], 'keywords': ['citizen science', 'relationship extraction', 'biomedical literature', 'abstracts'], 'TL;DR': 'Non-experts can perform relationship extraction from biomedical abstracts', 'abstract': 'Biomedical literature is growing at a rate that outpaces our ability to harness the knowledge contained therein. In order to mine valuable inferences from the large volume of literature, many researchers have turned to information extraction algorithms to harvest information in biomedical texts. Information extraction is usually accomplished via a combination of manual expert curation and computational methods. Advances in computational methods usually depends on the generation of gold standards by a limited number of expert curators. This process can be time consuming and represents an area of biomedical research that is ripe for exploration with citizen science. Citizen scientists have been previously found to be willing and capable of performing named entity recognition of disease mentions in biomedical abstracts, but it was uncertain whether or not the same could be said of relationship extraction. Relationship extraction requires training on identifying named entities as well as a deeper understanding of how different entity types can relate to one another. Here, we used the web-based application Mark2Cure (https://mark2cure.org) to demonstrate that citizen scientists can perform relationship extraction and confirm the importance of accurate named entity recognition on this task. We also discuss opportunities for future improvement of this system, as well as the potential synergies between citizen science, manual biocuration, and natural language processing. ', 'archival status': 'Non-Archival', 'subject areas': ['Natural Language Processing', 'Information Extraction', 'Human computation', 'Crowd-sourcing'], 'pdf': '/pdf/b6d5b31073faedb4660e67e28a3ba9f1f72a6b98.pdf', 'paperhash': 'anonymous|applying_citizen_science_to_gene_drug_disease_relationship_extraction_from_biomedical_abstracts', '_bibtex': '@inproceedings{    \nanonymous2019applying,    \ntitle={Applying Citizen Science to Gene, Drug, Disease Relationship Extraction from Biomedical Abstracts},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=r1loaec6pm},    \nnote={under review}    \n}'}		r1loaec6pm	r1loaec6pm	AKBC.ws/2019/Conference/-/Blind_Submission	[]	5	rklm2J-qaQ	['everyone']		['AKBC.ws/2019/Conference']	1542459587007		1548439469138	['AKBC.ws/2019/Conference']
525	1542371428504	"{'title': 'These problems have already been addressed in the text', 'comment': 'Thank you for your comment. Indeed we had a correspondence about this issue following a posting of a previous version of our paper on ArXiv. \n\nThe reviewer is referring to one particular result shown at the bottom of figure 1. Contrary to the title of the reviewer’s comment, he/she acknowledged in our correspondence that given the input frames shown in the bottom of figure 1, the output of the network that we show is reproducible and it varies drastically even between almost identical frames. \n\nThe issue that the reviewer is addressing is how to go from the YouTube video to the network input since the video has an aspect ratio very different from one. Indeed for this specific video, this “processing pipeline” makes a difference. However, and as we already mentioned in the correspondence, we easily found other YouTube videos of polar bears that give jagged predictions with the reviewer’s suggested pipeline.\n\nAs a result of the same correspondence, we added a footnote at the bottom of page 3 that explicitly says that different preprocessing pipelines give different results. \nImportantly, following the reviewer\'s comments, we verified that all the other results do not depend on the pipeline and all the graphs shown in the paper are with a pipeline that maintains the aspect ratio of the original image.\n\nThat being said, the question of ""the right pipeline"" is far from obvious. Using a center crop as suggested by the reviewer means that you will necessarily miss polar bears that do not appear in the center (an extreme case of taking advantage of photographer\'s bias). In fact, the default preprocessing suggested by Keras (https://keras.io/applications/)  prefers to reshape the full image (as we did in the bottom of figure 1) rather than to perform a center crop.\n\nFinally, we want to stress again that the three frames shown in the bottom of figure 1 are exactly the frames given to the network and given these frames anyone can reproduce our reported network output. The difference between these frames is a very small deformation that is imperceptible to humans and yet makes a huge difference to the output of the network. The goal of our paper is to explain why this happens.\n'}"		HJxYwiC5tm	Sye2D_EhaX	ICLR.cc/2019/Conference/-/Paper281/Official_Comment	['ICLR.cc/2019/Conference/Paper281/Reviewers/Unsubmitted']	1		['everyone']	B1lUFfAjpQ	['ICLR.cc/2019/Conference/Paper281/Authors']	1542371428504		1548402881789	['ICLR.cc/2019/Conference/Paper281/Authors', 'ICLR.cc/2019/Conference']
526	1548401971393	{'title': 'Thanks for your kind remainder', 'comment': 'Thank you for your information. We will carefully read the paper and refer to them in the future version.'}		HyMnYiR9Y7	rkgsNaN_Q4	ICLR.cc/2019/Conference/-/Paper483/Official_Comment	['ICLR.cc/2019/Conference/Paper483/Reviewers/Unsubmitted']	8		['everyone']	SygLERUjk4	['ICLR.cc/2019/Conference/Paper483/Authors']	1548401971393		1548401971393	['ICLR.cc/2019/Conference/Paper483/Authors', 'ICLR.cc/2019/Conference']
527	1548381921643	"{'comment': ""I also do not understand the AC's decision, for all the reasons you mentioned (going against all reviewers clearly rejecting the paper, failure of the authors to formulate rebuttals regarding content issues, seemingly disrespectful attitude of the authors and clear anonymity issues).\n\nHowever, I don't think mocking the AC or accusing the AC of favoritism is respectful or productive.\n\nI do wish the AC provides clear compelling technical arguments for going against all reviewers and ignoring the other issues."", 'title': 'Re: Respect?'}"		rJlWOj0qF7	S1xc1kldXE	ICLR.cc/2019/Conference/-/Paper331/Public_Comment	[]	13		['everyone']	H1gmsmaPX4	['(anonymous)']	1548381921643		1548381921643	['(anonymous)', 'ICLR.cc/2019/Conference']
528	1548371225963	{'title': 'Addressing the Representation Bottleneck in Neural Machine Translation with Lexical Shortcuts', 'abstract': 'The transformer is a state-of-the-art neural translation model that uses attention to iteratively refine lexical representations with information drawn from the surrounding context. Lexical features are fed into the first layer and propagated through a deep network of hidden layers. We argue that the need to represent and propagate lexical features in each layer limits the model’s capacity for learning and representing other information relevant to the task. To alleviate this bottleneck, we introduce gated shortcut connections between the embedding layer and each subsequent layer within the encoder and decoder. This enables the model to access relevant lexical content dynamically, without expending limited resources on storing it within intermediate states. We show that the proposed modification yields consistent improvements on standard WMT translation tasks and reduces the amount of lexical information passed along the hidden layers. We furthermore evaluate different ways to integrate lexical connections into the transformer architecture and present ablation experiments exploring the effect of proposed shortcuts on model behavior.', 'authors': ['Anonymous'], 'keywords': ['machine translation', 'natural langauge processing', 'skip connections', 'interpretability'], 'TL;DR': 'Equipping the transformer model with shortcuts to the embedding layer frees up model capacity for learning novel information.', 'authorids': ['OpenReview.net/Anonymous_Preprint/Paper46/Authors'], 'pdf': '/pdf/de3a7470c67510675ebfbfef479bd8e6e5a7696b.pdf', 'paperhash': 'anonymous|addressing_the_representation_bottleneck_in_neural_machine_translation_with_lexical_shortcuts', '_bibtex': '@unpublished{          \nanonymous2018addressing,          \ntitle={Addressing the Representation Bottleneck in Neural Machine Translation with Lexical Shortcuts},          \nauthor={Anonymous},          \njournal={OpenReview Preprint},          \nyear={2018},          \nnote={anonymous preprint under review}      \n}'}		SJgfQH6PQ4	SJgfQH6PQ4	OpenReview.net/Anonymous_Preprint/-/Blind_Submission	[]	46	SJxbXBTD7N	['everyone']		['OpenReview.net/Anonymous_Preprint']	1548371225963		1548371226153	['OpenReview.net/Anonymous_Preprint']
529	1548370843127	"{'comment': 'It is incredibly disappointing that this happens in a top tier conference like ICLR. It is also very disrespectful towards all the other authors and reviewers that the rules of the game are completely ignored for this particular paper. \n\n- The main argument of the AC is that he/she ""knows"" much better than all the reviewers how good this paper is. We all know \n that these reviews are biased, but ignoring 3 reviews for another one it not gonna make things better. All the official reviewers said ""reject"", but then the AC God came and saved the day, of course, ignoring all the reviewers work.  My question is: why didn\'t the AC review all the other papers himself, why does he/she need reviewers when it is now clear that the AC is the only person to correctly evaluate the quality of a paper?  \n\n- Even if, hypothetically speaking, the AC would hold the absolute truth, this decision is very unfair and disrespectful towards the other submitted papers which were judged based on the 3 reviews, thus subject to the randomness and bias inherent in this process. \n\n- Anonymity break: the github account linked in the abstract could have been easily used to detect the first author\'s name. This is clearly against the rules and this paper should have been rejected only because of this reason, in the first place.\n\n- Authors outright bullying reviewers: yeah, they have someone to protect them, why care more ?\n\nThe ICLR representatives continue up to date to refuse making justice in this case. Moreover, on twitter, they concluded that ""we do hope that it is not seen as representative of the 1600 other (mostly) productive conversations that happened concurrently."" . I also conclude that: ""in life you can succeed by being hard working or by hardly working, but knowing the right people"". ', 'title': 'Respect ?'}"		rJlWOj0qF7	H1gmsmaPX4	ICLR.cc/2019/Conference/-/Paper331/Public_Comment	[]	12		['everyone']	rJlWOj0qF7	['~suomynona_noitpurroc1']	1548370843127		1548371069219	['~suomynona_noitpurroc1', 'ICLR.cc/2019/Conference']
530	1548355400847	"{'comment': '(the author of this comment is the same as the original comment ""Can the AC participate in the discussion..."" Identities of anonymous authors of other responses in this thread are unknown to me)\n\nMy original comment was mainly directed at engaging the AC in the technical discussion. I think it is necessary now to step back and look at the technical content of the paper so that we can ""stay professional and friendly"". Appeals to the AC\'s authority and his intuition about the possible impact of the paper might be a valid way of defending the paper, however, since not all of us ""have worked and thought about this field for over a decade"", it might be hard for some of us to follow the AC\'s argument. While the AC is completely within his right to accept the paper, my argument is simply that it should be made transparent how the specific technical contribution of the paper affected this decision. \n\nIt is understandable that the meta-reviewer doesn\'t have time to review the paper and has to base the decision on the reviews and the authors comments. If the decision was based on a personal review by the AC, I believe such review (with a summary of technical part and experiments) should be posted so that the decision becomes transparent. It the next part of my comment, I will assume that the decision was instead based on the discussion here, and address AC\'s comments about this discussion.\n\n--------––------------------------––------------------------––------------------------––------------------------––----------------------\n\nThe AC comments that ""Detailed equations were discussed and improved our understanding of the paper."" While it\'s true that ""equations"" were discussed between Rev1 and the authors,  I do not believe our understanding of the paper was improved.\n\nThe only publicly visible discussion happened after the acceptance decision. The authors did not elect to address technical concerns of the reviewers before the AC decision, therefore the discussion the AC mentions could not affect the AC\'s decision. The AC claims that ""Nobody was ignored"", however, the reviewer comments were ignored by the authors (as far as I can tell from the public part of the discussion). \n\nThere was indeed a discussion between Rev1 and authors after the acceptance decision. if you follow the discussion, you will find out that in none of the comments Rev1 agrees with the authors, or the authors agree with any point raised by Rev1. The authors failed to defend their point, and withdrew from the discussion (the last comment is by Rev1). I, therefore, cannot follow the argument of AC that ""Detailed equations were discussed and improved our understanding of the paper.""\n\nThe AC is under the impression that ""All related work has been or will be added to a later paper version so having missed it in prior submission is now irrelevant. That\'s what this process is for.\nIf the authors still did not cite relevant work it would be bad."" Unfortunately, we have to accept that what is happening is, in fact, ""bad"". If you follow the discussion, the authors refuse to cite the recent relevant papers, instead stating that they will cite Venn, 1880 and Whitehead, 1929.  They add that ""If space available, we consider add one of the two references you mentions which appear in 2015 and 2017"", however, the submitted manuscript has not been revised once since submission.\n\nThe AC might reasonably state that the fact that the consensus was not reached does not imply rejection of the paper. However, the ICLR AC guidelines state that ""A key part of your role during the review period is to actively facilitate discussion among reviewers, with the aim of clarifying aspects of the papers’ claims, and other questions that have been raised, in fair and respectful ways."" It seems to me that it would be appropriate for the AC to moderate the discussion between Rev1 and the authors so that the truth can be found. If the AC chooses not to do so, the guidelines still instruct them to "" Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision."" None of the AC\'s comments address or state what the reviewers concerns are and why they have been downweighted in the acceptance decision. I hope that AC chooses to engage in technical discussion (possibly with Rev1 or even the authors) to clarify this.\n\n', 'title': 'Lack of technical discussion by AC'}"		rJlWOj0qF7	B1lWLvKD7N	ICLR.cc/2019/Conference/-/Paper331/Public_Comment	[]	11		['everyone']	SklYX6QPXV	['(anonymous)']	1548355400847		1548355400847	['(anonymous)', 'ICLR.cc/2019/Conference']
531	1548355119561	{'title': 'Thanks for the positive feedback', 'comment': 'We thank all of our reviewers for the positive feedback. In our final version of the paper we will be adding a broader discussion of work in open-world probabilistic databases, and highlight more interesting open problems that remain.'}		r1xTXZ9p6Q	SJluVIFDX4	AKBC.ws/2019/Conference/-/Paper42/Official_Comment	['AKBC.ws/2019/Conference/Paper42/Reviewers/Unsubmitted']	1		['everyone']	r1xTXZ9p6Q	['AKBC.ws/2019/Conference/Paper42/Authors']	1548355119561		1548355119561	['AKBC.ws/2019/Conference/Paper42/Authors', 'AKBC.ws/2019/Conference']
532	1538087863040	{'title': 'Recurrent Experience Replay in Distributed Reinforcement Learning', 'abstract': 'Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. Using a single network architecture and fixed set of hyper-parameters, the resulting agent, Recurrent Replay Distributed DQN, quadruples the previous state of the art on Atari-57, and matches the state of the art on DMLab-30. It is the first agent to exceed human-level performance in 52 of the 57 Atari games.', 'keywords': ['RNN', 'LSTM', 'experience replay', 'distributed training', 'reinforcement learning'], 'authorids': ['skapturowski@google.com', 'ostrovski@google.com', 'johnquan@google.com', 'munos@google.com', 'wdabney@google.com'], 'authors': ['Steven Kapturowski', 'Georg Ostrovski', 'John Quan', 'Remi Munos', 'Will Dabney'], 'TL;DR': 'Investigation on combining recurrent neural networks and experience replay leading to state-of-the-art agent on both Atari-57 and DMLab-30 using single set of hyper-parameters.', 'pdf': '/pdf/387fb2fcee8f74c53cf707a9856f40c458f33933.pdf', 'paperhash': 'kapturowski|recurrent_experience_replay_in_distributed_reinforcement_learning', '_bibtex': '@inproceedings{\nkapturowski2018recurrent,\ntitle={Recurrent Experience Replay in Distributed Reinforcement Learning},\nauthor={Steven Kapturowski and Georg Ostrovski and Will Dabney and John Quan and Remi Munos},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1lyTjAqYX},\n}'}		r1lyTjAqYX	r1lyTjAqYX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	765	rygr68TcYQ	['everyone']		['ICLR.cc/2019/Conference']	1538087863040		1548351939038	['ICLR.cc/2019/Conference']
533	1548351172962	{'pdf': '/pdf/5751997b35c639b61573217f50f775f63cb14746.pdf', 'title': 'MORTY Embedding: Improved Embeddings without Supervision', 'abstract': 'We demonstrate a low effort method that unsupervisedly constructs task-optimized embeddings from existing word embeddings to gain performance on a supervised end-task. This avoids additional labeling or building more complex model architectures by instead providing specialized embeddings better fit for the end-task(s). Furthermore, the method can be used to roughly estimate whether a specific kind of end-task(s) can be learned form, or is represented in, a given unlabeled dataset, e.g. using publicly available probing tasks. We evaluate our method for diverse word embedding probing tasks and by size of embedding training corpus -- i.e. to explore its use in reduced (pretraining-resource) settings.', 'authors': ['Anonymous'], 'keywords': ['Unsupervised Learning', 'Representation Learning', 'Transfer Learning'], 'TL;DR': 'Morty refits pretrained word embeddings to either: (a) improve overall embedding performance (for Multi-task settings) or improve Single-task performance, while requiring only minimal effort.', 'authorids': ['OpenReview.net/Anonymous_Preprint/Paper45/Authors'], 'paperhash': 'anonymous|morty_embedding_improved_embeddings_without_supervision', '_bibtex': '@unpublished{          \nanonymous2018morty,          \ntitle={MORTY Embedding: Improved Embeddings without Supervision},          \nauthor={Anonymous},          \njournal={OpenReview Preprint},          \nyear={2018},          \nnote={anonymous preprint under review}      \n}'}		rylTa8uvQ4	rylTa8uvQ4	OpenReview.net/Anonymous_Preprint/-/Blind_Submission	[]	45	HklhaI_D7N	['everyone']		['OpenReview.net/Anonymous_Preprint']	1548351172962		1548351173097	['OpenReview.net/Anonymous_Preprint']
534	1548347668107	{'title': 'Interesting idea but in need of more clarity', 'metareview': 'The paper proposes a simple approach for computing a sentence embedding as a weighted combination of pre-trained word embeddings, which obtains nice results on a number of tasks.  The approach is described as training-free but does require computing principal components of word embedding subspaces on the test set (similarly to some earlier work).  The reviewers are generally in agreement that the approach is interesting, and the results are encouraging.  However, there is some concern about the clarity of the paper and in particular the placement of the work in relation to other methods.  There is also a bit of concern about whether there is sufficient novelty compared to Arora et al. 2017, which also compose sentence embeddings as weighted combinations of word embeddings, and also use a principal subspace of embeddings in the test set.  This AC feels that the method here is sufficiently different from Arora et al., but agrees with the reviewers that the paper clarity needs to be improved, so that the community can appreciate what is gained from the new aspects of the approach and what conclusions should be drawn from each experimental comparison.', 'recommendation': 'Reject', 'confidence': '4: The area chair is confident but not absolutely certain'}		rJedbn0ctQ	B1gnMKDPmE	ICLR.cc/2019/Conference/-/Paper1187/Meta_Review	[]	1		['everyone']	rJedbn0ctQ	['ICLR.cc/2019/Conference/Paper1187/Area_Chair1']	1548347668107		1548347668107	['ICLR.cc/2019/Conference/Paper1187/Area_Chair1']
535	1548345065359	"{'comment': 'Repeating ""It may present a puzzle piece to solve one of the major issues of AI."" again and again does not sound like an objective comment/review, specially compared to the extensive and quantitive reviews given by the anonymous reviewers. Looking at the AC\'s comments I can\'t really gain an intuition as to why the paper was accepted, none of the reviewers concerns are addressed, no discussion attempt was made with the reviewers in the portal. How does this reflect reviews are important ? \n\nFurthermore sharing links containing the authors name is in clear violation of the anonymity rule. Interacting with the GitHub repository is definitely something natural to do for the author, have a quick check of how reproducible the work is, maybe how much work was put into making it reproducible. Some conferences like ICML encourage this heavily. Thus the statement made by the AC :\n\n""Searching for a title of a paper on Google is much easier than clicking through github repos and the reverse name github thing is only obvious in retrospect. So it\'s not relevant in my opinion.""\n\nIs heavily overlooking the conferences rules.  Why does everyone else have to make a proper effort in doing this ? and yet we have an exception for this paper.\n\n""Anonymity. ICLR is double-blind, which means that authors are not aware of reviewer identities and reviewers are not aware of author identities. If you believe a paper contains an anonymity violation, contact your AC immediately. Anonymity violations are not considered as part of reviewing criteria, they are requirements for submission. Unless your AC decides that the paper does indeed violate anonymity, proceed to review it as normal. Do not reveal your or the authors’ identities in the discussion.""\n\nFurthermore:\n\n""Arxiv and prior work. ICLR considers unpublished arxiv papers to be prior work. While we encourage reviewers to apply the reasonable standards of the relevant community in considering what does and does constitute prior work, the following minimum standards will be enforced: no paper will be considered prior work if it appeared on arxiv, or another online venue, less than 30 days prior to the ICLR deadline.""\n\n The latter exception with Arxiv and non proceedings does not override the former rule regarding anonymous submissions. Furthermore the rule itself has timing constraints, uploading linked prior work to arXiv or GitHub the day before the deadline would be in violation of this rule.\n', 'title': 'Reviews are important ?'}"		rJlWOj0qF7	H1ebxyDDQE	ICLR.cc/2019/Conference/-/Paper331/Public_Comment	[]	10		['everyone']	SklYX6QPXV	['(anonymous)']	1548345065359		1548345668040	['(anonymous)', 'ICLR.cc/2019/Conference']
536	1548344081244	{'title': 'review ', 'review': '-summary\nThis paper addresses the problem of model scaling and efficiency in coreference resolution. Commonly used models represent mentions as sparse feature vectors which are iteratively agglomerated to form clusters of mentions. The feature vector of a cluster is a function of all the mentions in the cluster which causes them to become more dense over time and computationally expensive to operate over. To address this, the authors investigate hashing schemes to expedite similarity calculation. Importantly, they propose a homomorphic hash function that is able to update the representation of a cluster online as new mentions are added or removed. \n\n\n-pros\n — a homomorphic simhash that can speed up similarity calculations while maintaining performance\n — experiments  show that the hashing schema is able to perform within 1 F1 point of the exact similarity model. \n — good coverage of related work and explanation of methods\n\n-cons\n— only tested on a single dataset\n— the lines in Fig 2 are not the clearest presentation of the speed accuracy tradeoff due to the large differences in scales and the fact that the maximum accuracies are never reached for some lines. Maybe a table or another presentation of these results could help.\n', 'rating': '7: Good paper, accept', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		H1gwRx5T6Q	HyetGj8PQN	AKBC.ws/2019/Conference/-/Paper10/Official_Review	['AKBC.ws/2019/Conference/Paper10/Reviewers/Unsubmitted']	3		['everyone']	H1gwRx5T6Q	['AKBC.ws/2019/Conference/Paper10/AnonReviewer4']	1548344081244		1548344081244	['AKBC.ws/2019/Conference/Paper10/AnonReviewer4', 'AKBC.ws/2019/Conference']
537	1538087853286	"{'title': 'Learning deep representations by mutual information estimation and maximization', 'abstract': ""This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classification tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation learning objectives for specific end-goals."", 'keywords': ['representation learning', 'unsupervised learning', 'deep learning'], 'authorids': ['devon.hjelm@microsoft.com', 'eidos92@gmail.com', 'samuel.lavoie-marchildon@umontreal.ca', 'karang@cs.toronto.edu', 'phil.bachman@gmail.com', 'adam.trischler@microsoft.com', 'yoshua.umontreal@gmail.com'], 'authors': ['R Devon Hjelm', 'Alex Fedorov', 'Samuel Lavoie-Marchildon', 'Karan Grewal', 'Phil Bachman', 'Adam Trischler', 'Yoshua Bengio'], 'TL;DR': 'We learn deep representation by maximizing mutual information, leveraging structure in the objective, and are able to compute with fully supervised classifiers with comparable architectures', 'pdf': '/pdf/7faa55257b95f1d42e93b97a62404631fb0cf897.pdf', 'paperhash': 'hjelm|learning_deep_representations_by_mutual_information_estimation_and_maximization', '_bibtex': '@inproceedings{\nhjelm2018learning,\ntitle={Learning deep representations by mutual information estimation and maximization},\nauthor={R Devon Hjelm and Alex Fedorov and Samuel Lavoie-Marchildon and Karan Grewal and Phil Bachman and Adam Trischler and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bklr3j0cKX},\n}'}"		Bklr3j0cKX	Bklr3j0cKX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	709	BkeC4T8qKQ	['everyone']		['ICLR.cc/2019/Conference']	1538087853286		1548342931182	['ICLR.cc/2019/Conference']
538	1548337940756	{'title': 'Thank you for your comment', 'comment': 'Thank you for your feedback and reproduce!\n\nI implemented my code via tensorflow. In my code, I set the elements which are lower than a given threshold to zero, which is stated in the paper. Also, the datasets are divided as GCN (https://github.com/tkipf/gcn). I will organize and release my code in Github as soon as possible, and update the link of Github here.'}		H1ewdiR5tQ	B1lazXHDXV	ICLR.cc/2019/Conference/-/Paper363/Official_Comment	['ICLR.cc/2019/Conference/Paper363/Reviewers/Unsubmitted']	27		['everyone']	ryxZNpEDQV	['ICLR.cc/2019/Conference/Paper363/Authors']	1548337940756		1548337940756	['ICLR.cc/2019/Conference/Paper363/Authors', 'ICLR.cc/2019/Conference']
539	1548336424771	{'comment': 'I attempted to reproduce the results. The accuracy scores are lower than reported, even with a larger training set. What was your exact approach to defining/creating a wavelet filter?\n\nhttps://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork', 'title': 'Attempt to reproduce results.'}		H1ewdiR5tQ	ryxZNpEDQV	ICLR.cc/2019/Conference/-/Paper363/Public_Comment	[]	6		['everyone']	H1ewdiR5tQ	['~Benedek_Rozemberczki1']	1548336424771		1548336424771	['~Benedek_Rozemberczki1', 'ICLR.cc/2019/Conference']
540	1548335425881	{'title': '더킹카지노 https://www.wac3636.com/thekingo', 'comment': 'https://www.wac3636.com/thekingo/ 더킹카지노/'}		BkjLkSqxg	Syl9SYEvX4	ICLR.cc/2017/conference/-/paper230/public/comment	['']	30		['everyone']	HJ2xSZXbg	['~FFFF_DDSS1']	1548335425881		1548335425881	['~FFFF_DDSS1']
541	1548335386896	{'title': '퍼스트카지노 https://www.wac3636.com/firstcasino11', 'comment': 'https://www.wac3636.com/firstcasino11/ 퍼스트카지노/'}		BkjLkSqxg	SJl7QFEvm4	ICLR.cc/2017/conference/-/paper230/public/comment	['']	29		['everyone']	HJ2xSZXbg	['~FFFF_DDSS1']	1548335386896		1548335386896	['~FFFF_DDSS1']
542	1548335364137	{'title': '예스카지노 https://www.wac3636.com/yescasino11', 'comment': 'https://www.wac3636.com/yescasino11/ 예스카지노/'}		BkjLkSqxg	B1lh-tEwQN	ICLR.cc/2017/conference/-/paper230/public/comment	['']	28		['everyone']	HJ2xSZXbg	['~FFFF_DDSS1']	1548335364137		1548335364137	['~FFFF_DDSS1']
543	1548335338457	{'title': '온라인카지노 https://www.wac3636.com', 'comment': 'Depending on what kind of experience you’re interested in, some games are better to play than others. Do you actually want to use skill to try to win some money? Or do you want to lounge around and sip on free cocktails? Do you want to feel the weight of casino chips in your hands? Or would you rather look at a machine showing off flashy effects and sounds?\n\n\n\n\n\nhttps://www.wac3636.com/-카지노사이트/\nhttps://www.ffaa7.com/-강친닷컴/\nhttps://www.xn--o79ak1s1tdy0owni.kr/-바카라게임/\nhttps://www.txt7777.com/-온라인카지노/\nhttps://www.css29.com/-블랙잭사이트/\n'}		BkjLkSqxg	S1xGeFNDQN	ICLR.cc/2017/conference/-/paper230/public/comment	['']	27		['everyone']	HJ2xSZXbg	['~FFFF_DDSS1']	1548335338457		1548335338457	['~FFFF_DDSS1']
544	1548335285414	{'title': '카지노사이트https://www.wac3636.com', 'comment': 'The first thing I did on my 21st birthday was go play a few hands of blackjack, and I’ve been playing ever since. I’m no high roller, but I know my way around a casino. If you’re not sure where to begin, let me help.\n\n\n\n\n\nhttps://www.wac3636.com/ 카지노사이트/\nhttps://www.wac3636.com/yescasino11/ 예스카지노/\nhttps://www.wac3636.com/firstcasino11/ 퍼스트카지노/\nhttps://www.wac3636.com/thekingo/ 더킹카지노/\nhttps://www.wac3636.com/suv/ 바카라사이트/\nhttps://www.wac3636.com/fire/ 온라인카지노/\nhttps://www.wac3636.com/yxt/ 룰렛사이트/\nhttps://www.wac3636.com/bigmoney/ 블랙잭사이트/\nhttps://www.wac3636.com/mcasino777/ 엠카지노/'}		BkjLkSqxg	rkg6h_NvQ4	ICLR.cc/2017/conference/-/paper230/public/comment	[]	26		['everyone']	HJ2xSZXbg	['~FFFF_DDSS1']	1548335285414		1548335314980	['~FFFF_DDSS1']
545	1548332320789	"{'title': 'Reviews are important ', 'comment': ""The reviewers raised important points.\nDetailed equations were discussed and improved our understanding of the paper.\nPlease read the below discussions that are public.\n\nI'm not saying this should get a best paper award. I'm not saying this should be an oral.\n\nBut in my opinion, despite its flaws, and having worked and thought about this field for over a decade, it is my opinion that this paper could be more important than many others.\nIt may present a puzzle piece to solve one of the major issues of AI. Connecting neural and logical/set reasoning will be impactful and may help with common sense reasoning.\n\nIt is not the most perfect piece of research but outlines an interesting direction and I think will be beneficial to be discussed at the conference.\n\nEspecially to also draw attention to the so far barely cited papers that are being discussed in these reviews. \nThese good papers too will be lifted and get more exposure.\n\nI can tell you that everybody here was listened to. Nobody was ignored. \nI believe it is a sign of quality that ACs do not simply use a threshold to make accept/reject decisions but try their best to think about impact.\n\nWe do not want to be the kind of field or conference that hold back impactful ideas and directions because they have issues.\n\nI do encourage all parties to stay professional and friendly. Even when  anonymous. \n""}"		rJlWOj0qF7	SklYX6QPXV	ICLR.cc/2019/Conference/-/Paper331/Official_Comment	['ICLR.cc/2019/Conference/Paper331/Reviewers/Unsubmitted']	20		['everyone']	SJed-iWDXN	['ICLR.cc/2019/Conference/Paper331/Area_Chair1']	1548332320789		1548332320789	['ICLR.cc/2019/Conference/Paper331/Area_Chair1', 'ICLR.cc/2019/Conference']
546	1548329889696	{'title': 'Comparison with state-of-the-art results', 'comment': 'We thank the reviewer for the comments.\n\nThe paper, in addition to evaluation of number of model variations, provides a comprehensive evaluation of comparing the proposed model against state-of-the-art results on both datasets. The comparison of results for Quirk and Poon Dataset is provided in Table 4 with the explanation provided in Section 4.4.6. Similarly, the comparison of results for Chemcial-induced dataset is provided in Table 5 and the explanation is provided in Section 4.4.6. As explained in Section 4.4.6, the proposed model clearly outperforms the state-of-the-art results on both datasets.\n\nFurther, since this research specifically considers n-ary relation extraction, these two datasets were considered which involves binary and ternary relation instances. This is the reason we do not perform evaluation on standard intra-sentence relation extraction datasets such as Semeval 2010 Task 8 dataset. Further, this is inline with previous work (Peng et al., 2016) who do not consider evaluation on Semeval 2010 Task 8 relation extraction dataset.'}		Sye0lZqp6Q	Hyg9iQXvmN	AKBC.ws/2019/Conference/-/Paper25/Official_Comment	['AKBC.ws/2019/Conference/Paper25/Reviewers/Unsubmitted']	1		['everyone']	ByxYGXmVME	['AKBC.ws/2019/Conference/Paper25/Authors']	1548329889696		1548329928252	['AKBC.ws/2019/Conference/Paper25/Authors', 'AKBC.ws/2019/Conference']
547	1548323584431	"{'comment': 'Three reviewers took an effort to provide feedback. Some reviewers were more elaborate than others but the authors didn\'t provide a direct response to any of the reviewers (as far as we can see publicly), except regarding the anonymization issues (starting off with ""thanks for the detailed comments, though not acceptable.""). \nNone of the content-related comments of reviewers seem to have been addressed by the authors publicly. \nInstead, one week after the reviews, the authors claim that all three reviewers neglected the main point of the paper, in a top-level comment.\n\nOnly one reviewer raised the anonymity issues on the linked resources. Whether or not knowledge of author identity would bias reviewers seems less of a point than the fact that it violates submission rules, which others do have to obey. Another issue of the same reviewer is similarity to previous work, which was not discussed until after the acceptance decision. Other issues about evaluation and comparison to previous work sound valid but have not been further addressed by the authors.\n\nThe acceptance decision would\'ve been understandable if the authors at least tried to convince the reviewers by addressing the issues they have about their content (which didn\'t happen as far as we can see publicly), and if the final review scores weren\'t at low as 4, 4, 3.\nFrom how it looks publicly, it seems the reviewers\' efforts were simply ignored.\n\nThat being said, the time the reviewers used to review this work could\'ve been used for novel research instead. And maybe this gets so much attention elsewhere because people are genuinely surprised that this actually happened at such a top venue as ICLR.', 'title': 'Re: More details'}"		rJlWOj0qF7	SJed-iWDXN	ICLR.cc/2019/Conference/-/Paper331/Public_Comment	[]	9		['everyone']	SJxDmT9LX4	['(anonymous)']	1548323584431		1548323837577	['(anonymous)', 'ICLR.cc/2019/Conference']
548	1548310197673	"{'title': 'Thank you for the feedback', 'comment': 'We thank the reviewer for the thoughtful reading and comments.\n\n> 1. What percentage of the facts in JKB are from web-crawled data? \n\nThe facts from Web-crawled data are about 0.02% of all facts in the JKB.\nWe got only Website facts from Web-crawled data, and 4.6% of websites in the JKB come from Web-crawled data.\n\n> 2. In figure 5, it makes sense that the number of extracted information will decrease when the threshold increases, but why is the ""~1.0"" bar taller than ""~0.9"", actually almost similar to ""~0.5""? \n\nThe number of extracted information in ""~0.5"" threshold in Figure 5 means the number of extracted information whose confidence score is in the range ""(0.4, 0.5]"".\nThe confidence score, which is calculated from two confidence scores with  ⊕ operator defined in Section 4.2, tends to be higher than 0.9 in our setting of α=0.1.\nThe following figure shows the merged score between two confidence scores as a heatmap; https://www.dropbox.com/s/95dxlydk4vp0vwk/\nIf either of the two scores is higher than 0.9, the merged score is also higher than 0.9.\nTherefore, the number of extracted information included in the ""(0.9, 1.0]"" range is larger than that in ""(0.8, 0.9]"" range.\n\nNote that we wanted to calculate the probabilistic that the same extracted facts from two different paths are correct such as P(bothOfPathsAreCorrect | extractedFactsAreTheSame).\nWe derivate the fomula in Section 4.2 from P(bothOfPathsAreCorrect | extractedFactsAreTheSame) by using Bayes\' theorem.\n\n> 3.""Entity Linker links between two entities where they do not connect each other in spite of their intercorrelations."" \nRegarding the name ""entity linker"", it is a bit different from the standard definition of entity linking, which usually means linking a mention in the text to an entity in the KB. (See also the Wikipedia page for Entity Linking)\n\nWe agree that the name ""EntityLinker"" causes misleading expressions.\nThus, we have changed ""EntityLinker"" to ""EntityConnecter.""\n\n> 4.""We solve the first problem by comparing our ontology with the range type of the predicate and second problem by narrowing down the target to accelerate and reduce the number of incorrect interlinks."" \nI didn\'t quite get how ""narrowing down the target to accelerate and reduce the number of incorrect interlinks"" is done. Are there any details about this? \n\n""narrowing down targets"" means that converting the object only when the entity whose pair of the object-type and the object-name is uniquely defined in the JKB.\nWe have added the explanation.\n\n> 5.""confirmed that only 0.0004% of entities changed their ids. We also observed that more than 94% of the entities did not change their ids, as shown in Figure 6"" \nThe description here is a bit confusing because (0.0004% + 94%) < 100%. I guess you are trying to say that ""0.0004%"" changed within a week and only ""94%"" changed within the whole experiment period (16 months)? \n\nYes, 0.0004% of entities changed within a week and about 6% (100% - 94%) of entities changed for 17 months.\nAdditionaly, we have fixed Figure 6 the beginning of the graph to 100%.\n\n> 6.""The Attribute Completer partially addresses a well-known challenge; knowledge base completion [Socher et al., 2013]."" Complementing attributes using symmetric properties and extracting information from URL seems to be different from the normal knowledge base completion, which, as far as I understand, tries to infer missing triples from the existing triples in the KB. \n\nA knowledge base completion tries to find Triples whose subjects, predicates, and objects are contained in the KB, but the Triplets do not exist in the KB.\n(Refer to ""Definition 1"" in [2018 AAAI] Open-World Knowledge Graph Completion)\nhttps://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16055/15901\nTherefore, we address the knowledge base completion problem by reasoning missing inversed Triples.\n\nWe show the detail below.\nLet a KB be G =(E, R, T), where E, R, and T are the entity set, relationship set, and Triple set respectively.\nWe define inverseOf complementing as follows:\ngiven Triple sets {＜s,p,o＞|s∈E, p∈R, o∈E, <s,p,o>∈T},\nan ""inverseOf"" relation complements {＜o,p_i,s＞|s∈E, p_i∈R, o∈E, <s,p,o>∉T}, where p_i is the inversed property of p.\n\n> typos\n\nWe have fixed all the typos as suggested in the revised version.\n'}"		SJxfV-q6Tm	rygA3IA8XE	AKBC.ws/2019/Conference/-/Paper44/Official_Comment	['AKBC.ws/2019/Conference/Paper44/Reviewers/Unsubmitted']	3		['everyone']	HJls07eUfV	['AKBC.ws/2019/Conference/Paper44/Authors']	1548310197673		1548316905628	['AKBC.ws/2019/Conference/Paper44/Authors', 'AKBC.ws/2019/Conference']
549	1538087746560	{'title': 'Dynamic Sparse Graph for Efficient Deep Learning', 'abstract': 'We propose to execute deep neural networks (DNNs) with dynamic and sparse graph (DSG) structure for compressive memory and accelerative execution during both training and inference. The great success of DNNs motivates the pursuing of lightweight models for the deployment onto embedded devices. However, most of the previous studies optimize for inference while neglect training or even complicate it. Training is far more intractable, since (i) the neurons dominate the memory cost rather than the weights in inference; (ii) the dynamic activation makes previous sparse acceleration via one-off optimization on fixed weight invalid; (iii) batch normalization (BN) is critical for maintaining accuracy while its activation reorganization damages the sparsity. To address these issues, DSG activates only a small amount of neurons with high selectivity at each iteration via a dimensionreduction search and obtains the BN compatibility via a double-mask selection. Experiments show significant memory saving (1.7-4.5x) and operation reduction (2.3-4.4x) with little accuracy loss on various benchmarks.', 'keywords': ['Sparsity', 'compression', 'training', 'acceleration'], 'authorids': ['liu_liu@ucsb.edu', 'leideng@ucsb.edu', 'huxing@ece.ucsb.edu', 'maohuazhu@ucsb.edu', 'liguoqi@mail.tsinghua.edu.cn', 'yufeiding@cs.ucsb.edu', 'yuanxie@ucsb.edu'], 'authors': ['Liu Liu', 'Lei Deng', 'Xing Hu', 'Maohua Zhu', 'Guoqi Li', 'Yufei Ding', 'Yuan Xie'], 'TL;DR': 'We construct dynamic sparse graph via dimension-reduction search to reduce compute and memory cost in both DNN training and inference.', 'pdf': '/pdf/08d1bc6386b7bc30bc01d298f53d36f0f7fc000e.pdf', 'paperhash': 'liu|dynamic_sparse_graph_for_efficient_deep_learning', '_bibtex': '@inproceedings{\nliu2018dynamic,\ntitle={Dynamic Sparse Graph for Efficient Deep Learning},\nauthor={Liu Liu and Lei Deng and Xing Hu and Maohua Zhu and Guoqi Li and Yufei Ding and Yuan Xie},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1goBoR9F7},\n}'}		H1goBoR9F7	H1goBoR9F7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	114	SyxU3JiKKQ	['everyone']		['ICLR.cc/2019/Conference']	1538087746560		1548314823045	['ICLR.cc/2019/Conference']
550	1548310054577	{'title': 'Thank you for the feedback', 'comment': 'We thank the reviewer for your valuable feedback!\n\n> That the system is limited in this respect is also reflected in the manual construction of the type mappings.\n\nSince our objective is to construct KB with high precision, we combine automatic and manual validation methods.\nFor the construction of type mappings, candidate mappings are automatically collected by traversing the type hierarchy of LOD, and we judge the correctness of these mappings by checking sampled data.\nThus, we consider that this semi-automatic construction procedure is not limited to a small class of entity types.\nWe have added the detailed explanation of these semi-automatical construction methods in Section 3.4 (Attribute Converter).\n\n> There are numerous small grammar and usage errors\n> It might be advisable to chose an illustrative example that does not require the inclusion of illustrations of partially naked (cartoon) humans.\n\nThank you for your essential comments, we revised some grammatical errors and the illustration.'}		SJxfV-q6Tm	BklyVUALQV	AKBC.ws/2019/Conference/-/Paper44/Official_Comment	['AKBC.ws/2019/Conference/Paper44/Reviewers/Unsubmitted']	2		['everyone']	B1gy4BgNfN	['AKBC.ws/2019/Conference/Paper44/Authors']	1548310054577		1548310072662	['AKBC.ws/2019/Conference/Paper44/Authors', 'AKBC.ws/2019/Conference']
551	1542459692914	{'title': 'Fine-grained Entity Recognition with Reduced False Negatives and Large Type Coverage', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper45/Authors'], 'keywords': ['Named Entity Recognition', 'Wikipedia', 'Freebase', 'Fine-grained Entity Recognition', 'Fine-grained Entity Typing', 'Automatic Dataset construction'], 'TL;DR': 'We initiate a push towards building ER systems to recognize thousands of types by providing a method to automatically construct suitable datasets based on the type hierarchy. ', 'abstract': 'Fine-grained Entity Recognition (FgER) is the task of detecting and classifying entity mentions to a large set of types spanning diverse domains such as biomedical, finance and sports.   We  observe  that  when  the  type  set  spans  several  domains,  detection  of  entity mention becomes a limitation for supervised learning models.  The primary reason being lack  of  dataset  where  entity  boundaries  are  properly  annotated  while  covering  a  large spectrum of entity types.  Our work directly addresses this issue.  We propose Heuristics Allied with Distant Supervision (HAnDS) framework to automatically construct a quality dataset suitable for the FgER task.  HAnDS framework exploits the high interlink among Wikipedia  and  Freebase  in  a  pipelined  manner,  reducing  annotation  errors  introduced by naively using distant supervision approach.  Using HAnDS framework,  we create two datasets, one suitable for building FgER systems recognizing up to 118 entity types based on the FIGER type hierarchy and another for up to 1115 entity types based on the TypeNet hierarchy.  Our extensive empirical experimentation warrants the quality of the generated datasets.  Along with this, we also provide a manually annotated dataset for benchmarking FgER systems.', 'pdf': '/pdf/3b5b4ad9b5a4aba533260cbe4884615e9aab8d6a.pdf', 'archival status': 'Archival', 'subject areas': ['Natural Language Processing', 'Information Extraction'], 'paperhash': 'anonymous|finegrained_entity_recognition_with_reduced_false_negatives_and_large_type_coverage', '_bibtex': '@inproceedings{    \nanonymous2019fine-grained,    \ntitle={Fine-grained Entity Recognition with Reduced False Negatives and Large Type Coverage},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=HylHE-9p6m},    \nnote={under review}    \n}'}		HylHE-9p6m	HylHE-9p6m	AKBC.ws/2019/Conference/-/Blind_Submission	[]	45	BygnR6VaaQ	['everyone']		['AKBC.ws/2019/Conference']	1542459692914		1548308781320	['AKBC.ws/2019/Conference']
552	1548307278881	"{'title': 'Re: Clarification on SCONE', 'comment': 'Hi!\n\nYes, we are following their settings in both training and testing.\n\nFrom the paper description, they are testing on final world state without access to intermediate answers. But during training, they have access to the intermediate answers. For example, in (Suhr and Artzi), they wrote ""During training, we create an example for each instruction."" in Section 2, Learning.'}"		ByftGnR9KX	BJgw8oT8QV	ICLR.cc/2019/Conference/-/Paper1287/Official_Comment	['ICLR.cc/2019/Conference/Paper1287/Reviewers/Unsubmitted']	7		['everyone']	S1gsA8yrmV	['ICLR.cc/2019/Conference/Paper1287/Authors']	1548307278881		1548307278881	['ICLR.cc/2019/Conference/Paper1287/Authors', 'ICLR.cc/2019/Conference']
553	1538087951481	"{'title': 'Learning what you can do before doing anything', 'abstract': ""Intelligent agents can learn to represent the action spaces of other agents simply by observing them act. Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences. Current deep learning methods can learn to predict and plan from actions, but only when given abundant, action-labeled data. In this work, we address the problem of learning an agent’s action space purely from visual observation. We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content. Standard techniques lead to reasonable predictions but fail to capture the structure of the action space, as they do not disentangle actions from static scene content. We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions. We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings. When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and visual servoing, while requiring orders of magnitude fewer action labels."", 'keywords': ['unsupervised learning', 'vision', 'motion', 'action space', 'video prediction', 'variational models'], 'authorids': ['oleh@seas.upenn.edu', 'pertsch@usc.edu', 'kosta@ryerson.ca', 'kostas@seas.upenn.edu', 'ajaegle@upenn.edu'], 'authors': ['Oleh Rybkin', 'Karl Pertsch', 'Konstantinos G. Derpanis', 'Kostas Daniilidis', 'Andrew Jaegle'], 'TL;DR': ""We learn a representation of an agent's action space from pure visual observations. We use a recurrent latent variable approach with a novel compositionality loss."", 'pdf': '/pdf/4c9b34e592ce5a5b2afe71a43a81ba185d22d6c5.pdf', 'paperhash': 'rybkin|learning_what_you_can_do_before_doing_anything', '_bibtex': '@inproceedings{\nrybkin2018learning,\ntitle={Learning what you can do before doing anything},\nauthor={Oleh Rybkin and Karl Pertsch and Andrew Jaegle and Konstantinos G. Derpanis and Kostas Daniilidis},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SylPMnR9Ym},\n}'}"		SylPMnR9Ym	SylPMnR9Ym	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1276	Byl9P2wpu7	['everyone']		['ICLR.cc/2019/Conference']	1538087951481		1548303513091	['ICLR.cc/2019/Conference']
554	1548295454687	"{'title': 'More details', 'comment': ""I feel like my comments already said everything needed.\nBut here's more.\n\nThe work is very important and has potential to bridge the gap between neural and logical reasoning methods. In my eyes that could have major impact on the field. I see my job as area chair not in just defining a cutoff based on scores. I've worked in neural nlp for a while and happen to disagree with the reviewers here. \nIt's the only paper in my batch for which I came to this conclusion.\nI do believe the content matters more than the formalities.\n\nMost reject reasons here are not even about the actual content and method of the paper. \n\nAll related work has been or will be added to a later paper version so having missed it in prior submission is now irrelevant. That's what this process is for.\nIf the authors still did not cite relevant work it would be bad.\n\nSearching for a title of a paper on Google is much easier than clicking through github repos and the reverse name github thing is only obvious in retrospect. So it's not relevant in my opinion.\n\nSince people speculate on Twitter:\nI have never heard of the authors and don't know any of them personally.\n\nI understand that people get upset if their papers get rejected or related work is not cited often or well enough. I've had many papers rejected in the past too. I've also seen papers related to my papers that didn't cite me properly. It happens. Most of the time it's an honest mistake that is easily fixable.\n\nThat being said I'm surprised people spend so much time and energy trying to reject a paper...\nI hope we could spend this energy on novel research instead.""}"		rJlWOj0qF7	SJxDmT9LX4	ICLR.cc/2019/Conference/-/Paper331/Official_Comment	['ICLR.cc/2019/Conference/Paper331/Reviewers/Unsubmitted']	19		['everyone']	Bke2NJEUmV	['ICLR.cc/2019/Conference/Paper331/Area_Chair1']	1548295454687		1548295454687	['ICLR.cc/2019/Conference/Paper331/Area_Chair1', 'ICLR.cc/2019/Conference']
555	1538087802358	{'title': 'Learning Recurrent Binary/Ternary Weights', 'abstract': 'Recurrent neural networks (RNNs) have shown excellent performance in processing sequence data. However, they are both complex and memory intensive due to their recursive nature. These limitations make RNNs difficult to embed on mobile devices requiring real-time processes with limited hardware resources. To address the above issues, we introduce a method that can learn binary and ternary weights during the training phase to facilitate hardware implementations of RNNs. As a result, using this approach replaces all multiply-accumulate operations by simple accumulations, bringing significant benefits to custom hardware in terms of silicon area and power consumption. On the software side, we evaluate the performance (in terms of accuracy) of our method using long short-term memories (LSTMs) and gated recurrent units (GRUs) on various sequential models including sequence classification and language modeling. We demonstrate that our method achieves competitive results on the aforementioned tasks while using binary/ternary weights during the runtime. On the hardware side, we present custom hardware for accelerating the recurrent computations of LSTMs with binary/ternary weights. Ultimately, we show that LSTMs with binary/ternary weights can achieve up to 12x memory saving and 10x inference speedup compared to the full-precision hardware implementation design.', 'keywords': ['Quantized Recurrent Neural Network', 'Hardware Implementation', 'Deep Learning'], 'authorids': ['arash.ardakani@mail.mcgill.ca', 'zhengyun.ji@mail.mcgill.ca', 'sean.smithson@mail.mcgill.ca', 'brett.meyer@mcgill.ca', 'warren.gross@mcgill.ca'], 'authors': ['Arash Ardakani', 'Zhengyun Ji', 'Sean C. Smithson', 'Brett H. Meyer', 'Warren J. Gross'], 'TL;DR': 'We propose high-performance LSTMs with binary/ternary weights, that can greatly reduce implementation complexity', 'pdf': '/pdf/20ed3f83c3c07d994e560f50e97fedbc65182702.pdf', 'paperhash': 'ardakani|learning_recurrent_binaryternary_weights', '_bibtex': '@inproceedings{\nardakani2018learning,\ntitle={Learning Recurrent Binary/Ternary Weights},\nauthor={Arash Ardakani and Zhengyun Ji and Sean C. Smithson and Brett H. Meyer and Warren J. Gross},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkNGYjR9FX},\n}'}		HkNGYjR9FX	HkNGYjR9FX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	427	S1erhnqcKQ	['everyone']		['ICLR.cc/2019/Conference']	1538087802358		1548291875275	['ICLR.cc/2019/Conference']
556	1548283177507	{'title': 'The motivation of obtaining information from search engine is not clear. It would be more interesting to derive a method that can automatically get useful common sense knowledge from the search engine without human judgement.', 'review': 'This paper proposes obtaining common sense knowledge from search engine and convert the Winograd Schema problem into natural language inference problem with the help of QASRL.\nThe idea of obtaining extra information from search engine and converting the problem to NLI through QASRL is interesting. However, I have several concerns regarding this paper:\n1. The motivation of obtaining information from search engine is not clear. Even though the overall claim of the paper is to get common sense knowledge to help solving the problem, by simple pattern matching of common verbs and properties can not guarantee the quality of the extracted knowledge. \n2. The knowledge sentences rarely contains any useful information in Table 2. For example, `He succeeded because he studied hard`, I can’t see much useful information from this sentence can be used to disambiguate the pronoun in Scenario sentence. I think the common sense the external knowledge is trying to get is probably cause and effect knowledge, but it can hardly be captured through rule-based system such as QASRL\n3. I wonder the performance of QASRL, since it may introduce lots of pipeline errors. It would be nice to add some analysis regarding that.\n4. It would be more interesting to derive a method that can automatically get useful common sense knowledge from the search engine without human judgement.', 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		H1xdMZ566m	HJe-V6D8XV	AKBC.ws/2019/Conference/-/Paper35/Official_Review	['AKBC.ws/2019/Conference/Paper35/Reviewers/Unsubmitted']	3		['everyone']	H1xdMZ566m	['AKBC.ws/2019/Conference/Paper35/AnonReviewer4']	1548283177507		1548283279174	['AKBC.ws/2019/Conference/Paper35/AnonReviewer4', 'AKBC.ws/2019/Conference']
557	1548277648521	"{'title': 'Comments for Reviewer 3', 'comment': ""We would like to thank the reviewer for the comments. We submitted to the AKBC conference assuming that it now has a wider scope as mentioned in the CFP (http://www.akbc.ws/2019/cfp/). As per the call, we assume that AKBC is now not just limited to knowledge base construction only but also encourages wider scope such as NLP.  \n\nAs also described in the paper, we are addressing an interesting research problem in a more practical setting where there is no training data available.\n\nSpecific Remarks:\n\n1. During our experiments we evaluated existing text simplification systems and a manual inspection showed very poor results. Hence, we didn’t include any quantitative results in the paper. Nevertheless, we have updated the paper with quantitative and qualitative results comparing with other text simplification systems. \n2. In our system, we employed several heuristics to order the actions contained in the same sentence during the sentence simplification stage, based on the timing information. We could not include these due to space constraints (we will describe it if the paper gets accepted). Each simplified sentence gets assigned with an incremental temporary ID, indicating the ordering. For example, in the Adverbial structure analyzer, the transform function will detect words with 'PREP' and 'MARK' tag, and modify the ordering according to a set of time indicators(e.g. [before, after, as, until, till, ...]). One example is: \n\nSuddenly there is a knock at the door, immediately after which Jim's mom enters. \n\nThe system will detect PREP 'which' and identifies the ordering of verbs. The outputs are: [('Jim's mom enters', 1), ('Suddenly there is a knock at the door', 0)], which means Jim's mom enters only after the knock. Similar heuristics are also applied in other sentence structure analyzers.\n3. We trained our annotators to be as consistent and objective as possible. However, annotation inconsistencies are due to subjectivity of the task and inherent ambiguities in natural language. Some of the fields being annotated are quite subjective and vary from person to person. This problem is not specific to our setting but happens in other NLP labelling tasks as well: for example, machine translation where multiple human annotators translate the same sentence differently. \n4. In future work we plan to upgrade the animation module to include more features.  \n""}"		Hkg5zW96p7	BJgY5DU8QE	AKBC.ws/2019/Conference/-/Paper36/Official_Comment	['AKBC.ws/2019/Conference/Paper36/Reviewers/Unsubmitted']	3		['everyone']	rkgl6DrsWN	['AKBC.ws/2019/Conference/Paper36/Authors']	1548277648521		1548277648521	['AKBC.ws/2019/Conference/Paper36/Authors', 'AKBC.ws/2019/Conference']
558	1548277527786	{'title': 'Comments for Reviewer 2', 'comment': 'We would like to thank the reviewer for the comments. We submitted to the AKBC conference assuming that it now has a wider scope as mentioned in the CFP (http://www.akbc.ws/2019/cfp/). As per the call, we assume that AKBC is now not just limited to knowledge base construction only but also encourages wider scope such as NLP.  Nevertheless, as also noted by Reviewer 3, the linguistic text simplification techniques we describe are equally applicable for information extraction as is done in Open IE based systems. \n\nAs also as described in the paper, we are addressing an interesting research problem in a more practical setting where there is no training data available. \n\nSpecific Remarks:\n\n1. We used training data to develop linguistic rules for sentence simplification and later for extracting information. The labelled corpus we created is only for evaluation. Creating a corpus is very labor intensive and time intensive task making it difficult to create a reasonable-sized training corpus which could be used for training an end to end system. \n2. In this paper we are addressing a more practical setting where there is no training data available for training an end to end system. The methods proposed in this paper are NLP oriented but, as we mentioned in the paper, the linguistic simplification techniques will be useful for information extraction and knowledge base creation especially for free form texts, or for use cases with little labelled data. Our work comes close to the Graphene System (https://arxiv.org/pdf/1807.11276.pdf) where the authors also propose using linguistic rules for simplifying text and for extracting information. \n3. Our paper does not include quantitative comparison between our system and existing systems. This is due to two reasons: Firstly, existing systems such as CONFUCIUS, SceneMaker and Cardinal do not provide any quantitative results. And secondly, there is no publicly available implementation of their work. Therefore, replicating their work is hard. Moreover, as stated in their original papers, they do not address sentences with complex structure, therefore it is difficult to compare against their work.\n4. Our main contribution in terms of  methodology is our set of linguistic simplification rules to simplify sentence structures and proposing a novel pipeline for text to animation system. As shown in the paper, these methods works well in practice. In terms of resource contribution, we have a created a gold test dataset for text to animation generation system. It is hoped that it will serve as a standard benchmark dataset for systems working on text to animation generation. '}		Hkg5zW96p7	r1xeQD8I7V	AKBC.ws/2019/Conference/-/Paper36/Official_Comment	['AKBC.ws/2019/Conference/Paper36/Reviewers/Unsubmitted']	2		['everyone']	SylUGTRkG4	['AKBC.ws/2019/Conference/Paper36/Authors']	1548277527786		1548277572930	['AKBC.ws/2019/Conference/Paper36/Authors', 'AKBC.ws/2019/Conference']
559	1548277427017	{'title': 'Updated version', 'comment': 'A new version of the paper has been uploaded (still working on some additional results) as well as our rebuttal.\n\nWe would like to stress again that at its core our IE method is based on  [Christensen] that used SRL for open-domain IE (i.e., domain independent). Although our examples and motivating use case is from the medical domain, all our algorithms use general purpose methods (SRL, dependency parsing) and can be used to extract information from sentences like “Barack Obama is an American politician who served as the 44th president of the United States”. They require no training and no annotated data to learn specific patterns per relation.\n\nIndeed some parts of the work have to do with engineering solutions since this is actually a production level system. However, many techniques extend previous works on SRL for openIE [Christensen] and copula verbs [Miwa 2010]. Non-trivial approaches to scoring and linking are also presented.\n\nhigher arity relations cannot be stored in existing triple-stores without causing a blow-up by using reification. we are in the process of investigating this further as such triple annotations are useful.\n\nOther openIE have been tested but they usually extract general triples which are not precise enough for a symptom checking scenario.\n\nMore detailed comments have been added below in our rebuttal.'}		HJxmxbq66Q	HJxj28ILmE	AKBC.ws/2019/Conference/-/Paper20/Official_Comment	['AKBC.ws/2019/Conference/Paper20/Reviewers/Unsubmitted']	5		['everyone']	HJxmxbq66Q	['AKBC.ws/2019/Conference/Paper20/Authors']	1548277427017		1548277427017	['AKBC.ws/2019/Conference/Paper20/Authors', 'AKBC.ws/2019/Conference']
560	1548277400846	"{'title': 'Comments for Reviewer 1', 'comment': ""We would like to thank the reviewer for the comments. We submitted to the AKBC conference assuming that it now has a wider scope as mentioned in the CFP (http://www.akbc.ws/2019/cfp/). As per the call, we assume that AKBC is now not just limited to knowledge base construction only but also encourages wider scope such as NLP.  Nevertheless, as also noted by Reviewer 3, the linguistic text simplification techniques we describe are equally applicable for information extraction as is done in Open IE based systems. \n\nAs also as described in the paper, we are addressing an interesting research problem in a more practical setting where there is no training data available. \n\nSpecific Remarks:\n\n1. In principle we could we use an end to end IE system. In fact we even tried to do that in our initial attempts. However, in our initial experiments with Open IE system gave very poor results. The sentence structures occurring in our corpus are complicated and cannot be readily processed by an IE system as we saw in our initial experiments. In order to make the text amenable to an IE system we used a text simplification approach (Text simplification module in figure 1). We crafted a set of linguistic rules to simplify the sentences so that information could be easily extracted from these (IE module in figure 1). Moreover, the linguistic simplification techniques we developed would be useful for IE systems dealing with free-form texts like the screenplays. In our initial attempts at addressing the research problem, we thought of trying an end to end approach as described in https://arxiv.org/pdf/1710.00421.pdf, however, we suffered from lack of a big domain-specific training corpus. Hence, we had to resort to learning an intermediate representation (ARF) which could be used by animation engine. \n2. We are not applying only readily available NLP tools. For example, we developed the text simplification module using linguistic patterns. This is a new contribution and is easily generalizable to other corpora and IE based tasks as well (also mentioned by Reviewer 3). There are a number of text simplification systems available but most of these have been trained on more encyclopedic data like Wikipedia. These tools fail to perform on free form text as we encountered. Extracting relationships between entities from simplified sentences is easier than extracting from complex sentences which contain multiple relationships between entities. Our task at hand is paraphrasing a sentence into several simpler ones without losing relational information. This is different from what has been addressed in existing tasks. In sentence simplification tasks, existing systems instead try to summarize/ extract the core information and ignore secondary information. These units of 'secondary information' are, in fact, information we do not want to discard in our task. Actually we applied this procedure in another published information extraction related task, where it proved to be very effective.\n3. Our system can be used for generating training data which could be useful for training end-to-end systems in the future. The gold dataset that we created would serve as a benchmark for the community to evaluate text to animation systems. \n4. We extensively evaluated of our system using intrinsic and extrinsic techniques. However, as mentioned in related work section (section 2), we could not compare to any of the existing text to animation systems as their implementations are not available and these systems have not been evaluated on any standard corpus, making it difficult to compare. We give examples in Table 2 and 4 and we analyze these in Section 5.1.1.\n""}"		Hkg5zW96p7	BkgZo8IUQV	AKBC.ws/2019/Conference/-/Paper36/Official_Comment	['AKBC.ws/2019/Conference/Paper36/Reviewers/Unsubmitted']	1		['everyone']	SJxDo7emfV	['AKBC.ws/2019/Conference/Paper36/Authors']	1548277400846		1548277400846	['AKBC.ws/2019/Conference/Paper36/Authors', 'AKBC.ws/2019/Conference']
561	1542459665956	{'title': 'Generating Animations from Screenplays', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper36/Authors'], 'keywords': ['Text to Animation', 'NLP'], 'TL;DR': 'We generate animations from screenplays by extracting information using NLP and linguistic techniques.', 'abstract': 'Automatically generating animation from natural language text finds application in a number of areas e.g. movie script writing, instructional videos, and public safety. However, translating natural language text into animation is a challenging task. Existing text-to-animation systems can handle only very simple sentences, which limits their applications. In this paper, we develop a text-to-animation system which is capable of handling complex sentences. We achieve this by introducing a text simplification step into the process. Building on an existing animation generation system for screenwriting, we create a robust NLP pipeline to extract information from screenplays and map them to the system’s knowledge base. We develop a set of linguistic transformation rules that simplify complex sentences. Information extracted from the simplified sentences is used to generate a rough storyboard and video depicting the text. Results show that our system performs reasonably well in terms of both efficiency and robustness.\n', 'pdf': '/pdf/b7c56bd18eb9054b689e9045acfa67dd2a9d4999.pdf', 'archival status': 'Archival', 'subject areas': ['Natural Language Processing', 'Applications: Other'], 'paperhash': 'anonymous|generating_animations_from_screenplays', '_bibtex': '@inproceedings{    \nanonymous2019generating,    \ntitle={Generating Animations from Screenplays},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=Hkg5zW96p7},    \nnote={under review}    \n}'}		Hkg5zW96p7	Hkg5zW96p7	AKBC.ws/2019/Conference/-/Blind_Submission	[]	36	Syl5rwtnaQ	['everyone']		['AKBC.ws/2019/Conference']	1542459665956		1548277090834	['AKBC.ws/2019/Conference']
562	1548276727191	{'title': 'reply on review', 'comment': 'SRL was used because as shown in [Christensen] off-the-shelf parsers can readily be used for IE with fairly good results and no need for training classifiers and large annotated data. We have tried to explain this rational in the intro, that is, that SRL is used out-of-the-box. \n\nOur tools and solutions are heavily based on Semantic Web standards, SPARQL, and triple stores. Most triple stores only store triples (some can store quads but only under strong restrictions), hence higher-arity relations cannot be stored unless reification is used which causes an “explosion” on their number. We are in the process of figuring out how to practically store such annotations on triples like TIME and LOCATION that come from SRL. \n\nWe have experimented with tools like Stanford’s (http://corenlp.run/) and Allen’s (https://demo.allennlp.org/open-information-extraction/) open Information Extraction in the past and the results were not very good for our purposes. These tools have been trained to be general purpose and the extracted facts are guide general. This can, e.g., be seen if one tries sentence “Optic neuritis is a common symptom that can cause blurred vision” at http://corenlp.run/. The NER component fails to identify “blurred vision” and hence the IE fails to extract relevant triples like <”Optic neuritis” “causes” “blurred vision”> which can be extracted using our approach. Regarding the Allen system, since it is based on SRL like ours it behaves well in this example but fails on sentence like “Common symptoms of malaria include fever” where a “lifting” of the noun-phrase relation is required. Given these observations and the fact that we wanted to have full control on the source code we decided to implement our own pipeline.\n\nWe updated the Evaluation section to state that indeed our implementation is using the system described by He et al. (AllenNLP). \n\nIndeed various sub-components of the system, like entity linking, can be improved but we can’t possibly make a state-of-the-art contribution to all faced problems. Just entity linking in the medical domain (disease normalisation) is a topic of research on it own and actually a paper specifically for this topic has been submitted to AKBC. Context can be used for disambiguation, however, note that our pipeline does not make a specific entity choice but returns a set of ranked top-n entities from the KB from which doctors select. \n\nWe have revised the paper regarding the issue of confidence values and SRL. SRL is a shallow parser and does not per se compute confidence scores for the extraction of information. Moreover, quoting from [Christensen]\n\t“Because of its use of an integer linear program, SRL-IE does not associate confidence values with extractions and is shown as a point in these figures.”\n\nWe have updated Table 1 with some corrected numbers. Regarding section 6.4 and 6.5, the two scoring approaches were developed in different points in time in which we had different sets of triples. Since these are different systems we don’t feel this is an issue.\n\nIn table 1 we only show the breakdown of accepted triples because for rejects, there is a number of different reasons that a triple can be rejected. It can be a failure to associate any or some precise IRI or even more, a doctor may change the proposed property into a different one and then accept the triple. Hence, it is almost impossible to track the rejects per property.  \n\nIndeed there is overlap with some of the relations used in Ernst et al. 2015. To make a comparison of the two works would require to run them over the exact same dataset and, to the best of our knowledge, the system is not available for download and testing.\n\nYes, it hasn’t happened that any triple with the affects relation has been generated by the doctor verification process. No treats, affects, and causes are different relations.\n\nThe reviewer is correct in the last point. The statistics in Table 1 do not use the ordering and pruning machinery that is presented in sections 6.4 and 6.5 because we hadn’t managed to integrate this work into the main production system yet. We are in the process of an additional evaluation and until the end of the rebuttal phase we will try provide some accuracy numbers with these machinery integrated in the main pipeline. \n'}		HJxmxbq66Q	r1gyZE8UXE	AKBC.ws/2019/Conference/-/Paper20/Official_Comment	['AKBC.ws/2019/Conference/Paper20/Reviewers/Unsubmitted']	4		['everyone']	Ske93-vjWV	['AKBC.ws/2019/Conference/Paper20/Authors']	1548276727191		1548276751079	['AKBC.ws/2019/Conference/Paper20/Authors', 'AKBC.ws/2019/Conference']
563	1548276667107	{'title': 'reply', 'comment': 'The proposed heuristics help the IE algorithm to improve the recall of the extracted triples. That is, without the heuristics the standard SRL would extract no triple or it will extract a triple that would eventually get rejected. Measuring recall increase in IE is a very hard problem and with the current state-of-the-art is almost impossible to measure. There is actually a paper submission at AKBC on precisely this issue. Consequently, although we agree it would be very interesting to assess the effects of the heuristics it is, however, currently almost impossible. \n\nThe acceptance rate of 48% does not reflect the amount of correct/incorrect triples. There are plenty of correct triples that doctors reject, e.g., because they are not relevant to our use case (the triples are vague, imprecise, too general). For example, a triple of the form <”Malaria” “spread” “by mosquitoes”> is correct and arguably relevant for building a general purpose medical Knowledge Graph, however, it can’t be used for any symptom checking purposes. In addition, a doctor may reject a triple like <”gangrene” “occurs” “when body tissue dies”> because no precise IRI is linked to the phrases. We have revised the paper to make an attempt to explain these points better. We feel that 48% is actually a quite high score and doctors are indeed very cautious and selective.\n\nWe agree with the view of the reviewer about practical system for practitioners, however, we don’t feel this should be considered as a shortcoming. Being able to combine in a not trivial way existing or new solutions to difficult research problems is far from uninteresting and arguably is of academic relevance. In addition, we are making various non-trivial extensions and contributions to previous academic works like the copula verb pre-processing [Miwa 2010] and the noun-based relation extraction. Both of these techniques improve the generic SRL-based open IE approach presented initially in [Christensen]. Moreover, we evaluate KB-embedding based link prediction works on a real-world scenario and propose our combined approach for scoring and filtering. Finally, note that many of our extensions are based on dependency parsing and general linguistic properties, hence they are domain independent and transfer easier to new domains compared to learning methods. \n\nWe addressed all typos and editing comments pointed out by the reviewer. Indeed SRL is based on ML in the first place. We revised the text.\n\nYes, the SRL parser used is the AllenNLP one described by He et al. We mention this in the new version. \n\nUnfortunately, due to the anonymous paper submission instructions we cannot provide a reference to the document that provides further details of the PGM model and our diagnostic approach. '}		HJxmxbq66Q	BygmpQLL7E	AKBC.ws/2019/Conference/-/Paper20/Official_Comment	['AKBC.ws/2019/Conference/Paper20/Reviewers/Unsubmitted']	3		['everyone']	rJeTq2NXfE	['AKBC.ws/2019/Conference/Paper20/Authors']	1548276667107		1548276667107	['AKBC.ws/2019/Conference/Paper20/Authors', 'AKBC.ws/2019/Conference']
564	1548276630589	{'title': 'reply on review', 'comment': 'Although our examples and our work is motivated by the medical domain, all our techniques are domain independent and can be applied to any type of sentence. At its core the algorithm is based on the work of [Christensen] that used SRL for open-domain IE. Then, Algorithm 1 can be applied to sentences like “Barack Obama is an American politician who served as the 44th president of the United States” to extract triple <“Obama” “served” “as the 44th president of the United States”>. Alg 1 generalises and formalises the approach proposed in [Miwa 2010] by using dependency parsing, a method that is domain independent and is based on general linguistic properties. Algorithm 2 is also easily adaptable to other noun-type relations by passing a different set of relations as a parameter. Finally, our entity linking and all our filtering, scoring, and ranking algorithms are also domain independent and use off-the-shelf state-of-the-art tools like Google’s Universal sentence embedder and KB-embedding algorithms. Actually, we feel that our approach generalises and adapts to other domain and different sets of relations much better compared to distance supervision learning approaches which need to be retrained whenever a new domain or a new set of relations is considered.  \n\nOur motivation for the definitions is to formalise the approach and disclose all details in a precise way. We feel that just using examples leaves things vague, half specified and may not address all edge cases compared to a formal definition. We have included examples throughout the paper to explain how the algorithms work and in the newly uploaded version of the paper we made those examples more explicit as well as added an example for Definition 1. \n\nWe have revised the text involving the GATE system. We now refer to the system MetaMap which is a fairly standard tool for annotating medical text with concepts from the UMLS vocabulary. A reference to the system is also given.\n\nWe have revised the text to introduce the concept of embeddings and we also added references on the subject. \n\n“Alternative labels” have been introduced in Section 2. As stated, they are taken from the SKOS W3C standard and intuitively are used to capture synonymous labels.  \n\nThere is some confusion on the interpretation of the 48% acceptance rate mostly originating from our side and the difficulty to explain this number. First, in information extraction one can measure precision at two different stages. One stage could be over the extracted RAW text triples before entity linking. For example, from text “Gangrene occurs when body tissue dies” an IE tool could extract text triple <”gangrene” “occurs” “when body tissue dies”>. This is a correct triple and many previous approaches would count it as a successful extraction. However, if one tries to associate a concept to each of the phrases, no biomedical KB contains an entity for phrase “when body tissue dies”. Failure to associate a precise IRI would lead a doctor to reject the triple. Second, even semantically correct triples can also be rejected by our doctors if they think they are imprecise. For example, triples like <”Malaria” “spread” “by mosquitoes”> or <”OpticNeuritis” “symptomOf” “ImmuneSystemDiseases”> may be rejected by doctors since they are not relevant for symptom checking and diagnosis but represent some general knowledge. Given this very selective filtering done by doctors we actually feel that 48% acceptance is in contrary a very high one. We have tried several online demos of existing open IE frameworks and in most cases they extract such arguably correct but vague triples. \n'}		HJxmxbq66Q	BkgJi7ULmV	AKBC.ws/2019/Conference/-/Paper20/Official_Comment	['AKBC.ws/2019/Conference/Paper20/Reviewers/Unsubmitted']	2		['everyone']	BJe-nn8vGN	['AKBC.ws/2019/Conference/Paper20/Authors']	1548276630589		1548276630589	['AKBC.ws/2019/Conference/Paper20/Authors', 'AKBC.ws/2019/Conference']
565	1538087893149	"{'title': 'Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning', 'abstract': ""Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n"", 'keywords': ['Multi-agent Reinforcement Learning', 'Recursive Reasoning'], 'authorids': ['ying.wen@cs.ucl.ac.uk', 'yaodong.yang@cs.ucl.ac.uk', 'rui.luo@cs.ucl.ac.uk', 'jun.wang@cs.ucl.ac.uk', 'wei.pan@tudelft.nl'], 'authors': ['Ying Wen', 'Yaodong Yang', 'Rui Luo', 'Jun Wang', 'Wei Pan'], 'TL;DR': 'We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.', 'pdf': '/pdf/12f430fb5a28c8adc39784fd4f45dda62a737de4.pdf', 'paperhash': 'wen|probabilistic_recursive_reasoning_for_multiagent_reinforcement_learning', '_bibtex': '@inproceedings{\nwen2018probabilistic,\ntitle={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},\nauthor={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl6As0cF7},\n}'}"		rkl6As0cF7	rkl6As0cF7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	938	SkgpPa69Y7	['everyone']		['ICLR.cc/2019/Conference']	1538087893149		1548269563269	['ICLR.cc/2019/Conference']
566	1548267316106	"{'comment': 'The AC\'s comment does not have any technical content, in particular, there is nothing that addresses any of the reviewers concerns, including Rev 1, who points out that the paper fails to cite prior work that already introduces concepts this paper is trying to present as proposed. There is no comment on this discussion page that would defend the authors\' claim.\n\nIn addition, the authors failed to comply with the anonymization policy. It takes three link clicks to get from the submitted manuscript to see 4 of the authors\' names (go to the ""mushroom"" repo, the names are at the bottom of the page). Authors\' comments reveal that they are under the impression that allowing Arxiv preprints invalidates the rest of the anonymity policy, which raises questions of whether they read the author\'s guidelines presented at the conference website iclr.cc.\n\nThe authors and Rev 1 failed to reach a consensus, while the authors were openly belligerent in both discussions that happened. See comments as ""your feeling is understandable"", ""unacceptable comment"", and ""the detective work of your kind"". \n\n--------––------------------------––------------------------––------------------------––------------------------––------------------------––----------------\n\nGiven that any single point of the above is substantive grounds for a rejection, the lack of a transparent decision by AC feels out of place here. OpenReview exists in part so that the reviewer entities can be held publicly accountable for their reviews. This is not what\'s happened here.', 'title': 'Can the AC participate in the discussion or at least explain the acceptance decision?'}"		rJlWOj0qF7	Bke2NJEUmV	ICLR.cc/2019/Conference/-/Paper331/Public_Comment	[]	8		['everyone']	rJlWOj0qF7	['(anonymous)']	1548267316106		1548267316106	['(anonymous)', 'ICLR.cc/2019/Conference']
567	1542459628926	{'title': 'Improving Relation Extraction by Pre-trained Language Representations', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper21/Authors'], 'keywords': ['relation extraction', 'deep language representations', 'transformer', 'transfer learning', 'unsupervised pre-training'], 'TL;DR': 'We propose a Transformer based relation extraction model that uses pre-trained language representations instead of explicit linguistic features.', 'abstract': 'Current state-of-the-art relation extraction methods typically rely on a set of lexical, syntactic, and semantic features, explicitly computed in a pre-processing step. Training feature extraction models requires additional annotated language resources, which severely restricts the applicability and portability of relation extraction to novel languages. Similarly, pre-processing introduces an additional source of error. To address these limitations, we introduce TRE, a Transformer for Relation Extraction, extending the OpenAI Generative Pre-trained Transformer [Radford et al., 2018]. Unlike previous relation extraction models, TRE uses pre-trained deep language representations instead of explicit linguistic features to inform the relation classification and combines it with the self-attentive Transformer architecture to effectively model long-range dependencies between entity mentions. TRE allows us to learn implicit linguistic features solely from plain text corpora by unsupervised pre-training, before fine-tuning the learned language representations on the relation extraction task. TRE obtains a new state-of-the-art result on the TACRED and SemEval 2010 Task 8 datasets, achieving a test F1 of 67.4 and 87.1, respectively. Furthermore, we observe a significant increase in sample efficiency. With only 20% of the training examples, TRE matches the performance of our baselines and our model trained from scratch on 100% of the TACRED dataset. We open-source our trained models, experiments, and source code.', 'pdf': '/pdf/3b843fca9466b93a7405d30ace5c9c94103e129b.pdf', 'archival status': 'Non-Archival', 'subject areas': ['Natural Language Processing', 'Information Extraction'], 'paperhash': 'anonymous|improving_relation_extraction_by_pretrained_language_representations', '_bibtex': '@inproceedings{    \nanonymous2019improving,    \ntitle={Improving Relation Extraction by Pre-trained Language Representations},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=BJgrxbqp67},    \nnote={under review}    \n}'}		BJgrxbqp67	BJgrxbqp67	AKBC.ws/2019/Conference/-/Blind_Submission	[]	21	B1l3x7zsaQ	['everyone']		['AKBC.ws/2019/Conference']	1542459628926		1548266363284	['AKBC.ws/2019/Conference']
568	1548258390292	{'title': 'Response to Reviewer 3', 'comment': 'Thanks for your comments.  As you point out, the lines do not cross for MRR but do cross for coverage.   This is due to the fact that while the Char 4-gram has higher coverage at high K levels, the correct candidates are still ranked very low, and have a low effect on the MRR.  DiscK overall does a much better job of ranking correct concepts highly, but omits some that Char 4-gram assigns a low ranking. \n\nWe agree that the size and contents of the candidate set would have significant implications for the final linking results.  We are limited in the flexibility we have to experiment with this, since the candidate set is determined by the annotators, but it is an important point.  Regarding the terminology, DiscK weighted uses the parameters learned by the training procedure (discussed in Response #1), while DiscK binary only considers the presence of features (i.e. all parameters are 1) and does not use the learned weights.  The combined model uses the best DiscK weighted with a model that has higher coverage at K=1000 (Char 4-gram).  For each candidate, we take the higher of the two normalized scores to achieve a combined ranking.  \n'}		r1xP1W56pQ	S1xC83WU7E	AKBC.ws/2019/Conference/-/Paper15/Official_Comment	['AKBC.ws/2019/Conference/Paper15/Reviewers/Unsubmitted']	3		['everyone']	S1x_J6gElE	['AKBC.ws/2019/Conference/Paper15/Authors']	1548258390292		1548258390292	['AKBC.ws/2019/Conference/Paper15/Authors', 'AKBC.ws/2019/Conference']
569	1548258356248	{'title': 'Response to Reviewer 2', 'comment': 'Thanks for the comments.  While we do not report downstream results in Section 6, we do in Section 8.  We did so in this setup so that we could first compare potential retrieval methods, and then test the DiscK method downstream.  We agree that reporting timed results would be ideal.  However, after a significant amount of engineering effort, we were unable to directly integrate the DiscK method into DNorm, as it is an older codebase.  Therefore, we run the DiscK retrieval method, save the results, and finally loading the results into DNorm.  This doesn’t allow for a fair comparison on timed efficiency, so we are left with the efficiency measurements we report in the paper.  In addition, thanks for correctly pointing out that instead of linear time, we should state that DiscK is sublinear time - this will be corrected in the paper.\n\nIn response to comment (1), it is true that the size of this dataset (125k concepts) could possibly allow for a pairwise model, as you suggest.  However, the mentions in this dataset are only linked to a subset of one ontology, SNOMED Disorders.  In real clinical settings, the size of the ontology is often much larger, as the entire ontology (SNOMED has 300K concepts) or several ontologies may be used in combination, leading to a concept set of 1 million or more.  Therefore, we wanted to approach this problem in a way that will scale to very large ontologies, and a pairwise approach, while potentially feasible for this dataset, would get computationally expensive quickly for larger ontology sets.  The downside of the pairwise approach extends to DNorm, which essentially performs a weighted similarity calculation between the tf-idf vector of the mention and the concept.  In larger settings, this will grow very slowly.  While most of the time concept linking is performed offline, there are several scenarios where online concept linking would be useful, such as a clinician seeking to process a specific clinical note.\n\nFollowing off of the above point, this is part of the reason we don’t simply make DNorm more efficient (as suggested in (2)), as it is still a pairwise method.  Furthermore, we wanted to propose a general method that would allow candidate generation for any system, and not simply speed up an existing one.\n\nAs noted in the response to reviewer #2, the DNorm system was one of the best performing systems on the task (at the release of the shared task results, see Pradhan et al 2015), so that point of comparison is available.  However, those results are on the test set, and aren’t directly comparable.  While we don’t currently have the code for any of the other systems reported in that paper, we do have access to a later system (“Sieve-Based Entity Linking for the Biomedical Domain”, D’Souza and Ng 2015), and we can use that as a point of comparison for the final paper.'}		r1xP1W56pQ	B1lnN2-L74	AKBC.ws/2019/Conference/-/Paper15/Official_Comment	['AKBC.ws/2019/Conference/Paper15/Reviewers/Unsubmitted']	2		['everyone']	HJebHxu6x4	['AKBC.ws/2019/Conference/Paper15/Authors']	1548258356248		1548258356248	['AKBC.ws/2019/Conference/Paper15/Authors', 'AKBC.ws/2019/Conference']
570	1548258311511	{'title': 'Response to reviewer 1', 'comment': 'Thanks for your comments on our work.  First, we can clarify several points on DiscK.  The goal of DiscK is to use a simple feature set to learn a weighted similarity between a query and a set of documents.  In our task, the query is a mention and the set of documents is a set of concepts from the UMLS.  It is parameterized in a way that allows for retrieval in sublinear time (as reviewer #2 points out, we incorrectly said “linear” - this will be corrected to “sublinear”).\n\nFor the training procedure, the weights θ are trained using a negative sampling procedure, with the goal of learning a set of weights that will correctly predict mention-concept pairs.  For each training mention, it is paired with the correct ontology entry and 50 incorrect ontology entries.  The resulting weights are used to project which ontology entry is most suited to the mention feature set.  This is trained using a log-linear model.    \n\nThe feature set we explored that included additional mention properties (e.g. the surrounding sentence and the definition of the concept, if available) used the same approach as the mention features, except that they included the text of the sentence or definition.  For example, one proposed feature was a bag-of-words approach using the surrounding sentence and the concept definition.\n\nRegarding the data, we cannot directly compare to previously reported results on this dataset for several reasons.  As pointed out, we do use a specific subset of the ontology, and it isn’t clear which specific subsets other papers used to evaluate. However, the more important point is that we do not have access to the test set (as noted in Section 5), and therefore we don’t have the necessary data for a direct comparison.  The DNorm system was one of the best performing systems on the task (at the release of the shared task results, see Pradhan et al 2015), so that point of comparison is available.\n'}		r1xP1W56pQ	BJxJz3-I7E	AKBC.ws/2019/Conference/-/Paper15/Official_Comment	['AKBC.ws/2019/Conference/Paper15/Reviewers/Unsubmitted']	1		['everyone']	Bkxhm1YMfN	['AKBC.ws/2019/Conference/Paper15/Authors']	1548258311511		1548258311511	['AKBC.ws/2019/Conference/Paper15/Authors', 'AKBC.ws/2019/Conference']
571	1538087769360	{'title': 'Identifying and Controlling Important Neurons in Neural Machine Translation', 'abstract': 'Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons. We develop unsupervised methods for discovering important neurons in NMT models. Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision. We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena. Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons.', 'keywords': ['neural machine translation', 'individual neurons', 'unsupervised', 'analysis', 'correlation', 'translation control', 'distributivity', 'localization'], 'authorids': ['abau@mit.edu', 'belinkov@mit.edu', 'hsajjad@hbku.edu.qa', 'ndurrani@qf.org.qa', 'faimaduddin@qf.org.qa', 'glass@mit.edu'], 'authors': ['Anthony Bau', 'Yonatan Belinkov', 'Hassan Sajjad', 'Nadir Durrani', 'Fahim Dalvi', 'James Glass'], 'TL;DR': 'Unsupervised methods for finding, analyzing, and controlling important neurons in NMT', 'pdf': '/pdf/9843ebbf50e6a26bc61bff4758075fd2e79730f1.pdf', 'paperhash': 'bau|identifying_and_controlling_important_neurons_in_neural_machine_translation', '_bibtex': '@inproceedings{\nbau2018identifying,\ntitle={Identifying and Controlling Important Neurons in Neural Machine Translation},\nauthor={Anthony Bau and Yonatan Belinkov and Hassan Sajjad and Nadir Durrani and Fahim Dalvi and James Glass},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1z-PsR5KX},\n}'}		H1z-PsR5KX	H1z-PsR5KX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	240	ryl84XP5YX	['everyone']		['ICLR.cc/2019/Conference']	1538087769360		1548257514574	['ICLR.cc/2019/Conference']
572	1548246850435	"{'title': 'Response to the AC metareview overriding reviewer consensus', 'comment': 'We would like to thank again all the reviewers and the area chair for their participation during the reviewing period. We honestly believe that it significantly contributed to improve our work. Being said that and given the nature of OpenReview, we would like to provide a response to the AC metareview which resulted in a reject decision. We think that his/her arguments were too limited to warrant over-riding the reviewer consensus for acceptance. In summary, from our point of view, the reasons appear vague, arbitrary in some aspects and/or were never raised during the reviewing period:\n\n\n\n1 (AC) “the quantitative results with weak supervision are not a big improvement over beta-vae-like methods or Mathieu et al.”\n \n→ It is not clear how “big improvements” should be for the AC not to be used as a reason to reject a paper. Experimentally, we find that our method improves over all the compared state-of-the-art methods on the evaluated datasets. The comparison with Mathieu et. al was added according to a suggestion by AnonReviewer2, who raise his/her score based on the provided results. Additionally, we show how our model can be applied to different problems (conditional image generation and attribute transfer) which can not be addressed using the compared methods.\n \n\n\n2 (AC) “a red flag of sorts to me is that it is not very clear where the gains are coming from: the authors claim to have done a fair comparison with the various baselines, but they introduce an entirely new encoder/decoder architecture that was likely (involuntarily, but still) tuned more to their method than others.”\n \n→ This point was not raised in any of the reviews and, therefore, we had not the opportunity to clarify it. As described in the paper, our networks are based on a standard conv/deconv networks with upsampling and downsampling operations. This is the most standard architecture used in VAE-based models and GAN literature. Moreover, our architecture uses exactly the same building blocks as [Karras et. al, 2018]. We think that characterising this as an “entirely new encoder/decoder architecture” is unreasonable. We did not explore other types of architecture designs and all the evaluated methods used the same hyper-parameters without any tuning.\n \n\n\n3 (AC) ""the setup as presented is somewhat artificial and less general than it could be (however, this was not a major factor in my decision). It is easy to get confused by the kind of disentangled representations that this work is aiming to get.""\n \n→ This point was raised by AnonReviewer1 during the revision. We clarified our concept of disentanglement and discussed different potential applications of our setup. We updated our paper accordingly. The reviewer was convinced by our response and raised his score to (Good paper, accept). No additional comments on this issue are provided by the AC.\n \n\n\n4 (AC) ""I think this has the potential to be a solid paper, but at this stage it\'s missing a number of ablation studies to truly understand what sets it apart from the previous work. At the very least, there is a number of architectural and training choices in Appendix D -- like the 0.25 dropout -- that require more explanation / empirical understanding and how they generalize to other datasets.""\n \n--> No description about the “number of ablation studies” is provided. A single concrete point is raised here: that we do not provide data on how the used dropout-rate of 0.25 generalizes to other datasets. First of all, we did not fine-tune this hyper-parameter given that we found that the default value worked well in all our experiments. Moreover, the lack of in-depth evaluation of how the used dropout rates generalize to other datasets seems plentiful across the papers in ICLR/ICML/NIPS/CVPR/ICCV/ECCV that use dropout strategies, and does not appear to be a commonly accepted reason to reject papers from publication. Finally, it is worth to clarify that the “dropout strategy” is only used in our model (because it is only applied when the discriminator uses features as input) and therefore, the results of the other baselines are not affected by this hyper-parameter.\n'}"		rkxraoRcF7	Byx9HkJUXN	ICLR.cc/2019/Conference/-/Paper797/Official_Comment	['ICLR.cc/2019/Conference/Paper797/Reviewers/Unsubmitted']	18		['everyone']	Sygjczi-xE	['ICLR.cc/2019/Conference/Paper797/Authors']	1548246850435		1548246850435	['ICLR.cc/2019/Conference/Paper797/Authors', 'ICLR.cc/2019/Conference']
573	1548214525904	"{'title': 'All resources described in the paper are included in the dataset provided', 'awarded badges': ['Artifacts Available'], 'comment': 'I checked the dataset shared at GitHub and it contains the whole interaction history between user and retriever while processing the information needs. It also contains a code book to explain how ""Action"" field is generated from the raw data. I think the results in the CHIIR\'18 paper can be verified with the dataset and researchers can try to develop their own dataset with the strategy and codebook provided in the dataset and the paper. '}"		rJgGxq1_z4	HygUZWvrm4	ACM.org/SIGIR/Badging/-/Paper4/Review	[]	1		['everyone']	rJgGxq1_z4	['~Yiqun_LIU1']	1548214525904		1548214525904	['~Yiqun_LIU1']
574	1548214112605	{'comment': 'Thanks for your great work.\n\nI have one question about DeepMind Lab experiments in this paper.\nIn Appendix D, you mentioned that p_D(x_{t_2}) is Bernoulli distribution and the log-likelihood is calculated using the logits outputted by the network in MNIST experiments.\nIs it same in the DeepMind Lab experiments?\nI think Normal distribution with a fixed variance is often used as decoder distribution in such color image generation, so if you used a different setting for DeepMind Lab experiments, I hope the setting is clearly written in the paper.\n\nThank you.\n', 'title': 'About DeepMind Lab experiments'}		S1x4ghC9tQ	HJeFv1wBm4	ICLR.cc/2019/Conference/-/Paper1067/Public_Comment	[]	1		['everyone']	S1x4ghC9tQ	['(anonymous)']	1548214112605		1548214175129	['(anonymous)', 'ICLR.cc/2019/Conference']
575	1538087747495	{'title': 'Slalom: Fast, Verifiable and Private Execution of Neural Networks in Trusted Hardware', 'abstract': 'As Machine Learning (ML) gets applied to security-critical or sensitive domains, there is a growing need for integrity and privacy for outsourced ML computations. A pragmatic solution comes from Trusted Execution Environments (TEEs), which use hardware and software protections to isolate sensitive computations from the untrusted software stack. However, these isolation guarantees come at a price in performance, compared to untrusted alternatives. This paper initiates the study of high performance execution of Deep Neural Networks (DNNs) in TEEs by efficiently partitioning DNN computations between trusted and untrusted devices. Building upon an efficient outsourcing scheme for matrix multiplication, we propose Slalom, a framework that securely delegates execution of all linear layers in a DNN from a TEE (e.g., Intel SGX or Sanctum) to a faster, yet untrusted, co-located processor. We evaluate Slalom by running DNNs in an Intel SGX enclave, which selectively delegates work to an untrusted GPU. For canonical DNNs (VGG16, MobileNet and ResNet variants) we obtain 6x to 20x increases in throughput for verifiable inference, and 4x to 11x for verifiable and private inference.', 'keywords': ['Trusted hardware', 'integrity', 'privacy', 'secure inference', 'SGX'], 'authorids': ['tramer@cs.stanford.edu', 'dabo@cs.stanford.edu'], 'authors': ['Florian Tramer', 'Dan Boneh'], 'TL;DR': 'We accelerate secure DNN inference in trusted execution environments (by a factor 4x-20x) by selectively outsourcing the computation of linear layers to a faster yet untrusted co-processor.', 'pdf': '/pdf/5e103b447a94ab65439d808e98681ddee667107e.pdf', 'paperhash': 'tramer|slalom_fast_verifiable_and_private_execution_of_neural_networks_in_trusted_hardware', '_bibtex': '@inproceedings{\ntramer2018slalom,\ntitle={Slalom: Fast, Verifiable and Private Execution of Neural Networks in Trusted Hardware},\nauthor={Florian Tramer and Dan Boneh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJVorjCcKQ},\n}'}		rJVorjCcKQ	rJVorjCcKQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	119	HJxAxmiYt7	['everyone']		['ICLR.cc/2019/Conference']	1538087747495		1548197473319	['ICLR.cc/2019/Conference']
576	1548183250971	{'comment': 'This is a nice work on conversational QA. The authors compare with previous works on SCONE in Table 5. However, some of the previous models (Long et al., Guu et al., Suhr and Artzi) cited in Table 5 assumes a harder problem setting of learning with only the final word state, without access to intermediate answers (i.e., the gold answer_i for i = 1, 2, ..., N-1). It would be great if the authors could clarify this :)', 'title': 'Clarification on SCONE'}		ByftGnR9KX	S1gsA8yrmV	ICLR.cc/2019/Conference/-/Paper1287/Public_Comment	[]	2		['everyone']	ByftGnR9KX	['(anonymous)']	1548183250971		1548183250971	['(anonymous)', 'ICLR.cc/2019/Conference']
577	1548178673177	{'title': 'Submission Withdrawn by the Authors', 'withdrawal confirmation': 'I have read and agree with the withdrawal statement on behalf of myself and my co-authors.'}		SyeD-b9T6m	HyxKxrA474	AKBC.ws/2019/Conference/-/Paper28/Withdraw_Submission	[]	1		['everyone']	SyeD-b9T6m	['AKBC.ws/2019/Conference/Paper28/Authors']	1548178673177		1548178673177	[]
578	1548169208409	{'title': 'Thank you for your helpful feedback ', 'comment': 'We thank the reviewers for providing productive comments and critiques. We believe this input is very useful. We are working on improving the presentation of the paper as well as extending the non-extensive theory to several other applications. We will present our enhanced model in future opportunities.\n\n \n'}		SJMZRsC9Y7	Syxgbg34mV	ICLR.cc/2019/Conference/-/Paper869/Official_Comment	['ICLR.cc/2019/Conference/Paper869/Reviewers/Unsubmitted']	2		['everyone']	SJg6oHJK1N	['ICLR.cc/2019/Conference/Paper869/Authors']	1548169208409		1548171650300	['ICLR.cc/2019/Conference/Paper869/Authors', 'ICLR.cc/2019/Conference']
579	1548165122109	{'title': 'Response to comments', 'comment': 'We thank the reviewer for the encouraging review.\n\nWe have observed that the proposed models learn to relate the two entity types that are involved in a certain relationship/predicate. This is illustrated in Figure 6. However, it sometimes struggles to find an image of the entity that completes the query. This is related to your concern regarding the feasibility of the task. This highly depends on the entities involved in the query. Queries involving entities with canonical images (“Statue of Liberty”) are easier to answer than those involving entities that can be represented with heterogeneous images (“United States of America”). This is a nice suggestion, so we will include this discussion in the paper.\n\nOur queries solely include visual information. This means that we do not address queries where the underlying entity for a given image is known.\n'}		BylEpe9ppX	BJlqZej4m4	AKBC.ws/2019/Conference/-/Paper2/Official_Comment	['AKBC.ws/2019/Conference/Paper2/Reviewers/Unsubmitted']	3		['everyone']	H1x8rhmCWV	['AKBC.ws/2019/Conference/Paper2/Authors']	1548165122109		1548165122109	['AKBC.ws/2019/Conference/Paper2/Authors', 'AKBC.ws/2019/Conference']
580	1548163714372	"{'title': 'We thank the reviewers for their valuable comments and constructive suggestions.', 'comment': 'We thank the reviewers for their valuable comments and constructive suggestions. We acknowledge that our work partially requires more experimental evidence in terms of multilingual and in-domain comparison.\n\n“The authors fail to compare their LSTM-CRF approach to adequate baselines. In the results section, they focus on comparing only the LSTM-CRF architecture initialized without (baseline) vs. with pre-trained FastText embeddings.”\n“No comparison to existing botanical NER systems, such as those mentioned in the related work section.”\n- Regarding the comparison to in-domain baselines, we focused on the comparison to our own baseline system due to the presence of entities on a vernacular and scientific level. Using a different botanical NER system without vernacular names as a baseline did, in our eyes, not seem justified. However, on the level of scientific names, a comparison to other in-domain systems could be beneficial to test the relative performance of our neural models.\n\n\n“Why didn’t they run the same experiment for English, which is presumably less morphologically inflected?”\n“It is also unclear to me why they do not report corresponding results for a hand-labeled English test set.”\n“as an English reader I might appreciate more English examples”\n- Due to experiments and annotation work that we did not manage to conclude before the submission deadline, we will hereinafter integrate a manually corrected gold standard for English as well, and, additional English examples for multilingual comparison of the LSTM-CRF models. Moreover, we will run the equivalent experiments (i.e. character embedding dimension) for both languages to achieve better cross-lingual model comparability.\n\n“At the very least, I would like an evaluation of using the dictionary-based annotations alone (Section 3.3) as test-time predictions. If I understand correctly, this would obtain 100% accuracy on the ""silver-labeled"" data, but there is an open question of how well it generalizes to the gold labels.”\n- We agree that our explanations regarding the evaluation on the silver- and gold standard are insufficient and need to be expanded. We used the dictionary-based annotation system to obtain low-effort semi-automatic annotations of our data in order to avoid time-consuming manual annotations and guarantee high-quality annotations at the same time. Since the dictionary-based annotations hence already correspond to the silver standard, the performance evaluation of this annotation system is limited to the direct comparison of the manually corrected gold standard fold and the associated silver standard fold.'}"		rkeFyWcTTm	rygcFc5EQV	AKBC.ws/2019/Conference/-/Paper16/Official_Comment	['AKBC.ws/2019/Conference/Paper16/Reviewers/Unsubmitted']	1		['everyone']	rkeFyWcTTm	['AKBC.ws/2019/Conference/Paper16/Authors']	1548163714372		1548163714372	['AKBC.ws/2019/Conference/Paper16/Authors', 'AKBC.ws/2019/Conference']
581	1548162513192	{'title': 'Rebuttal Review#3', 'comment': 'Thanks for your helpful review.\n\nLet us address individually each of your concerns:\n\n1. Prediction errors: We agree that, for the data sets discussed in the paper, the performance is not up to industrial standard and thus this technique may not be useful “as-is” in the industry. We believe that this paper is the first step which will hopefully spur more interest in this problem, and would eventually lead to industry-ready algorithms. However, this (current) limitation is not specific to this new problem, but common to many research problems (e.g. link prediction methods, for the moment, are far from meeting industrial requirements).\n\n2. Embeddings: This is again an excellent point. Please also see rebuttal for Reviewer #2. Subsets of latent factors capture signals related to different relational and numerical information. A metric learning approach that “selects” a group of latent factors relevant to a certain numerical attribute would make sense. However, during our experiments, we learned Mahalanobis metrics by incorporating an additional loss during the learning of knowledge graph embeddings, and it did not improve the overall performance. We will discuss this in the paper. Nevertheless, we suggest that future work should address this research direction in greater depth.'}		BJlh0x9ppQ	BJlFCHqEQ4	AKBC.ws/2019/Conference/-/Paper12/Official_Comment	['AKBC.ws/2019/Conference/Paper12/Reviewers/Unsubmitted']	3		['everyone']	B1xo8C8iZV	['AKBC.ws/2019/Conference/Paper12/Authors']	1548162513192		1548162513192	['AKBC.ws/2019/Conference/Paper12/Authors', 'AKBC.ws/2019/Conference']
582	1548162440622	{'title': 'Rebuttal Review#2', 'comment': 'Thanks a lot for your feedback. \n\nLet us address individually each of your concerns:\n\n1. Previous work: Thank you for bringing the work by Davidov and Rappoport (ACL 2010)  and Gupta et al. (EMNLP 2015) to our attention, we were not aware of this work. We will elaborate on these papers in the related work section. Regarding Gupta et al. (EMNLP 2015), the paper uses word2vec embeddings of named entities as inputs to a number of regression models. Similar to us, they aim to predict numerical attributes of knowledge base entities. Different to us, they leverage text information to do so. This is important, as in our problem we do not assume the existence of information other than the own graph structure. This relates to knowledge bases where entities’ names are unknown/anonymized  (e.g. medical knowledge bases). Nevertheless, this is very interesting and nicely complements our approach of learning from KG relationships as opposed to free text. Similarly, Davidov and Rappoport leverages text and/or class label information.\n\n2. Linear Regression: As you correctly observed, different output variables have different distributions. Ideally, for attributes that do not permit a normal distribution assumption one would need to validate (i) the power transformation to be applied to the data, and (ii) the regression function. This comes at an exponential cost, as this validation should be jointly done for a number of regression models. To circumvent this issue we used a common normalization procedure for all attributes. We also trained with L1-loss (Robust regression), which is more robust (w.r.t. outliers) than OLS, for all attributes but the results were similar.\n\n3. Label Propagation: Our assumption is that if two entities relate to the rest of the world in a similar manner, then they have similar numerical attributes. Nevertheless, your observation is an excellent point.\n\nLabel Propagation uses Euclidean distance between vectors to build the k-nearest neighbor graph. We speculate that subsets of entities’ latent factors could be encoding different relational and numerical information. For instance, it is possible that a few dimensions of the entity embeddings encode location information, while others encode population information and so on. For this reason, we learned Mahalanobis metrics for capturing different entity similarities. We did this while learning knowledge base embeddings by using an additional nearest neighbor loss. It did slightly improve the performance for few attributes, but overall it did not improve the results. We will include this discussion in the paper. Nevertheless, we suggest that future work should address this research direction in greater depth.\n'}		BJlh0x9ppQ	HJlb5B9EmV	AKBC.ws/2019/Conference/-/Paper12/Official_Comment	['AKBC.ws/2019/Conference/Paper12/Reviewers/Unsubmitted']	2		['everyone']	BJeOWyoxGN	['AKBC.ws/2019/Conference/Paper12/Authors']	1548162440622		1548162440622	['AKBC.ws/2019/Conference/Paper12/Authors', 'AKBC.ws/2019/Conference']
583	1548162276198	{'title': 'Rebuttal Review#1', 'comment': 'Thanks for the positive and encouraging comments.\n\nGranularity:  You have raised a very valid concern regarding the granularity of different numerical attributes. We agree on that the normalization step is really important to this problem, as one has to jointly train a number of regression models (with shared input vector representations) where output variables have different scales. We also tried min-max scaling, however this gave worse performance in comparison to standard scaling. We will add this comment to the paper.'}		BJlh0x9ppQ	Sylnkr9NX4	AKBC.ws/2019/Conference/-/Paper12/Official_Comment	['AKBC.ws/2019/Conference/Paper12/Reviewers/Unsubmitted']	1		['everyone']	rJxMHDcfGN	['AKBC.ws/2019/Conference/Paper12/Authors']	1548162276198		1548162276198	['AKBC.ws/2019/Conference/Paper12/Authors', 'AKBC.ws/2019/Conference']
584	1540835786923	{'title': 'Towards Reproducible and Reusable Deep Learning Systems Research Artifacts', 'abstract': '  This paper discusses results and insights from the 1st ReQuEST workshop, a collective effort to promote reusability, portability and reproducibility of deep learning research artifacts within the Architecture/PL/Systems communities.\n  ReQuEST (Reproducible Quality-Efficient Systems Tournament) exploits the open-source Collective Knowledge framework (CK) to unify benchmarking, optimization, and co-design of deep learning systems implementations and exchange results via a live multi-objective scoreboard.\n  Systems evaluated under ReQuEST are diverse and include an FPGA-based accelerator, optimized deep learning libraries for x86 and ARM systems, and distributed inference in Amazon Cloud and over a cluster of Raspberry Pis.\n  We finally discuss limitations to our approach, and how we plan improve upon those limitations for the upcoming SysML artifact evaluation effort.', 'paperhash': 'moreau|towards_reproducible_and_reusable_deep_learning_systems_research_artifacts', 'TL;DR': 'We describe insights from introducing reproducible and reusable artifact evaluation to the deep learning systems community.', 'authorids': ['moreau@uw.edu', 'anton@dividiti.com', 'grigori.fursin@ctuning.org'], 'authors': ['Thierry Moreau', 'Anton Lokhmotov', 'Grigori Fursin'], 'keywords': ['artifact evaluation', 'deep learning', 'systems', 'workflows', 'reproducibility', 'open-source'], 'pdf': '/pdf/41db83dd63eb9c3c8d79bafd1cfe7848d999e395.pdf', 'decision': 'accept'}		rJxXAtpV2X	rJxXAtpV2X	NIPS.cc/2018/Workshop/MLOSS/-/Paper20/Decision	[]	1	Ske-2Gyk9X	['everyone']		['NIPS.cc/2018/Workshop/MLOSS']	1540835786923		1548159833400	['NIPS.cc/2018/Workshop/MLOSS']
585	1538351784910	{'title': 'Towards Reproducible and Reusable Deep Learning Systems Research Artifacts', 'abstract': '  This paper discusses results and insights from the 1st ReQuEST workshop, a collective effort to promote reusability, portability and reproducibility of deep learning research artifacts within the Architecture/PL/Systems communities.\n  ReQuEST (Reproducible Quality-Efficient Systems Tournament) exploits the open-source Collective Knowledge framework (CK) to unify benchmarking, optimization, and co-design of deep learning systems implementations and exchange results via a live multi-objective scoreboard.\n  Systems evaluated under ReQuEST are diverse and include an FPGA-based accelerator, optimized deep learning libraries for x86 and ARM systems, and distributed inference in Amazon Cloud and over a cluster of Raspberry Pis.\n  We finally discuss limitations to our approach, and how we plan improve upon those limitations for the upcoming SysML artifact evaluation effort.', 'paperhash': 'moreau|towards_reproducible_and_reusable_deep_learning_systems_research_artifacts', 'TL;DR': 'We describe insights from introducing reproducible and reusable artifact evaluation to the deep learning systems community.', 'authorids': ['moreau@uw.edu', 'anton@dividiti.com', 'grigori.fursin@ctuning.org'], 'authors': ['Thierry Moreau', 'Anton Lokhmotov', 'Grigori Fursin'], 'keywords': ['artifact evaluation', 'deep learning', 'systems', 'workflows', 'reproducibility', 'open-source'], 'pdf': '/pdf/41db83dd63eb9c3c8d79bafd1cfe7848d999e395.pdf'}		Ske-2Gyk9X	Ske-2Gyk9X	NIPS.cc/2018/Workshop/MLOSS/-/Submission	[]	20		['everyone']		['~Thierry_J_Moreau1']	1538351784910		1548159833400	['~Thierry_J_Moreau1']
586	1548158219837	"{'comment': ""If it is receiving a reward of -1 for all other states, isn't this no longer a sparse reward problem? It is getting feedback for every move taken, which biases it against getting into cycles, and towards shorter solution lengths (in episodic problems).\n\nAlso, how long was Kociemba's solver given to run? Doesn't that solver eventually converge on the optimal solution, or was the first outputted solution what was compared to? This solver is widely used by the speedcubing community and it generally finds solutions in the low 20s or better within a couple of seconds. The algorithm itself states that the worse-case scenario of Kociemba is 30 moves, yet the graph seems to show solution lengths longer than this.\n\nThere is also relatively recent work in bi-directional search which has been capable of optimally solving the hardest positions (as opposed to just proving the solution length is <= 20 moves) reasonably quickly with far fewer node expansions- this might be useful to compare to."", 'title': 'Not Really Sparse Rewards?'}"		Hyfn2jCcKm	S1lEfrKE7E	ICLR.cc/2019/Conference/-/Paper750/Public_Comment	[]	1		['everyone']	Skg0UYN5Cm	['(anonymous)']	1548158219837		1548158600125	['(anonymous)', 'ICLR.cc/2019/Conference']
587	1548155491991	"{'comment': ""blow ? \n\nAnyways. Whilst non proceeding publications are allowed, that  does not override the anonymity rule.  Whilst the authors can be googled for, that does not mean that the author should post two links containing their name (just clicking on it leads to the name, and reading it also leads to the name ... ).\n\nSubmission instructions clearly state:\n\n> Submissions and reviews are both anonymous.\n\nHaving the soft constraint on non-proceeding publications does not override this instruction. Even if you have an arXiv entry you should not be linking it in the papers page, or any other information that makes the submission not be anonymous. \n\nGoing away and searching for the paper on arXiv is not the same as clicking a link that directly reveals the authors name. The attempt to follow the conferences rules seems deliberately poorly executed by the author. \n\nHas ICLR reached the point where its rules can be completely disregarded, without comment ? This issue should be discussed and addressed by the PC's , if its ok to break such rules then the should be removed or adapted appropiately."", 'title': 'RE: this issued has been replied below'}"		rJlWOj0qF7	BJehvcdEQV	ICLR.cc/2019/Conference/-/Paper331/Public_Comment	[]	7		['everyone']	rJxuuPBEm4	['(anonymous)']	1548155491991		1548155491991	['(anonymous)', 'ICLR.cc/2019/Conference']
588	1548155437114	{'title': 'Response to comments', 'comment': 'Thank you for the helpful review.\n\nThe novelty of our submission is in the visual KB that we have created, the novel query types we propose, and the combination of standard CNNs with KB embedding scoring functions. We do not claim that the scoring functions are novel. What is novel is the combination of CNNs on image data with these scoring functions for the novel query types we propose. Our work shows that some existing scoring functions are not better than a concatenation, at least for the image retrieval task. We have explored the zero-shot learning setting in the link prediction problem for the first time in the literature. We think that this also makes the work novel.\n\nExperiments show that the difficulty and performance are different from that of the traditional link prediction problem. We have (adapted and) benchmarked some of the most standard scoring functions from the link prediction literature in this problem and show that: i) they work, to some extent, in the relation prediction task, and ii) they perform poorly in the image retrieval task, where the (naive) concatenation approach performs the best. We remark again the complexity of the latter problem (see rebuttal for reviewer 1). We think that this set of experiments constitutes a solid answer to the question regarding the ability of the traditional scoring function to answer these new types of visual queries.\n\nPlease also note the setup descriptions in Section 4.1.  In the image retrieval and link prediction scenarios, (experiments  (1) and (2)) we have images for which we do not know the underlying KG entities. This is the reason why we have not considered approaches that leverage entity embeddings, as this information is not available in our setting.\n\nThank you for your suggestion regarding the notation used to define certain operators. We completely agree. However, we think the notation (head, relation, tail) is not that uncommon, as it has been used in previous works (e.g. (Bordes et al. 2013), (Garcia-Duran&Niepert, 2017)).\n\nFinally, the probabilistic baseline is explained in Section 5.2 at the end of the second paragraph: “The second baseline (probabilistic baseline) computes the\nprobability of each relation type using the set of training and validation triples. The baseline\nranks relation types based on these prior probabilities.”\n'}		BylEpe9ppX	S1lSE5O4Q4	AKBC.ws/2019/Conference/-/Paper2/Official_Comment	['AKBC.ws/2019/Conference/Paper2/Reviewers/Unsubmitted']	2		['everyone']	Byenu7AhbN	['AKBC.ws/2019/Conference/Paper2/Authors']	1548155437114		1548155437114	['AKBC.ws/2019/Conference/Paper2/Authors', 'AKBC.ws/2019/Conference']
589	1548154674303	{'title': 'Response to comments ', 'comment': 'Thank you for your helpful review.\n\nFB15k contain entities like “ISO_3166-1:SO”, or “C16:1(n-7)”, for which the crawler returned few images, or even no image at all. After removing those entities from the graph, some relations were not present in the data set.\n\nWe could have replaced our feature extractor VGG-16 with more sophisticated/up-to-date CNNs such as ResNet or DenseNet. These models obtain moderate gains in accuracy on ImageNet, but they come at the cost of (largely) slower running times. In our opinion, the low performance relates to the difficulty of the problem. Our data set has 15 times more of categories/entities than ImageNet, hence the low performance in the image retrieval task. However, for the relation prediction task, where the output space is comparable to that of ImageNet, performances are much more competitive.\n\nWe agree that the experiments provide insights into the bias of the data set. Most importantly, the experiments show that the introduced problem is much harder than the traditional link prediction problem. We wanted to evaluate the scoring functions of state-of-the-art link prediction methods on the problems. At least for the visual entity prediction problem, these scoring functions did not work better than a concatenation. We hope this serves as a first step to this new problem.'}		BylEpe9ppX	SygcEDu474	AKBC.ws/2019/Conference/-/Paper2/Official_Comment	['AKBC.ws/2019/Conference/Paper2/Reviewers/Unsubmitted']	1		['everyone']	rJxQ2N6mfE	['AKBC.ws/2019/Conference/Paper2/Authors']	1548154674303		1548154674303	['AKBC.ws/2019/Conference/Paper2/Authors', 'AKBC.ws/2019/Conference']
590	1538087890843	{'title': 'A comprehensive, application-oriented study of catastrophic forgetting in DNNs', 'abstract': 'We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning.\nA new experimental protocol is proposed that takes into account typical constraints encountered in application scenarios.\nAs the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF.\nOur results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.', 'keywords': ['incremental learning', 'deep neural networks', 'catatrophic forgetting', 'sequential learning'], 'authorids': ['benedikt.pfuelb@cs.hs-fulda.de', 'alexander.gepperth@cs.hs-fulda.de'], 'authors': ['B. Pfülb', 'A. Gepperth'], 'TL;DR': 'We check DNN models for catastrophic forgetting using a new evaluation scheme that reflects typical application conditions, with surprising results.', 'pdf': '/pdf/65af1b654096d947a212ab642b657ea444e43fb7.pdf', 'paperhash': 'pfülb|a_comprehensive_applicationoriented_study_of_catastrophic_forgetting_in_dnns', '_bibtex': '@inproceedings{\npfülb2018a,\ntitle={A comprehensive, application-oriented study of catastrophic forgetting in {DNN}s},\nauthor={B. Pfülb and A. Gepperth},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkloRs0qK7},\n}'}		BkloRs0qK7	BkloRs0qK7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	925	BJeNwd_qK7	['everyone']		['ICLR.cc/2019/Conference']	1538087890843		1548151415845	['ICLR.cc/2019/Conference']
591	1538087877800	{'title': 'Phase-Aware Speech Enhancement with Deep Complex U-Net', 'abstract': 'Most deep learning-based models for speech enhancement have mainly focused on estimating the magnitude of spectrogram while reusing the phase from noisy speech for reconstruction. This is due to the difficulty of estimating the phase of clean speech. To improve speech enhancement performance, we tackle the phase estimation problem in three ways. First, we propose Deep Complex U-Net, an advanced U-Net structured model incorporating well-defined complex-valued building blocks to deal with complex-valued spectrograms. Second, we propose a polar coordinate-wise complex-valued masking method to reflect the distribution of complex ideal ratio masks. Third, we define a novel loss function, weighted source-to-distortion ratio (wSDR) loss, which is designed to directly correlate with a quantitative evaluation measure. Our model was evaluated on a mixture of the Voice Bank corpus and DEMAND database, which has been widely used by many deep learning models for speech enhancement. Ablation experiments were conducted on the mixed dataset showing that all three proposed approaches are empirically valid. Experimental results show that the proposed method achieves state-of-the-art performance in all metrics, outperforming previous approaches by a large margin.', 'keywords': ['speech enhancement', 'deep learning', 'complex neural networks', 'phase estimation'], 'authorids': ['kekepa15@snu.ac.kr', 'blue378@snu.ac.kr', 'jaesung.huh@navercorp.com', 'adrian.kim@navercorp.com', 'jungwoo.ha@navercorp.com', 'kglee@snu.ac.kr'], 'authors': ['Hyeong-Seok Choi', 'Jang-Hyun Kim', 'Jaesung Huh', 'Adrian Kim', 'Jung-Woo Ha', 'Kyogu Lee'], 'TL;DR': 'This paper proposes a novel complex masking method for speech enhancement along with a loss function for efficient phase estimation.', 'pdf': '/pdf/8e24d4fd5b556acc2804680a9a7f08b7864db875.pdf', 'paperhash': 'choi|phaseaware_speech_enhancement_with_deep_complex_unet', '_bibtex': '@inproceedings{\nchoi2018phaseaware,\ntitle={Phase-Aware Speech Enhancement with Deep Complex U-Net},\nauthor={Hyeong-Seok Choi and Janghyun Kim and Jaesung Huh and Adrian Kim and Jung-Woo Ha and Kyogu Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeRTsAcYm},\n}'}		SkeRTsAcYm	SkeRTsAcYm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	850	ryg5G0OqYm	['everyone']		['ICLR.cc/2019/Conference']	1538087877800		1548151083964	['ICLR.cc/2019/Conference']
592	1548149494713	"{'comment': ""Any model can have high precision with low recall.\n\nThen what criteria to categorize models?\n\nA scientist with integrity should refer to the base theory (Venn's diagram, set theories) *and*  constructively compare to previous work instead of trying to avoid."", 'title': '100% precision and low recall?'}"		rJlWOj0qF7	SyxJ-mP4X4	ICLR.cc/2019/Conference/-/Paper331/Public_Comment	[]	6		['everyone']	S1lpBN786Q	['(anonymous)']	1548149494713		1548149536589	['(anonymous)', 'ICLR.cc/2019/Conference']
593	1548142814372	{'title': 'it is also an issue raised some weeks ago', 'comment': 'the point is that all reviewers neglect the main point of this paper : a novel geometric method to perfectly impose structures onto vector space -- using higher dimensional regions than vectors. '}		rJlWOj0qF7	BygIkYrVm4	ICLR.cc/2019/Conference/-/Paper331/Official_Comment	['ICLR.cc/2019/Conference/Paper331/Reviewers/Unsubmitted']	18		['everyone']	r1eKFZSEX4	['ICLR.cc/2019/Conference/Paper331/Authors']	1548142814372		1548142814372	['ICLR.cc/2019/Conference/Paper331/Authors', 'ICLR.cc/2019/Conference']
594	1548142448335	"{'title': 'this issued has been replied blow', 'comment': 'here is the earlier reply:\n(1) this year\'s submission policy accepts papers from arXiv. ""papers that have appeared on non-peered reviewed websites (like arXiv) or that have been presented at workshops (i.e., venues that do not have a publication proceedings) do not violate the policy"". So, you can detect authors of submissions that exist in arXiv.  \n\n(2) the detective work of your kind may round up names with less or no claims to authorship, because they may be master students from our institute or partner universities who help data collection and implementation. \n\n------\naccording to the policy, it is possible for reviewers to identify authors. in this case, you raised the reverse technique, after the real names are published, instead of the reviewing process. That is, the anonymization technique actually works. '}"		rJlWOj0qF7	rJxuuPBEm4	ICLR.cc/2019/Conference/-/Paper331/Official_Comment	['ICLR.cc/2019/Conference/Paper331/Reviewers/Unsubmitted']	17		['everyone']	rJg1Mw07QN	['ICLR.cc/2019/Conference/Paper331/Authors']	1548142448335		1548142448335	['ICLR.cc/2019/Conference/Paper331/Authors', 'ICLR.cc/2019/Conference']
595	1548140929258	"{'comment': 'Especially when at least one of the anonymous reviewers is an expert in this subfield and gave strong and compelling arguments for rejection.', 'title': ""Isn't it a joke for AC to accept the paper given all the reviewers vote for rejection?""}"		rJlWOj0qF7	r1eKFZSEX4	ICLR.cc/2019/Conference/-/Paper331/Public_Comment	[]	5		['everyone']	rJlWOj0qF7	['(anonymous)']	1548140929258		1548140929258	['(anonymous)', 'ICLR.cc/2019/Conference']
596	1548135304417	{'title': 'Submission Withdrawn by the Authors', 'withdrawal confirmation': 'I have read and agree with the withdrawal statement on behalf of myself and my co-authors.'}		BJlsVZ966m	BJgx5oQ4QE	AKBC.ws/2019/Conference/-/Paper47/Withdraw_Submission	[]	1		['everyone']	BJlsVZ966m	['AKBC.ws/2019/Conference/Paper47/Authors']	1548135304417		1548135304417	[]
597	1538087927658	{'title': 'Stable Recurrent Models', 'abstract': 'Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n', 'keywords': ['stability', 'gradient descent', 'non-convex optimization', 'recurrent neural networks'], 'authorids': ['miller_john@berkeley.edu', 'hardt@berkeley.edu'], 'authors': ['John Miller', 'Moritz Hardt'], 'TL;DR': 'Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.', 'pdf': '/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf', 'paperhash': 'miller|stable_recurrent_models', '_bibtex': '@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}'}		Hygxb2CqKm	Hygxb2CqKm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1136	H1xtrh-cYm	['everyone']		['ICLR.cc/2019/Conference']	1538087927658		1548123791197	['ICLR.cc/2019/Conference']
598	1548116098634	"{'comment': ""So let me get this right. It is perfectly normal to not include people who are directly involved with curating/creating the data and carrying out part of the implementation ? which themselves are two very core tasks in machine learning.\n\nI think in most modern institutions it is perfectly normal to expect the names found in the official GitHub repository to be names of authors directly involved in the paper.  \n\nIt seems unlikely to find all commits in a work pointing to a single name (which is not of a masters student but of a senior research fellow).\n\nSuch detective work should be a valid point to raise, and it was not that thorough specially since the anonymous account is the name of the first author in reverse. You definitely don't need to be Sherlock Holme's to spot that. "", 'title': ""Master students' deserve no crediting ?! ""}"		rJlWOj0qF7	SyejYekVQN	ICLR.cc/2019/Conference/-/Paper331/Public_Comment	[]	4		['everyone']	BkgXhXRezE	['(anonymous)']	1548116098634		1548116312577	['(anonymous)', 'ICLR.cc/2019/Conference']
599	1548113671493	{'comment': 'Apart from the multitude of issues that the reviewers have raised with this work, it seems that the authors indeed broke the double-blind review process. The link https://github.com/gnodisnait/bp94nball.git in the paper abstract clearly contains the username gnodisnait, which is the name of the main author Tiansi Dong spelled in reverse. I urge the area chair to reconsider their acceptance decision based on this very obvious and intentional act of breaking ICLR rules.', 'title': 'Name reversal is not a valid anonymization technique'}		rJlWOj0qF7	rJg1Mw07QN	ICLR.cc/2019/Conference/-/Paper331/Public_Comment	[]	3		['everyone']	rJlWOj0qF7	['(anonymous)']	1548113671493		1548113863656	['(anonymous)', 'ICLR.cc/2019/Conference']
600	1538087904831	"{'title': 'Variational Autoencoder with Arbitrary Conditioning', 'abstract': 'We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in ""one shot"". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples.', 'keywords': ['unsupervised learning', 'generative models', 'conditional variational autoencoder', 'variational autoencoder', 'missing features multiple imputation', 'inpainting'], 'authorids': ['tigvarts@gmail.com', 'michael@figurnov.ru', 'vetrovd@yandex.ru'], 'authors': ['Oleg Ivanov', 'Michael Figurnov', 'Dmitry Vetrov'], 'TL;DR': 'We propose an extension of conditional variational autoencoder that allows conditioning on an arbitrary subset of the features and sampling the remaining ones.', 'pdf': '/pdf/1e3782d9d1eeddb13c604dbf64d39255b671955f.pdf', 'paperhash': 'ivanov|variational_autoencoder_with_arbitrary_conditioning', '_bibtex': '@inproceedings{\nivanov2018variational,\ntitle={Variational Autoencoder with Arbitrary Conditioning},\nauthor={Oleg Ivanov and Michael Figurnov and Dmitry Vetrov},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxtJh0qYm},\n}'}"		SyxtJh0qYm	SyxtJh0qYm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1003	rylrQ2T5KQ	['everyone']		['ICLR.cc/2019/Conference']	1538087904831		1548092496964	['ICLR.cc/2019/Conference']
601	1538087828909	"{'title': 'Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks', 'abstract': 'In general, natural language is governed by a tree structure: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). This is a strict hierarchy: when a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the standard LSTM allows different neurons to track information at different time scales, the architecture does not impose a strict hierarchy. This paper proposes to add such a constraint to the system by ordering the neurons; a vector of ""master"" input and forget gates ensure that when a given unit is updated, all of the units that follow it in the ordering are also updated. To this end, we propose a new RNN unit: ON-LSTM, which achieves good performance on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference.', 'keywords': ['Deep Learning', 'Natural Language Processing', 'Recurrent Neural Networks', 'Language Modeling'], 'authorids': ['yikang.shn@gmail.com', 'shawn@wtf.sg', 'alsordon@microsoft.com', 'aaron.courville@gmail.com'], 'authors': ['Yikang Shen', 'Shawn Tan', 'Alessandro Sordoni', 'Aaron Courville'], 'TL;DR': 'We introduce a new inductive bias that integrates tree structures in recurrent neural networks.', 'pdf': '/pdf/f59f99ed9d4324d9c03a35fcf803319dbcec9d9b.pdf', 'paperhash': 'shen|ordered_neurons_integrating_tree_structures_into_recurrent_neural_networks', '_bibtex': '@inproceedings{\nshen2018ordered,\ntitle={Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks},\nauthor={Yikang Shen and Shawn Tan and Alessandro Sordoni and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l6qiR5F7},\n}'}"		B1l6qiR5F7	B1l6qiR5F7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	574	HklP3Bi9Y7	['everyone']		['ICLR.cc/2019/Conference']	1538087828909		1548086880273	['ICLR.cc/2019/Conference']
602	1548057527315	"{'comment': ""As already pointed out in one of the author's responses (https://openreview.net/forum?id=rkgpCoRctm&noteId=ryx10nht07), Lee at al. (NIPS 2018) uses OOD dataset to train the logistic regressor and also to finetune other hyperparameters in preprocessing. In that case, how is their setting for experiments is any different from the current paper? Or am I totally missing something?"", 'title': 'How is Lee at al. (NIPS 2018) settings for experiments the right one?'}"		rkgpCoRctm	BJeyTsxQ7E	ICLR.cc/2019/Conference/-/Paper937/Public_Comment	[]	9		['everyone']	Bklaqej0JV	['(anonymous)']	1548057527315		1548057553662	['(anonymous)', 'ICLR.cc/2019/Conference']
603	1538087734907	{'title': 'Large Scale Graph Learning From Smooth Signals', 'abstract': 'Graphs are a prevalent tool in data science, as they model the inherent structure of the data. Typically they are constructed either by connecting nearest samples, or by learning them from data, solving an optimization problem. While graph learning does achieve a better quality, it also comes with a higher computational cost. In particular, the current state-of-the-art model cost is O(n^2) for n samples.\nIn this paper, we show how to scale it, obtaining an approximation with leading cost of O(n log(n)), with quality that approaches the exact graph learning model. Our algorithm uses known approximate nearest neighbor techniques to reduce the number of variables, and automatically selects the correct parameters of the model, requiring a single intuitive input: the desired edge density.', 'paperhash': 'kalofolias|large_scale_graph_learning_from_smooth_signals', 'authorids': ['v.kalofolias@gmail.com', 'nathanael.perraudin@sdsc.ethz.ch'], 'authors': ['Vassilis Kalofolias', 'Nathanaël Perraudin'], 'keywords': ['Graph learning', 'Graph signal processing', 'Network inference'], 'pdf': '/pdf/def56f715a464b2fbec9bec2780092eaf31a8697.pdf', '_bibtex': '@inproceedings{\nkalofolias2018large,\ntitle={Large Scale Graph Learning From Smooth Signals},\nauthor={Vassilis Kalofolias and Nathanaël Perraudin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGkSo0qYm},\n}'}		ryGkSo0qYm	ryGkSo0qYm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	51	rJl6V6TUKQ	['everyone']		['ICLR.cc/2019/Conference']	1538087734907		1548054947595	['ICLR.cc/2019/Conference']
604	1548036161240	{'title': 'We thank the reviewer for their detailed comments and suggestions. It has helped us revise our submission appropriately. We have responded to the comments below.', 'comment': '1. For instance, the basic definitions, e.g., the definitions of “mentions”, “events”, “attributes” and “relations”, are not clearly given. Due to brevity of space, we skipped over the definitions. Thank you for the suggestion, we are working on adding a supplementary section in the next revision, where in we will be adding the definitions and an end-to-end example.\n\n2. This work lacks the references to some works that also centers on constructing KG from a probabilistic perspective. (1) \tJay Pujara, Probabilistic Models for Scalable Knowledge Graph Construction. University of Maryland, College Park, MD, USA 2016. (2) Knowledge Graph Identification. Jay Pujara, Hui Miao, Lise Getoor, William Cohen. 2013 International Semantic Web Conference (ISWC). What is the difference between this work and Pujara’s works?\nThank for you providing us with this information. We have updated the Related Work and compared our work with the above. \n\n3. The experiment is comparatively weak since there is no baseline or competitors. Plus, the scale of the dataset is relatively small as far as a normal KG is concerned. In consequence, it is hard to judge whether this method can achieve superior results on this problem.\nWe acknowledge this issue, however this dataset was first of its kind in having conflicting events with multiple viewpoints. Additionally, the dataset offered annotations across multiple languages. We are in the process of acquiring more data from LDC. Parallely, in our continued work we plan to apply our approach for the datasets: NELL and synthetic knowledge base from the LinkedBrainz project, and at least check the performance on entity and relation resolution if not events.\n\n1. What is the exact difference between “relation” and “event” in this work? It seems that much more emphasis is laid on “events”, while “relations” are barely mentioned.\nWe focused on more on events since they posed a harder challenge, often having multiple arguments. Relations were handled similar to events, as they can be considered events with two arguments always. Additionally, there were many conflicting events as compared to relations, hence due to brevity of space we focused more on events. \n\n2.Why is this approach effective and why should “patient argument”, instead of “instrument argument” be required?\nSuch an approach is effective because an event knowledge element (KE), is defined uniquely majorly by its event-type, patient and time. So, “attack on Mh17” is a different event KE than “attack on Kramatorsk”. Additionally,  we can’t use “instrument argument” as the restrictive constraint as same instruments like “missile”, “ak47”, could have been used in different attack events, which would cause noise in our candidate set. \n\n3. In the subsection “Event and Relation Grounding”, it is said that “In our domain, relations usually encode affiliations and location hierarchy, ...”. What about broader scenarios?\nThe parent-child analogy is a misnomer which we have corrected in our manuscript. For broader scenarios, since we treat  relations similar to events, a  factor graph  can be constructed over the event KE and a possible relevant relation KE.  Event sequencing on wikipedia or domain-specific corpora can be used to get the co-occurring relations and events, to handle broader scenarios.\n\n4. In the “Factors” subsection of Section 2.3, the given example seems not to be matched with Fig.4. In addition, what does the right part of Fig.4 try to express?\nThank for this helpful suggestion, we have fixed the figure. Since the observation has uniform probability distribution over either being a Spanish truck or a French truck, therefore the candidate set of {truck1, truck2} have entity KEs as a Spanish Truck and French truck. The right part of the figure shows the properties of the candidate set, which helped in selecting the existing KEs Truck1 and Truck2 for grounding.\n\n 5. Some use of terms are too casual.\nThank you for pointing that out, we have corrected the manuscript.\n\n6. Are there any measures to control the error propagations?\nOur current work, in continuation of this work, addresses this issue. We use an EM based approach, where we first keep the entity grounding constant and update the event/relation grounding. Then after some n number of documents, we fix the event/relation grounding and use that information to update the entity grounding. A joint learning of entity and event and relation grounding should be able to handle it. For instance, we ground a “missile” mention to ‘Russian missile’ Knowledge Element node. Keeping that fixed, we perform event/relation grounding and we observe that the probability of “Ukranian missile” is higher in all the events and relations where ‘Russian missile’ is also one interpretation. This could suggest that the grounding of the earlier ‘missile’ mention was incorrect and instead it should get grounded to ‘Ukrainian missile’ with higher probability. \n\n'}		SJeHzZqapm	SygFHOizXV	AKBC.ws/2019/Conference/-/Paper34/Official_Comment	['AKBC.ws/2019/Conference/Paper34/Reviewers/Unsubmitted']	2		['everyone']	rye0dAOPZN	['AKBC.ws/2019/Conference/Paper34/Authors']	1548036161240		1548036652085	['AKBC.ws/2019/Conference/Paper34/Authors', 'AKBC.ws/2019/Conference']
605	1548036639476	"{'title': 'We thank the reviewer for their detailed comments and suggestions. It has helped us revise our submission appropriately and think on future directions. We have responded to the comments below.', 'comment': 'If this understanding is correct, then I find Figure 4 misleading. \n-We have updated the figure to reflect our ideas more clearly.  The understanding you described in the prior comment is what we did indeed. \n\nIf variables represent distributions, how many variables are there actually for each observation? -The variables are bounded by the number of properties we consider for entity/event resolution. For instance, an event can have maximum variables as #(num of semantic roles) + 1 (target variable).\n\nActually, it is inference, unless with grounding they mean actually ""rolling"" out the factor graph. The terminology is quite unclear and misleading I find here. \n-By grounding we refer to the “rolling” out of the factor graph. Inference refers to the computation of marginal probabilities which we achieve through belief propagation. \n\nI understand that inference is performed using belief propagation. First of all, the authors mention that they graphs are loop-free. However, I do not see really why this would be the case. This would served some further explanation. Further, I do not see how exact inference can be tractable as there is an infinite number of possible distributions. There is some discretization involved, but this is not mentioned. \n-The reviewer is right. Exact inference is generally intractable. This is why the LEAPFROG engine uses  sum-product (belief propagation). In our application, we use a number of heuristics described in Section 2 to form a limited factor graph for belief propagation. Somewhat surprisingly, most of the resulting graphs didn’t have loops and sum-product worked fine. But we are not counting on that. We also use coarse discretization of numeric variables. Our representation of beliefs and belief propagation are definitely approximate. Unfortunately, space limitations forced us to leave out many details.\n\n\nThe evaluation shows that the approach delivers good results, but does not compare the results to other state-of-the-art approaches.\nThe main purpose of this work was to demonstrate an approach for conflating information by modeling different kinds of certainties across languages, especially for events. The dataset used in our work had the required annotations for uncertainties at even role level, which we didn’t find in any other dataset. Most datasets in KG construction focus on entity and relation resolution. But we do plan to test our approach at the entity and relation level, using factual knowledge from NELL and a synthetic KG  from the LinkedBrainz project. \n\nI was puzzled by the fact that the authors only provide Precision and Recall figures, but no F-Measures. Why is this the case? \nThere was no particular reason, usually in only F-measure the effect of an approach on Precision and recall is often not mentioned, hence we mentioned each of the metric separately. Thank you for the suggestion, we have updated the Table 2 and Table 4.\n\nThe results simulating the biases in Table 4 are interesting, but I wondered why the results do not come even close to the results in Table 2. \nThere were three primary reasons why Table 4 is far from Table2:\nThe bulk of the metrics rely on the efficiency of belief propagation to perform co-reference across multiple documents and languages.  Since, we used simple string-based heuristics for entity-co-references, many of the co-referent links which were paraphrases couldn’t be handled. We plan to include low-dimensional vectors to represent the entity nodes for better entity-coreference. Within a document, there weren’t many name variations hence these heuristics were able to achieve good entity co-coreference. To also avoid error propagation, in our ongoing we work we plan to address using a joint EM like learning approach where we update the entity co-references keeping event grounding constant and vice-versa.\n\nSecondly, many of the entities/events required commonsense and world knowledge for correct resolution. Like MH17 is also referred to as Malaysian aircraft, etc. \n\nTable 2 only considered information local to a document. Usually, a single document did not have many alternative interpretations and had <5 events on an average.\n\nIn general, the approach is interesting and promising, but large parts of the paper are imprecise, do not introduce properly the relevant notions, use non-standard terminology and obfuscate/omit important details.\nWe apologize, due to brevity of space, we couldn’t expand on the definitions and some model details. We are working on adding a supplementary section to the next revision.\n\nWhat I found inappropriate are the unwarranted and unjustified claims about how people infer interpretations. \n \nThe reviewer is absolutely right and we should not make such unsupported statements. Our intent was to acknowledge the possibility that our approximations may lead to mistakes, but that these mistakes seem to resemble human mistakes. We corrected the text accordingly.\n'}"		SJeHzZqapm	HkevQqiM74	AKBC.ws/2019/Conference/-/Paper34/Official_Comment	['AKBC.ws/2019/Conference/Paper34/Reviewers/Unsubmitted']	3		['everyone']	BkeDfugVMN	['AKBC.ws/2019/Conference/Paper34/Authors']	1548036639476		1548036639476	['AKBC.ws/2019/Conference/Paper34/Authors', 'AKBC.ws/2019/Conference']
606	1542459661240	{'title': 'LEAPFROG: Adapting Belief Propagation for Knowledge Graph Construction', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper34/Authors'], 'keywords': ['Belief propagation', 'probabilistic graphical models', 'probabilistic knowledge base', 'Multi-modal knowledge bases', 'multilingual NLP', 'event coreference', 'entity coreference', 'relation coreference'], 'TL;DR': 'We present LEAPFROG, a probabilistic graphical model based framework, that maintains alternative probabilistic interpretations for conflicting entities, events and relations across multiple modalities in the automatically constructed knowledge graph.', 'abstract': 'Populating a knowledge base (KB) from unstructured information has been a widely studied problem. Capturing complex events and relations is especially challenging. Even more challenging is providing coherent interpretations of uncertain and even contradictory information. In this work, we present a novel probabilistic framework — LEAPFROG, for automated knowledge base construction that maintains alternative probabilistic interpretations for entities, events, and relations. To the best of our knowledge, this work is the first attempt at capturing multiple uncertain alternatives. Furthermore, we allow for a domain expert to inject their beliefs and prior knowledge into the system. We show how the expert’s beliefs about the reliability of an information source affect information interpretation.\n', 'archival status': 'Archival', 'subject areas': ['Machine Learning', 'Natural Language Processing', 'Information Integration', 'Knowledge Representation'], 'pdf': '/pdf/0a6386e19531811383644174f52af70c0f159b32.pdf', 'paperhash': 'anonymous|leapfrog_adapting_belief_propagation_for_knowledge_graph_construction', '_bibtex': '@inproceedings{    \nanonymous2019leapfrog:,    \ntitle={LEAPFROG: Adapting Belief Propagation for Knowledge Graph Construction},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=SJeHzZqapm},    \nnote={under review}    \n}'}		SJeHzZqapm	SJeHzZqapm	AKBC.ws/2019/Conference/-/Blind_Submission	[]	34	rye9bobaam	['everyone']		['AKBC.ws/2019/Conference']	1542459661240		1548035557180	['AKBC.ws/2019/Conference']
607	1548035480406	"{'title': 'We thank the reviewer for their detailed comments and suggestions. We have updated our manuscript based on the reviewer’s suggestion and below we have provided responses for each comment. ', 'comment': '""In Figure 1, the alternatives are really alternative interpretations or simply alternative mentions?  \n-These are alternative interpretations because SU25 and military jet are two different entities.  \nThere is no evidence in the documents suggesting SU25 and military jet are the same entity. The military jet is a Ukrainian military jet whereas SU25 referred to a jet without any affiliation. Thank you for pointing it out,  we have updated the figure.\n\n""In the beginning of section 2, it says ""the belief propagation engine performs inference ..."". \n-By Inference, we mean calculating the marginal probabilities (beliefs) for the knowledge graph nodes (which are event/relation/entity properties) for entity/event resolution. We have updated the paper.\n\n""It seems LEAPFROG needs an ontology as its initial input""\n-We referred to the ontology in Section 2.1, but didn’t mention in the system description. Thank you for the suggestion, we have updated the section and the supporting figure. \n\n""Section 2.2 explains about evidence graph construction. Also, this part is not evaluated at all, as the evaluation relied on annotations provided by LDC. I understand that this part is not the main focus of the manuscript. However, it is a problem that a thorough explanation is missing and also evaluation is also totally missing.""\n-Due to brevity of space, we kept this section short. We have updated the description to add more clarity to the section. Regarding evaluation - since we used the LDC annotated mentions, the evidence graph was constructed from these mentions, which would imply the evaluation over the evidence graph to be 100% accurate. Additionally, LDC provided only the complete knowledge graph and therefore we could evaluate only  our final KG.\n\n""Section 2.3, what do you mean : we ""leverage"" relation mentions.""\n-We use information from relation mentions like Sponsorship or Ownership  for entity disambiguation. For example, the “Spanish truck” has “entity-name: truck” and a relation “owned-by: Spain”. Another entity “French truck” has the same “entity-name: truck” but a different owner “owned-by: France”. Thus, using only name based heuristics will fail to disambiguate these entities and having additional information in the form of relation helps in entity disambiguation.\n\n""Section 2.3 it does not explain when and how a new KE is created.""\n-A new KE will be created in two cases: 1) if no candidate KEs are found 2) if after belief propagation the probability of a new KE ( for.eg the Truck3 in the target variable T in Figure 4) is above a specified threshold. \n\n""I could not understand the ""parent-child analogy"".""\n-The parent-child analogy was a misnomer, it has no significance. We have corrected it. \n\n""How the posterior of Obs(O2) is computed?""\n-The posterior of Obs(O2) is computed the exact same as computed for Obs(O1). Thus, \nP(obs2.property=x) = P(Obs1= x) P(certainty) + P(guess=x)*(1-P(certainty)). Due to brevity of space, we omitted this explicit description & have updated the paper. \n\t\n""Factor f1 captures the relation between the four parts of the observation"". Do you mean three parts?""\n-Yes, that is right. We have fixed that. \n\n""In section 3.2, how certainty is represented at an event level? Is it explained somewhere in the manuscript?""\n-At  the  event  level,  the  certainty  information  is  encoded  as  priors  of  the  target  variable T,  described in Section 4.2 similar to how its done for entities. In an ideal case, where observation event O1 is not hedged or negated, T has a uniform prior implying that all existing KEs (EventCandidate_1, EventCandidate_2) and the new possibility (Event_New) are equally likely T={EventCandidate_1: 0.33, EventCandidate_2: 0.33, Event_New: 0.33}. However, if O1 is hedged, the prior for the new event possibility (Event_New) is lowered, and the prior distribution  T={EventCandidate_1: 0.41, EventCandidate_2: 0.41, Event_New: 0.18}\n\n""which part of the framework is evaluated by which part of the experiment?""\n-Apologize for the confusion. The evaluation experiments in Table 2 and Table 4, show the ability of a belief propagation based engine to achieve cross-document and cross-lingual co-referencing across entities, events and relations, essentially constructing a knowledge graph from multi-lingual documents. \n\n""The discussion in section 4.4 is not very interesting. I do not think the case shows the benefit of using the proposed framework very well.""\n-The case study of showing different viewpoints based on country-specific sources is to demonstrate that how important it is to consider the information of a document’s source. If we change the country-wise bucketing strategy of sources and replace it with bucketing sources based on their reliability, the downstream application of this knowledge graph would help filtering or downweighting fake news or information from unreliable sources. Since we didn’t have the reliability scores for each document, we simulated a case study instead.'}"		SJeHzZqapm	HkegiroMQ4	AKBC.ws/2019/Conference/-/Paper34/Official_Comment	['AKBC.ws/2019/Conference/Paper34/Reviewers/Unsubmitted']	1		['everyone']	HJxG1FR4zN	['AKBC.ws/2019/Conference/Paper34/Authors']	1548035480406		1548035480406	['AKBC.ws/2019/Conference/Paper34/Authors', 'AKBC.ws/2019/Conference']
608	1538087935020	{'title': 'Biologically-Plausible Learning Algorithms Can Scale to Large Datasets', 'abstract': 'The backpropagation (BP) algorithm is often thought to be biologically implausible in the brain. One of the main reasons is that BP requires symmetric weight matrices in the feedforward and feedback pathways. To address this “weight transport problem” (Grossberg, 1987), two biologically-plausible algorithms, proposed by Liao et al. (2016) and Lillicrap et al. (2016), relax BP’s weight symmetry requirements and demonstrate comparable learning capabilities to that of BP on small datasets. However, a recent study by Bartunov et al. (2018) finds that although feedback alignment (FA) and some variants of target-propagation (TP) perform well on MNIST and CIFAR, they perform significantly worse than BP on ImageNet. Here, we additionally evaluate the sign-symmetry (SS) algorithm (Liao et al., 2016), which differs from both BP and FA in that the feedback and feedforward weights do not share magnitudes but share signs. We examined the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using different network architectures (ResNet-18 and AlexNet for ImageNet; RetinaNet for MS COCO). Surprisingly, networks trained with sign-symmetry can attain classification performance approaching that of BP-trained networks. These results complement the study by Bartunov et al. (2018) and establish a new benchmark for future biologically-plausible learning algorithms on more difficult datasets and more complex architectures.', 'keywords': ['biologically plausible learning algorithm', 'ImageNet', 'sign-symmetry', 'feedback alignment'], 'authorids': ['xiaow@fas.harvard.edu', 'chenhonglin@g.ucla.edu', 'lql@mit.edu', 'tp@csail.mit.edu'], 'authors': ['Will Xiao', 'Honglin Chen', 'Qianli Liao', 'Tomaso Poggio'], 'TL;DR': 'Biologically plausible learning algorithms, particularly sign-symmetry, work well on ImageNet', 'pdf': '/pdf/ee627a1d05507e9114b2183aed27d4e10e709247.pdf', 'paperhash': 'xiao|biologicallyplausible_learning_algorithms_can_scale_to_large_datasets', '_bibtex': '@inproceedings{\nxiao2018biologicallyplausible,\ntitle={Biologically-Plausible Learning Algorithms Can Scale to Large Datasets},\nauthor={Will Xiao and Honglin Chen and Qianli Liao and Tomaso Poggio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SygvZ209F7},\n}'}		SygvZ209F7	SygvZ209F7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1180	H1xxraS9Y7	['everyone']		['ICLR.cc/2019/Conference']	1538087935020		1548020007060	['ICLR.cc/2019/Conference']
609	1538087835260	{'title': 'Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network', 'abstract': 'We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.', 'keywords': [], 'authorids': ['xqliu@cs.ucla.edu', 'yaoli@ucdavis.edu', 'crwu@ucdavis.edu', 'chohsieh@cs.ucla.edu'], 'authors': ['Xuanqing Liu', 'Yao Li', 'Chongruo Wu', 'Cho-Jui Hsieh'], 'TL;DR': 'We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks', 'pdf': '/pdf/044946fbafc92fb33fc13947d293cdaa1f1ba0c1.pdf', 'paperhash': 'liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network', '_bibtex': '@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}'}		rk4Qso0cKm	rk4Qso0cKm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	610	HJgGbfk5tQ	['everyone']		['ICLR.cc/2019/Conference']	1538087835260		1548014632058	['ICLR.cc/2019/Conference']
610	1548009701538	"{'title': 'response', 'comment': 'Hi Rajarshi, thanks a lot for your comments. For the $m$, we define it in the paragraph right before section 3.3. It refers to the ""memory"", which can contain $h_{\\tau - 1}^{n-1}$ or additionally more faraway segments like $h_{\\tau - 2}^{n-1}$. '}"		HJePno0cYm	SJgCkbBfXV	ICLR.cc/2019/Conference/-/Paper717/Official_Comment	['ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted']	13		['everyone']	Skx4CMJfQN	['ICLR.cc/2019/Conference/Paper717/Authors']	1548009701538		1548009701538	['ICLR.cc/2019/Conference/Paper717/Authors', 'ICLR.cc/2019/Conference']
611	1547995251567	{'title': 'Experiment with mixed detailed and concise feedback', 'comment': 'Thank you for your positive review! \nWe would be happy to expand on our experimental results. While it is not possible for us to carry out a study with real human feedback (which requires IRB approval, etc.) we can perform experiments with a mixture of detailed and concise feedback and with different proportions of positive and negative feedback. Would this address your criticism (at least in part)?\n'}		SygLHbcapm	BJx2OdWf74	AKBC.ws/2019/Conference/-/Paper51/Official_Comment	['AKBC.ws/2019/Conference/Paper51/Reviewers/Unsubmitted']	2		['everyone']	Bye0ls1mfV	['AKBC.ws/2019/Conference/Paper51/Authors']	1547995251567		1547995251567	['AKBC.ws/2019/Conference/Paper51/Authors', 'AKBC.ws/2019/Conference']
612	1547995186367	{'title': 'Details are included in the paper', 'comment': 'Thank you for your review. \nThe details you mention do appear in our paper, but perhaps they can be further clarified.  We address each of your criticisms below:\n\n1. The implementation of feedback mentions is described in the third paragraph of section 2.3: “FMs are composed of two attribute maps” the first called “packaging” and the second called “payload”.  Immediately after this description, we provide intuition for why both attribute maps are needed. In section 3.1.1, we describe precisely how both of these attribute maps are used during canonicalization.\n\n2. The model, g, computes the linkage score between two nodes using their attribute maps.  This is described in the second paragraph in 3.1.1: “Since g scores the compatibility of two nodes based on their attribute maps...”. Perhaps we can further clarify this in the paper by including some text describing the linkage function g more precisely. \n\nThe precision of a node pair is used during training. In the first paragraph of section 4: “We train g to regress to the precision…” Thus, during learning we run agglomerative clustering and, at each merge, train g to predict the precision of the corresponding node pair.\n\n3. The function g is computed from the attribute maps of the corresponding nodes (which differ by dataset).  It is true that we omit the specific features that we use, for example, whether two nodes have the same title with a corresponding positive count in their respective attribute maps, or whether one node contains an attribute A with a positive count and the second node contains the attribute A with a negative count.  We are open to including these features in a supplement.\n\n4. We agree that the definition for complete could be revised to be clearer. We will modify the definition as follows:\nA node v is complete if for some $i$,\n\\lvs{e^\\star_i} \\subseteq lvs{v}.\nThus, if for some $i$, the leaves of $e^\\star_i$ are a subset of the leaves of $v$, then $v$ is complete. We note that the definition for complete is described in words immediately after the mathematical statement in the paper and is different from the definition of pure.\n\n5. What part of the detailed/concise feedback needs further explanation? As described in 5.2.1 and 5.2.2 and as detailed in the figure, the simulation procedure finds a “destination” and “target” for each edit.  The simulated edit contains a packaging that includes all of the positive attributes at the destination and a payload containing the attributes at the target (either with positive or negative weight, as described in the paper). We’re happy to describe the feedback generation process in more detail.\n\n6. We believe that qualitative results would not be very illuminating, but we’re open to including some. Would you find informative an  example of an edit added to a tree and the subsequent modification of inferred entities?\n'}		SygLHbcapm	Hkl94_WMQE	AKBC.ws/2019/Conference/-/Paper51/Official_Comment	['AKBC.ws/2019/Conference/Paper51/Reviewers/Unsubmitted']	1		['everyone']	SylJd4urG4	['AKBC.ws/2019/Conference/Paper51/Authors']	1547995186367		1547995186367	['AKBC.ws/2019/Conference/Paper51/Authors', 'AKBC.ws/2019/Conference']
613	1547985611813	"{'comment': 'This paper makes great contributions and like many, I was sad to see it get rejected. \n\nI was looking at the equations closely and the final equations describing the model has a variable ""m"" which hasn\'t been defined before. Specifically, I am referring to the ""m"" within the stop-gradient operator in the equation below.\n\n$\\tilde{h}_{\\tau}^{n - 1} = [SG(m_{\\tau}^{n-1}) \\cdot h_{tau}^{n-1}]$\n\nThis set of equations does not have a direct dependence on $h_{\\tau - 1}^{n-1}$ (the previous segment), so I am guessing ""m"" is capturing it somehow and it is not very clear presently.\n\nThank you in advance for the clarification.', 'title': 'Variable ""m"" in equation (end of page 5)'}"		HJePno0cYm	Skx4CMJfQN	ICLR.cc/2019/Conference/-/Paper717/Public_Comment	[]	3		['everyone']	HJePno0cYm	['~Rajarshi_Das1']	1547985611813		1547992605950	['~Rajarshi_Das1', 'ICLR.cc/2019/Conference']
614	1547958613265	"{'comment': 'How much does iRevNet differ from fiRevNet?\nFrom my understanding, the logits (N-classes output) from DCT-II output is optimized with class labels, and the remaining output is optionally optimized with the proposed loss function.\nSorry if I missed something.', 'title': ""isn't fiRevNet is iRevNet with average pooling swapped with DCT-II as (spectral) invertible pooling ?""}"		BkfbpsAcF7	Byx6IYubX4	ICLR.cc/2019/Conference/-/Paper778/Public_Comment	[]	2		['everyone']	BylSu67bQV	['(anonymous)']	1547958613265		1547958872007	['(anonymous)', 'ICLR.cc/2019/Conference']
615	1547939181038	"{'title': 'Not related to our submission', 'comment': ""1) The codebase you are referring to is not related to this paper.\n\n2) The paper presented here makes no claims about Cifar10. Our focus is on Imagenet, which is a much more challenging problem. MNIST is used to illustrate our proposed solution clearly.\n\n3) The Imagenet model we trained works just as well without invertible DCT pooling. We simply found DCT pooling to make our fi-RevNet conceptually closer to standard ResNets that apply a final global average pooling step, but this is a matter of taste.\n\n4) iRevNets differ from fiRevNets, we describe this in the paper, so you should not be surprised if they perform differently as well.\n\nIt is unfortunate that DCT pooling does not work for you on Cifar10, but neither did we make any claims about it, nor did we experiment with it or provide any implementation of DCT-pooled fiRevNets on Cifar10. However, feel free to drop me an email and I'll try to do what I can to help you.\n\n- Jörn""}"		BkfbpsAcF7	BylSu67bQV	ICLR.cc/2019/Conference/-/Paper778/Official_Comment	['ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted']	12		['everyone']	BylmnLXZm4	['ICLR.cc/2019/Conference/Paper778/Authors']	1547939181038		1547939297448	['ICLR.cc/2019/Conference/Paper778/Authors', 'ICLR.cc/2019/Conference']
616	1547937450584	"{'comment': ""The big jump from MNIST then Imagnet is intriguing.\nI tried training with CIFAR-10 using author's open source i-RevNet code, modified the original code to do pooling with DCT II, \n\nand the result is, accuracy is so bad, because everything must fit in 10-class logits.\n\nImagenet has 1000-class where the logits is much informative.\n\nThe author should clarify the limitation of DCT as invertible pooling"", 'title': 'Why no CIFAR-10 experiment'}"		BkfbpsAcF7	BylmnLXZm4	ICLR.cc/2019/Conference/-/Paper778/Public_Comment	[]	1		['everyone']	BkfbpsAcF7	['(anonymous)']	1547937450584		1547937592026	['(anonymous)', 'ICLR.cc/2019/Conference']
617	1538087834026	{'title': 'Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks', 'abstract': 'Efforts to reduce the numerical precision of computations in deep learning training have yielded systems that aggressively quantize weights and activations, yet employ wide high-precision accumulators for partial sums in inner-product operations to preserve the quality of convergence. The absence of any framework to analyze the precision requirements of partial sum accumulations results in conservative design choices. This imposes an upper-bound on the reduction of complexity of multiply-accumulate units. We present a statistical approach to analyze the impact of reduced accumulation precision on deep learning training. Observing that a bad choice for accumulation precision results in loss of information that manifests itself as a reduction in variance in an ensemble of partial sums, we derive a set of equations that relate this variance to the length of accumulation and the minimum number of bits needed for accumulation. We apply our analysis to three benchmark networks: CIFAR-10 ResNet 32, ImageNet ResNet 18 and ImageNet AlexNet. In each case, with accumulation precision set in accordance with our proposed equations, the networks successfully converge to the single precision floating-point baseline. We also show that reducing accumulation precision further degrades the quality of the trained network, proving that our equations produce tight bounds. Overall this analysis enables precise tailoring of computation hardware to the application, yielding area- and power-optimal systems.', 'keywords': ['reduced precision floating-point', 'partial sum accumulation bit-width', 'deep learning', 'training'], 'authorids': ['sakr2@illinois.edu', 'nwang@us.ibm.com', 'cchen@us.ibm.com', 'choij@us.ibm.com', 'ankuragr@us.ibm.com', 'shanbhag@illinois.edu', 'kailash@us.ibm.com'], 'authors': ['Charbel Sakr', 'Naigang Wang', 'Chia-Yu Chen', 'Jungwook Choi', 'Ankur Agrawal', 'Naresh Shanbhag', 'Kailash Gopalakrishnan'], 'TL;DR': 'We present an analytical framework to determine accumulation bit-width requirements in all three deep learning training GEMMs and verify the validity and tightness of our method via benchmarking experiments.', 'pdf': '/pdf/5eabacc116976cbbde3289eeeb5f99df10c2543b.pdf', 'paperhash': 'sakr|accumulation_bitwidth_scaling_for_ultralow_precision_training_of_deep_networks', '_bibtex': '@inproceedings{\nsakr2018accumulation,\ntitle={Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks},\nauthor={Charbel Sakr and Naigang Wang and Chia-Yu Chen and Jungwook Choi and Ankur Agrawal and Naresh Shanbhag and Kailash Gopalakrishnan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BklMjsRqY7},\n}'}		BklMjsRqY7	BklMjsRqY7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	603	S1lWNM39FX	['everyone']		['ICLR.cc/2019/Conference']	1538087834026		1547930865746	['ICLR.cc/2019/Conference']
618	1538087777732	{'title': 'Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds', 'abstract': 'Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. \n', 'keywords': ['crowdsourcing', 'information theory'], 'authorids': ['caopeng2016@pku.edu.cn', 'xuyilun@pku.edu.cn', 'yuqing.kong@pku.edu.cn', 'yizhou.wang@pku.edu.cn'], 'authors': ['Peng Cao', 'Yilun Xu', 'Yuqing Kong', 'Yizhou  Wang'], 'pdf': '/pdf/6d826c78b6d8b90396fcb156ac1799e0ffe56737.pdf', 'paperhash': 'cao|maxmig_an_information_theoretic_approach_for_joint_learning_from_crowds', '_bibtex': '@inproceedings{\ncao2018maxmig,\ntitle={Max-{MIG}: an Information Theoretic Approach for Joint Learning from Crowds},\nauthor={Peng Cao and Yilun Xu and Yuqing Kong and Yizhou  Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJg9DoR9t7},\n}'}		BJg9DoR9t7	BJg9DoR9t7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	287	rklvqGzYYX	['everyone']		['ICLR.cc/2019/Conference']	1538087777732		1547882140727	['ICLR.cc/2019/Conference']
619	1544465878444	{'title': 'Thanks for your continued feedback, but imitation learning is not the same as inverse RL', 'comment': 'Our paper is on inverse reinforcement learning. The goal of this setting to learn the cost function of another agent through observing behavior. Evaluation of the learned cost function should naturally be with respect to the original cost function. Since we are trying to explicitly recover the cost function of the expert, the value difference metric seems to us reasonable. We emphasize again that this comes from prior work.\n\nThis problem statement is different from the imitation setup. In fact, imitation learning very often does not recover a cost function at all.\n\nWe would like to address your concern and thank you for your time, but can you provide a more specific description of what normalized metric are you suggesting? \n\n'}		SyeLno09Fm	BJg0ATQ2yE	ICLR.cc/2019/Conference/-/Paper713/Official_Comment	['ICLR.cc/2019/Conference/Paper713/Reviewers/Unsubmitted']	10		['everyone']	ryxrkKm2kV	['ICLR.cc/2019/Conference/Paper713/Authors']	1544465878444		1547869803491	['ICLR.cc/2019/Conference/Paper713/Authors', 'ICLR.cc/2019/Conference']
620	1544463777972	"{'title': 'Comment: the value difference metric is ""normalized""', 'comment': 'We would just like to point out that the value difference metric is ""normalized"". When the value difference is zero, the recovered policy is optimal. The difference is computed with respect to V*, which we explain in the paper. There seems to have a been a persistent misunderstanding of this metric, which we hope is now clarified.'}"		SyeLno09Fm	SJl9jB7nJV	ICLR.cc/2019/Conference/-/Paper713/Official_Comment	['ICLR.cc/2019/Conference/Paper713/Reviewers/Unsubmitted']	8		['everyone']	SJgvMVsuCm	['ICLR.cc/2019/Conference/Paper713/Authors']	1544463777972		1547869743203	['ICLR.cc/2019/Conference/Paper713/Authors', 'ICLR.cc/2019/Conference']
621	1538087767719	{'title': 'Learning to Remember More with Less Memorization', 'abstract': 'Memory-augmented neural networks consisting of a neural controller and an external memory have shown potentials in long-term sequential learning. Current RAM-like memory models maintain memory accessing every timesteps, thus they do not effectively leverage the short-term memory held in the controller. We hypothesize that this scheme of writing is suboptimal in memory utilization and introduces redundant computation. To validate our hypothesis, we derive a theoretical bound on the amount of information stored in a RAM-like system and formulate an optimization problem that maximizes the bound. The proposed solution dubbed Uniform Writing is proved to be optimal under the assumption of equal timestep contributions. To relax this assumption, we introduce modifications to the original solution, resulting in a solution termed Cached Uniform Writing. This method aims to balance between maximizing memorization and forgetting via overwriting mechanisms. Through an extensive set of experiments, we empirically demonstrate the advantages of our solutions over other recurrent architectures, claiming the state-of-the-arts in various sequential modeling tasks. ', 'keywords': ['memory-augmented neural networks', 'writing optimization'], 'authorids': ['lethai@deakin.edu.au', 'truyen.tran@deakin.edu.au', 'svetha.venkatesh@deakin.edu.au'], 'authors': ['Hung Le', 'Truyen Tran', 'Svetha Venkatesh'], 'pdf': '/pdf/8eef966ac61262af865603f479fdeb3e415445b6.pdf', 'paperhash': 'le|learning_to_remember_more_with_less_memorization', '_bibtex': '@inproceedings{\nle2018learning,\ntitle={Learning to Remember More with Less Memorization},\nauthor={Hung Le and Truyen Tran and Svetha Venkatesh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1xlvi0qYm},\n}'}		r1xlvi0qYm	r1xlvi0qYm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	231	BJeZEytdKm	['everyone']		['ICLR.cc/2019/Conference']	1538087767719		1547860944843	['ICLR.cc/2019/Conference']
622	1538087930011	{'title': 'Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet', 'abstract': 'Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6% top-5 for 32 x 32 px features and Alexnet performance for 16 x16 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.', 'keywords': ['interpretability', 'representation learning', 'bag of features', 'deep learning', 'object recognition'], 'authorids': ['wieland.brendel@bethgelab.org', 'matthias.bethge@uni-tuebingen.de'], 'authors': ['Wieland Brendel', 'Matthias Bethge'], 'TL;DR': 'Aggregating class evidence from many small image patches suffices to solve ImageNet, yields more interpretable models and can explain aspects of the decision-making of popular DNNs.', 'pdf': '/pdf/8a4703a5ecc953348745d78472c842c3636cc051.pdf', 'paperhash': 'brendel|approximating_cnns_with_bagoflocalfeatures_models_works_surprisingly_well_on_imagenet', '_bibtex': '@inproceedings{\nbrendel2018approximating,\ntitle={Approximating {CNN}s with Bag-of-local-Features models works surprisingly well on ImageNet},\nauthor={Wieland Brendel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkfMWhAqYQ},\n}'}		SkfMWhAqYQ	SkfMWhAqYQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1150	HkgToZ05Ym	['everyone']		['ICLR.cc/2019/Conference']	1538087930011		1547846814083	['ICLR.cc/2019/Conference']
623	1538087739641	"{'title': 'Caveats for information bottleneck in deterministic scenarios', 'abstract': 'Information bottleneck (IB) is a method for extracting information from one random variable X that is relevant for predicting another random variable Y. To do so, IB identifies an intermediate ""bottleneck"" variable T that has low mutual information I(X;T) and high mutual information I(Y;T). The ""IB curve"" characterizes the set of bottleneck variables that achieve maximal I(Y;T) for a given I(X;T), and is typically explored by maximizing the ""IB Lagrangian"", I(Y;T) - βI(X;T). In some cases, Y is a deterministic function of X, including many classification problems in supervised learning where the output class Y is a deterministic function of the input X. We demonstrate three caveats when using IB in any situation where Y is a deterministic function of X: (1) the IB curve cannot be recovered by maximizing the IB Lagrangian for different values of β; (2) there are ""uninteresting"" trivial solutions at all points of the IB curve; and (3) for multi-layer classifiers that achieve low prediction error, different layers cannot exhibit a strict trade-off between compression and prediction, contrary to a recent proposal. We also show that when Y is a small perturbation away from being a deterministic function of X, these three caveats arise in an approximate way. To address problem (1), we propose a functional that, unlike the IB Lagrangian, can recover the IB curve in all cases. We demonstrate the three caveats on the MNIST dataset.', 'paperhash': 'kolchinsky|caveats_for_information_bottleneck_in_deterministic_scenarios', 'TL;DR': 'Information bottleneck behaves in surprising ways whenever the output is a deterministic function of the input.', 'authorids': ['artemyk@gmail.com', 'tracey.brendan@gmail.com', 'steven.jvk@gmail.com'], 'authors': ['Artemy Kolchinsky', 'Brendan D. Tracey', 'Steven Van Kuyk'], 'keywords': ['information bottleneck', 'supervised learning', 'deep learning', 'information theory'], 'pdf': '/pdf/9b21b28b3f206ce24ad627416358608550789999.pdf', '_bibtex': '@inproceedings{\nkolchinsky2018caveats,\ntitle={Caveats for information bottleneck in deterministic scenarios},\nauthor={Artemy Kolchinsky and Brendan D. Tracey and Steven Van Kuyk},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rke4HiAcY7},\n}'}"		rke4HiAcY7	rke4HiAcY7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	77	rJxVUUUJtQ	['everyone']		['ICLR.cc/2019/Conference']	1538087739641		1547844668853	['ICLR.cc/2019/Conference']
624	1547825513623	{'title': 'comments to AnonReview2', 'comment': 'The use of word “short” in the paper was quite unfortunate from our side. We are handling the same kind of phrases like all previous entity-oriented retrieval approaches [Zhang 07, Pound 12, Hou and Nayak 13, Han 17]. The EL language we use is a W3C standard, it is used in SNOMED CT, and it is more complex compared to the logic used in [Pound 12]. This language is to a great extent sufficient to capture the meaning in user queries since users never type special quantifiers like “in exactly two of my finger” or “in every arm”. Our approach extends and adapts in a trivial way a previous approach that uses dependency parsing and then recursively maps nodes to concepts to perform text interpretation. \n\nDuring submission we were working towards handling coordinated conjunctions but this part was not ready before the deadline. We have updated the paper with our new results (alg 2). It is in our plans to constantly extend the work to capture additional features like temporal information or negation but these can be topics of dedicated papers. In addition, in our case we would also need to extend the reasoning algorithm (section 4) to support these constructs. With such constructs reasoning becomes intractable (NP-hard) and hence the problem is far from trivial.\n\nOur medical KB and the construction pipeline have been published in the International Semantic Web conference in the main research track, the poster track as well as in various workshops. Due to double-blind review policies we cannot reveal more details about the KB or fully cite those papers. \n\nWe have added a reference and a small note about GATE. We apologize that we assume this systems as a standard one, it is almost a standard in the Semantic Web community. https://gate.ac.uk \n\nNED is indeed quite important but there are several reasons why we picked ElasticSearch. First, we wanted to provide a system that provides a max 200msec response time guarantee. Given all complex components of the algorithm this requirement is almost impossible to be achieved with heavy weight NED algorithms. Second, disambiguation approaches are more effective when processing paragraphs or large sentences rather than noun phrases. Some NED approaches had been attempted in the past but did not necessarily give better results. We have updated the paper to mention these design choices. \n\nThe datasets we used were indeed created in-house however they were not created by our team, hence, our results are not biased. Moreover, our work targets activating complex entities from a KB (i.e., not just a single entity - Da Vinci - but entities defined as a combination of KB entities - Severe Chest Pain). To the best of our knowledge no annotated dataset exists for this scenario and in the only other related work for text subsumption [Movshovitz-Attias et al., 2015] proprietary datasets were devised and used. '}		H1l8ag9a6m	HkxfdZ_yQV	AKBC.ws/2019/Conference/-/Paper3/Official_Comment	['AKBC.ws/2019/Conference/Paper3/Reviewers/Unsubmitted']	1		['everyone']	S1gWjsDLGE	['AKBC.ws/2019/Conference/Paper3/Authors']	1547825513623		1547827654523	['AKBC.ws/2019/Conference/Paper3/Authors', 'AKBC.ws/2019/Conference']
625	1547827489388	{'title': 'updated version', 'comment': 'We thank the reviewers for their comments. A new version of the paper has been updated. There is an extension of the extraction algorithm mainly due to work that was ongoing during submission and which we did not manage to fit in before the deadline. Some clarifications to various points are also made'}		H1l8ag9a6m	BklF7F_1QN	AKBC.ws/2019/Conference/-/Paper3/Official_Comment	['AKBC.ws/2019/Conference/Paper3/Reviewers/Unsubmitted']	4		['everyone']	H1l8ag9a6m	['AKBC.ws/2019/Conference/Paper3/Authors']	1547827489388		1547827489388	['AKBC.ws/2019/Conference/Paper3/Authors', 'AKBC.ws/2019/Conference']
626	1547827418948	{'title': 'comments to AnonReview2', 'comment': 'Indeed to make a practical and production ready system we had to make various design and architectural choices and in the paper we have tried to disclose all of them. We still feel that this has its merits and is valid applied research. However, we would like to also point out that we also do actually make theoretical contributions to many different research problems for which many works have been published in the past and these are described in the detailed Related Work section. In summary we do concrete contributions, improvements and have clear advantages compared to the current state-of-the-art on i) concept extraction from text ii) subsumption checking from text, iii) entity-oriented retrieval and iv) we also present the first ever approach on matching complex keyword-based queries to complex ontological entities.\n\nAs we use KR technologies the behavior of the algorithm is highly explainable and traceable. Some ablation analysis is given in Table 4 where we breakdown on how many cases the expansion module or the builder was used internally. \n\nAll algorithms are general (no medical domain specific rules) and can be used over any domain if a different KB is used. However, of course there is no solution that fits everything. Adaptations and extensions would be required. '}		H1l8ag9a6m	H1eQkFOJ74	AKBC.ws/2019/Conference/-/Paper3/Official_Comment	['AKBC.ws/2019/Conference/Paper3/Reviewers/Unsubmitted']	3		['everyone']	B1lBn1XQz4	['AKBC.ws/2019/Conference/Paper3/Authors']	1547827418948		1547827418948	['AKBC.ws/2019/Conference/Paper3/Authors', 'AKBC.ws/2019/Conference']
627	1547825944221	{'title': 'comments to AnonReview3', 'comment': 'Our extraction method is based on a published theory about using dependency parsing and KBs for text interpretation [Romacker]. We adapt this theory in a non-trivial way to construct concise concepts (as detailed in Related Work) and enable subsequent reasoning. \n\nAmbiguity is quite inherent in natural language and dealing with it in a general way that suits all use cases is impossible. Since we use NLP techniques we need to assume that the dependency tree we obtain from the parser is the correct one. The same assumption is followed in the work by Romacker. \n\nAmbiguity on entity linking was also considered. Entity linking can again be improved or adapted depending on the use case. E.g., if it is symptom matching one may prefer to activate Diseases/Symptoms (Clinical Findings), as opposed to general health monitoring where one may prioritise entities corresponding to BiologicalFunctions. Some Named Entity Disambiguation algorithms were tried in the past but did not work particularly better than ElasticSearch. We added a note on the choice of ElasticSearch in the newly uploaded version. We would be grateful if the reviewer elaborated his comment on ambiguity of dependency structure more and the previous answer does not cover his/her concerns.\n\nAlgorithm 3 is based on theory and tools that come from Knowledge Representation & Reasoning (Description Logics) and Semantic Web. We present non-trivial contributions to scalable and expressive SPARQL query answering over triple-stores and a novel combination of these logic-based approaches with NLP. This has not been done before in the Semantic Web community. The extensive Related Work section shows clear advancement on all research topics that our paper deals with (entity-oriented retrieval, concept extraction, discovering subsumptions from text). As such our solution is not based on Machine Learning and statistic-based models, however, KRR, semantics, and reasoning are clearly stated in the topic of the conference. \n\nWe have added a reference and a small note about GATE. We apologize that we assume this systems as a standard one, it is almost a standard one in the Semantic Web community. https://gate.ac.uk.\n\nTo the best of our knowledge no annotated dataset for checking text-level subsumption exists. In the only other related work to ours on text-level subsumption [Movshovitz-Attias et al., 2015] proprietary datasets were devised and used. We would appreciate any concrete pointers to datasets and annotated true/false subsumptions. \n\nWe used two baselines. One is simple text annotation, which is the approach proposed in [Thanh Tran], and the other one is sentence embeddings which is also used in [Movshovitz-Attias].  '}		H1l8ag9a6m	BJge77u1XV	AKBC.ws/2019/Conference/-/Paper3/Official_Comment	['AKBC.ws/2019/Conference/Paper3/Reviewers/Unsubmitted']	2		['everyone']	rkxBeKmSME	['AKBC.ws/2019/Conference/Paper3/Authors']	1547825944221		1547825944221	['AKBC.ws/2019/Conference/Paper3/Authors', 'AKBC.ws/2019/Conference']
628	1542459582241	"{'title': 'Understanding And Reasoning With Short Phrase Queries Over Large Knowledge Bases', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper3/Authors'], 'keywords': ['knowledge extraction', 'entity-oriented querying', 'hybrid reasoning'], 'abstract': ""The use of large Knowledge Bases (KBs) in modern semantic intensive applications is gradually becoming the norm. Entities from a KB can be used to annotate and semantically enrich data (e.g., holiday destinations or patient profiles) as well as build intelligent services (e.g., holiday booking or symptom checking engines). To access such information or services users usually form entity-centric queries expressed using noun phrases like ``Small Mediterranean Islands'', ``Cheap Pizza Restaurants'', or ``I have a pain in my left leg''. Answering such queries or matching them to the annotated entities is a challenging task since it requires a deeper understanding of their semantics as well as taking into account the axioms defined in the KB (both hierarchy and relations).\n\nIn the current paper we propose a framework for KB-based understanding and reasoning with such queries that is heavily based on formal Semantic Web technologies. First, we develop a method which given such short phrases it extracts a Description Logic concept in an attempt to formally capture their semantics. Second, we develop a novel reasoning algorithm which given two concepts constructed using entities from a KB compares them by possibly also exploiting information that is encoded in their text labels. The latter is necessary due to possible ``underspecification'' of KBs which hinders recall of formal reasoning methods. We demonstrate the feasibility and practical relevance of our approach by testing it in a real-world industrial strength application in Anonymous which develops an AI-based symptom checking dialogue system. Our approach coupled with sentence embeddings as a fall-back is used to activate Anonymous' symptom checking engine and provides the best combination of precision and recall compared to any other method used alone."", 'pdf': '/pdf/cac3e55cce8ad0efcf56594b9df5cc5ce402bc78.pdf', 'archival status': 'Archival', 'subject areas': ['Natural Language Processing', 'Question Answering', 'Reasoning', 'Knowledge Representation', 'Semantic Web'], 'paperhash': 'anonymous|understanding_and_reasoning_with_short_phrase_queries_over_large_knowledge_bases', '_bibtex': '@inproceedings{    \nanonymous2019understanding,    \ntitle={Understanding And Reasoning With Short Phrase Queries Over Large Knowledge Bases},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=H1l8ag9a6m},    \nnote={under review}    \n}'}"		H1l8ag9a6m	H1l8ag9a6m	AKBC.ws/2019/Conference/-/Blind_Submission	[]	3	SklaTCvDaX	['everyone']		['AKBC.ws/2019/Conference']	1542459582241		1547821027251	['AKBC.ws/2019/Conference']
629	1547640330188	"{'title': 'Response to Reviewer 3', 'comment': 'We would like to thank Reviewer 3 for their review and constructive suggestions.\n\nWe acknowledge that distantly- and semi-supervised methods are a vital part of (large-scale) relation extraction. In this work we explicitly focus on the supervised scenario for the following reasons:\nOur main goal is to show that pre-trained language representations, in combination with a self-attentive architecture, are able to perform comparable or better than methods relying on explicit syntactic and semantic features, which are common in current state-of-the-art relation extraction methods. Due to the automated annotation process, distantly- and semi-supervised methods introduce a considerable amount of noise. This requires extending the approach to explicitly account for the noisiness of the data, which makes it more difficult to assess the efficacy of our approach in isolation. In ongoing work we address the distantly-supervised scenario, extending our approach to account for the noise introduced during the automated annotation process.\n\nNo special care has to be taken for passive and active voice. Our approach implicitly assumes the head entity to be provided first, followed by the tail entity. I.e. if the entities are provided as <Person A> <Organisation A> (assuming that person A is an employee of Organisation A), the system predicts ""per:employee_of"", whereas <Organisation A> <Person A> results in the inverse relation ""org:top_members/employees"".'}"		BJgrxbqp67	H1gGfCchGN	AKBC.ws/2019/Conference/-/Paper21/Official_Comment	['AKBC.ws/2019/Conference/Paper21/Reviewers/Unsubmitted']	3		['everyone']	BkxqDfEWfE	['AKBC.ws/2019/Conference/Paper21/Authors']	1547640330188		1547809596474	['AKBC.ws/2019/Conference/Paper21/Authors', 'AKBC.ws/2019/Conference']
630	1547640140561	{'title': 'Response to Reviewer 1', 'comment': 'We would like to thank Reviewer 1 for their review and constructive suggestions.\n\nWe agree that adapting our approach to BERT is rather easy, in fact, we have already done so for our ongoing experiments.\n\nRegarding the last comment: We agree that our paper is a slightly adapted application of the OpenAI GPT for relation extraction. However, in the introduction, our goal was to motivate this application, because most state-of-the-art approaches (e.g. all competing approaches listed in table 4 and 5) still rely on dependency parse information and other manually-engineered features.\n\nWe will rephrase our paper to clearly indicate the adaptation of the OpenAI GPT.'}		BJgrxbqp67	HyeSIa92ME	AKBC.ws/2019/Conference/-/Paper21/Official_Comment	['AKBC.ws/2019/Conference/Paper21/Reviewers/Unsubmitted']	2		['everyone']	S1ercHpmMV	['AKBC.ws/2019/Conference/Paper21/Authors']	1547640140561		1547809589283	['AKBC.ws/2019/Conference/Paper21/Authors', 'AKBC.ws/2019/Conference']
631	1547640021225	{'title': 'Response to Reviewer 2', 'comment': 'We would like to thank Reviewer 2 for their review and constructive suggestions.\n\nWe report the best single-model performance (PA-LSTM) from the original TACRED paper in Table 4. Zhang et al. report slightly different results, 65.4 (2017) vs. 65.1 (2018), and we report the latter. The overall best performing model reported in the original TACRED paper is an ensemble combining 5 independently trained models.\n\nWe will submit a revised version clearly indicating that we compare single-model performance.'}		BJgrxbqp67	rJlpR253GE	AKBC.ws/2019/Conference/-/Paper21/Official_Comment	['AKBC.ws/2019/Conference/Paper21/Reviewers/Unsubmitted']	1		['everyone']	Hke1NobVMN	['AKBC.ws/2019/Conference/Paper21/Authors']	1547640021225		1547809574845	['AKBC.ws/2019/Conference/Paper21/Authors', 'AKBC.ws/2019/Conference']
632	1538087940245	{'title': 'Meta-learning with differentiable closed-form solvers', 'abstract': 'Adapting deep networks to new concepts from few examples is challenging, due to the high computational and data requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this work we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as logistic regression, as part of its own internal model, enabling it to quickly adapt to novel tasks.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.', 'keywords': ['few-shot learning', 'one-shot learning', 'meta-learning', 'deep learning', 'ridge regression', 'classification'], 'authorids': ['luca.bertinetto@robots.ox.ac.uk', 'joao@robots.ox.ac.uk', 'philip.torr@eng.ox.ac.uk', 'vedaldi@robots.ox.ac.uk'], 'authors': ['Luca Bertinetto', 'Joao F. Henriques', 'Philip Torr', 'Andrea Vedaldi'], 'TL;DR': 'We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.', 'pdf': '/pdf/2910e2b93c944ceac56b0800085c3e994016ae78.pdf', 'paperhash': 'bertinetto|metalearning_with_differentiable_closedform_solvers', '_bibtex': '@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}'}		HyxnZh0ct7	HyxnZh0ct7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1211	S1xHc_n5tQ	['everyone']		['ICLR.cc/2019/Conference']	1538087940245		1547807314731	['ICLR.cc/2019/Conference']
633	1547806388281	"{'title': 'Doubts about generalizability, design decisions and contribution', 'comment': ""Thank's to all reviewers for their comments. It seems that all reviewers agree that the paper is on an important problem, however, the main criticism seems to be:\n\n* The system is manually optimized for a narrow domain and generalizability to other domains might be doubted. That has been brought up by reviewer 1. Do the other reviewers agree on this point? Is the domain too narrow?\n* Related is the criticism by reviewer 2, who points out that it is mostly an engineering and not so much a research contribution. What about the other reviewers? Do you agree? I am not sure if perhaps the optimization for this domain and the evaluation that it works here could be seen as a substantial contribution.\n* Reviewer 3 pointed out that the design choices are somewhat unclear. SRL with n-ary relations might not be the most straight-forward choice for a triple generation approach. I see that, but what might have been an alternative if want to build on top of existing tools? Would a comparison to alternative open information extraction approach be a sufficient argument?\n\nThanks again, also for all the detailed comments how the paper could be made more clear.""}"		HJxmxbq66Q	SJlhnIQkXV	AKBC.ws/2019/Conference/-/Paper20/Official_Comment	['AKBC.ws/2019/Conference/Paper20/Reviewers/Unsubmitted']	1		['everyone']	HJxmxbq66Q	['AKBC.ws/2019/Conference/Paper20/Area_Chair1']	1547806388281		1547806388281	['AKBC.ws/2019/Conference/Paper20/Area_Chair1', 'AKBC.ws/2019/Conference']
634	1538087856949	{'title': 'Bayesian Prediction of Future Street Scenes using Synthetic Likelihoods', 'abstract': 'For autonomous agents to successfully operate in the real world, the ability to anticipate future scene states is a key competence. In real-world scenarios, future states become increasingly uncertain and multi-modal, particularly on long time horizons. Dropout based Bayesian inference provides a computationally tractable, theoretically well grounded approach to learn different hypotheses/models to deal with uncertain futures and make predictions that correspond well to observations -- are well calibrated. However, it turns out that such approaches fall short to capture complex real-world scenes, even falling behind in accuracy when compared to the plain deterministic approaches. This is because the used log-likelihood estimate discourages diversity. In this work, we propose a novel Bayesian formulation for anticipating future scene states which leverages synthetic likelihoods that encourage the learning of diverse models to accurately capture the multi-modal nature of future scene states. We show that our approach achieves accurate state-of-the-art predictions and calibrated probabilities through extensive experiments for scene anticipation on Cityscapes dataset. Moreover, we show that our approach generalizes across diverse tasks such as digit generation and precipitation forecasting.', 'keywords': ['bayesian inference', 'segmentation', 'anticipation', 'multi-modality'], 'authorids': ['abhattac@mpi-inf.mpg.de', 'mfritz@mpi-inf.mpg.de', 'schiele@mpi-inf.mpg.de'], 'authors': ['Apratim Bhattacharyya', 'Mario Fritz', 'Bernt Schiele'], 'TL;DR': 'Dropout based Bayesian inference is extended to deal with multi-modality and is evaluated on scene anticipation tasks.', 'pdf': '/pdf/48a0121cbf54a93fea66c3ab54f55ae239a87733.pdf', 'paperhash': 'bhattacharyya|bayesian_prediction_of_future_street_scenes_using_synthetic_likelihoods', '_bibtex': '@inproceedings{\nbhattacharyya2018bayesian,\ntitle={Bayesian Prediction of Future Street Scenes using Synthetic Likelihoods},\nauthor={Apratim Bhattacharyya and Mario Fritz and Bernt Schiele},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkgK3oC5Fm},\n}'}		rkgK3oC5Fm	rkgK3oC5Fm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	730	r1xTQsHqtm	['everyone']		['ICLR.cc/2019/Conference']	1538087856949		1547799310933	['ICLR.cc/2019/Conference']
635	1538087731517	"{'title': 'Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets', 'abstract': 'Training activation quantized neural networks involves minimizing a piecewise constant training loss whose gradient vanishes almost everywhere, which is undesirable for the standard back-propagation or chain rule. An empirical way around this issue is to use a straight-through estimator (STE) (Bengio et al., 2013) in the backward pass, so that the ""gradient"" through the modified chain rule becomes non-trivial. Since this unusual ""gradient"" is certainly not the gradient of loss function, the following question arises: why searching in its negative direction minimizes the training loss? In this paper, we provide the theoretical justification of the concept of STE by answering this question. We consider the problem of learning a two-linear-layer network with binarized ReLU activation and Gaussian input data. We shall refer to the unusual ""gradient"" given by the STE-modifed chain rule as coarse gradient. The choice of STE is not unique. We prove that if the STE is properly chosen, the expected coarse gradient correlates positively with the population gradient (not available for the training), and its negation is a descent direction for minimizing the population loss. We further show the associated coarse gradient descent algorithm converges to a critical point of the population loss minimization problem. Moreover, we show that a poor choice of STE may lead to instability of the training algorithm near certain local minima, which is also observed in our CIFAR-10 experiments.', 'paperhash': 'yin|understanding_straightthrough_estimator_in_training_activation_quantized_neural_nets', 'keywords': ['straight-through estimator', 'quantized activation', 'binary neuron'], 'authorids': ['yph@ucla.edu', 'jianchel@uci.edu', 'shuazhan@qti.qualcomm.com', 'sjo@math.ucla.edu', 'yingyong@qti.qualcomm.com', 'jxin@math.uci.edu'], 'authors': ['Penghang Yin', 'Jiancheng Lyu', 'Shuai Zhang', 'Stanley Osher', 'Yingyong Qi', 'Jack Xin'], 'TL;DR': 'We make theoretical justification for the concept of straight-through estimator.', 'pdf': '/pdf/f766c866d898a1bcfb76ec5cd3f524726fd8d67d.pdf', '_bibtex': '@inproceedings{\nyin2018understanding,\ntitle={Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets},\nauthor={Penghang Yin and Jiancheng Lyu and Shuai Zhang and Stanley J. Osher and Yingyong Qi and Jack Xin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Skh4jRcKQ},\n}'}"		Skh4jRcKQ	Skh4jRcKQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	33	HkxB3XguKm	['everyone']		['ICLR.cc/2019/Conference']	1538087731517		1547798180254	['ICLR.cc/2019/Conference']
636	1547796071282	{'title': 'The work would be a better fit for Recommender communities', 'review': 'The paper presents TransRev, a product recommender. \nThe recommendation is problem is tackled by learning vector representations for users, products and reviews and using various prediction techniques (e.g. linear regression) to perform the recommendations.\n\nWhile the problem is extremely interesting, the paper itself has some drawbacks:\n- sentiment analysis, Knowledge Base population and various theme are touched and deemed essential components of the work, but I struggle to identify how are they incorporated in the method (Section 2)\n- from a presentation point of view, the contributions are not clearly stated. Also the abstract is not crisp and diverges to external references\n- finally, the connection to Knowledge Bases seems somehow “forced” as for fitting the venue ', 'rating': '4: Ok but not good enough - rejection', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		HJxikb5T6m	ryxyORxyXN	AKBC.ws/2019/Conference/-/Paper17/Official_Review	['AKBC.ws/2019/Conference/Paper17/Reviewers/Unsubmitted']	3		['everyone']	HJxikb5T6m	['AKBC.ws/2019/Conference/Paper17/AnonReviewer3']	1547796071282		1547796091574	['AKBC.ws/2019/Conference/Paper17/AnonReviewer3', 'AKBC.ws/2019/Conference']
637	1547784288782	{'title': 'Thank you for the feedback.', 'comment': 'We thank the reviewer for the feedback!\n\nOur main contribution is proposing a production-level KB construction system, and we developed an information extraction system from the Web to improve the quality of our KB (JKB).\nIn other words, the information extraction system is a part of our KB construction system.\nThus, we evaluated our KB construction system along with enterprise-specific issues in Section 2 and confirmed the validity of our KB construction system in our experiments.\nTo clarify the aforementioned contribution, we will revise the manuscript.'}		SJxfV-q6Tm	SJeKwxRRME	AKBC.ws/2019/Conference/-/Paper44/Official_Comment	['AKBC.ws/2019/Conference/Paper44/Reviewers/Unsubmitted']	1		['everyone']	S1lJ_aIRb4	['AKBC.ws/2019/Conference/Paper44/Authors']	1547784288782		1547784288782	['AKBC.ws/2019/Conference/Paper44/Authors', 'AKBC.ws/2019/Conference']
638	1540314505205	"{'title': 'A Deep Generative Acoustic Model for Compositional Automatic Speech Recognition', 'abstract': 'Inspired by the recent successes of deep generative models for Text-To-Speech (TTS) such as WaveNet (van den Oord et al., 2016) and Tacotron (Wang et al., 2017), this article proposes the use of a deep generative model tailored for Automatic Speech Recognition (ASR) as the primary acoustic model (AM) for an overall recognition system with a separate language model (LM). Two dimensions of depth are considered: (1) the use of mixture density networks, both autoregressive and non-autoregressive, to generate density functions capable of modeling acoustic input sequences with much more powerful conditioning than the first-generation generative models for ASR, Gaussian Mixture Models / Hidden Markov Models (GMM/HMMs), and (2) the use of standard LSTMs, in the spirit of the original tandem approach, to produce discriminative feature vectors for generative modeling. Combining mixture density networks and deep discriminative features leads to a novel dual-stack LSTM architecture directly related to the RNN Transducer (Graves, 2012), but with the explicit functional form of a density, and combining naturally with a separate language model, using Bayes rule. The generative models discussed here are compared experimentally in terms of log-likelihoods and frame accuracies.', 'keywords': ['Automatic Speech Recognition', 'Deep generative models', 'Acoustic modeling', 'End-to-end speech recognition'], 'authorids': ['erikmcd@google.com'], 'authors': ['Erik McDermott'], 'TL;DR': ""This paper proposes the use of a deep generative acoustic model for automatic speech recognition, combining naturally with other deep sequence-to-sequence modules using Bayes' rule."", 'pdf': '/pdf/828a25c950ffbee4204c518197c597814d4befc0.pdf', 'paperhash': 'mcdermott|a_deep_generative_acoustic_model_for_compositional_automatic_speech_recognition'}"		S1fbqB0noQ	S1fbqB0noQ	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	59	BklZ5SRnim	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540314505205		1547767355376	['NIPS.cc/2018/Workshop/IRASL']
639	1547745448576	"{'title': 'Response to AnonReviewer1', 'comment': ""The introduction has been updated to state that a new annotated gold standard resource ('benchmark') is being introduced.\n\nThe main contribution in this paper is the new 'benchmark' - a manually annotated resource, for training and evaluating biomedical concept recognition systems. This new resource addresses 2 key needs:\n(i) Larger annotated resource, useful for training today's more complex ML models\n(ii) Broader coverage of biology and medicine concepts, by targeting UMLS.\n\nCRAFT is the closest, in size and coverage of biology. MedMentions can be viewed as a supplement -- the sizes of the ontologies is so much greater than the available annotated corpora that for ML models more data will always be useful. MedMentions has the added benefit of better coverage of concepts from some biomedical disciplines, e.g. diseases and drugs. This comes from the use of UMLS as the target ontology.\n\nTaggerOne is an established model for biomedical concept recognition and is offered here as a baseline concept recognition model, that researchers developing new models may compare their results against.\n\nSection 3 on related annotated corpora has been expanded slightly. Main differences are the size of the MedMentions benchmark corpus, and through the use of UMLS as the target ontology, more comprehensive coverage of biomedical concepts.""}"		SylxCx5pTQ	B1g-hdECG4	AKBC.ws/2019/Conference/-/Paper7/Official_Comment	['AKBC.ws/2019/Conference/Paper7/Reviewers/Unsubmitted']	4		['everyone']	rkeJBbeEl4	['AKBC.ws/2019/Conference/Paper7/Authors']	1547745448576		1547757386927	['AKBC.ws/2019/Conference/Paper7/Authors', 'AKBC.ws/2019/Conference']
640	1547754579270	{'title': 'Interesting paper, incomplete experiments', 'review': 'This paper proposes to use homomorphic compression for hierarchical coreference resolution. Contributions of the paper are threefold. First, it proposes a homomorphic cosine preserving hashing version of simhash. Secondly, it presents another homomorphic cosine preserving representation based on random projections. Thirdly, the paper proposes a homomorphic version of minhash for Jaccard similarity. The paper applies these representations to the hierarchical coreference resolution problem using CRF. The authors also provide statistical analysis. Experimental results on real-world datasets are also presented. The paper is generally well written.\n\nMy main concern with the paper is that it falls short of comparing with other valid baselines. One of the main points of the papers is that sparse and high-dimensional representations of mentions creates problems during probabilistic inference. In order to overcome this problem, the authors propose various hashing-based methods. However, in light of recent advances in word embeddings, one is not limited to using such sparse and high-dimensional representations. One can potentially use off-the-shelf dense and relatively lower-dimensional embeddings such as word2vec, glove etc, or use context-dependent embeddings such as ELMO. Unfortunately, the paper completely ignores this line of research, and neither compares nor cites them. This is clearly not satisfactory, and the paper is not complete with such vital missing pieces.\n\nOverall, I think the paper proposes some interesting ideas, but it is incomplete in light of the issues above. Moreover, applying the proposed hashed representation on at least one other task will make the paper stronger.', 'rating': '5: Marginally below acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		H1gwRx5T6Q	ryejI2LAMN	AKBC.ws/2019/Conference/-/Paper10/Official_Review	['AKBC.ws/2019/Conference/Paper10/Reviewers/Unsubmitted']	2		['everyone']	H1gwRx5T6Q	['AKBC.ws/2019/Conference/Paper10/AnonReviewer3']	1547754579270		1547754741485	['AKBC.ws/2019/Conference/Paper10/AnonReviewer3', 'AKBC.ws/2019/Conference']
641	1538087889476	{'title': 'Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset', 'abstract': 'Generating musical audio directly with neural networks is notoriously difficult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude (~0.1 ms to ~100 s), a process we call Wave2Midi2Wave. This large advance in the state of the art is enabled by our release of the new MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) dataset, composed of over 172 hours of virtuosic piano performances captured with fine alignment (~3 ms) between note labels and audio waveforms. The networks and the dataset together present a promising approach toward creating new expressive and interpretable neural models of music.', 'keywords': ['music', 'piano transcription', 'transformer', 'wavnet', 'audio synthesis', 'dataset', 'midi'], 'authorids': ['fjord@google.com', 'astas@google.com', 'adarob@google.com', 'iansimon@google.com', 'annahuang@google.com', 'sedielem@google.com', 'eriche@google.com', 'jesseengel@google.com', 'deck@google.com'], 'authors': ['Curtis Hawthorne', 'Andrew Stasyuk', 'Adam Roberts', 'Ian Simon', 'Cheng-Zhi Anna Huang', 'Sander Dieleman', 'Erich Elsen', 'Jesse Engel', 'Douglas Eck'], 'TL;DR': 'We train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure, enabled by the new MAESTRO dataset.', 'pdf': '/pdf/ced843930e16b6829293532407cbae254ab1a5af.pdf', 'paperhash': 'hawthorne|enabling_factorized_piano_music_modeling_and_generation_with_the_maestro_dataset', '_bibtex': '@inproceedings{\nhawthorne2018enabling,\ntitle={Enabling Factorized Piano Music Modeling and Generation with the {MAESTRO} Dataset},\nauthor={Curtis Hawthorne and Andrew Stasyuk and Adam Roberts and Ian Simon and Cheng-Zhi Anna Huang and Sander Dieleman and Erich Elsen and Jesse Engel and Douglas Eck},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1lYRjC9F7},\n}'}		r1lYRjC9F7	r1lYRjC9F7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	917	r1l2yRT9KX	['everyone']		['ICLR.cc/2019/Conference']	1538087889476		1547754151121	['ICLR.cc/2019/Conference']
642	1547753177484	{'title': 'Novel and Interesting', 'review': 'The authors propose to apply adversarial attacks techniques to the task of link prediction for Knowledge Bases - Adversarial Attack on Link Prediction (AALP).\nThey introduce a novel adversarial attack model and show with experiments that it effectively (reducing accuracy) attack the link prediction models. \nAs an application they exploit the attack model to detect incorrect facts in the knowledge base, achieving up to 55% accuracy in identifying errors.\n\nThe work is well thought, novel and supported by experiments.\n\nIn terms of presentation, the authors should make sure experimental data and settings are clearly described and reproducible.', 'rating': '7: Good paper, accept', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		Hkg7rbcp67	rye-yPICGN	AKBC.ws/2019/Conference/-/Paper50/Official_Review	['AKBC.ws/2019/Conference/Paper50/Reviewers/Unsubmitted']	3		['everyone']	Hkg7rbcp67	['AKBC.ws/2019/Conference/Paper50/AnonReviewer3']	1547753177484		1547753382512	['AKBC.ws/2019/Conference/Paper50/AnonReviewer3', 'AKBC.ws/2019/Conference']
643	1540860156566	{'title': 'On the Effectiveness of Minimal Context Selection for Robust Question Answering', 'abstract': 'Machine learning models for question-answering (QA), where given a question and a passage, the learner must select some span in the passage as an answer, are known to be brittle. By inserting a single nuisance sentence into the passage, an adversary can fool the model into selecting the wrong span. A promising new approach for QA decomposes the task into two stages: (i) select relevant sentences from the passage; and (ii) select a span among those sentences. Intuitively, if the sentence selector excludes the offending sentence, then the downstream span selector will be robust. While recent work has hinted at the potential robustness of two-stage QA, these methods have never, to our knowledge, been explicitly combined with adversarial training. This paper offers a thorough empirical investigation of adversarial robustness, demonstrating that although the two-stage approach lags behind single-stage span selection, adversarial training improves its performance significantly, leading to an improvement of over 22 points in F1 score over the adversarially-trained single-stage model.', 'keywords': ['Machine Reading Comprehension', 'Question Answering', 'Robustness', 'Adversarial Training'], 'authorids': ['bvp@cs.cmu.edu', 'alankarj@cs.cmu.edu', 'zlipton@cs.cmu.edu'], 'authors': ['Anonymous'], 'TL;DR': 'A two-stage approach consisting of sentence selection followed by span selection can be made more robust to adversarial attacks in comparison to a single-stage model trained on full context.', 'pdf': '/pdf/2936b2e49f578d0a909b3306f83140a864dfea27.pdf', 'paperhash': 'anonymous|on_the_effectiveness_of_minimal_context_selection_for_robust_question_answering'}		SJeB-KQHnm	SJeB-KQHnm	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	67	Bkx4ZYXrhQ	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540860156566		1547748642982	['NIPS.cc/2018/Workshop/IRASL']
644	1540239377467	{'title': 'Improved Speech Enhancement with the Wave-U-Net', 'abstract': 'We study the use of the Wave-U-Net architecture for speech enhancement, a model introduced by Stoller et al for the separation of music vocals and accompaniment.  This end-to-end learning method for audio source separation operates directly in the time domain, permitting the integrated modelling of phase information and being able to take large  temporal contexts into account.  Our experiments show that the proposed method improves several metrics, namely PESQ, CSIG, CBAK, COVL and SSNR, over the  state-of-the-art with respect to the speech enhancement task on the Voice Bank corpus (VCTK) dataset. We find that a reduced number of hidden layers is sufficient for speech enhancement in comparison to the original system designed for singing voice separation in music. We see this initial result as an encouraging signal to further explore speech enhancement in the time-domain, both as an end in itself and as a pre-processing step to speech recognition systems. ', 'authorids': ['craig.macartney@city.ac.uk', 't.e.weyde@city.ac.uk'], 'authors': ['Anonymous'], 'keywords': ['speech enhancement', 'deep learning', 'U-Net', 'convolutional neural networks'], 'paperhash': 'anonymous|improved_speech_enhancement_with_the_waveunet', 'pdf': '/pdf/9bd38a155f7f00395d2635333a8eacfa91ba7715.pdf', 'TL;DR': 'The Wave-U-Net architecture, recently introduced by Stoller et al for music source separation, is highly effective for speech enhancement, beating the state of the art.'}		B1zKGg3soX	B1zKGg3soX	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	42	rkltfehos7	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540239377467		1547748642773	['NIPS.cc/2018/Workshop/IRASL']
645	1537762806666	{'keywords': ['Captioning', 'Language Generation', 'Modularized Method', 'Inverse of Language Parsing'], 'authorids': ['doubledaibo@gmail.com'], 'title': 'A Neural Compositional Paradigm for Image Captioning', 'TL;DR': 'a hierarchical and compositional way to generate captions', 'abstract': 'Mainstream captioning models often follow a sequential structure to generate cap-\ntions, leading to issues such as introduction of irrelevant semantics, lack of diversity\nin the generated captions, and inadequate generalization performance. In this paper,\nwe present an alternative paradigm for image captioning, which factorizes the\ncaptioning procedure into two stages: (1) extracting an explicit semantic represen-\ntation from the given image; and (2) constructing the caption based on a recursive\ncompositional procedure in a bottom-up manner. Compared to conventional ones,\nour paradigm better preserves the semantic content through an explicit factorization\nof semantics and syntax. By using the compositional generation procedure, caption\nconstruction follows a recursive structure, which naturally fits the properties of\nhuman language. Moreover, the proposed compositional procedure requires less\ndata to train, generalizes better, and yields more diverse captions.', 'authors': ['Anonymous'], 'paperhash': 'anonymous|a_neural_compositional_paradigm_for_image_captioning', 'pdf': '/pdf/169cb7521273262f0a56c264a0d0a1d31b6270fe.pdf'}		SJxyZ81IYQ	SJxyZ81IYQ	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	3	HkxAlIyUK7	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1537762806666		1547748642560	['NIPS.cc/2018/Workshop/IRASL']
646	1538451996478	"{'keywords': ['speech', 'noise', 'invariance', 'representations', 'regularization', 'librispeech'], 'authorids': ['davisblaine.liang@gmail.com', 'zlipton@cmu.edu'], 'title': 'Learning Noise-Invariant Representations for Robust Speech Recognition', 'TL;DR': ' In this paper, we hypothesize that superficially perturbed data points shouldn’t merely map to the same class---they should map to the same representation.', 'abstract': ""Despite rapid advances in speech recognition, current models remain brittle to superficial perturbations to their inputs. Small amounts of noise can destroy the performance of an otherwise state-of-the-art model. To harden models against background noise, practitioners often perform data augmentation, adding artificially-noised examples to the training set, carrying over the original label. In this paper, we hypothesize that a clean example and its superficially perturbed counterparts shouldn't merely map to the same class--- they should map to the same representation. We propose invariant-representation-learning (IRL): At each training iteration, for each training example, we sample a noisy counterpart. We then apply a penalty term to coerce matched representations at each layer (above some chosen layer). Our key results, demonstrated on the LibriSpeech dataset are the following: (i) IRL significantly reduces character error rates (CER)on both `clean' (3.3% vs 6.5%) and `other' (11.0% vs 18.1%) test sets; (ii) on several out-of-domain noise settings (different from those seen during training), IRL's benefits are even more pronounced. Careful ablations confirm that our results are not simply due to shrinking activations at the chosen layers. "", 'authors': ['Anonymous'], 'pdf': '/pdf/de585e1fcc253488ca6306a1459d68b4ca62c4e1.pdf', 'paperhash': 'anonymous|learning_noiseinvariant_representations_for_robust_speech_recognition'}"		ryz4mqPx9m	ryz4mqPx9m	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	4	SJgN79wlcX	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1538451996478		1547748642346	['NIPS.cc/2018/Workshop/IRASL']
647	1540248377149	{'title': 'Improving label efficiency through multi-task learning on auditory data', 'abstract': 'Collecting high-quality, large scale datasets typically requires significant resources. The aim of the present work is to improve the label efficiency of large neural networks operating on audio data through multitask learning with self-supervised tasks on unlabeled data. To this end, we trained an end-to-end audio feature extractor based on WaveNet that feeds into simple, yet versatile task-specific neural networks. We describe three self-supervised learning tasks that can operate on any large, unlabeled audio corpus. We demonstrate that, in a scenario with limited labeled training data, one can significantly improve the performance of a supervised classification task by simultaneously training it with these additional self-supervised tasks. We show that one can improve performance on a diverse sound events classification task by nearly 6\\% when jointly trained with up to three distinct self-supervised tasks. This improvement scales with the number of additional auxiliary tasks as well as the amount of unsupervised data. We also show that incorporating data augmentation into our multitask setting leads to even further gains in performance.', 'TL;DR': 'Improving label efficiency through multi-task learning on auditory data', 'authorids': ['anthony.ndirango@intel.com', 'tyler.p.lee@intel.com', 'ting.gong@intel.com', 'suchismita.padhy@intel.com'], 'authors': ['Anonymous'], 'keywords': ['multitask learning', 'self-supervised learning', 'end-to-end audio classification'], 'pdf': '/pdf/69643e4dd00f80400be6075e6539d6c0a2bdc38f.pdf', 'paperhash': 'anonymous|improving_label_efficiency_through_multitask_learning_on_auditory_data'}		ryl-BQRisQ	ryl-BQRisQ	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	52	rkllHQ0isX	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540248377149		1547748642128	['NIPS.cc/2018/Workshop/IRASL']
648	1540225724451	{'title': 'A Fully Time-domain Neural Model for Subband-based Speech Synthesizer', 'abstract': '—This  paper  introduces  a  deep  neural  network  model for  subband-based  speech  synthesizer.  The  model  benefits  from the  short  bandwidth  of  the  subband  signals  to  reduce  the complexity  of  the time-domain speech generator. We employed the multi-level  wavelet  analysis/synthesis  to  decompose/reconstruct the  signal  to  subbands  in  time  domain.  Inspired  from  the WaveNet,  a  convolutional  neural  network  (CNN)  model  predicts subband  speech  signals  fully  in  time  domain.  Due  to  the  short bandwidth  of  the  subbands,  a  simple  network  architecture  is enough  to  train  the  simple  patterns  of  the  subbands  accurately.  In the  ground  truth  experiments  with  teacher  forcing,  the  subband synthesizer  outperforms  the  fullband  model  significantly.  In addition,  by  conditioning  the  model  on  the  phoneme  sequence using  a  pronunciation  dictionary,  we  have  achieved  the  first  fully time-domain  neural  text-to-speech  (TTS)  system.  The  generated speech  of  the  subband  TTS  shows  comparable  quality  as  the fullband  one  with  a  slighter  network  architecture  for  each subband.', 'authorids': ['azrabiee@kaist.ac.kr', 'ken.geonmin.kim@gmail.com', 'ktho894@gmail.com', 'sy-lee@kaist.ac.kr'], 'authors': ['Anonymous'], 'keywords': ['Deep  learning', 'speech  synthesis', 'text-to-speech', 'wavelet  transforms', 'WaveNet'], 'paperhash': 'anonymous|a_fully_timedomain_neural_model_for_subbandbased_speech_synthesizer', 'pdf': '/pdf/41f217789ff4c83a107204f06f6c9774f3df535a.pdf'}		r1GEa5uoim	r1GEa5uoim	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	32	rklEpcuoiQ	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540225724451		1547748641915	['NIPS.cc/2018/Workshop/IRASL']
649	1540239865523	{'title': 'Adversarial Gain', 'abstract': 'Adversarial examples can be defined as inputs to a model which induce a mistake -- where the model output is different than that of an oracle, perhaps in surprising or malicious ways. Original models of adversarial attacks are primarily studied in the context of classification and computer vision tasks. While several attacks have been proposed in natural language processing (NLP) settings, they often vary in defining the parameters of an attack and what a successful attack would look like. The goal of this work is to propose a unifying model of adversarial examples suitable for NLP tasks in both generative and classification settings. We define the notion of adversarial gain: based in control theory, it is a measure of the change in the output of a system relative to the perturbation of the input (caused by the so-called adversary) presented to the learner. This definition, as we show, can be used under different feature spaces and distance conditions to determine attack or defense effectiveness across different intuitive manifolds. This notion of adversarial gain not only provides a useful way for evaluating adversaries and defenses, but can act as a building block for future work in robustness under adversaries due to its rooted nature in stability and manifold theory.', 'TL;DR': 'We propose an alternative measure for determining effectiveness of adversarial attacks in NLP models according to a distance measure-based method like incremental L2-gain in control theory.', 'authorids': ['peter.henderson@mail.mcgill.ca', 'rosemary.nan.ke@gmail.com', 'koustuv.sinha@mail.mcgill.ca', 'jpineau@cs.mcgill.ca'], 'authors': ['Anonymous'], 'keywords': ['adversarial attacks', 'gain', 'stability', 'robustness', 'natural language processing'], 'pdf': '/pdf/8efb8bb225eaba0cd8ef8a3eb6a02ef2f71868d2.pdf', 'paperhash': 'anonymous|adversarial_gain'}		HkgGWM3som	HkgGWM3som	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	43	HklWZzhoiX	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540239865523		1547748641702	['NIPS.cc/2018/Workshop/IRASL']
650	1540242292107	{'title': 'On the Confidence of Neural Network Predictions for some NLP Tasks', 'abstract': 'Neural networks are known to produce unexpected results on inputs that are far from the training distribution. One approach to tackle this problem is to detect the samples on which the trained network can not answer reliably. ODIN is a recently proposed method for out-of-distribution detection that does not modify the trained network and achieves good performance for various image classification tasks. In this paper we adapt ODIN for sentence classification and word tagging tasks. We show that the scores produced by ODIN can be used as a confidence measure for the predictions on both in-distribution and out-of-distribution datasets.', 'TL;DR': 'A recent out-of-distribution detection method helps to measure the confidence of RNN predictions for some NLP tasks', 'authorids': ['mahnerak@yerevann.com', 'hrant@yerevann.com'], 'authors': ['Anonymous'], 'keywords': ['nlp', 'confidence', 'out-of-distribution detection', 'odin', 'rnn', 'sentiment analysis', 'POS tagging'], 'paperhash': 'anonymous|on_the_confidence_of_neural_network_predictions_for_some_nlp_tasks', 'pdf': '/pdf/de4cf8d671b8210da97770088810af983a6aa2c1.pdf'}		HJf2ds2ssm	HJf2ds2ssm	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	45	Bkl2uonjjQ	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540242292107		1547748641493	['NIPS.cc/2018/Workshop/IRASL']
651	1540325384545	{'title': 'Word Sense Induction using Knowledge Embeddings', 'abstract': 'Word Embeddings are able to capture lexico-semantic information but remain flawed in their inability to assign unique representations to different senses of a polysemous words. They also fail to include information from well curated semantic lexicons and dictionaries. Previous approaches that integrate polysemy and knowledge bases fall distinctly under two categories - retrofitting vectors to ontologies or learning from sense tagged corpora. While these embeddings are superior in understanding contextual similarity, they are outperformed by single prototype word vectors on several relatedness tasks. \nIn this work, we introduce a new approach that can induce polysemy to any pre-trained embedding space by jointly grounding contextualized sense representations with word embeddings to an ontology and a thesaurus. Along with word sense induction, the resulting representations reduces the effect of vocabulary bias that arises in natural language corpora and in turn embedding spaces. By grounding them to knowledge bases  they are able to learn multi-word representations and are also interpretable. We evaluate our vectors across 12 datasets on several similarity and relatedness tasks along with two extrinsic tasks and find that our approach consistently outperforms current state of the art.', 'keywords': ['ontology grounding', 'sense representations', 'knowledge bases'], 'authorids': ['sanjanaramprasad1@gmail.com', 'james.maddox@hiremya.com'], 'authors': ['Anonymous'], 'paperhash': 'anonymous|word_sense_induction_using_knowledge_embeddings', 'pdf': '/pdf/1475175da4a6bc451be467c13feaf96e4c2384b1.pdf'}		rkl-GeWasQ	rkl-GeWasQ	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	60	Syeezxbpi7	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540325384545		1547748641280	['NIPS.cc/2018/Workshop/IRASL']
652	1540363250531	{'title': 'Adversarial Machine Learning And Speech Emotion Recognition: Utilizing Generative Adversarial Networks For Robustness', 'abstract': ' Although deep learning has enabled unprecedented improvements in the performance of the state-of-the-art speech emotion recognition (SER) systems, recent research on adversarial examples has cast a shadow of doubt on the robustness of SER systems by showing the susceptibility of deep neural networks to adversarial examples that rely only on small and imperceptible perturbations. In this study, we evaluate how adversarial examples can be used to attack SER systems and propose the first black-box adversarial attack on SER systems. We also explore potential defenses including adversarial training and generative adversarial network (GAN) to enhance robustness. Experimental evaluations suggest various interesting aspects of the effective utilization of adversarial examples that can be useful not only for SER robustness but also other speech-based intelligent systems.', 'keywords': ['deep neural networks', 'adversarial examples', 'speech emotion recognition'], 'authorids': ['siddique.latif@itu.edu.pk', 'rajib.rana@usq.edu.au', 'junaid.qadir@itu.edu.pk'], 'authors': ['Anonymous'], 'paperhash': 'anonymous|adversarial_machine_learning_and_speech_emotion_recognition_utilizing_generative_adversarial_networks_for_robustness', 'pdf': '/pdf/3f05f800aed8f74edbb7f81d5d18484f5b524826.pdf'}		BkeoxN5aiX	BkeoxN5aiX	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	63	B1ecgNcpjQ	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540363250531		1547748641063	['NIPS.cc/2018/Workshop/IRASL']
653	1540194383614	{'title': 'Multi-Domain Processing via Hybrid Denoising Networks for Speech Enhancement', 'abstract': 'We present a hybrid framework that leverages the trade-off between temporal and frequency precision in audio representations to improve the performance of speech enhancement task. We first show that conventional approaches using specific representations such as raw-audio and spectrograms are each effective at targeting different types of noise.\nBy integrating both approaches, our model can learn multi-scale and multi-domain features, effectively removing noise existing on different regions on the time-frequency space in a complementary way. Experimental results show that the proposed hybrid model yields better performance and robustness than using each model individually.', 'TL;DR': 'A hybrid model utilizing both raw-audio and spectrogram information for speech enhancement tasks.', 'authorids': ['blue378@snu.ac.kr', 'jaejun.yoo@navercorp.com', 'sanghyuk.c@navercorp.com', 'adrian.kim@navercorp.com', 'jungwoo.ha@navercorp.com'], 'authors': ['Anonymous'], 'keywords': ['speech enhancement', 'multi-scale analysis', 'time-frequency domain', 'hybrid network'], 'paperhash': 'anonymous|multidomain_processing_via_hybrid_denoising_networks_for_speech_enhancement', 'pdf': '/pdf/088134d7d7262d1afad9a96e240bc667327332f4.pdf'}		B1xOLgWijQ	B1xOLgWijQ	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	19	BklPIeboom	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540194383614		1547748640853	['NIPS.cc/2018/Workshop/IRASL']
654	1540229679963	{'title': 'Autoencoding Documents for Topic Modeling with L-2 Sparsity Regularization', 'abstract': 'We propose a novel yet simple neural network architecture for topic modelling. The method is based on training an autoencoder structure where the bottleneck represents the space of the topics distribution and the decoder outputs represent the space of the words distributions over the topics. We exploit an auxiliary decoder to prevent mode collapsing in our model.  A key feature for an effective topic modelling method is having sparse topics and words distributions, where there is a trade-off between the sparsity level of topics and words. This feature is implemented in our model by L-2 regularization and the model hyperparameters take care of the trade-off.  We show in our experiments that our model achieves competitive results compared to the state-of-the-art deep models for topic modelling, despite its simple architecture and training procedure. The “New York Times” and “20 Newsgroups” datasets are used in the experiments.\n\n', 'TL;DR': 'A deep model for topic modelling', 'authorids': ['sbanijam@uwaterloo.ca', 'wei.zhou1@huawei.com', 'wei.li.crc@huawei.com', 'aghodsib@uwaterloo.ca'], 'authors': ['Anonymous'], 'keywords': [], 'paperhash': 'anonymous|autoencoding_documents_for_topic_modeling_with_l2_sparsity_regularization', 'pdf': '/pdf/e3f079cdc24dd483b3db89b7f133d6a4f6bce810.pdf'}		BJluV5tjiQ	BJluV5tjiQ	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	34	H1xDEqYssQ	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540229679963		1547748640641	['NIPS.cc/2018/Workshop/IRASL']
655	1540218513717	{'title': 'ROBUST SPEECH COMMAND RECOGNITION USING LABEL-DRIVEN TIME-FREQUENCY MASKING', 'abstract': 'Speech enhancement driven robust Automatic Speech Recognition (ASR) systems typically require parallel corpus with noisy and clean speech utterances for training. Moreover, many studies have reported that such front-ends, even though improve speech quality, do not always improve the recognition performance. On the other hand, the multi-condition training of ASR systems have little visualization or interpretability capabilities of how these systems achieve robustness. In this paper, we propose a novel neural architecture with unified enhancement and sequence classification block, that is trained in an end-to-end manner only using noisy speech without having information of clean speech. The enhancement block is a fully convolutional network that is designed to perform Time Frequency (T-F) masking like operation, followed by an LSTM sequence classification block. The T-F masking formulation enables visualization of learned mask and helps us to visualize the T-F points important for classification of a speech command. Experiments performed on Google Speech Command dataset show that our proposed network achieves better results than the baseline model without an enhancement front-end.', 'authorids': ['meet.soni@tcs.com', 'imran.as@tcs.com', 'sunilkumar.kopparapu@tcs.com'], 'authors': ['Anonymous'], 'keywords': [], 'paperhash': 'anonymous|robust_speech_command_recognition_using_labeldriven_timefrequency_masking', 'pdf': '/pdf/beb287b1ba255073456c00faf74dd0ba33cf1458.pdf'}		SJl990Ussm	SJl990Ussm	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	23	SklF5AUos7	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540218513717		1547748640430	['NIPS.cc/2018/Workshop/IRASL']
656	1540190789045	{'title': 'Targeted Adversarial Examples for Black Box Audio Systems', 'abstract': 'The application of deep recurrent networks to audio transcription has led to impressive gains in automatic speech recognition (ASR) systems. Many have demonstrated that small adversarial perturbations can fool deep neural networks into incorrectly predicting a specified target with high confidence. Current work on fooling ASR systems have focused on white-box attacks, in which the model architecture and parameters are known. In this paper, we adopt a black-box approach to adversarial generation, combining the approaches of both genetic algorithms and gradient estimation to solve the task. We achieve a 89.25% targeted attack similarity after 3000 generations while maintaining 94.6% audio file similarity.', 'TL;DR': 'We present a novel black-box targeted attack that is able to fool state of the art speech to text transcription.', 'authorids': ['rohantaori@berkeley.edu', 'amogkamsetty@berkeley.edu', 'brentonlongchu@berkeley.edu', 'nikitavemuri@berkeley.edu'], 'authors': ['Anonymous'], 'keywords': ['adversarial attack', 'deep learning', 'adversarial examples', 'audio processing', 'speech to text', 'adversarial audio', 'black box', 'machine learning'], 'pdf': '/pdf/80217ba9ffa8e7513c0658abfd19e1639690576e.pdf', 'paperhash': 'anonymous|targeted_adversarial_examples_for_black_box_audio_systems'}		HklaBGxoo7	HklaBGxoo7	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	16	H1g3rMgoim	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540190789045		1547748640214	['NIPS.cc/2018/Workshop/IRASL']
657	1540191440722	"{'title': 'Learning pronunciation from a foreign language in speech synthesis networks', 'abstract': ""Although there are more than 65,000 languages in the world, the pronunciations of many phonemes sound similar across the languages. When people learn a foreign language, their pronunciation often reflect their native language's characteristics. That motivates us to investigate how the speech synthesis network learns the pronunciation when multi-lingual dataset is given. In this study, we train the speech synthesis network bilingually in English and Korean, and analyze how the network learns the relations of phoneme pronunciation between the languages. Our experimental result shows that the learned phoneme embedding vectors are located closer if their pronunciations are similar across the languages. Based on the result, we also show that it is possible to train networks that synthesize English speaker's Korean speech and vice versa. In another experiment, we train the network with limited amount of English dataset and large Korean dataset, and analyze the required amount of dataset to train a resource-poor language with the help of resource-rich languages. "", 'TL;DR': 'Learned phoneme embeddings of multilingual neural speech synthesis network could represent relations of phoneme pronunciation between the languages.', 'authorids': ['yg@neosapience.com', 'taesu@neosapience.com'], 'authors': ['Anonymous'], 'keywords': ['multilingual data', 'phoneme representation', 'text-to-speech', 'speech synthesis'], 'paperhash': 'anonymous|learning_pronunciation_from_a_foreign_language_in_speech_synthesis_networks', 'pdf': '/pdf/eccb472ed6e3447a7b2bb1d0ca30ba593d50b7bb.pdf'}"		HkeYCNgooQ	HkeYCNgooQ	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	17	S1x_AVxjs7	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540191440722		1547748639991	['NIPS.cc/2018/Workshop/IRASL']
658	1540250251628	"{'title': 'Investigation of using disentangled and interpretable representations with language conditioning for cross-lingual voice conversion', 'abstract': ""We study the problem of cross-lingual voice conversion in non-parallel speech corpora and one-shot learning setting. Most prior work require either parallel speech corpora or enough amount of training data from a target speaker. However, we convert an arbitrary sentences of an arbitrary source speaker to target speaker's given only one target speaker training utterance. To achieve this, we formulate the problem as learning disentangled speaker-specific and context-specific representations and follow the idea of [1] which uses Factorized Hierarchical Variational Autoencoder (FHVAE). After training FHVAE on multi-speaker training data, given arbitrary source and target speakers' utterance, we estimate those latent representations and then reconstruct the desired utterance of converted voice to that of target speaker. We use multi-language speech corpus to learn a universal model that works for all of the languages. We investigate the use of a one-hot language embedding to condition the model on the language of the utterance being queried and show the effectiveness of the approach. We conduct voice conversion experiments with varying size of training utterances and it was able to achieve reasonable performance with even just one training utterance. We also investigate the effect of using or not using the language conditioning. Furthermore, we visualize the embeddings of the different languages and sexes. Finally, in the subjective tests, for one language and cross-lingual voice conversion, our approach achieved moderately better or comparable results compared to the baseline in speech quality and similarity."", 'TL;DR': 'We use a Variational Autoencoder to separate style and content, and achieve voice conversion by modifying style embedding and decoding. We investigate using a multi-language speech corpus and investigate its effects.', 'authorids': ['hamid@oben.com', 'taehwan@oben.com'], 'authors': ['Anonymous'], 'keywords': ['voice conversion', 'one-shot learning', 'cross-lingual', 'variational autoencoder'], 'paperhash': 'anonymous|investigation_of_using_disentangled_and_interpretable_representations_with_language_conditioning_for_crosslingual_voice_conversion', 'pdf': '/pdf/d3a151ccf7f6b87afc223579a25e2665ed8fdf1c.pdf'}"		S1eN99RioX	S1eN99RioX	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	56	H1gmc5RjiQ	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540250251628		1547748639780	['NIPS.cc/2018/Workshop/IRASL']
659	1540230407242	{'title': 'A Gray Box Interpretable Visual Debugging Approach for Deep Sequence Learning Model', 'abstract': 'Deep Learning algorithms are often used as black box type learning and they are too complex to understand. The widespread usability of Deep Learning algorithms to solve various machine learning problems demands deep and transparent understanding of the internal representation as well as decision making. Moreover, the learning models, trained on sequential data, such as audio and video data, have intricate internal reasoning process due to their complex distribution of features. Thus, a visual simulator might be helpful to trace the internal decision making mechanisms in response to adversarial input data, and it would help to debug and design appropriate deep learning models. However, interpreting the internal reasoning of deep learning model is not well studied in the literature. In this work, we have developed a visual interactive web application, namely d-DeVIS, which helps to visualize the internal reasoning of the learning model which is trained on the audio data. The proposed system allows to perceive the behavior as well as to debug the model by interactively generating adversarial audio data point. The web application of d-DeVIS is available at ddevis.herokuapp.com.', 'authorids': ['akash.cse.du@gmail.com', 'amar.csedu@gmail.com', 'tahsinalsayeed@gmail.com', 'jyotirmaynag.cse@gmail.com', 'mrr.rana13@gmail.com', 'sakib.csedu21@gmail.com', 'razzaque@du.ac.bd', 'mosaddek@cse.univdhaka.edu', 'swakkhar@cse.uiu.ac.bd'], 'authors': ['Anonymous'], 'keywords': ['Interpretable machine learning', 'Deep Learning', 'Gray Box', 'Adversarial Examples'], 'pdf': '/pdf/faa4b79f04fbd15824495e65a4aff731a68f2978.pdf', 'paperhash': 'anonymous|a_gray_box_interpretable_visual_debugging_approach_for_deep_sequence_learning_model'}		BJgkMaYism	BJgkMaYism	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	35	HkxyGaFoi7	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540230407242		1547748639564	['NIPS.cc/2018/Workshop/IRASL']
660	1540199089570	{'title': 'Natural Language Detectors Emerge in Individual Neurons', 'abstract': 'Although deep convolutional networks have achieved improved performance in many natural language tasks, they have been treated as black boxes because they are difficult to interpret. Especially, little is known about how they represent language in their intermediate layers. In an attempt to understand the representations of deep convolutional networks trained on language tasks, we show that individual units are selectively responsive to specific morphemes, words, and phrases, rather than responding to arbitrary and uninterpretable patterns. In order to quantitatively analyze such intriguing phenomenon, we propose a concept alignment method based on how units respond to replicated text. We conduct analyses with different architectures on multiple datasets for classification and translation tasks and provide new insights into how deep models understand natural language.', 'TL;DR': 'We show that individual units in CNN representations learned in NLP tasks are selectively responsive to specific natural language concepts.', 'authorids': ['seil.na@vision.snu.ac.kr', 'yj.c@kakaocorp.com', 'benjamin.lee@kakaobrain.com', 'gunhee@snu.ac.kr'], 'authors': ['Anonymous'], 'keywords': ['interpretability of deep neural networks', 'natural language representation'], 'paperhash': 'anonymous|natural_language_detectors_emerge_in_individual_neurons', 'pdf': '/pdf/047c296ad21b3e2a147f777865849a60c94aba89.pdf'}		BJec2Mfosm	BJec2Mfosm	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	21	B1eFnGfoj7	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540199089570		1547748639340	['NIPS.cc/2018/Workshop/IRASL']
661	1540249115341	{'title': 'Training Neural Speech Recognition Systems with Synthetic Speech Augmentation', 'abstract': 'Building an accurate automatic speech recognition (ASR) system requires a large dataset that contains many hours of labeled speech samples produced by a diverse set of speakers. The lack of such open free datasets is one of the main issues preventing advancements in ASR research. To address this problem, we propose to augment a natural speech dataset with synthetic speech. We train very large end-to-end neural speech recognition models using the LibriSpeech dataset augmented with synthetic speech. These new models achieve state of the art Word Error Rate (WER) for character-level based models without an external language model.', 'authorids': ['jasoli@nvidia.com', 'rgadde@nvidia.com', 'bginsburg@nvidia.com', 'vlavrukhin@nvidia.com'], 'authors': ['Anonymous'], 'keywords': [], 'paperhash': 'anonymous|training_neural_speech_recognition_systems_with_synthetic_speech_augmentation', 'pdf': '/pdf/004b0de1b5f76506dbe1e385feb36867c9b79376.pdf'}		HyxmQ8Coo7	HyxmQ8Coo7	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	53	ryxXQLRsjQ	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540249115341		1547748639120	['NIPS.cc/2018/Workshop/IRASL']
662	1540197028171	{'title': 'On Zero-shot Cross-lingual Transfer of Multilingual Neural Machine Translation', 'abstract': 'Transferring representations from large-scale supervised tasks to downstream tasks have shown outstanding results in Machine Learning in both Computer Vision and natural language processing (NLP). One particular example can be sequence-to-sequence models for Machine Translation (Neural Machine Translation - NMT). It is because, once trained in a multilingual setup, NMT systems can translate between multiple languages and are also capable of performing zero-shot translation between unseen source-target pairs at test time. In this paper, we first investigate if we can extend the zero-shot transfer capability of multilingual NMT systems to cross-lingual NLP tasks (tasks other than MT, e.g. sentiment classification and natural language inference). We demonstrate a simple framework by reusing the encoder from a multilingual NMT system, a multilingual Encoder-Classifier, achieves remarkable zero-shot cross-lingual classification performance, almost out-of-the-box on three downstream benchmark tasks - Amazon Reviews, Stanford sentiment treebank (SST) and Stanford natural language inference (SNLI). In order to understand the underlying factors contributing to this finding, we conducted a series of analyses on the effect of the shared vocabulary, the training data type for NMT models, classifier complexity, encoder representation power, and model generalization on zero-shot performance. Our results provide strong evidence that the representations learned from multilingual NMT systems are widely applicable across languages and tasks, and the high, out-of-the-box classification performance is correlated with the generalization capability of such systems.', 'authorids': ['erigchi@gmail.com', 'melvinp@google.com', 'orhanf@google.com', 'kazawa@google.com', 'wmach@google.com'], 'authors': ['Anonymous'], 'keywords': ['Multilingual Neural Machine Translation', 'Zero-shot Cross-lingual Classification'], 'paperhash': 'anonymous|on_zeroshot_crosslingual_transfer_of_multilingual_neural_machine_translation', 'TL;DR': 'Zero-shot cross-lingual transfer by using multilingual neural machine translation ', 'pdf': '/pdf/94a1f8b9549041ed9388f7a429bc5f6c40c25e53.pdf'}		H1gni9-ojX	H1gni9-ojX	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	20	rkeso9Wsi7	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540197028171		1547748638904	['NIPS.cc/2018/Workshop/IRASL']
663	1540194175493	{'title': 'Variational learning across domains with triplet information', 'abstract': 'The work investigates deep generative models, which allow us to use training data from one domain to build a model for another domain. We propose the Variational Bi-domain Triplet Autoencoder (VBTA) that learns a joint distribution of objects from different domains. We extend the VBTAs objective function by the relative constraints or triplets that sampled from the shared latent space across domains. In other words, we combine the deep generative models with a  metric learning ideas in order to improve the final objective with the triplets information. The performance of the VBTA model is demonstrated on different tasks: image-to-image translation, bi-directional image generation and cross-lingual document classification.', 'authorids': ['rita.kuznetsova@phystech.edu', 'bakhteev@phystech.edu', 'ogaltsov@ap-team.ru', 'safin@ap-team.ru'], 'authors': ['Anonymous'], 'keywords': [], 'paperhash': 'anonymous|variational_learning_across_domains_with_triplet_information', 'pdf': '/pdf/21b5e5c8b7a45e67edc2e0ddd96fac53ea2131ab.pdf'}		ryfwFJWioX	ryfwFJWioX	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	18	HJewty-ssX	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540194175493		1547748638691	['NIPS.cc/2018/Workshop/IRASL']
664	1540249130636	{'title': 'Learned in Speech Recognition: Contextual Acoustic Word Embeddings', 'abstract': 'End-to-end acoustic-to-word speech recognition models have recently gained popularity because they are easy to train, scale well to large amounts of training data, and do not require a lexicon. In addition, word models may also be easier to integrate with downstream tasks such as spoken language understanding, because inference (search) is much simplified compared to phoneme, character or any other sort of sub-word units. In this paper, we describe methods to construct contextual acoustic word embeddings directly from a supervised sequence-to-sequence acoustic-to-word speech recognition model using the learned attention distribution. On a suite of 16 standard sentence evaluation tasks, our embeddings show competitive performance against a word2vec model trained on the speech transcriptions. In addition, we evaluate these embeddings on a spoken language understanding task and observe that our embeddings match the performance of text-based embeddings in a pipeline of first performing speech recognition and then constructing word embeddings from transcriptions.', 'authorids': ['spalaska@cs.cmu.edu', 'vraunak@cs.cmu.edu', 'fmetze@cs.cmu.edu'], 'authors': ['Anonymous'], 'keywords': ['acoustic word embeddings', 'contextual embeddings', 'attention', 'acoustic-to-word speech recognition'], 'paperhash': 'anonymous|learned_in_speech_recognition_contextual_acoustic_word_embeddings', 'TL;DR': 'Methods to learn contextual acoustic word embeddings from an end-to-end speech recognition model that perform competitively with text-based word embeddings.', 'pdf': '/pdf/797774e91b8d87d51b9a126d164a5fe26a8a7210.pdf'}		SJlmNI0ojQ	SJlmNI0ojQ	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	54	HJlfVURisQ	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540249130636		1547748638474	['NIPS.cc/2018/Workshop/IRASL']
665	1540176383661	{'title': 'Boosting pathology detection in infants by deep transfer learning from adult speech', 'abstract': 'Can knowledge extracted from adult speech help improve the performance models developed for infant cry? This work investigates this question in the context of pathology detection in newborns. The analysis of infant crying patterns to detect pathology is of interest as it opens the possibility of more accessible diagnostic tools in resource-constrained settings. Classical machine learning approaches leveraging features extracted as Mel frequency cepstral coefficients, have supported the viability of the infant cry as a diagnostic input, but performance is not yet at a level of clinical utility. The application of deep learning models has been limited due to the unavailability of large infant cry databases which are costly to acquire. This work argues that the transfer of useful knowledge from adult speech is possible because it is driven by the same underlying physiologic process as that of infants. Our experiments demonstrate that on the task of predicting perinatal asphyxia from infant cry, such transfer learning provides an overall improvement of 13.5\\% in F1 score over a model trained from random initialization.', 'authorids': ['onucharles@gmail.com', 'gautam.b.85@gmail.com', 'dprecup@cs.mcgill.ca'], 'authors': ['Anonymous'], 'keywords': ['transfer learning', 'infant cry', 'keyword spotting', 'pathology detection', 'MFCC'], 'paperhash': 'anonymous|boosting_pathology_detection_in_infants_by_deep_transfer_learning_from_adult_speech', 'pdf': '/pdf/2946c02605253f50c615537eac9be15fe55e2453.pdf'}		Hkg_-53cj7	Hkg_-53cj7	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	14	rylDbqh5j7	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540176383661		1547748638243	['NIPS.cc/2018/Workshop/IRASL']
666	1539730064582	{'title': 'Are You Sure YouWant To Do That? Classification with Interpretable Queries', 'abstract': 'Classification systems typically act in isolation, meaning they are required to implicitly memorize the characteristics of all candidate classes in order to classify. The cost of this is increased memory usage and poor sample efficiency. We propose a model which instead verifies using reference images during the classification process, reducing the burden of memorization. The model uses iterative non-differentiable queries in order to classify an image. We demonstrate that such a model is feasible to train and can match baseline accuracy while being more parameter efficient. However, we show that finding the correct balance between image recognition and verification is essential to pushing the model towards desired behavior, suggesting that a pipeline of recognition followed by verification is a more promising approach towards designing more powerful networks with simpler architectures.', 'TL;DR': 'Image classification via iteratively querying for reference image from a candidate class with a RNN and use CNN to compare to the input image', 'authorids': ['hchan@cs.toronto.edu', 'atef@cs.toronto.edu', 'shenkev@cs.toronto.edu'], 'authors': ['Anonymous'], 'keywords': ['Image Classification', 'Knowledge Base', 'Verification', 'Query model', 'Gumbel-Softmax', 'Non-differentiable Model', 'Few Shot learning'], 'pdf': '/pdf/280901513b2adfba84b1b37ac620dfa4ef4c65d6.pdf', 'paperhash': 'anonymous|are_you_sure_youwant_to_do_that_classification_with_interpretable_queries'}		HygF59JVo7	HygF59JVo7	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	9	B1gO5ck4j7	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1539730064582		1547748638012	['NIPS.cc/2018/Workshop/IRASL']
667	1540218716789	{'title': 'Variational Autoencoders with implicit priors for short-duration text-independent speaker verification', 'abstract': 'In this work, we exploited different strategies to provide prior knowledge to commonly used generative modeling approaches aiming to obtain speaker-dependent low dimensional representations from short-duration segments of speech data, making use of available information of speaker identities. Namely, convolutional variational autoencoders are employed, and statistics of its learned posterior distribution are used as low dimensional representations of fixed length short-duration utterances. In order to enforce speaker dependency in the latent layer, we introduced a variation of the commonly used prior within the variational autoencoders framework, i.e. the model is simultaneously trained for reconstruction of inputs along with a discriminative task performed on top of latent layers outputs. The effectiveness of both triplet loss minimization and speaker recognition are evaluated as implicit priors on the challenging cross-language NIST SRE 2016 setting and compared against fully supervised and unsupervised baselines.', 'TL;DR': 'We evaluate the effectiveness of having auxiliary discriminative tasks performed on top of statistics of the posterior distribution learned by variational autoencoders to enforce speaker dependency.', 'authorids': ['joaomonteirof@gmail.com', 'jahangir.alam@crim.ca', 'falk@emt.inrs.ca'], 'authors': ['Anonymous'], 'keywords': ['Speaker verification', 'Variational autoencoders'], 'paperhash': 'anonymous|variational_autoencoders_with_implicit_priors_for_shortduration_textindependent_speaker_verification', 'pdf': '/pdf/19599c1a51cca6488ddcfdeb5a6db5556a73eb0b.pdf'}		ryeHw1vjiQ	ryeHw1vjiQ	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	24	SJl4PkPojX	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540218716789		1547748637792	['NIPS.cc/2018/Workshop/IRASL']
668	1539700845558	{'title': 'Skip-Thought GAN: Generating Text through Adversarial Training using Skip-Thought Vectors', 'abstract': 'In the past few years, various advancements have been made in generative models owing to the formulation of Generative Adversarial Networks (GANs). GANs have been shown to perform exceedingly well on a wide variety of tasks pertaining to image generation and style transfer. In the field of Natural Language Processing, word embeddings such as word2vec and GLoVe are state-of-the-art methods for applying neural network models on textual data. Attempts have been made for utilizing GANs with word embeddings for text generation. This work presents an approach to text generation using Skip-Thought sentence embeddings in conjunction with GANs based on gradient penalty functions and f-measures. The results of using sentence embeddings with GANs for generating text conditioned on input information are comparable to the approaches where word embeddings are used. ', 'TL;DR': 'Generating text using sentence embeddings from Skip-Thought Vectors with the help of Generative Adversarial Networks.', 'authorids': ['afrozsahamad@gmail.com'], 'authors': ['Anonymous'], 'keywords': ['Natural Language Generation', 'Computation and Language', 'Machine Learning', 'Generative Adversarial Networks', 'Sentence Embeddings'], 'pdf': '/pdf/0baa291ac961d7390c2484a6347d5d39aa96b8c9.pdf', 'paperhash': 'anonymous|skipthought_gan_generating_text_through_adversarial_training_using_skipthought_vectors'}		rylLud_moQ	rylLud_moQ	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	8	HJeS_O_mim	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1539700845558		1547748637565	['NIPS.cc/2018/Workshop/IRASL']
669	1540237110170	"{'title': 'Transferable and Configurable Audio Adversarial Attack from Low-Level Features', 'abstract': ""Recent works revealed that state-of-the-art machine learning based Automatic Speech Recognition systems (ASR) have a considerable vulnerability to the crafted adversarial examples. However, limited by individual ASR system's specific machine learning models, the current audio adversarial attacks still lack certain model transferability as well as configurability for different deployment scenarios. In this work, we propose a novel untargeted adversarial example generation method to ASR systems, which shifts the adversarial example generation from the high-level machine learning models to the low-level feature extraction stage. By taking advantage of the fundamental impact and direct configuration of the low-level features, the proposed method can generate transferable and configurable adversarial examples for ASR system perturbation.\nDuring the evaluation, we use 6 commercial ASR models to test the proposed attack method. The results show that the proposed method can achieve strong transferability and good perturbation effectiveness. Also, it can configure the adversarial examples with desired audio attributes for better scenario adaptation capability. "", 'authorids': ['zxu21@gmu.edu', 'fyu2@gmu.edu', 'xchen26@gmu.edu'], 'authors': ['Anonymous'], 'keywords': ['Automatic Speech Recognition', 'Adversarial Example', 'Transferable', 'Black-box', 'Reconfiguration'], 'paperhash': 'anonymous|transferable_and_configurable_audio_adversarial_attack_from_lowlevel_features', 'pdf': '/pdf/de6f666754ac52784c25ef18367e6a0ab25e0f33.pdf'}"		Hkl04wsjiQ	Hkl04wsjiQ	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	40	S1lTNDisim	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540237110170		1547748637330	['NIPS.cc/2018/Workshop/IRASL']
670	1540226994251	{'title': 'Training Neural Nets to Achieve Audio-to-Score Translation: Opening the Black-Box', 'abstract': 'It is suggested that the task of audio-to-score translation offers an adequate testbed to investigate the division of labor between background knowledge and machine learning in the domain of audio pattern recognition, with a controllable level of difficulty and the ability to synthesize a limitless amount of labelled data.\n  \n  As a proof of concept, this paper focuses on pitch detection from audio signals. Extensive background knowledge is used to initialize simple convolutional neural nets (NN) and achieve the recognition of single notes with a decent accuracy. The performance achieved by trained NNs, however, is significantly higher. Some tentative interpretations of this fact are obtained by opening the black box and inspecting the modifications of the NN filters due to supervised learning. ', 'authorids': ['eleonore.bartenlian@u-psud.fr', 'simon@myndblue.io', 'sebag@lri.fr'], 'authors': ['Anonymous'], 'keywords': ['Pitch detection', 'automatic music transcription', 'convolutional neural networks', 'prior knowledge', 'raw signal'], 'paperhash': 'anonymous|training_neural_nets_to_achieve_audiotoscore_translation_opening_the_blackbox', 'pdf': '/pdf/75d68eb27cb85c651374ec149daf65fc5b09ec4c.pdf'}		SyMc3JYoim	SyMc3JYoim	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	33	Sye52kYsi7	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540226994251		1547748637102	['NIPS.cc/2018/Workshop/IRASL']
671	1540865856735	"{'title': ""How to make someone speak a language that they don't know."", 'abstract': 'We present a simple idea that allows to record a speaker in a given language and synthesize their voice in other languages that they may not even know. These techniques open a wide range of potential applications such as cross-language communication, language learning or automatic video dubbing. We call this general problem multi-language speaker-conditioned speech synthesis and we present a simple but strong baseline for it.\n\nOur model architecture is similar to the encoder-decoder Char2Wav model or Tacotron. The main difference is that, instead of conditioning on characters or phonemes that are specific to a given language, we condition on a shared phonetic representation that is universal to all languages. This cross-language phonetic representation of text allows to synthesize speech in any language while preserving the vocal characteristics of the original speaker. Furthermore, we show that fine-tuning the weights of our model allows us to extend our results to speakers outside of the training dataset.', 'keywords': ['Speech synthesis', 'Voice cloning', 'TTS'], 'authorids': ['lucas@lyrebird.ai', 'rdz.sotelo@gmail.com', 'kundankumar2510@gmail.com', 'adbrebs@gmail.com', 'wezteoh@lyrebird.ai', 'thibault@lyrebird.ai'], 'authors': ['Anonymous'], 'TL;DR': 'We present a simple idea that allows to record a speaker in a given language and synthesize their voice in other languages that they may not even know.', 'pdf': '/pdf/d0d04cec76b30c717308d5984062e45180392bd8.pdf', 'paperhash': 'anonymous|how_to_make_someone_speak_a_language_that_they_dont_know'}"		HkltHkBB37	HkltHkBB37	NIPS.cc/2018/Workshop/IRASL/-/Blind_Submission	[]	68	rygOSyBB37	['everyone']		['NIPS.cc/2018/Workshop/IRASL']	1540865856735		1547748636862	['NIPS.cc/2018/Workshop/IRASL']
672	1538087999341	{'title': 'Label super-resolution networks', 'abstract': 'We present a deep learning-based method for super-resolving coarse (low-resolution) labels assigned to groups of image pixels into pixel-level (high-resolution) labels, given the joint distribution between those low- and high-resolution labels. This method involves a novel loss function that minimizes the distance between a distribution determined by a set of model outputs and the corresponding distribution given by low-resolution labels over the same set of outputs. This setup does not require that the high-resolution classes match the low-resolution classes and can be used in high-resolution semantic segmentation tasks where high-resolution labeled data is not available. Furthermore, our proposed method is able to utilize both data with low-resolution labels and any available high-resolution labels, which we show improves performance compared to a network trained only with the same amount of high-resolution data.\nWe test our proposed algorithm in a challenging land cover mapping task to super-resolve labels at a 30m resolution to a separate set of labels at a 1m resolution. We compare our algorithm with models that are trained on high-resolution data and show that 1) we can achieve similar performance using only low-resolution data; and 2) we can achieve better performance when we incorporate a small amount of high-resolution data in our training. We also test our approach on a medical imaging problem, resolving low-resolution probability maps into high-resolution segmentation of lymphocytes with accuracy equal to that of fully supervised models.', 'keywords': ['weakly supervised segmentation', 'land cover mapping', 'medical imaging'], 'authorids': ['kolya_malkin@hotmail.com', 'dcrobins@gatech.edu', 'le.hou@stonybrook.edu', 'rsoobitsky@chesapeakeconservancy.org', 'jczawlytko@chesapeakeconservancy.org', 'samaras@cs.stonybrook.edu', 'joel.saltz@stonybrookmedicine.edu', 'lujoppa@microsoft.com', 'jojic@microsoft.com'], 'authors': ['Kolya Malkin', 'Caleb Robinson', 'Le Hou', 'Rachel Soobitsky', 'Jacob Czawlytko', 'Dimitris Samaras', 'Joel Saltz', 'Lucas Joppa', 'Nebojsa Jojic'], 'TL;DR': 'Super-resolving coarse labels into pixel-level labels, applied to aerial imagery and medical scans.', 'pdf': '/pdf/c67df427800bf5178ceb18ba82580d8613d042dd.pdf', 'paperhash': 'malkin|label_superresolution_networks', '_bibtex': '@inproceedings{\nmalkin2018label,\ntitle={Label super-resolution networks},\nauthor={Kolya Malkin and Caleb Robinson and Le Hou and Nebojsa Jojic},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxwShA9Ym},\n}'}		rkxwShA9Ym	rkxwShA9Ym	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1552	ryxWcmhqKQ	['everyone']		['ICLR.cc/2019/Conference']	1538087999341		1547746194607	['ICLR.cc/2019/Conference']
673	1547744981872	"{'title': 'Response to AnonReviewer3', 'comment': ""The Introduction has been expanded in the revised submission to make the motivation more explicit.\n\nIn this paper we wanted to describe the new resource we have created for training and evaluating biomedical concept recognition in scientific literature. The resource addresses 2 key needs:\n(i) Larger annotated resource, useful for training today's more complex ML models\n(ii) Broader coverage of biology and medicine concepts, by targeting UMLS.\n\nInformation Retrieval models is a research area on its own, and in this paper we just wanted to focus on concept recognition (CR). Metrics for CR models are quite standardized now, and the specific ones we use are described in a new section 4.1 added to the revised submission.\n\nCitation of the (Murty et al., ACL 2018) paper was excluded to anonymize the paper for review. It will be included in the final copy.\n\nThanks for the detailed proof-reading. These should now be fixed in the uploaded revision.""}"		SylxCx5pTQ	ByeRRU4Rf4	AKBC.ws/2019/Conference/-/Paper7/Official_Comment	['AKBC.ws/2019/Conference/Paper7/Reviewers/Unsubmitted']	3		['everyone']	SyejyHp1MN	['AKBC.ws/2019/Conference/Paper7/Authors']	1547744981872		1547744981872	['AKBC.ws/2019/Conference/Paper7/Authors', 'AKBC.ws/2019/Conference']
674	1547744353242	"{'title': 'Responses to  AnonReviewer2', 'comment': 'More detailed information on the annotation process and inter-annotator agreement is being gathered and will be published on the release site.\n\nSection 2.4 point 2 has been expanded to describe ""biomedical relevance"" as used to select semantic types.\n\nThe TaggerOne based model is now described in (expanded) section 4.2, and reason for its selection is also included. The TaggerOne paper referenced does include performance metrics on other biomedical datasets. Its demonstrated performance on recognizing biomedical entities from multiple types was our reason for using it as a baseline.  Our goal at present is to simply offer baseline metrics for concept recognition models trained on MedMentions ST21pv.\n\nTypos fixed, thanks!'}"		SylxCx5pTQ	rJeYPVVAM4	AKBC.ws/2019/Conference/-/Paper7/Official_Comment	['AKBC.ws/2019/Conference/Paper7/Reviewers/Unsubmitted']	2		['everyone']	H1gAZoslfN	['AKBC.ws/2019/Conference/Paper7/Authors']	1547744353242		1547744378931	['AKBC.ws/2019/Conference/Paper7/Authors', 'AKBC.ws/2019/Conference']
675	1547743730156	{'title': 'Revised submission', 'comment': 'We would like to thank the reviewers for their detailed review. A revised version of the paper has been uploaded that should address most of the points raised.  Various sections have been expanded in the revision, including: the introduction, to make the motivation more explicit; a clearer description of the metrics used for evaluating concept recognition models. See also responses to some specific questions below.'}		SylxCx5pTQ	Syl5lzVAME	AKBC.ws/2019/Conference/-/Paper7/Official_Comment	['AKBC.ws/2019/Conference/Paper7/Reviewers/Unsubmitted']	1		['everyone']	SylxCx5pTQ	['AKBC.ws/2019/Conference/Paper7/Authors']	1547743730156		1547743730156	['AKBC.ws/2019/Conference/Paper7/Authors', 'AKBC.ws/2019/Conference']
676	1542459592042	{'title': 'MedMentions: A Large Biomedical Corpus Annotated with UMLS Concepts', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper7/Authors'], 'keywords': ['gold-standard corpus', 'biomedical concept recognition', 'named entity recognition and linking'], 'TL;DR': 'The paper introduces a new gold-standard corpus corpus of biomedical scientific literature manually annotated with UMLS concept mentions.', 'abstract': 'This paper presents the formal release of {\\em MedMentions}, a new manually annotated resource for the recognition of biomedical concepts. What distinguishes MedMentions from other annotated biomedical corpora is its size (over 4,000 abstracts and over 350,000 linked mentions), as well as the size of the concept ontology (over 3 million concepts from UMLS 2017) and its broad coverage of biomedical disciplines. In addition to the full corpus, a sub-corpus of MedMentions is also presented, comprising annotations for a subset of UMLS 2017 targeted towards document retrieval. To encourage research in Biomedical Named Entity Recognition and Linking, data splits for training and testing are included in the release, and a baseline model and its metrics for entity linking are also described.', 'archival status': 'Archival', 'subject areas': ['Natural Language Processing', 'Information Extraction', 'Applications: Biomedicine'], 'pdf': '/pdf/a9e9c60779d6982b14fc249dfe30063e6d0fac40.pdf', 'paperhash': 'anonymous|medmentions_a_large_biomedical_corpus_annotated_with_umls_concepts', '_bibtex': '@inproceedings{    \nanonymous2019medmentions:,    \ntitle={MedMentions: A Large Biomedical Corpus Annotated with UMLS Concepts},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=SylxCx5pTQ},    \nnote={under review}    \n}'}		SylxCx5pTQ	SylxCx5pTQ	AKBC.ws/2019/Conference/-/Blind_Submission	[]	7	rJxtiCNcpm	['everyone']		['AKBC.ws/2019/Conference']	1542459592042		1547743610570	['AKBC.ws/2019/Conference']
677	1547740384642	"{'title': 'Answer for Reviewer 3', 'comment': 'Dear Reviewer, thank you for your review.\nI would like to take this opportunity to react to your questions:\n\nIn the paper, I argue that using only NE is not a good approach: ""focusing only on annotations including named-entities not necessarily leads to annotations describing the content of documents"". \nWe use subject-predicate-object notation for each annotation. It is possible to use another granularity for an annotation. However, we choose this kind of annotations to have the option to describe subjects and relations between subjects. Having only terms like ""sport"" or ""politic"" are very generic and do not describe the content of documents. \nThe G-similarity is a heuristic method. Depending on the application different weighting factors can be applied for matches in only the subject, or subject and predicate, subject and object and so on.\n\nWe do not represent documents as vectors. Each document is represented as a topic distribution, and the distribution depends on the composition of documents within the corpus. Therefore, we use the Hellinger distance to compare the distributions instead of the cosine similarity.\n\nThank you for the suggestion comparing the document similarity based on topic models and word embeddings.\n'}"		HylzClqp67	HylKySXRzV	AKBC.ws/2019/Conference/-/Paper8/Official_Comment	['AKBC.ws/2019/Conference/Paper8/Reviewers/Unsubmitted']	3		['everyone']	r1x3h1EbG4	['AKBC.ws/2019/Conference/Paper8/Authors']	1547740384642		1547740384642	['AKBC.ws/2019/Conference/Paper8/Authors', 'AKBC.ws/2019/Conference']
678	1547740344322	{'title': 'Answer for Reviewer 2', 'comment': 'Dear Reviewer, thank you for your review.\nThank you for the positive arguments. I would like to take this opportunity to react to your argument that this paper has no clear quantitative empirical results to evaluate the algorithm.\n\nDo you have more details for me what you mean that there are no clear quantitative empirical results to evaluate the algorithm?  We give a proof for the correctness and the termination and present empirical results in Section 5. \nSection 5 contains a detailed description about preprocessing the three different datasets. Additionally, we plot the results of the results using different configurations for Algorithm 1.\n\nMaybe, you mean the fact that we have no baseline or other systems to compare with.\nI searched for a baseline in the domain of annotating documents. However, I did not find any baseline and I think there is no baseline. In general, there is no correct or wrong annotation for a document. The added value of an annotation depends on the person reading a document and the corresponding annotations. \nPlease let me know if you have a gold standard for document annotations.\nUnfortunately, none of the approaches from the related work can be used as baseline. Most of the approaches simply add external data from ontologies to the documents and argue that these annotations are semantic annotations. However, they ignore the composition of documents in a corpus and only compare data from one document with data from an external data source.\n'}		HylzClqp67	SkxepEXRf4	AKBC.ws/2019/Conference/-/Paper8/Official_Comment	['AKBC.ws/2019/Conference/Paper8/Reviewers/Unsubmitted']	2		['everyone']	rkle11iMG4	['AKBC.ws/2019/Conference/Paper8/Authors']	1547740344322		1547740344322	['AKBC.ws/2019/Conference/Paper8/Authors', 'AKBC.ws/2019/Conference']
679	1547740275564	{'title': 'Answer for Reviewer 1', 'comment': 'Dear Reviewer, thank you for your review.\nI would like to take this opportunity to react to the few possible issues:\n\n1) We tried to present the effects of the D- and G-similarity. We had different setups in our evaluation using different number of topics, number of documents, and various thresholds:\n- The D-similarity is based on the Hellinger distance working on the topic distribution the topic model and the topic distribution strongly depend on the number of topics.\n- We used three different datasets and the documents in each dataset have different properties like the length and the vocabulary. Again, the topic model highly depends on the documents (length and words).\n- The G-similarity is based on the data from DBpedia. We used different thresholds for the data of each dataset and identified (see Figure 1)\n\nI agree with you that the results depend on different techniques, but we tried to show the influence of different techniques (D-similarity, G-similarity, number of documents, etc.) in the evaluation of our paper.\n\n2) I searched for a baseline in the domain of annotating documents. However, I did not find any baseline and I think there is no baseline. Generally, there is no correct or wrong annotation for a document. The added value of an annotation depends on the person reading a document and the corresponding annotations. \nPlease let me know if you have a gold standard for document annotations.\nUnfortunately, none of the approaches from the related work can be used as baseline. Most of the approaches simply add external data from ontologies to the documents and argue that these annotations are semantic annotations. However, they ignore the composition of documents in a corpus and only compare data from one document with data from an external data source.\n'}		HylzClqp67	rkehdEm0z4	AKBC.ws/2019/Conference/-/Paper8/Official_Comment	['AKBC.ws/2019/Conference/Paper8/Reviewers/Unsubmitted']	1		['everyone']	H1ghtgPOf4	['AKBC.ws/2019/Conference/Paper8/Authors']	1547740275564		1547740275564	['AKBC.ws/2019/Conference/Paper8/Authors', 'AKBC.ws/2019/Conference']
680	1547735466290	{'title': 'Learning in Reproducing Kernel Kreı̆n Spaces', 'authors': ['Dino Oglic', 'Thomas Gaertner'], 'authorids': ['dino.oglic@uni-bonn.de'], 'pdf': 'http://proceedings.mlr.press/v80/oglic18a/oglic18a.pdf', 'paperhash': 'oglic|learning_in_reproducing_kernel_kren_spaces'}		Byxz2WM0GE	Byxz2WM0GE	OpenReview.net/Archive/-/Direct_Upload	[]	205		['everyone']		['~Dino_Oglic1']	1547735466290		1547735466290	['~Dino_Oglic1']
681	1547735372160	{'title': 'Nyström Method with Kernel K-means++ Samples as Landmarks', 'authors': ['Dino Oglic', 'Thomas Gärtner'], 'authorids': ['dino.oglic@uni-bonn.de'], 'pdf': 'http://proceedings.mlr.press/v70/oglic17a/oglic17a.pdf', 'paperhash': 'oglic|nyström_method_with_kernel_kmeans_samples_as_landmarks'}		S1x48bGAf4	S1x48bGAf4	OpenReview.net/Archive/-/Direct_Upload	[]	204		['everyone']		['~Dino_Oglic1']	1547735372160		1547735372160	['~Dino_Oglic1']
682	1547735309405	{'title': 'Greedy Feature Construction', 'authors': ['Dino Oglic', 'Thomas Gaertner'], 'authorids': ['dino.oglic@uni-bonn.de'], 'pdf': 'https://papers.nips.cc/paper/6557-greedy-feature-construction.pdf', 'paperhash': 'oglic|greedy_feature_construction'}		BJxrfbfAfE	BJxrfbfAfE	OpenReview.net/Archive/-/Direct_Upload	[]	203		['everyone']		['~Dino_Oglic1']	1547735309405		1547735309405	['~Dino_Oglic1']
683	1542459651747	{'title': 'Learning Relation Representations from Word Representations', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper30/Authors'], 'keywords': ['Relation representations', 'relation embeddings'], 'TL;DR': 'Identifying the relations that connect words is important for various NLP tasks. We model relation representation as a supervised learning problem and learn parametrised operators that map pre-trained word embeddings to relation representations.', 'abstract': 'Identifying the relations that connect words is an important step towards understanding human languages and is useful for various NLP tasks such as knowledge base completion and analogical reasoning. Simple unsupervised operators such as vector offset between two-word embeddings have shown to recover some specific relationships between those words, if any. Despite this, how to accurately learn generic relation representations from word representations remains unclear. We model relation representation as a supervised learning problem and learn parametrised operators that map pre-trained word embeddings to relation representations. We propose a method for learning relation representations using a feed-forward neural network that performs relation prediction. Our evaluations on two benchmark datasets reveal that the penultimate layer of the trained neural network-based relational predictor acts as a good representation for the relations between words.', 'pdf': '/pdf/721e7e3b66c09c98c90c857cd174588a506aa5e8.pdf', 'archival status': 'Non-Archival', 'subject areas': ['Natural Language Processing'], 'paperhash': 'anonymous|learning_relation_representations_from_word_representations', '_bibtex': '@inproceedings{    \nanonymous2019learning,    \ntitle={Learning Relation Representations from Word Representations},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=r1e3WW5aTX},    \nnote={under review}    \n}'}		r1e3WW5aTX	r1e3WW5aTX	AKBC.ws/2019/Conference/-/Blind_Submission	[]	30	SJl9VKI3aX	['everyone']		['AKBC.ws/2019/Conference']	1542459651747		1547729593328	['AKBC.ws/2019/Conference']
684	1538087800971	{'title': 'Dynamically Unfolding Recurrent Restorer: A Moving Endpoint Control Method for Image Restoration', 'abstract': 'In this paper, we propose a new control framework called the moving endpoint control to restore images corrupted by different degradation levels in one model. The proposed control problem contains a restoration dynamics which is modeled by an RNN. The moving endpoint, which is essentially the terminal time of the associated dynamics, is determined by a policy network. We call the proposed model the dynamically unfolding recurrent restorer (DURR). Numerical experiments show that DURR is able to achieve state-of-the-art performances on blind image denoising and JPEG image deblocking. Furthermore, DURR can well generalize to images with higher degradation levels that are not included in the training stage.', 'keywords': ['image restoration', 'differential equation'], 'authorids': ['jet@pku.edu.cn', 'luyiping9712@pku.edu.cn', 'liujiaying@pku.edu.cn', 'dongbin@math.pku.edu.cn'], 'authors': ['Xiaoshuai Zhang', 'Yiping Lu', 'Jiaying Liu', 'Bin Dong'], 'pdf': '/pdf/5bb516414b9aaab76f3a904c0624dd75b7e4f82a.pdf', 'paperhash': 'zhang|dynamically_unfolding_recurrent_restorer_a_moving_endpoint_control_method_for_image_restoration', 'TL;DR': 'We propose a novel method to handle image degradations of different levels by learning a diffusion terminal time. Our model can generalize to unseen degradation level and different noise statistic.', '_bibtex': '@inproceedings{\nzhang2018dynamically,\ntitle={Dynamically Unfolding Recurrent Restorer: A Moving Endpoint Control Method for Image Restoration},\nauthor={Xiaoshuai Zhang and Yiping Lu and Jiaying Liu and Bin Dong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJfZKiC5FX},\n}'}		SJfZKiC5FX	SJfZKiC5FX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	419	SyeuxYEjuX	['everyone']		['ICLR.cc/2019/Conference']	1538087800971		1547710667024	['ICLR.cc/2019/Conference']
685	1538087949928	{'title': 'Improving the Generalization of Adversarial Training with Domain Adaptation', 'abstract': 'By injecting adversarial examples into training data, adversarial training is promising for improving the robustness of deep learning models. However, most existing adversarial training approaches are based on a specific type of adversarial attack. It may not provide sufficiently representative samples from the adversarial domain, leading to a weak generalization ability on adversarial examples from other attacks. Moreover, during the adversarial training, adversarial perturbations on inputs are usually crafted by fast single-step adversaries so as to scale to large datasets. This work is mainly focused on the adversarial training yet efficient FGSM adversary. In this scenario, it is difficult to train a model with great generalization due to the lack of representative adversarial samples, aka the samples are unable to accurately reflect the adversarial domain. To alleviate this problem, we propose a novel Adversarial Training with Domain Adaptation (ATDA) method. Our intuition is to regard the adversarial training on FGSM adversary as a domain adaption task with limited number of target domain samples. The main idea is to learn a representation that is semantically meaningful and domain invariant on the clean domain as well as the adversarial domain. Empirical evaluations on Fashion-MNIST, SVHN, CIFAR-10 and CIFAR-100 demonstrate that ATDA can greatly improve the generalization of adversarial training and the smoothness of the learned models, and outperforms state-of-the-art methods on standard benchmark datasets. To show the transfer ability of our method, we also extend ATDA to the adversarial training on iterative attacks such as PGD-Adversial Training (PAT) and the defense performance is improved considerably.', 'keywords': ['adversarial training', 'domain adaptation', 'adversarial example', 'deep learning'], 'authorids': ['cbsong@hust.edu.cn', 'brooklet60@hust.edu.cn', 'wanglw@pku.edu.cn', 'jeh@cs.cornell.edu'], 'authors': ['Chuanbiao Song', 'Kun He', 'Liwei Wang', 'John E. Hopcroft'], 'TL;DR': 'We propose a novel adversarial training with domain adaptation method that significantly improves the generalization ability on adversarial examples from different attacks.', 'pdf': '/pdf/2b0578d22c1cf1a6b00f801fdeac0b2c9482c39d.pdf', 'paperhash': 'song|improving_the_generalization_of_adversarial_training_with_domain_adaptation', '_bibtex': '@inproceedings{\nsong2018improving,\ntitle={Improving the Generalization of Adversarial Training with Domain Adaptation},\nauthor={Chuanbiao Song and Kun He and Liwei Wang and John E. Hopcroft},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyfIfnC5Ym},\n}'}		SyfIfnC5Ym	SyfIfnC5Ym	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1267	HygMqg65F7	['everyone']		['ICLR.cc/2019/Conference']	1538087949928		1547701520951	['ICLR.cc/2019/Conference']
686	1538087827345	{'title': 'Improving MMD-GAN Training with Repulsive Loss Function', 'abstract': 'Generative adversarial nets (GANs) are widely used to learn the data sampling process and their performance may heavily depend on the loss functions, given a limited computational budget. This study revisits MMD-GAN that uses the maximum mean discrepancy (MMD) as the loss function for GAN and makes two contributions. First, we argue that the existing MMD loss function may discourage the learning of fine details in data as it attempts to contract the discriminator outputs of real data. To address this issue, we propose a repulsive loss function to actively learn the difference among the real data by simply rearranging the terms in MMD. Second, inspired by the hinge loss, we propose a bounded Gaussian kernel to stabilize the training of MMD-GAN with the repulsive loss function. The proposed methods are applied to the unsupervised image generation tasks on CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets. Results show that the repulsive loss function significantly improves over the MMD loss at no additional computational cost and outperforms other representative loss functions. The proposed methods achieve an FID score of 16.21 on the CIFAR-10 dataset using a single DCGAN network and spectral normalization.', 'keywords': ['generative adversarial nets', 'loss function', 'maximum mean discrepancy', 'image generation', 'unsupervised learning'], 'authorids': ['weiw8@student.unimelb.edu.au', 'yuan.sun@rmit.edu.au', 'saman@unimelb.edu.au'], 'authors': ['Wei Wang', 'Yuan Sun', 'Saman Halgamuge'], 'TL;DR': 'Rearranging the terms in maximum mean discrepancy yields a much better loss function for the discriminator of generative adversarial nets', 'pdf': '/pdf/42f0f2862617e7f3db97414a00846cfcd8eff694.pdf', 'paperhash': 'wang|improving_mmdgan_training_with_repulsive_loss_function', '_bibtex': '@inproceedings{\nwang2018improving,\ntitle={Improving {MMD}-{GAN} Training with Repulsive Loss Function},\nauthor={Wei Wang and Yuan Sun and Saman Halgamuge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HygjqjR9Km},\n}'}		HygjqjR9Km	HygjqjR9Km	ICLR.cc/2019/Conference/-/Blind_Submission	[]	565	SkxDo2iqFm	['everyone']		['ICLR.cc/2019/Conference']	1538087827345		1547699798894	['ICLR.cc/2019/Conference']
687	1547690094967	{'title': 'Interesting but missing solid takeaway messages', 'review': 'The work described in this paper concerns performing Relation Extraction on medical data, via non-expert crowdsourcing.\nThe input text for the analysis comes from (1058) PubMed abstracts. \nConcepts (Named Entities) are extracted and pair of 1009 concepts are submitted to the Mark2Cure  platform workers (volunteer) to perform relation annotation.\nEach of those pairs was annotated by at least 15 volunteers, for a total of 15,739 annotations.\nResults are assessed against 10% manually validated ground truth (although no details are provided on who performed the manual validation: medical experts? How many?)\n\nWhile the authors report several findings on the specific selected dataset, as a reader I struggle to easily identify: \n- the novelty: most of the findings are consistent with previous literature, both on the viability of performing medical relation extraction via non-expert crowd (Aroyo and Welty, 2013) and and the fact that NER performance affects RE performance\n- clear takeaway guidelines and messages: the discussion is somewhat limited to specific issue of the one analyzed datasets, but fail to provide reusable insights\n', 'rating': '6: Marginally above acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		r1loaec6pm	ryewOgvTMV	AKBC.ws/2019/Conference/-/Paper5/Official_Review	['AKBC.ws/2019/Conference/Paper5/Reviewers/Unsubmitted']	3		['everyone']	r1loaec6pm	['AKBC.ws/2019/Conference/Paper5/AnonReviewer1']	1547690094967		1547690155277	['AKBC.ws/2019/Conference/Paper5/AnonReviewer1', 'AKBC.ws/2019/Conference']
688	1538087770775	{'title': 'Variational Smoothing in Recurrent Neural Network Language Models', 'abstract': 'We present a new theoretical perspective of data noising in recurrent neural network language models (Xie et al., 2017). We show that each variant of data noising is an instance of Bayesian recurrent neural networks with a particular variational distribution (i.e.,  a mixture of Gaussians whose weights depend on statistics derived from the corpus such as the unigram distribution). We use this insight to propose a more principled  method to apply at prediction time and propose natural extensions to data noising under the variational framework. In particular, we propose variational smoothing  with tied input and output embedding matrices and an element-wise variational smoothing method. We empirically verify our analysis on two benchmark language modeling datasets and demonstrate performance improvements over existing data noising methods.', 'keywords': [], 'authorids': ['lingpenk@cs.cmu.edu', 'melisgl@google.com', 'lingwang@google.com', 'leiyu@google.com', 'dyogatama@google.com'], 'authors': ['Lingpeng Kong', 'Gabor Melis', 'Wang Ling', 'Lei Yu', 'Dani Yogatama'], 'pdf': '/pdf/5970cf68724ba093b0d4b28dc5bb884d1499f3ed.pdf', 'paperhash': 'kong|variational_smoothing_in_recurrent_neural_network_language_models', '_bibtex': '@inproceedings{\nkong2018variational,\ntitle={Variational Smoothing in Recurrent Neural Network Language Models},\nauthor={Lingpeng Kong and Gabor Melis and Wang Ling and Lei Yu and Dani Yogatama},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SygQvs0cFQ},\n}'}		SygQvs0cFQ	SygQvs0cFQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	248	rJlXheVqFQ	['everyone']		['ICLR.cc/2019/Conference']	1538087770775		1547678824623	['ICLR.cc/2019/Conference']
689	1547651050624	{'title': 'Looks good', 'review': 'Solid work and centrally in scope for the conference.  Authors not even slightly disguised.\n\nAlfio - Anca did some work on relation similarity presented at the last AKBC - \n\nAnca Dumitrache, Lora Aroyo, Chris Welty: False Positive and Cross-relation Signals in Distant Supervision Data. AKBC at NIPS 2017.\n\n\n-Chris', 'rating': '9: Top 15% of accepted papers, strong accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		SkxE1b56TQ	B1lmldphM4	AKBC.ws/2019/Conference/-/Paper14/Official_Review	['AKBC.ws/2019/Conference/Paper14/Reviewers/Unsubmitted']	4		['everyone']	SkxE1b56TQ	['AKBC.ws/2019/Conference/Paper14/AnonReviewer3']	1547651050624		1547662997319	['AKBC.ws/2019/Conference/Paper14/AnonReviewer3', 'AKBC.ws/2019/Conference']
690	1547489682401	{'title': 'Interesting application, but out of scope for the conference', 'review': 'The authors present a study of algorithms for compound repositioning. The paper is written for a bio-informatics audience, focusing mostly on the findings of the study rather than on the technical approach.\n\nThe compound repositioning problem is not defined for a broad audience (repurposing drugs to treat a disease other than the originally intended one). The methods for constructing the KGs from the original sources are explained in narrative form, so it is difficult for a computer science audience to get a clear picture of what information is contained in the KG.\n\nThe main point of the paper is to compare two evaluation approaches for the prediction problem. One uses cross-validation, the second creates a time-sequence of KGs where the goal is to predict edges that are reported later in the literature. The focus is on the finding rather than on the machine learning methods (which had been reported in the literature).\n\nI cannot evaluate the significance of the results. However, for a computer science audience the paper is of marginal interest.', 'rating': '4: Ok but not good enough - rejection', 'confidence': '2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper'}		BkeCW-q6aQ	B1e55Z89GV	AKBC.ws/2019/Conference/-/Paper31/Official_Review	['AKBC.ws/2019/Conference/Paper31/Reviewers/Unsubmitted']	3		['everyone']	BkeCW-q6aQ	['AKBC.ws/2019/Conference/Paper31/AnonReviewer3']	1547489682401		1547662996260	['AKBC.ws/2019/Conference/Paper31/AnonReviewer3', 'AKBC.ws/2019/Conference']
691	1547471462872	"{'title': 'Embedding entity pairs using their contexts for distantly supervised relation extraction ', 'review': ""Summary\nThe paper casts relation extraction as an analogy problem over pairs of entities. An entity pair (A, B) is analogous to (C,D) if the same relation holds between (A, B) and (C, D). They train a Siamese-network based binary classifier over representations of entity pairs which predicts whether a given instance of two entity pairs is analogous or not. The entity pair representations are computed by embedding sentences in which a pair occurs (assuming distant supervision). The method is evaluated on traditional as well as one-shot relation extraction tasks.\n\nStrengths\n1. The paper is generally clearly written.\n2. The approach is simple and the results are reasonable.\n\nComments\n1. The paper seems to suggest (page 3, last para) that USchema approaches don't handle unseen relation types. This isn't completely accurate considering that USchema has both row-less and column-less variants (Toutanova et al., 2015; Verga et al., 2016, Verga et al., 2017). When combined (Verga et al., 2017), these are capable of generalizing to unseen textual relations and entity-pairs. \n2. Is there a reason why the analogy model was trained only (once) on relations in T-REX instead of training separate models for each dataset?\n3. Missing relevant work -- Jameel et al., 2017 (Modeling Semantic Relatedness using Global Relation Vectors). They learn word pair (or relation) vectors from co-occurrence statistics and evaluate on relation-extraction.\n"", 'rating': '7: Good paper, accept', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		SkxE1b56TQ	r1g1OqZ5fV	AKBC.ws/2019/Conference/-/Paper14/Official_Review	['AKBC.ws/2019/Conference/Paper14/Reviewers/Unsubmitted']	3		['everyone']	SkxE1b56TQ	['AKBC.ws/2019/Conference/Paper14/AnonReviewer4']	1547471462872		1547662996034	['AKBC.ws/2019/Conference/Paper14/AnonReviewer4', 'AKBC.ws/2019/Conference']
692	1547411289844	"{'title': 'The paper described a information extraction task, but too many questions are unanswered', 'review': 'The paper tackles an important problem: extraction of structured data from unstructured text, but lack of comparison with existing approaches.\n\nSection 1\nWikipedia is not only a knowledge based of the names in the world. Maybe the authors wanted to say the ""entities of the world""?\nThe motivation of the paper is limited: what is the goal of the structured knowledge base? I the goal is better consistency, how to we improve consistency. There is nothing in the paper that indicate that the consistency is better than, say, manually created data with a voting mechanism. Why this approach to KB structuring is inherently coherent? \n\n\nSection 2\nIf the only problem of CYC is the coverage, why the authors did not try to improve the coverage of CYC instead of inventing a new method?\n\nSection 3\nWhy top-down design of ontology is needed. If the authors have learned it, what are the supporting evidence for it?\n\nSection 4\nNo annotation reliability (e.g. the inter-annotator agreement score). \n\nSection 5\nWhy ""chemical compound"" was selected and not ""movie"" or ""building"" as more common sub-categories?\n600 data points in quite small compared to standard datasets. What was the cost of the annotations?\n\nSection 6\nWhy the Workers (Lancers) were not used? What was their accuracy/cost ? Maybe the cost could compensate the low accuracy.\n\nSection 7\nWhy is it scientifically interesting to know that the authors are happy ?\n\nSection 8\nWhy did the authors participate in the shared task? \nWhat are the references for stacking? \nAs far as I know, stacking performs poorly compared to proper inference techniques, such as CRF. Why is it different in this case? \n\nOverall: the English writing is very approximate. I\'m not a native speaker myself, but I would suggest the authors to send the paper to a native English speaker for correction.', 'rating': '4: Ok but not good enough - rejection', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		HygfXWqTpm	BkgGPkXtMN	AKBC.ws/2019/Conference/-/Paper38/Official_Review	['AKBC.ws/2019/Conference/Paper38/Reviewers/Unsubmitted']	3		['everyone']	HygfXWqTpm	['AKBC.ws/2019/Conference/Paper38/AnonReviewer3']	1547411289844		1547662995811	['AKBC.ws/2019/Conference/Paper38/AnonReviewer3', 'AKBC.ws/2019/Conference']
693	1547365534209	"{'title': 'review', 'review': 'The paper proposes to improve open-world probabilistic databases by using additional mean tuple constraints. Theoretically, the authors show the constraints increase hardness but with approximation tight bounds could be obtained.\n\nI believe the paper is tackling an important problem, and the theoretical analysis looks solid. It seems to have a potential impact in the field of probabilistic knowledge representation.\n\nThe paper will be further improved if experimental results could be presented.', 'rating': '6: Marginally above acceptance threshold', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		r1xTXZ9p6Q	SyeIsnvuM4	AKBC.ws/2019/Conference/-/Paper42/Official_Review	['AKBC.ws/2019/Conference/Paper42/Reviewers/Unsubmitted']	3		['everyone']	r1xTXZ9p6Q	['AKBC.ws/2019/Conference/Paper42/AnonReviewer1']	1547365534209		1547662995596	['AKBC.ws/2019/Conference/Paper42/AnonReviewer1', 'AKBC.ws/2019/Conference']
694	1547362436465	{'title': 'review', 'review': 'The paper proposes techniques to enrich the annotations of a documents using the annotations of other documents in the same corpus. Methods to adapt to the dynamic changes of the corpus have been proposed.\n\nThe problem being tackled seems to have useful real world applications. The experimental section has studied the effects of multiple hyper-parameters and settings. However, there are a few possible issues:\n1. There is no ablation study to see the effects of different components of the approach. Since the solution is a set of techniques, it is not clear which technique contributes to the final result.\n2. There is no direct baseline available. It is straightforward to come up with possibly competitive methods such as training a machine learning model based on existing documents to predict annotations. Moreover, a lot of related work has been discussed, which could also serve as baselines. However, none of them has been considered as baselines for comparison in the experiments. As a result, it is hard to establish the significance of the proposed method.', 'rating': '5: Marginally below acceptance threshold', 'confidence': '2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper'}		HylzClqp67	H1ghtgPOf4	AKBC.ws/2019/Conference/-/Paper8/Official_Review	['AKBC.ws/2019/Conference/Paper8/Reviewers/Unsubmitted']	3		['everyone']	HylzClqp67	['AKBC.ws/2019/Conference/Paper8/AnonReviewer1']	1547362436465		1547662995370	['AKBC.ws/2019/Conference/Paper8/AnonReviewer1', 'AKBC.ws/2019/Conference']
695	1547295913469	{'title': 'Review', 'review': 'This paper introduces a method to extract facts (triples) from biomedical text using semantic role labeling. To make it work in their particular domain, they propose a variety of domain specific changes to each stage of SRL. For example, handling of copula verbs, relations expressed through nouns instead of verbs (which most SRL system extract) etc.\n\n(-) Even though the problem addressed in the paper is very important, I am not convinced that the proposed method would generalize well and can be adopted widely. For example algorithm 1 and 2 seem very domain specific and rule based and I am not sure if it would generalize. \n(-) The writing of the paper could be made much clearer. For example, I found the two definitions in section 3 very hard to read and pretty much unnecessary. It would have been much simpler to just explain with an example. For example, definition 1 could have been much more clearer if the input and outputs of SRL were explained with a simple example.\n(-) In 3.3, you mention an annotator like GATE. What exactly is GATE? It seems out of context.\n(-) Also in section 3.3, there was a sudden mention of using embeddings without any context which left me very confused. What exactly do you mean by vector representations of KB labels?\n(-) Definition 4 also brings in a new concept called “alternative labels” in KB. What are alternative labels?\n(+) I appreciate the evaluation by medical professionals. 48% acceptance rate is however not too high.\n(-) I would have liked to see an error analysis of where the system made mistake. That would also help to make the system better.\n\nOverall, even though the paper addresses a very important problem, but at its current state, the paper needs to improve by a lot (both interns of writing, reasoning behind the models, and analysis).', 'rating': '3: Clear rejection', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		HJxmxbq66Q	BJe-nn8vGN	AKBC.ws/2019/Conference/-/Paper20/Official_Review	['AKBC.ws/2019/Conference/Paper20/Reviewers/Unsubmitted']	3		['everyone']	HJxmxbq66Q	['AKBC.ws/2019/Conference/Paper20/AnonReviewer2']	1547295913469		1547662995158	['AKBC.ws/2019/Conference/Paper20/AnonReviewer2', 'AKBC.ws/2019/Conference']
696	1547234200875	{'title': 'Review', 'review': 'The paper proposes an approach for parsing short noun-phrase queries into a Description Logic concept that can be executed against a KB. For example, a phrase like “pain around the heart” should be parsed into a concept “Pain ^ \\exists locatedIn.Heart.”\n\nThe noun-phrases considered by the proposed approach are short, follow a similar pattern and are only comprised of a single central entity which is modified by the rest of the phrase. The description logic language is made from a simple grammar consisting of concept intersections (eg. Pain ^ Swelling), and checking if a certain property holds for a given concept (eg. locatedIn.Heart). Given these restrictions, the paper proposes a simple approach of using the dependency tree of the query phrase by first disambiguating the root node with the relevant concept, and then recursively traversing the tree to add constraints based on the modifiers to the root concept. The method requires named entity disambiguation to match the tokens in the query phrase to the concepts in the KB, for which the paper uses ElasticSearch. \n\nThe paper uses a new, unpublished medical KB made by integrating multiple KBs such as SNOMED CT, NCI, etc. The associated paper is made anonymous in the publication to get further details. The paper presents an evaluation on an in-house dataset made by doctors in the authors’ group which contain 1878 labeled pairs. \n\nThe paper compares to an approach called GATE that presumably performs exact match. (I failed to find the citation for this approach. I would appreciate if the authors could point that out in the author response.) The method is also compared to Google’s universal sentence embeddings and use it by finding the nearest neighboring concepts by encoding both the query and concepts using the sentence embedder. The proposed approach performs the best amongst the baseline approaches.\n\nI find that the paper has certain limitations which I think should be addressed before it is ready for publication. \n1. One major limitation of the proposed approach is that it can only be used for short noun phrases and it is unclear how the proposed algorithm can be improved to handle long and difficult queries containing multiple central concepts and quantifiers.\n2. In my understanding, the major heavy-lifting is done by the named-entity-disambiguation module, as also found out empirically in Sec 5.2. The paper should at least provide experimentation with different NED systems used, and better try to learn/fine-tune it using some kind of training data.\n3. I find that the evaluation experimentation is weak as the proposed is only tested on a single in-house KB, on relatively small number of queries. The evaluation could comprise of datasets based on Freebase, DBPedia or other large KBs, and compare to other methods proposed in the literature.\n\n', 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		H1l8ag9a6m	S1gWjsDLGE	AKBC.ws/2019/Conference/-/Paper3/Official_Review	['AKBC.ws/2019/Conference/Paper3/Reviewers/Unsubmitted']	3		['everyone']	H1l8ag9a6m	['AKBC.ws/2019/Conference/Paper3/AnonReviewer2']	1547234200875		1547662994932	['AKBC.ws/2019/Conference/Paper3/AnonReviewer2', 'AKBC.ws/2019/Conference']
697	1547203539330	"{'title': 'Interesting paper describing a production system', 'review': 'This work presents a production KB construction system that is used to create a large Japanese KB. The paper stated the 6 main issues when constructing a production KB and described the 12 different components in the system to address the issues. It also proposed an information extraction method that uses the DOM structure for extracting facts from Web-crawled data. In the experiments, it showed the \n\nThe paper is relatively well-written. However, except for the information extraction part, most of the other components are only described informally. We can see that the proposed IE method is a useful component in the system, but it is hard to judge its strength as a general method as there aren’t any baselines to compare to when evaluating it. \n\nSome specific questions: \n\n1. What percentage of the facts in JKB are from web-crawled data? \n \n2. In figure 5, it makes sense that the number of extracted information will decrease when the threshold increases, but why is the “~1.0” bar taller than “~0.9”, actually almost similar to “~0.5”? \n\n3.“Entity Linker links between two entities where they do not connect each other in spite of their intercorrelations.” \nRegarding the name “entity linker”, it is a bit different from the standard definition of entity linking, which usually means linking a mention in the text to an entity in the KB. (See also the Wikipedia page for Entity Linking)\n\n4.""We solve the first problem by comparing our ontology with the range type of the predicate and second problem by narrowing down the target to accelerate and reduce the number of incorrect interlinks.” \nI didn’t quite get how “narrowing down the target to accelerate and reduce the number of incorrect interlinks” is done. Are there any details about this? \n\n5.”confirmed that only 0.0004% of entities changed their ids. We also observed that more than 94% of the entities did not change their ids, as shown in Figure 6” \nThe description here is a bit confusing because (0.0004% + 94%) < 100%. I guess you are trying to say that “0.0004%” changed within a week and only “94%” changed within the whole experiment period (16 months)? \n\n6.”The Attribute Completer partially addresses a well-known challenge; knowledge base completion [Socher et al., 2013].”\nComplementing attributes using symmetric properties and extracting information from URL seems to be different from the usual knowledge base completion, which, as far as I understand, tries to infer missing triples from the existing triples in the KB. \n\nTypo or grammatical errors:\n\n""Rule-based matching matches entities whose Wikipedia URL or some identifiers, such as IMDb “ seem to be an incomplete sentence. . \n\n""Therefore many entity-matching systems heve been proposed” => “have""\n\n""we regard the website is incorrect” => “as""\n', 'rating': '6: Marginally above acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		SJxfV-q6Tm	HJls07eUfV	AKBC.ws/2019/Conference/-/Paper44/Official_Review	['AKBC.ws/2019/Conference/Paper44/Reviewers/Unsubmitted']	3		['everyone']	SJxfV-q6Tm	['AKBC.ws/2019/Conference/Paper44/AnonReviewer1']	1547203539330		1547662994720	['AKBC.ws/2019/Conference/Paper44/AnonReviewer1', 'AKBC.ws/2019/Conference']
698	1547185978947	{'title': 'Super-exciting stuff, great analysis and overall a well written and executed paper', 'review': 'This paper studies the problem of fine grained entity recognition (\\fger) and typing . They initially present an excellent analysis of how fine grained entity type sets are not a direct extension of the usual coarse grained types (person, organization, etc) and hence training \\fger systems on data annotated on coarse grained typed datasets would lead to low recall systems. Secondly, they show that automatically created fine grained typed datasets are also not sufficient since it leads to low recall because of noisy distant supervision. This analysis was both an interesting read and also sets the stage for the main contribution of this work.\n\nThe main contribution of this work is to create a high recall large training corpora to facilitate \\fger and typing. The authors propose a staged pipeline which takes in input (a) a text corpus linked to a knowledge base (using noisy distant supervision), (b) a knowledge base (and also a list of aliases for entities) and (c) a typed hierarchy. All of these are readily available. The output is a high-recall linked corpus with linked entities to the KB and the type hierarchy. To show the usability of the training corpus the paper performs excellent intrinsic and extrinsic analysis. In the intrinsic analysis, they show that the new corpus indeed has a lot more mentions annotated. In the extrinsic analysis, they show that state art of the models trained on the new corpus has very impressive gains in recall when compared to the original existing datasets and also wiki dataset with distant supervision. There is a loss in precision though, but it is fairly small when compared to the massive gains in recall. This experiments warrants the usability of the generated training corpus and also the framework they stipulate which I think everyone should follow.\n\nGreat work!\n\nQuestions:\n\n1. In table 3, can you also report the difference in entities (and not entity mentions). I would be interested to see if you were able to find new entities. \n2. Are you planning to release the two training datasets (and if possible the code ?)\n\nSuggestion: The heuristics definitely work great but I think we can still do better if parts of stage  2 and 3 were replaced by a simple machine learned system. For example, in stage 2, just relying on heuristics to match to the prefix trees would result in always choosing the shortest alias and would be problematic if aliases of different entities share the same prefix. Restricting to entities in the document would definitely help here but still there might be unrecoverable errors. A simple model conditioned on the context would definitely perform better. Similarly in sentence 3, the POS based heuristics would just be more robust if a classifier is learned. ', 'rating': '9: Top 15% of accepted papers, strong accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		HylHE-9p6m	Hkl7S1hHfN	AKBC.ws/2019/Conference/-/Paper45/Official_Review	['AKBC.ws/2019/Conference/Paper45/Reviewers/Unsubmitted']	3		['everyone']	HylHE-9p6m	['AKBC.ws/2019/Conference/Paper45/AnonReviewer1']	1547185978947		1547662994466	['AKBC.ws/2019/Conference/Paper45/AnonReviewer1', 'AKBC.ws/2019/Conference']
699	1547170919139	"{'title': 'lack of details, few novelty', 'review': ""This paper presents a hierarchical framework for integrating user feedback for KB construction under identity uncertainty.\n\n1. it is unclear about the algorithm implementation, such as what is the implementation of feedback mention.\n\n2. There is a definition about attribute map, but I can't find where the model uses it. Same thing for the precision of a node pair.\n\n3. How to calculate the function g(.) is unclear either. \n\n4. In section 5.2, the COMPLETE definition seems not correct, it is still the definition of PURE.\n\n5. The example for constructing positve/negative feedback is too vague.\n\n6. The experiment section needs more analysis including qualitative result."", 'rating': '3: Clear rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		SygLHbcapm	SylJd4urG4	AKBC.ws/2019/Conference/-/Paper51/Official_Review	['AKBC.ws/2019/Conference/Paper51/Reviewers/Unsubmitted']	2		['everyone']	SygLHbcapm	['AKBC.ws/2019/Conference/Paper51/AnonReviewer2']	1547170919139		1547662994247	['AKBC.ws/2019/Conference/Paper51/AnonReviewer2', 'AKBC.ws/2019/Conference']
700	1547157108129	"{'title': 'New distant supervision method focusing on mention coverage, but incomplete evaluation ', 'review': ""The paper proposes a new way to get distant supervision for fine grained NER using wikipedia, freebase and a type system. The paper explores using links in wikipedia as entities, matching them to freebase, and then expanding the using some string matching, and then finally pruning sentences using heuristics. Methods are compared on FIGER. The paper also introduces a dataset, but it is not fully described. \n\nOne interesting aspect about this paper is, as far as I can tell, one of the few works actually doing mention detection, and exploring the role mention detection. \n\nThat being said, its unclear what is new about the proposed source of supervision. The first two stages seem similar to standard methods and the last method (generally speaking pruning noisy supervision), has also been explored (e.g. in the context of distant supervision for IE, http://www.aclweb.org/anthology/P18-2015, and see Section 2.2) . Its also not clear to me what specifically targets good mention detection in this method. The experiments do somewhat argue that mention detection is improving, but not really on FIGER, but instead on the new dataset (this inconsistency causes me some pause). \n\nAll that being said, I don't think it would matter much if the supervision were incorporated into an existing system (e.g. https://github.com/uwnlp/open_type or https://github.com/MurtyShikhar/Hierarchical-Typing) and demonstrated competitive results (I understand that these systems use gt mention, but any stand in would be sufficient).  Table 5 on the other hand has some results that show baselines that are significantly worse than the original FIGER paper (without gt mentions) and the proposed supervision beating it (on the positive side, beating the original FIGER results too, but not included it in the table, see Section 3 of http://xiaoling.github.io/pubs/ling-aaai12.pdf ). So in the end, I'm not convinced on the experimental side.\n\nThis paper could be significantly improved on the evaluation. Incomplete reference to previous work on FIGER and insufficient description of their new datasets are areas of concern. Cursory reference, instead of evaluation, on new fine grained datasets (like open-type) also seem like missing pieces of a broader story introducing a new form of supervision.\n"", 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		HylHE-9p6m	H1enOAVBGE	AKBC.ws/2019/Conference/-/Paper45/Official_Review	['AKBC.ws/2019/Conference/Paper45/Reviewers/Unsubmitted']	2		['everyone']	HylHE-9p6m	['AKBC.ws/2019/Conference/Paper45/AnonReviewer3']	1547157108129		1547662994036	['AKBC.ws/2019/Conference/Paper45/AnonReviewer3', 'AKBC.ws/2019/Conference']
701	1547155003949	"{'title': 'The authors design an open knowledge graph (OKG) dataset, an evaluation protocol and also present and evaluate a number of baseline models for open link prediction. I am inclined to recommend the paper for a weak accept. I believe the paper overall is solid, but maybe not as significant or novel as other submissions.', 'review': ""strengths:\n\n--although link prediction benchmarks do exist, some have now been over-used. Although I do not completely agree with the distinction (as the authors have presented it) between 'open' and normal link prediction, I think the presence of a novel benchmark would be welcome in the community.\n\n--related work section has been covered in fairly impressive detail.\n\n--there are a fair number of new experimental results.\n\n\nweaknesses:\n\n--first, I am not completely sure that I agree with the distinction between OKGs and KGs. The primary distinction seems to be that the entities have not been 'disambiguated' or resolved to a common entity, and because nodes and relations are still so much at the 'extraction' level there is more noise and ambiguity. However, the link prediction task as it is set up is very similar to link prediction as it is set up in KGs. Since this is what the authors are ultimately proposing a benchmark for, the novelty is depressed a little bit.\n\n--the OKG dataset the authors extracted from Wikipedia seems very reminscent of a raw dataset that is available on the DBpedia downloads page (along with a much cleaner version that most people equate with the 'DBpedia' dataset itself). Again, I'm not sure how novel such a benchmark really is."", 'rating': '6: Marginally above acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		HJl_NZ5T67	r1gNBU4SGE	AKBC.ws/2019/Conference/-/Paper46/Official_Review	['AKBC.ws/2019/Conference/Paper46/Reviewers/Unsubmitted']	3		['everyone']	HJl_NZ5T67	['AKBC.ws/2019/Conference/Paper46/AnonReviewer1']	1547155003949		1547662993829	['AKBC.ws/2019/Conference/Paper46/AnonReviewer1', 'AKBC.ws/2019/Conference']
702	1547153859032	"{'title': 'Needs better framing and justification for why this approach should work.', 'review': ""KG-Cleaner\n\nHere researchers present a standalone system for evaluating the credibility of extracted knowledge tuples\nand for possibly correcting them if they are wrong.\n\nI do not have many comments on the modeling choices. The authors do a really good job of baselining their model and providing quantitative metrics. However, I would like to see more intuition for *why* this approach\nworks. Perhaps a discussion from a cognitive computing or linguistic perspective focusing on how the inputs to the models might give signal. I think the authors err on the side of expanding on quantitative and modeling discussions, many of which would be better suited for an appendix. Figure 4 needs to be better explained, and the general model conception should be better contextualized.\n\nOverall, this paper seems well thought out, well researched and overall convincingly presented,\nhowever, it reads like one in which authors cut many relevant parts of their thinking in order to meet a page limit. I would like to see the model framed better. I would suggest a Revise and Resubmit.\n\nSome specific comments:\n\nThe end of the introduction is not helpful. The discussion on model choice on Page 3, paragraph 2 \nis inappropriate because the reader has no idea what the framework is yet. In fact, much of the introduction contains \nuncontextualized methodology that is hard for the reader to really get. Remove all of that from the intro... \nwe don't need to know any of that until later.\n\nOne small comment on the approach: why the average of word embeddings? If this is your space:\n\n   | x \n__|__\nx |\n\nthe average will be in the middle, and will be meaningless. Can you show an example of this or provide a justification?\n"", 'rating': '6: Marginally above acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		ryleB-56pQ	Bkgi6WVSzE	AKBC.ws/2019/Conference/-/Paper49/Official_Review	['AKBC.ws/2019/Conference/Paper49/Reviewers/Unsubmitted']	3		['everyone']	ryleB-56pQ	['AKBC.ws/2019/Conference/Paper49/AnonReviewer3']	1547153859032		1547662993614	['AKBC.ws/2019/Conference/Paper49/AnonReviewer3', 'AKBC.ws/2019/Conference']
703	1547151942943	{'title': 'Authors propose a two stage synonym expansion framework for large shopping taxonomies. Paper is well written and empirical study is well conducted.', 'review': 'Authors present a method for expanding taxonomies with synonyms or aliases. The proposed method has two stages, 1) generate synonym candidates from WordNet and then 2) use a binary classifier to filter the candidates. The method is simple and effective. Paper is well written with ample empirical study and analysis. Couple of minor comments:\n1) Rather than using WordNet, for step 1, is it possible to use a similarity based clustering method to mine candidates for each concept from a corpus?\n2) For the word embeddings used in step 2, did the authors use off-the-shelf pre-compuated embeddings or compute the embeddings from a domain specific (in this case shopping) corpus? Will the performance improve if a domain specific embedding is applied?\n', 'rating': '7: Good paper, accept', 'confidence': '5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		rJx2g-qaTm	rJxJU57rz4	AKBC.ws/2019/Conference/-/Paper24/Official_Review	['AKBC.ws/2019/Conference/Paper24/Reviewers/Unsubmitted']	3		['everyone']	rJx2g-qaTm	['AKBC.ws/2019/Conference/Paper24/AnonReviewer3']	1547151942943		1547662993389	['AKBC.ws/2019/Conference/Paper24/AnonReviewer3', 'AKBC.ws/2019/Conference']
704	1547151596707	{'title': 'Lack of Clarity in contribution, model novelty and experimentation is very vague', 'review': 'This paper proposes a framework for KB based understanding and reasoning for queries based on the semantic web technology. To do so, the authors develop a method to extract a Description Logic form from natural queries, capturing their semantics. \nSome limitations of the work\n1. The method of extracting concepts from text, involving the normalizing function and the concept builder proposed by the authors is fairly trivial. The actual problem of extracting concepts from text has other complexities, ambiguity in the dependency structure or the semantic level, which has not been incorporated in the proposed algorithms.\n2. Section 4 (On Reasoning using textual Knowledge) which should have been one of the main contributions of the work, does not have a concrete end-to-end model. This is a major lacking and makes it hard to understand the contribution of this work.\n3. The Experimental section raises a lot of questions. First despite various existing datasets that are more popular, the authors have chosen a much less active dataset (Anonymous) and focused on a very specific problem of Symptom Activation. Also as baselines they have used an existing approach GATE, for which the reference is missing, it is not clear how GATE works exactly and whether it is state-of-the-art. While GATE seems to be trained model, the proposed model in this paper (KAL) is rule-based, it is not clear whether GATE and KAL should be even comparable, as the latter may have handcrafted rules (as in Algo 1 and 2).  \nThe main concern with the paper is the experiment is very vague and there are lots of gaps in the explanation and clarity of the experiment setting, baselines and choice of dataset. The other main concern is there is no crisp learning algorithm presented as the model, which makes the contribution hard to understand and generalize.', 'rating': '4: Ok but not good enough - rejection', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		H1l8ag9a6m	rkxBeKmSME	AKBC.ws/2019/Conference/-/Paper3/Official_Review	['AKBC.ws/2019/Conference/Paper3/Reviewers/Unsubmitted']	2		['everyone']	H1l8ag9a6m	['AKBC.ws/2019/Conference/Paper3/AnonReviewer3']	1547151596707		1547662993171	['AKBC.ws/2019/Conference/Paper3/AnonReviewer3', 'AKBC.ws/2019/Conference']
705	1547148483359	"{'title': 'RecallIE is an interesting concept and provides a novel perspective on IE evaluation. However the definition and examples are problematic. The introduced methods lacks details and technical depth. Empirical study is well studied and analyzed. ', 'review': 'Authors present a method named RecallIE to estimate the recall of information extraction from a text passage. Two classification methods are proposed and evaluated with different experiment settings.\nThe problem is defined as given a subject and a predicate, and a list of facts extracted with any IE method from a given text passage, build a binary classifier to determine either a) the text passage has all the facts following subject predicate schema, or b) the text passage doesn\'t have all the facts.\n1, I am not certain the usage of Recall is correct here. There are different types of predicates. Some only requires a short and single fact (e.g., birthday of Obama). Some may require a list of facts to be correct (e.g., children of Obama). Modifiers on predicates change meanings and also types. In the example given in the paper, ""first adopted son"" is a simple predicate, the answer to which is a single fact. Authors of this paper simply removed the modifiers to ""son"" and use the fact which is meant for ""first adopted son"" for ""son"". This seems to be wrong and incoherent.\n2, The problem is defined to evaluate a set of facts from a text passage but the proposed methods did not seem to use the facts. They are classification methods on the text passages. So the evaluation seems to be on the text passages rather than the IE facts.\nOverall, I do not think this paper is quite ready for publication.', 'rating': '5: Marginally below acceptance threshold', 'confidence': '5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		HkgAyb5aaQ	SJeja3GrME	AKBC.ws/2019/Conference/-/Paper18/Official_Review	['AKBC.ws/2019/Conference/Paper18/Reviewers/Unsubmitted']	3		['everyone']	HkgAyb5aaQ	['AKBC.ws/2019/Conference/Paper18/AnonReviewer3']	1547148483359		1547662992740	['AKBC.ws/2019/Conference/Paper18/AnonReviewer3', 'AKBC.ws/2019/Conference']
706	1547148339283	"{'title': 'Cool idea to built large scale unsupervised reps for tables but weak experiments', 'review': 'The paper proposes to train a skip gram embedding model on contexts constructed by merging related cells of web tables together. Table representations are computed by looking at different average statistics over cell vectors.  Finally, the representation is evaluated by trying to classify tables into five types (entity, relational, matrix , list, non-data). A dataset of 2500 instances is constructed for the purpose of evaluation. \n\nI like the idea, as it leverages a huge amount of free data from the web. It might be interesting to also think of this as building representations for web ""entities"" as well. I would have liked more thorough exploration of potential context construction strategies, including addressing the \'non-local\' nature of a table. \n\nIn general, the idea is intuitive and could potentially have some interesting applications. I don\'t think table classification is one such example. Potential real application could be directly for information extraction from tables, or question answering on tables (ex. https://nlp.stanford.edu/software/sempre/wikitable/) . As the dataset presented in the paper is small, specifically collected for this paper and with unclear data splits, its difficult to accept this as a sufficient evaluation of the method.', 'rating': '4: Ok but not good enough - rejection', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		Hye97Zcaam	r1li4hzBGN	AKBC.ws/2019/Conference/-/Paper41/Official_Review	['AKBC.ws/2019/Conference/Paper41/Reviewers/Unsubmitted']	3		['everyone']	Hye97Zcaam	['AKBC.ws/2019/Conference/Paper41/AnonReviewer3']	1547148339283		1547662992520	['AKBC.ws/2019/Conference/Paper41/AnonReviewer3', 'AKBC.ws/2019/Conference']
707	1547148033281	{'title': 'Sound approach for rule learning but heavy dependence on black-box algorithm to propose candidate rules', 'review': 'The paper proposes a model for probabilistic rule learning to automate the completion of probabilistic databases. The proposed method uses lifted inference which helps in computational efficiency given that non-lifted inference in rules containing ungrounded variables can be extremely computationally expensive.\nThe databases used contain binary relations and the probabilistic rules that are learned, are also learned for discovering new binary relations. The use of lifted inference restricts the proposed model to only discover rules that are a union of conjunctive queries.\nThe proposed approach uses AMIE+, a method to generate deterministic rules, to generate a set of candidate rules for which probabilistic weights are then learned. The model initializes the rule probabilities as the confidence scores estimated from the conditional probability of the head being true given that the body is true, and then uses a maximum likelihood estimate of the training data to learn the rule probabilities. \n\nThe paper presents empirical comparison to deterministic rules and ProbFOIL+ on the NELL knowledge base. The proposed approach marginally performs better than deterministic rule learning.\n\nThe approach proposed is straightforward and depends heavily on the candidate rules produced by the AMIE+ algorithm. The paper does not provide insights into the drawbacks of using AMIE+, the kind of rules that will be hard for AMIE+ to propose, how can the proposed method be improved to learn rules beyond the candidate rules. \n\n‘End-to-end differentiable proving’ from NeurIPS (NIPS) 2017 also tackles the same problem and it would be nice to see comparison to that work. \n', 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		HkyI-5667	HyxYWizrfV	AKBC.ws/2019/Conference/-/Paper54/Official_Review	['AKBC.ws/2019/Conference/Paper54/Reviewers/Unsubmitted']	3		['everyone']	HkyI-5667	['AKBC.ws/2019/Conference/Paper54/AnonReviewer2']	1547148033281		1547662992308	['AKBC.ws/2019/Conference/Paper54/AnonReviewer2', 'AKBC.ws/2019/Conference']
708	1547147246536	{'title': 'Interesting problem but no model contribution and clarity in the experimentation lacking', 'review': 'The paper takes an interesting problem of answering structured Queries directly over Text, without the need for constructing an intermediate structured database to represent the text. Their specific contributions are in generating training data for the said task they discuss a model for answering i) slot filling queries and ii) structured queries over a corpus. \nSome of the limitations of the work\n1. The problem of training data generation involves mapping triples associated with a given predicate in a RDF graph with a textual document describing the entities. The Algorithm 1 describing this mapping process and the functions, type, lexicalize, textual_description seem quite straight-forward. Techniques like these have been applied in various settings where end-to-end question-answering needs to be done on a combination of structured data like Knowledge Bases and unstructured data for e.g. Wikipedia like corpus. It is not clear what are the non-trivial challenges of this data generation process.\n2. In the model section, the authors have not proposed any new model for the question answering problem. Both FastQA and JackQA (reference missing) are previous works referred by the authors. Also the Table 2 in the experiment section is not explained properly: mean, std, min, max is not defined and the main take-away from the table is not clear. Also what do SP and PO mean for FastQA in Table 2, it has not been explained in the paper. Also based on Fig 3 the authors concluded models based on deep learning notably outperform the baseline models on average. It is now clear from Fig 3 how. Also what are the other notable points about Fig 3? \n3. In the analysis section, some of the points are very straight-forward and too intuitive to be mentioned for e.g. ‘simple syntactic patterns seem to be learned well’ or ‘properties that have specific value constraints within Wikidata generate good results’. \nThe experiment section has lots of gaps, and is much less rigorous than most published works in this area. There is no model-level contribution as such and it is not clear from these experiments, what is the main-takeaway. \nAlso the authors have focused on slot-filling queries, which are most simplest forms of queries. To make the study more comprehensive the authors should also study more complex queries than just slot-filling. \n', 'rating': '4: Ok but not good enough - rejection', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		BygfyW5pam	HkewlOzSzN	AKBC.ws/2019/Conference/-/Paper13/Official_Review	['AKBC.ws/2019/Conference/Paper13/Reviewers/Unsubmitted']	3		['everyone']	BygfyW5pam	['AKBC.ws/2019/Conference/Paper13/AnonReviewer3']	1547147246536		1547662992081	['AKBC.ws/2019/Conference/Paper13/AnonReviewer3', 'AKBC.ws/2019/Conference']
709	1547147108346	{'title': 'Novel solution and comprehensive experimentation', 'review': 'This paper proposes a novel solution to the relation composition problem when you already have pre trained word/entity embeddings and are interested only in learning to compose the relation representations . The proposed supervised relational composition method learns a neural network to classify the relation between any given pair of words and as a by-product, the penultimate layer of this neural network, as observed by the authors, can be directly used for relation representation.\nThe experiments have been performed on the BATS dataset and the DiffVec Dataset.\nThe inferences that were made to advocate for the usefulness of proposed MnnPL are as follows-\n- Out_of_domain relation prediction experiment to test for generalisability showed that MnnPL outperformed other baselines at this task\n- The interesting analysis in table 3 highlights the difficulty in representing lexicographic relations as compared to encyclopedic. MnnPL outperforms others here too. The authors provide a reasonable explanation (Figure 2) as to why the PairDiff operator that was proposed to work well on the Google analogy dataset works worse in this scenario.\n- The experiment to measure the degree of relational similarity using Pearson correlation coefficient, showcases that the relational embeddings from MnnPL are better correlated with human notion of relational similarity between word pairs\n- They also showed that MnnPl is less biased to attributional similarity between words\nThe authors show that the proposed MnnPL had outperformed other baselines on several experiments.\n\nSome of the positive aspects about the paper\n- elaborately highlighted all the implementations details in a crisp manner\n- Extensive experimentation done and a very due-diligent evaluation protocol.\n- In the experiments the authors have compared their proposed supervised operator extensively with other unsupervised operators like PairDiff,  CONCAT, e.t.c and some supervised operators like SUPER_CONCAT, SUPER_DIFF e.t.c. They also compared against the bilinear operator proposed by Hakami et al., 2018, which was published very recently.\nSome of the limitations of the work\n- Though extensive experiments have been done and elaborate evaluation  protocols have been followed to advocate for MnnPL, I believe that it lacked slightly on novelty side.\n- Reference to Table 2 on page 12 should actually be a reference to figure 2\n\nQuestions for rebuttal:-\n- Some reasoning on why does the CONCAT baseline show a better Pearson correlation coefficient than PairDIff?\n- Interesting to see that CBOW performed better than others, especially GLOVE, on all experiments. Some analysis on this.\n- Break down of the performance for the two semantic relation types, could be shown on a few other datasets to strengthen claim.\n', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		r1e3WW5aTX	S1l3DPGSfN	AKBC.ws/2019/Conference/-/Paper30/Official_Review	['AKBC.ws/2019/Conference/Paper30/Reviewers/Unsubmitted']	3		['everyone']	r1e3WW5aTX	['AKBC.ws/2019/Conference/Paper30/AnonReviewer2']	1547147108346		1547662991864	['AKBC.ws/2019/Conference/Paper30/AnonReviewer2', 'AKBC.ws/2019/Conference']
710	1547146585791	"{'title': ""The paper covers unsupervised domain adaptation (UDA), an important topic in machine learning especially given the amount of effort that can go into labeling large datasets. I think the paper is good, albeit with some small concerns. I've provided details below"", 'review': ""Short summary: A machine learning model trained using data from one domain (source domain) might not\nnecessarily perform well on a different (target) domain when their distributions are different. The authors propose a novel unsupervised domain adaptation (UDA)\nmethod that combines projection and self-training based approaches. For evaluation, the authors use cross-domain sentiment classification as an evaluation task citing precedent.\n\nstrengths:\n\n--The related work and explanation of the problem is relatively complete and well written. I would have liked to see some more intuition on projection in the earlier sections. \n\n--the approach is simple enough and I always consider that a significant strength, though it does appear a little bit like the authors tried some of the more established techniques experimentally and found that their workflow worked better than the individual techniques. This is fine, but it would be good to have some more insight (even if after-the-fact) on why the improvements were obtained.\n\n--overall, the experiments are well-described and replicable. They seem convincing and significant, and I think the notion that relatively simple pipelines can outperform more complex neural UDA is an important one that should be presented to the community. I have some minor concerns with experimental details that I have pointed out below.\n\n\nweaknesses:\n\n--I have heard (and personally witnessed) from many practitioners that summing the word embeddings is not always the best strategy for representing documents. Certainly, there are better methods now for learning document representations (even the now-classic topic models serve the purpose quite well with relatively normal datasets). So why not use one of those?\n\n--Is the pseudocode of Alg. 1 really necessary? The text explanation was clear enough. It's taking up space and 'looks' more impressive than it really is (training a classifier on S_L* and applying on T with threshold to obtain T_L')\n\n--from the evaluation description, it seems that labeled data was used from the target domain (for validation). Although this seems like (and has been presented as) a small detail my intuition suggests it may have ended up yielding better performance than would otherwise have been obtained. Some parameter sensitivity was explored in the experiments -- a little more on this issue at the beginning of the experiments section would have been useful. \n\n--Although I understand that acquiring datasets can be a problem, and I laud the authors' efforts to expand the 4-domain task that has been over-used at this point in UDA, it would still have been good to try the approach on one more problem. Although the authors considered many 'domains' they were still very similar to one another (sentiment classification on Amazon). How generalizable is the approach in terms of other applications?"", 'rating': '7: Good paper, accept', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		H1xtpgqTaQ	ryxGwrMSG4	AKBC.ws/2019/Conference/-/Paper4/Official_Review	['AKBC.ws/2019/Conference/Paper4/Reviewers/Unsubmitted']	3		['everyone']	H1xtpgqTaQ	['AKBC.ws/2019/Conference/Paper4/AnonReviewer1']	1547146585791		1547662991647	['AKBC.ws/2019/Conference/Paper4/AnonReviewer1', 'AKBC.ws/2019/Conference']
711	1547145799001	"{'title': 'Interesting modification of Winograd challenge, but unclear method and seemingly incomparable results', 'review': 'The paper proposes to factor the Winograd challenge into retrieval of knowledge relevant (a particular sentence) for solving a winograd co-ref problem, then transformation of that sentence to align in some way to the original scenario, and then an NLI system.  While this kind of factoring is not new to this paper, the idea of creating some sort of ground truth for the knowledge competent is new. \n\nTo be honest, I struggle to understand the examples of the knowledge in the paper, or how they relate scenario.  An example:\n\n""The fireman arrived before the police because they were coming from so far"" (challenge: find the referent of they), the knowledge proposed is : ""My teachers know that I arrived late sometimes. They do not punish me because the know I live far away"".  Could you explain this more explicitly?\n\nI understand that these sentences are transformed to be later used in NLI, but I just don\'t get it. Perhaps Table 2 could be better presented with the examples through the whole pipeline (transformed / aligned, and then the actual query we would give the NLI system). \n\nIn terms of the actual transformation of the knowledge into NLI instances, I am also confused about the method.  I like the case breakdown, but the examples seem incomplete. What I really need to see are examples that are going into the NLI system after these transforms.  \n\nRegardless, the knowledge seems to be manually sourced with google, but the comparisons offered in the paper to other work seem to either not use explicit knowledge, or do not use ground truth retrievals from the internet. This really makes the comparisons inappropriate since this methods involves manual curation of knowledge specific to the instances in the winograd challenge. This could be improved by using automatic retrievals and evaluating end-to-end (you could keep existing experiments as a type of ablation). \n\nIn general, I felt the paper is a work in progress that is going in an interesting direction that (a) needs a more clear presentation (fewer examples but more complete end-to-end examples, with explicit explanations) and (b) a more comparable set of experiments.\n\n\n\n', 'rating': '3: Clear rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		H1xdMZ566m	ByxJLMzrfE	AKBC.ws/2019/Conference/-/Paper35/Official_Review	['AKBC.ws/2019/Conference/Paper35/Reviewers/Unsubmitted']	2		['everyone']	H1xdMZ566m	['AKBC.ws/2019/Conference/Paper35/AnonReviewer2']	1547145799001		1547662991393	['AKBC.ws/2019/Conference/Paper35/AnonReviewer2', 'AKBC.ws/2019/Conference']
712	1547141193926	"{'title': 'Nice review of the development and recent advances in semantic parsing ', 'review': 'This work provided a comprehensive review of important works in semantic parsing. It starts with the rule-based systems in the early days. Then it described the introduction of statistical methods to learn from natural language and logical form pairs. Finally, it summarized the recent advances in weakly supervised semantic parsing or learning semantic parsing from denotations and the rise of seq2seq models. It also briefly compared different learning strategies (MML, RL, Max-Margin).  \n\nThe paper is well written and easy to follow. The survey covers most of the important works in the field. It provides a good summary of the development of the field as well as the most recent advances. I support the acceptance of this paper. \n\nSome minor comments: \n\n""the ATIS domain is : What states border Texas :λx.state(x)borders(x, texas)."" \nIs this example from ATIS? It seems more like a GeoQuery example. \n\nRegarding Reinforcement Learning (section 7.2), there is a recent work (Liang et al, 2018) that is quite relevant. It introduced a principled RL method for semantic parsing, and compared it with other objectives like MML. It also introduced a systematic exploration strategy to address the exploration problem mentioned in this section. Might be worth discussing here. \n\nMemory Augmented Policy Optimization for Program Synthesis and Semantic Parsing, Liang, Chen and Norouzi, Mohammad and Berant, Jonathan and Le, Quoc V and Lao, Ni, Advances in Neural Information Processing Systems, 2018', 'rating': '8: Top 50% of accepted papers, clear accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		HylaEWcTT7	S1gGIl-HMN	AKBC.ws/2019/Conference/-/Paper48/Official_Review	['AKBC.ws/2019/Conference/Paper48/Reviewers/Unsubmitted']	3		['everyone']	HylaEWcTT7	['AKBC.ws/2019/Conference/Paper48/AnonReviewer1']	1547141193926		1547662991174	['AKBC.ws/2019/Conference/Paper48/AnonReviewer1', 'AKBC.ws/2019/Conference']
713	1547137981794	"{'title': '""Siamese"" networks trained to distinguish relational pairs from non pairs produce relational embeddings', 'review': 'In this paper, a ""Siamese"" network is trained to produce embeddings that enable sentence contexts containing similarly related terms to be distinguished from those with dissimilarly related terms.  This encourages the embeddings to represent the relation consistently.\n\nThese embeddings allow the system to outperform the state of the art in KB completion tasks.\n\nSome specific comments:  Although the paper is in the most part clear and well written,  there are sections and isolated sentences with a high rate of grammatical and usage errors; thorough proof-reading is advised.\n\nWould performance be improved by using BERT embeddings instead of Glove?\n\nIt\'s surprising that the networks do not include specific inputs for the relational arguments.  This might be expected to improve performance by allowing attention to be more finely focussed on the predicate rather than the arguments. This is supported by the authors\' intent to try placeholders in future work', 'rating': '8: Top 50% of accepted papers, clear accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		SkxE1b56TQ	HkxLTQxrMN	AKBC.ws/2019/Conference/-/Paper14/Official_Review	['AKBC.ws/2019/Conference/Paper14/Reviewers/Unsubmitted']	2		['everyone']	SkxE1b56TQ	['AKBC.ws/2019/Conference/Paper14/AnonReviewer2']	1547137981794		1547662990953	['AKBC.ws/2019/Conference/Paper14/AnonReviewer2', 'AKBC.ws/2019/Conference']
714	1546811158415	"{'title': 'Marginally interesting results but uninteresting techniques and questionable soundness', 'review': 'In this paper the authors propose two techniques for SRL domain adaptation, where the new domain has a different label set (vs. assigning the same labels to a new data domain). Both approaches pre-train on CoNLL-2005 and CoNLL-2012 PropBank SRL data, followed by fine-tuning on the target domain (ProcessBank biological processes). One approach uses a manual mapping between source domain labels to target domain labels in an LSTM-CRF, and the other approach replaces the source domain CRF layer in a CNN-LSTM-CRF with a target-domain CRF (more here?) They evaluate their techniques by adapting PropBank SRL to ProcessBank annotations, showing that their best models improve over the state-of-the-art by >20 F1 absolute for both techniques, and that an additional 2.6 F1 can be obtained by modeling event-event relationships in ProcessBank (annotation not available in ProbBank).\n\nThe main contribution of this paper is empirical results showing that pre-training LSTM-CRF models on CoNLL-2005/2012 improves ProcessBank performance by about 20 F1 over Berant et al. (2014). This result is marginally interesting, but none of the proposed techniques are novel, the paper is not very clearly written, and while there are many experimental results, I\'m not convinced of their soundness. Why does the DeepSRLCRF perform worse than DeepSRL on the CoNLL datasets? I would not expect this result, and you do not provide a compelling explanation or analysis, which makes me think this result might be due to a bug, and raises suspicion about other results using this model. I also find it strange that the two adaptation techniques are applied to different base models, and there is no justification for this in the paper. The best performing model requires manual alignment between label sets, which is not very desirable, and would not extend well to transfer to more diverse data.\n\nQuestions:\n- Why use an LSTM-CRF with the first technique, and a CNN-LSTM-CRF for the second technique? Is there any rationale for this? It takes away from the results.\n\nStyle/writing comments:\n- may want to note that the source domain is PropBank in the abstract/intro\n- I find the edges in Figure 1 a bit strange. I think usually there would be one edge from the predicate (""reduced"") to the argument span (""the genetic variation fo the population""), rather than edges between each token in the span. Unless this is actually how you model dependencies between tokens/predicates/arguments (but I don\'t think this is the case). Using the same notation for ""Cause"" and ""Theme"" (big boxes at the top) is confusing since the semantics of the edges is quite different -- one is between predicates and the other is between a predicate and an argument. \n- Section 2, under ""Domain adaptation for SRL"": ""domain"" should not be capitalized. I also don\'t think question answering should be capitalized\n- Section 3.1: LSTM update formulas are completely unnecessary in this paper, since you don\'t modify them. Simply refer to Zhou and Xu (2015). This applies to many of the equations in this section (CRF, recurrent dropout, gated highway connections...) As a general rule, if you don\'t modify the equations you don\'t need to include them in the paper.\n- You say that you don\'t have any explicit constraints for decoding, but using Viterbi decoding is a slightly fancier version of enforcing BIO constraints (as in He et al. 2017). I think this statement is a bit misleading/confusing.\n- ""figure"" in ""figure 2"" should be capitalized\n- ""Viterbi"" should be capitalized\n- Section 3.2: you say that the CRF ""jointly decodes the best sequence,"" what is joint here? \n- In the introduction you state that the source domain is news, but CoNLL-2012 actually contains non-news domains.\n- Section 4: please clarify how you add the event-event interactions to the input.\n- Table 1: strange choice of ordering of the arguments, I would expect ARG0-4 to be in order.\n- I\'m interested in why you chose to map ARGM-MNR to other in ProcessBank. It\'s not the obvious choice without looking at the ProcessBank data.\n- Table 2: typo in caption (resuls -> results)\n- Section 6, ""Semantic role labeling"": typo: should be structured\n- Section 6, ""Domain Adaptation"": ""specially"" should be ""especially""', 'rating': '4: Ok but not good enough - rejection', 'confidence': '5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		Hklbplqp6X	S1gRGPlgzE	AKBC.ws/2019/Conference/-/Paper1/Official_Review	['AKBC.ws/2019/Conference/Paper1/Reviewers/Unsubmitted']	1		['everyone']	Hklbplqp6X	['AKBC.ws/2019/Conference/Paper1/AnonReviewer2']	1546811158415		1547662990735	['AKBC.ws/2019/Conference']
715	1546952098885	{'title': 'domain adaptation study -- needs more work / better analysis', 'review': 'The paper studies domain adaptation of semantic role labelling systems. The paper focuses primarily on label shift: they consider adapting a system estimated on PropBank to ProcessBank labeled with a different annotation schema.\n\nUnfortunately, the paper is not very well written, and it is hard to figure out what exactly the contributions are. The two considered approaches to  domain adaption are somewhat trivial\n— using a human-developed mapping between PropBank and ProcessBank roles \n— adapting CRF layer of a neural model then performing a couple of extra epochs on the target-domain dataset while updating (‘fine-tuning’) all the parameters. \n\nThere are lots of small things in the paper which make any findings questionable:\n\n— The different systems compared are different not only in architecture but also in many other modelling / optimization choices. I really do not understand what kind of conclusions I can draw from the results.\n\n— The DeepSRL system without ELMO used in the paper is trained only for 50 epoch, and it’s accuracy on the source domain is very low (40% whereas the state of the art is closer to 90%)\n\n— The authors use gold-standard event-to-event relations, as features. In practice they are not available at test time. Moreover, some of these relations are very informative for predicting roles — e.g., CAUSE events often correspond to agent arguments in the predicate argument structure (see even the example in Figure 1). The discussion at the end of section 4 suggesting much weaker correspondences, I believe, somewhat misleading. \n\n— Why not combine the two ideas together?\n\n\n\nUnfortunately, the paper is not well written, and this is not only about many typos and having some excessive details. Unfortunately, some important discussions and analyses are missing, e.g.: \n\n1)  Process bank uses nouns as predicates (as far as I can see from Figure 1 — drift is marked as predicate), while only verbs are annotated in PropBank. This must be an important problem which does not seem to be even mentioned. \n\n2) PropBank roles An, for  n > 2 are predicate (actually, frame / sense) specific. Why does it make sense to use a fixed mapping in Table 1? Any kind of analysis what is actually happening? \n\n3) More details on divergences between ProcessBank and PropBank annotations are necessary. \n\nExtra questions:\n— Typically arguments are marked for a specific verb, i.e. the semantic role labeling problem is approached as multiple labeling problems (one per predicate). Why is ‘drift’ labeled as B-V in Figure 1? Should not it be “O”?\n— Is there some way to disentangle the drop in performance due to divergences in annotation (label bias) and the domain mismatch (covariant shift)? \n\nUnfortunately, though I agree that domain adaptation for SRL is an important topic, this work is not ready for publication. The approach is not particularly novel, and the main contribution is an empirical study and analysis, which are not very well done.\n\n\n\n\n\n\n', 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		Hklbplqp6X	B1gsoazMzE	AKBC.ws/2019/Conference/-/Paper1/Official_Review	['AKBC.ws/2019/Conference/Paper1/Reviewers/Unsubmitted']	2		['everyone']	Hklbplqp6X	['AKBC.ws/2019/Conference/Paper1/AnonReviewer1']	1546952098885		1547662990468	['AKBC.ws/2019/Conference']
716	1546985054728	"{'title': 'Promising initial results for domain adaptation but needs a clearer explanation of prior work and more experimental evidence.', 'review': 'The authors address the task of adapting an SRL system to a low-resource setting. They provide two domain adaptation approaches: 1) manually building a mapping between source and target label domains, 2) re-training the decoder layer on the target domain. In both cases, they pre-train models on a larger (source) dataset and then fine-tune on the target data. They compare two model architectures: 1) DeepSRL-CRF, similar to He et. al. (2017, 2018), except they replace the constrained A* decoder with a CRF and 2) CNN-LSTM-CRF as in Ma & Hovy (2016). \n\nTheir experimental results show competitive performance for their proposed model architectures on CoNLL compared to strong (SOTA) baselines. Their domain adaptation approaches combined with their proposed models outperformed non-trivial baselines.\n\nPro’s:\n-\tClear description of the task, motivation, and challenges associated with SRL in low resource domains.\n-\tExperimental results are promising. In particular, the “replace-CRF-layer” domain adaptation strategy both shows strong performance and is easily applicable to other sequence tagging tasks and underlying model architectures.\n-\tReproducibility: Their experiments included non-trivial, previously-published baselines on well-known, public benchmark datasets. The authors will release their code and models publicly, and they provide details about the hyperparameter settings used to train models. \n\nCon’s/Suggestions:\n-\tRelated Work: It was unclear to me which of the prior works cited on domain adaptation is directly related to the authors’ proposed approaches. Which work(s) have used the label-mapping strategy? Is the idea of adapting the CRF decoder layer of a LSTM-CRF (or other neural model) to the target domain a novel contribution and if so, which works inspired the idea? If not, which work initially proposed this approach and how has this strategy been applied since?\n\nThe authors should provide more detail about the Berant et. al. (2014) work, including concrete examples from the ProcessBank dataset and a description of the baseline system. Why was this corpus chosen? Also, why was biological-domain SRL chosen over other low-resource domains?\n\nThe authors should cite the CRF paper: Lafferty, John, Andrew McCallum, and Fernando CN Pereira. ""Conditional random fields: Probabilistic models for segmenting and labeling sequence data."" (2001).\n\n-\tModels: The authors should succinctly state significant differences from approach He et. al. (2017) rather than re-explaining the whole paper. There was too much detail on this in Section 3.1. Also: What motivated these differences? Why did the authors decide to use a CRF instead of the original constrained A* decoding method?\n\nCNN-LSTM-CRF: What was the purpose of the 100-dimension predicate indicator? At the end of Section 3.1, they note that “We explicitly do not model predicate predication as we focus our efforts on the domain adaptation capability of the SRL given gold predicates."" Given this point, why did the authors then incorporate a predicate representation into the CNN-LSTM-CRF? It seems like the approach could be applied in both model architectures (i.e. concatenate a predicate indicator vector to the input word representations). As it stands, the fact that the CNN-LSTM-CRF has this additional information about the input might give it an unfair advantage. However, it’s possible I’ve misunderstood something about the two model architectures/implementations; if so, I would appreciate clarification. Also, why did they choose 100 dimensions? \n\n-\tCharacter-level features: The CNN-LSTM-CRF incorporates character-level features. It seems likely that this would be a significant advantage over word-only models in a low resource setting, since character-level features can help deal with out-of-vocabulary inputs with more expressive power than a single “UNK” word vector. I recommend that the authors provide some additional experiments and analysis to tease apart the effect of character-level representations vs. domain adaptation strategy vs. choice of model architecture. \n\n-\tCoNLL Experiments: The authors do not include the best CoNLL results from He et. al. (2017) in Table 2. In that paper, He et. al. report P/R/F of 85.0/84.3/84.6 (CoNLL 2005 WSJ) and 83.5/83.3/83.4 (CoNLL 2012). It is unclear to me why the authors chose to report a weaker result. If it was due to the fact that these results were for the ensembled “product-of-experts” model, why didn’t the authors attempt to incorporate this technique into their own work? Regardless, the authors should make a note of why they made this decision, so as to avoid misleading readers.\n\nWhy didn’t the authors report results for the CoNLL 2005 Brown test set in addition to WSJ? Since this test set is specifically designed to test systems’ ability to generalize to out-of-domain data, this would be a compelling additional experiment for assessing the strength of the “replace-CRF-layer” domain adaptation approach (the label-mapping approach does not apply, of course).  \n\nIt seems like more analysis is needed to explain why the DeepSRL-CRF performance is so much worse than that of the other systems. Was there a bug? Why types of errors did the system make? Are these errors that could be corrected by incorporating some of the constraints in the original DeepSRL system (e.g. BIO errors)? \n\n-\tProcessBank Experiments: The authors note that “pre-training on CoNLL 2012 was more effective than pre-training on CoNLL 2005”, and speculate that this was due to the fact that CoNLL 2012 is slightly larger. For completeness, why not just run another evaluation where all models are pre-trained on both datasets, combined? Or, even better, shuffle both together and evaluate performance over varying source dataset sizes? Which approach is most robust to smaller source dataset sizes, and which approach best takes advantage of additional data?\n\nCombining Tables 3 and 4 would improve readability.\n\n-\tAdditional Suggestions: The discussion of incorporating event-event relations is distracting and (in my opinion) irrelevant to the core proposed contribution of the paper. I recommend that the authors instead make brief a note of the fact that incorporating event-event relations improved results a lot, then handle further investigation of the subject in a separate paper. \n\nAlthough the authors leave this for future work, results for additional low-resource datasets (including CoNLL 2005/Brown) would make this paper more compelling. They should also consider applying these approaches to other sequence tagging tasks like POS tagging and NER. \n', 'rating': '6: Marginally above acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		Hklbplqp6X	SJxwwAqzGN	AKBC.ws/2019/Conference/-/Paper1/Official_Review	['AKBC.ws/2019/Conference/Paper1/Reviewers/Unsubmitted']	4		['everyone']	Hklbplqp6X	['AKBC.ws/2019/Conference/Paper1/AnonReviewer3']	1546985054728		1547662990204	['AKBC.ws/2019/Conference']
717	1546605427967	"{'title': 'REVIEW', 'review': 'The paper proposes to extend knowledge base completion benchmarks with visual data to explore novel query types that allow searching and completing knowledge bases by images. Experiments were conducted on standard KB completion tasks using images as entity representations instead of one-hot vectors, as well as a zero shot tasks using unseen images of unknown entities.\n\nOverall I think that enriching KBs with visual data is appealing and important. Using images to query knowledge bases can be a practical tool for several applications. However, the overall experimental setup suffers from several problems. The results are overall very low. In the non-zero shot experiments I would like to see a comparison to using entity embeddings, and maybe even using a combination of both, as this is the more interesting setup. For instance, I would like to see whether using images as additional information can help building better entity representations. The explored link prediction models are all known, so apart from using images instead of entities there is very limited novelty. The authors find that concatenation followed by dot-product with relation-vector works best. This is very unfortunate because it means that there is no interaction between h and t at all, i.e.: s(h, r, t) = [h;t] * r = h * r_1 + t * r_2. This means that finding t given h,r only depends on r and not on h at all. Finally, this shows that the proposed image embeddings derived from the pretrained VGG16 model are not very useful for establishing relations.\n\nGiven the mentioned problems I can unfortunately not recommend this paper for acceptence.\n\nOther comments: \n- I wouldn\'t consider a combination of pretrained image embeddings bsaed on CNNs with KB embeddings a ""novel machine learning approach"", but rather a standard technique   \n- redefine operators when describing LP models: \\odot is typically used for element-wise multiplication, for concatenation use  [h; t] for instance\n- (head, relation, tail) is quite unusual -->  better: (subject, predicate, object)\n- baselines are super weak. Concatenation should be the baseline as it connects h and t with r indepedent of each other. What is the probabilistic baseline?', 'rating': '3: Clear rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		BylEpe9ppX	Byenu7AhbN	AKBC.ws/2019/Conference/-/Paper2/Official_Review	['AKBC.ws/2019/Conference/Paper2/Reviewers/Unsubmitted']	1		['everyone']	BylEpe9ppX	['AKBC.ws/2019/Conference/Paper2/AnonReviewer2']	1546605427967		1547662989926	['AKBC.ws/2019/Conference']
718	1546693694029	{'title': 'Compelling new task and dataset', 'review': 'The paper introduces several novel tasks for visual reasoning resembling knowledge base completion tasks but involving images linked to entities: finding relations between entities represented by images and finding images given an image and a relation. The task is accompanied with a new dataset, which links images crawled from the web to FreeBase entities. The authors propose and evaluate the first approach on this dataset.\n\nThe paper is well written and clearly positions the novelty of the contributions with respect to the related work.\n\nQuestions:\n* What are the types of errors of the proposed approach? The error analysis is missing. A brief summary or a table based on the sample from the test set can provide insights of the limitations and future directions.\n* Is this task feasible? In some cases information contained in the image can be insufficient to answer the query. Error analysis and human baseline would help to determine the expected upper-bound for this task.\n* Which queries involving images and KG are not addressed in this work? The list of questions in 4.1. can be better structured, e.g. in a table/matrix: Target (relation/entity/image), Data (relation/entity/image)', 'rating': '9: Top 15% of accepted papers, strong accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		BylEpe9ppX	H1x8rhmCWV	AKBC.ws/2019/Conference/-/Paper2/Official_Review	['AKBC.ws/2019/Conference/Paper2/Reviewers/Unsubmitted']	2		['everyone']	BylEpe9ppX	['AKBC.ws/2019/Conference/Paper2/AnonReviewer3']	1546693694029		1547662989707	['AKBC.ws/2019/Conference']
719	1547060394904	"{'title': 'Interesting approach to a new task + new data set', 'review': 'This work aims to address the problem of answering visual-relational queries in knowledge graphs where the entities are associated with web-extracted images.\n\nThe paper introduces a newly constructed large scale visual-relational knowledge graph built by scraping the web. Going beyond previous data sets like VisualGenome having annotations within the image, the ImageGraph data set that this work proposes allows for queries over relations between multiple images and will be useful to the community for future work. Some additional details about the dataset would have been useful such as the criteria used to decide ""low quality images"" that were omitted from the web crawl as well as  the reason for omitting 15 relations and 81 entities from FB15k. \n\nWhile existing relational-learning models on knowledge graphs employ an embedding matrix to learn a representation for the entity, this paper proposes to use deep neural networks to extract a representation for the images. By employing deep representations of images associated with previously unseen entities, their method is also able to answer questions by generalizing to novel visual concepts, providing the ability to zero-shot answer questions about these unseen entities.\n\nThe baselines reported by the paper are weak especially the VGG+DistMult baseline with very low classifier score leading to its uncompetitive result. It would be worth at this point to try and build a better classifier that allows for more reasonable comparison with the proposed method. (Accuracy 0.082 is really below par) As for the probabilistic baseline, it only serves to provide insights into the prior biases of the data and is also not a strong enough baseline to make the results convincing.\n\nWell written paper covering relevant background work, but would be much stronger with better baselines.', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		BylEpe9ppX	rJxQ2N6mfE	AKBC.ws/2019/Conference/-/Paper2/Official_Review	['AKBC.ws/2019/Conference/Paper2/Reviewers/Unsubmitted']	3		['everyone']	BylEpe9ppX	['AKBC.ws/2019/Conference/Paper2/AnonReviewer1']	1547060394904		1547662989490	['AKBC.ws/2019/Conference']
720	1547018156872	{'title': 'A detailed system description for a specific problem', 'review': 'This paper presents a rule-based framework for extracting formal description logic concepts from a phrase describing an entity. This framework is used for answering entity-centric queries over a KB, and also to enrich the KB using the labels associated to the entities. In addition, the paper also presents a system for checking if one concept is subsumed by another to link phrase queries to concepts in the KB.\n\nThe paper is clearly written and easy to follow for the most part (except section 2, where some notations are not defined and a working knowledge of SPARQL is assumed). There are motivating examples throughout which show the need for the many hand-crafted rules and components in the system. Finally, the system is tested on two real-world problems for symptom checking and migrating the concepts from one KB to another.\n\nMy main reservation is that this paper is structured more as a system demonstration than a full research paper. The evaluation just tells us that the system as a whole is somewhat effective, rather than how its components contribute to the overall performance. For example, I would have liked to see an ablation study of the different rules used in Algorithm 3. As it stands, it is not clear what someone working on the same problem for a different KB can take away from this paper.\n\nOverall, I am slightly inclined towards acceptance since, within its domain, the paper describes an effective working solution for a difficult problem. However, its broader impact on the field will be limited.', 'rating': '6: Marginally above acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		H1l8ag9a6m	B1lBn1XQz4	AKBC.ws/2019/Conference/-/Paper3/Official_Review	['AKBC.ws/2019/Conference/Paper3/Reviewers/Unsubmitted']	1		['everyone']	H1l8ag9a6m	['AKBC.ws/2019/Conference/Paper3/AnonReviewer1']	1547018156872		1547662989273	['AKBC.ws/2019/Conference']
721	1546875321655	"{'title': 'Mostly well-written paper with relatively little original content and a few caveats', 'review': 'This paper presents work on domain adaptation for sentiment classification. More specifically, the authors present a new combination of already existing ideas in this problem space, test it on one existing dataset for which results from other domain adaptation techniques are available, and find that in some combinations of sub-domains in the dataset, their approach outperforms  recent competitors.\n\nI like about the paper that it is mostly well written and clear about what is being done. The presented idea makes intuitive sense on a high level, and I think it\'s a good thing that this specific idea has been tested and that its performance is documented in this paper.\n\nMy main concern with the paper is that the proposed technique is not really novel, and seems like a minor natural extension of existing domain adaption literature. Another point is that it\'s not entirely clear to me how the pseudo-labels for the unlabelled target-domain instances can be produced already in step 2, since the projection of these instances is only done in step 3. Concretely, the noisy classifier trained in step 2 operates in the projection space S^*_L, but the pseudo-labels are produced for instances in the not-yet-projected space T_U. The authors consistently describe their three-step approach in this way, so I feel like this is not a small hiccup in one spot of the paper, but I must be missing something essential. It would be great if the authors could extend the description of that part in the paper.\n\nNits:\n\nI wonder if it made sense to already mention in the abstract/first page what task is used in this paper as a testbed for the proposed method, in particular since AKBC itself is very much motivated by a specific application context.\n\nSelf-training: Iterations limited to one - would be nice to see results for higher iteration numbers\n\nIn Sec. 4.1, the authors state that for this part of the experiments, the number of considered neighbors is fixed to one, ""for the ease of comparisons"". Off the top of my head, I\'m not seeing how the comparisons get easier by introducing this restriction.\n\nOn page 12, the authors try to make the point that a method should use as much unlabelled data as possible. I\'d say this is only true if utilizing additional data points actually improves results.\n\nIn Table 6, the proposed method is often outperformed by the ""neural"" competitors, while in Table 4/5 the presented method wins for all domain pairs against the ""classic"" algorithms. I find the separation of competitors a bit arbitrary, in particular because all (?) recent competitors are in the same class.', 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		H1xtpgqTaQ	r1eM6-l-ME	AKBC.ws/2019/Conference/-/Paper4/Official_Review	['AKBC.ws/2019/Conference/Paper4/Reviewers/Unsubmitted']	1		['everyone']	H1xtpgqTaQ	['AKBC.ws/2019/Conference/Paper4/AnonReviewer3']	1546875321655		1547662988992	['AKBC.ws/2019/Conference']
722	1545630311583	"{'title': 'Interesting insights into crowd-provided relation annotations', 'review': 'This paper analyzes relation annotations collected on the Mark2Cure platform. On the platform, citizen scientists annotated abstracts of biomedical papers for relations between previously identified entities. Overall, this paper generates some interesting observations that could be used to improve future annotation efforts in this domain.\n\nPros:\n- Thorough analysis of the effect of number of crowdworkers needed to annotate each candidate. Practically this is important to measure, in order to minimize noise within budget constraints.\n- Evidence linking low crowdworker accuracies to systematic misunderstanding of instructions, which could be remedied in future versions of the data collection platform.\n- Nice analysis of cases in which relations types are missing, leading to errors (Figures 2, 3)\n- Confirmation that cross-sentence relations can be annotated reliably (Figure 4). Other recent work in biomedical relation extraction (e.g., Peng et al., 2017; Verga et al., 2018) has also argued for the importance of cross-sentence relations.\n\nCons:\n- Overall this paper generates a lot of interesting insights, but does not close the loop by feeding these back into the Mark2Cure platform and improving annotation quality. So while there is potential for impact, a lot of this potential has not yet been actualized.\n- Based on Figure 1C, the system maxes out at around 73%, even with an ensemble of workers. It is not clear to me how satisfactory this level of accuracy is. For example, would this be good enough to train a relation extraction model?\n- Some design choices seem odd and perhaps merit some explanation. For example, why not use Mark2Cure on relations between entities of the same type? It was noted that SemMedDB has such relations, and in general there seem to be many valid cases of these (drug-drug interactions, gene regulation, etc.).\n\nOther comments:\n- I don\'t understand the grey dots in Figure 4. What does it mean that no identifier was available?\n- Some minor formatting things: M2C was used as an acronym but not defined. Use ""(i.e., xyz)"" instead of ""(ie- xyz)"", and same for ""e.g."".', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		r1loaec6pm	SkxedGg0eN	AKBC.ws/2019/Conference/-/Paper5/Official_Review	['AKBC.ws/2019/Conference/Paper5/Reviewers/Unsubmitted']	1		['everyone']	r1loaec6pm	['AKBC.ws/2019/Conference/Paper5/AnonReviewer3']	1545630311583		1547662988557	['AKBC.ws/2019/Conference']
723	1546789714520	"{'title': 'promising initial results, but missing comparison with different relation extraction methods', 'review': 'The paper describes a Citizen Science initiative to collect medical relations from PubMed abstracts by asking volunteers to perform the task. The authors show an evaluation proving the quality of the annotations, as well as identifying the main issues that cause mistakes in this data: quality of NER tools, issues with the training of the annotators on the platform, and problems with the documents themselves.\n\nThe paper can be a bit difficult to understand at times, due to the heavy use of acronyms. This is particularly the case for readers lacking a biomedical background. To make it more readable, I suggest adding an explanation of SemMedDB in the introduction, as well as explaining the meaning of UMLS and the UMLS CUI in the section they are mentioned in.\n\nAs the main challenge of citizen science applications is to get a consistent high quality user base, the results of this paper appear promising. What is missing is a more thorough positioning within the related work on relation extraction. In particular, it would be useful to evaluate this application in comparison with with well-known general-purpose automated relation extraction models [1], and models that are tailored for medical relation extraction [2]. Another interesting comparison to make is with the different active [3] and semi-supervised learning methods [4], that also have experimented with different ways of aggregating crowd data.\n\n[1] Mintz, Mike, et al. ""Distant supervision for relation extraction without labeled data."" Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2. Association for Computational Linguistics, 2009.\n[2] Wang, Chang, and James Fan. ""Medical relation extraction with manifold models."" Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2014.\n[3] Liu, Angli, et al. ""Effective crowd annotation for relation extraction."" Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.\n[4] Angeli, Gabor, et al. ""Combining distant and partial supervision for relation extraction."" Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014.', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		r1loaec6pm	rJloLmskGE	AKBC.ws/2019/Conference/-/Paper5/Official_Review	['AKBC.ws/2019/Conference/Paper5/Reviewers/Unsubmitted']	2		['everyone']	r1loaec6pm	['AKBC.ws/2019/Conference/Paper5/AnonReviewer2']	1546789714520		1547662988329	['AKBC.ws/2019/Conference']
724	1547070735819	"{'title': 'The paper presents an approach to feed back knowledge curated by Wikidata editors into the primary sources from which the knowledge was ingested.  In particular, the paper proposes to rely on Wikidata as a way to curate and extend biomedical and disease ontologies. It proposes a mechanism by which curators of primary ontologies can compute diffs between the knowledge in the primary source and Wikidata to decide which changes to include in the primary data source.', 'review': 'The paper presents an approach to feed back knowledge curated by Wikidata editors into the primary sources from which the knowledge was ingested.  In particular, the paper proposes to rely on Wikidata as a way to curate and extend biomedical and disease ontologies. It proposes a mechanism by which curators of primary ontologies can compute diffs between the knowledge in the primary source and Wikidata to decide which changes to include in the primary data source. \n\nThe paper proposes an interesting approach to extending ""closed"" ontologies that are typically managed, published and extended by some owning entity. This is the case for many biomedical and disease ontologies.\nThe starting point of the authors is that Wikidata as a collaboratively created knowledge base is seeded with knowledge coming from such primary data sources / existing ontologies. Following a crowd-sourcing approach, anyone can extend the knowledge coming from these ontologies in Wikipedia. The authors of the paper focus in particular on two type of changes: adding or changing cross-references as well as adding or changing superconcepts. In both cases, errors can be made.\n\nThe entities creating, hosting and evolving the primary ontologies have an interest in importing all those changes made by editors in Wikidata to extend their resource in a cost-effective way. However, as errors exist, some quality control is needed. The authors propose a mechanism by which curators of ontologies can compute diffs between the knowledge in Wikidata and the knowledge in their own ontology. Observed differences can trigger automatically change proposals that can be verified by human editors.\n\nOverall, the approach is interesting and is of practical relevance. However, the technical contribution of the paper is really marginal and the scientific novelty is low. The only technical part of the paper is the one corresponding to the implementation of the diff operation via SPARQL queries. However, the SPARQL queries are not really described at sufficient level of detail to understand how the approach works. ', 'rating': '4: Ok but not good enough - rejection', 'confidence': '5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		SylC6gcTTm	ryeOfaJEMN	AKBC.ws/2019/Conference/-/Paper6/Official_Review	['AKBC.ws/2019/Conference/Paper6/Reviewers/Unsubmitted']	3		['everyone']	SylC6gcTTm	['AKBC.ws/2019/Conference/Paper6/AnonReviewer3']	1547070735819		1547662988118	['AKBC.ws/2019/Conference']
725	1544906554190	"{'title': 'Interesting project, though limited wider take-away.', 'review': 'The paper describes a technical and organizational process for KB curation in which the crowd is used to curate a domain-specific biomedical KB. Specifically, data from the external KB was loaded into Wikidata, then triggers/bots are implemented that harvest changes to the Wikidata items, and hands them to an expert for review, before the changes can be loaded back into the external KB.\n\nThis process is an interesting approach to KB curation, especially for maintenance in the temporal dimension, so I see its topic as a valuable contribution to the AKBC conference. Nonetheless, I\'m ambivalent about the current state of the paper: While it provides an interesting and well-written read, and it\'s specific approach is well documented, I feel it falls short of delivering wider lessons learned. Also, it lacks detail at several places, though this can probably be easily fixed.\n\nSpecific aspects for discussion:\n - Given that efforts are duplicated (WD editors, then expert review), how economic is the approach? Can you provide a back-of-the-envelope calculation of whether this approach pays of? I\'m crucially missing details on how big the external resources are, could the expert instead directly review them? How dynamic are the resources, i.e., how much is gained by a community continuously reviewing them, versus a one-off review by an expert?\n - What are the social/ethical dimensions of this approach? Are individual Wikidata editors acknowledged in the external resource? It seems both sides give and take, i.e., the external resources contribute data to Wikidata as well, so this should be fine, nonetheless a short discussion would be appreciated.\n - What level of knowledge is required to contribute to the present domain? How many Wikidata editors were actually doing the edits? 3? 15? 150? Are these John Doe\'s adding information about ""headache"" and ""coughing"", or likely medical experts themselves, possibly even contributing to Wikidata in professional roles?\n - Some placement of the work in the wider area would be appreciated, is this a singular approach? Are there other organizations (GLAMs maybe?) that run similar processes over Wikidata?\n - It\'s surely naive, but what are reasons why the external resources are not fully merged into Wikidata, from then on only edited there? What is their dataset size/commercial status/community size?\n \n\nMinor comments\n - Abstract line 2: ""prioritize"" - over what?\n - Intro 1st paragraph: ""our team"" - subject to anonymity, please be specific who that is, which organization?\n - Page 2: ""In previous work, we ..."" - violation of anonymity\n - ""the primary resource\'s curation team"" - the paragraph above lists 3 resources (NCBI Entrez, PubChem, HDO), now singular is used? How big is that team?\n - ""the Human Disease Ontology Project"" - same as Kibbe et al., 2015 above? Is it a static resource, or an active project?\n - ""were fixed in future releases"" - I see what you mean, but style is a little odd\n - 1.3 ""As described previously, we ..."" - not good style, not self-contained. Also confusing that the next sentence talks about ""details will be discussed in a future publication"". Anyway, at least the key numbers need to be mentioned here, how many facts were added?\n - 1.3 ""Resources [in Wikidata] are updated on a schedule..."" - details would be appreciated how the approval for this was obtained, and how the general state of automated data imports to Wikidata is - to my possibly outdated knowledge, Wikidata is rather skeptical towards automated external imports and updates\n - Consider reducing font size of Table 1\n - ""There are currently many other properties"" - wiggly language, please specify an order of magnitude at least (5? 50? 200?). Why are they out of scope of this publication?\n - ""Starting in late 2015"" - Please add month\n - ""We manually reviewed 2,241 ..."" - out of?\n - ""The curation of these 1,774 ontology changes is ongoing [...] only curated 14 cases in which XYZ"" - is this to say that case XYZ is so rare, or that little was reviewed so far?\n', 'rating': '6: Marginally above acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		SylC6gcTTm	SkeGHvyQxV	AKBC.ws/2019/Conference/-/Paper6/Official_Review	['AKBC.ws/2019/Conference/Paper6/Reviewers/Unsubmitted']	1		['everyone']	SylC6gcTTm	['AKBC.ws/2019/Conference/Paper6/AnonReviewer2']	1544906554190		1547662987898	['AKBC.ws/2019/Conference']
726	1547008032954	{'title': 'Review', 'review': 'This paper describes a pipeline approach to monitor and filter the changes of Wikidata and integrate them into a biomedical ontology: Human Disease Ontology.\n\nThe paper is clearly-written and gives a very detailed description of the Wikidata data model and community activities, and how these changes have been handled via a feedback loop.\n\nWhile it is an interesting read to me (and perhaps to the AKBC audience), this paper doesn’t present any computational approaches and also doesn’t provide any rigorous evaluation process (thus no comparisons to any previous approaches). Therefore, I find it difficult to judge the scientific value of the paper and whether it is a good fit to AKBC. At least for the experiments in the paper, I wish to see the progress of this project after a certain time period so we can judge the effectiveness of the proposed approach. I also wish to see more discussions about the pros and cons of the approach and how it is related to previous work.\n\n\n', 'rating': '4: Ok but not good enough - rejection', 'confidence': '2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper'}		SylC6gcTTm	Syetmdxmz4	AKBC.ws/2019/Conference/-/Paper6/Official_Review	['AKBC.ws/2019/Conference/Paper6/Reviewers/Unsubmitted']	2		['everyone']	SylC6gcTTm	['AKBC.ws/2019/Conference/Paper6/AnonReviewer1']	1547008032954		1547662987650	['AKBC.ws/2019/Conference']
727	1544974646961	"{'title': 'Useful resource, but claims could be better supported, and the uniqueness of the resource better argued', 'review': 'The paper describes a new resource ""Med Mentions"" for entity linking of Pubmed abstracts, where entities are concepts in the UMLS type hierarchy -- for example ""Medical Device"".\n\nThe annotations were manually verified. (I assume the intention is to use this as a benchmark, but the paper does not say)\n\nThe paper is very rigorous in describing which concepts were considered, and which were pruned. Authors suggest to combine it with ""TaggerOne"" to obtain end-to-end entity recognition and linking system.\n\nIt is a little bit unclear what the main contribution of this paper is. Is it a benchmark for method development and evaluation (the paper mentions the train/dev/test split twice)? or do the authors propose a new system based on this benchmark?, or was the intent to test a range of baselines on this corpus (and what is the purpose?) -- I believe this lack of clarity could be easily addressed with a change in structure of headings. (Headings are currently not helping the reader, a more traditional paper outline would be helpful.)\n\nI appreciate that the paper lists a range of related benchmarks. However, I am missing a discussion of: where the advantage of MedMentions is in contrast to these benchmarks? What is MedMentions offering that none of the other benchmarks couldn\'t?\n\n\nIt is indisputable that a new resource provides value to the community, and therefore should be disseminated. However, the paper quality is more reminiscent of a technical report. A lot of space is dedicated to supplemental information (e.g. page 6) which would be better spent on a clear argumentation and motivation of the steps taken.\n\n', 'rating': '6: Marginally above acceptance threshold', 'confidence': '5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		SylxCx5pTQ	rkeJBbeEl4	AKBC.ws/2019/Conference/-/Paper7/Official_Review	['AKBC.ws/2019/Conference/Paper7/Reviewers/Unsubmitted']	1		['everyone']	SylxCx5pTQ	['AKBC.ws/2019/Conference/Paper7/AnonReviewer1']	1544974646961		1547662987432	['AKBC.ws/2019/Conference']
728	1546798306685	"{'title': 'Solid biomedical entity extraction/linking dataset', 'review': 'In this paper the authors introduce MedMentions, a new dataset of biomedical abstracts (PubMed) labeled with biomedical concepts/entities. The concepts some from the broad-coverage UMLS ontology, which contains ~3 million concepts. They also annotate a subset of the data with a filtered version of UMLS more suitable for document retrieval. The authors present data splits and results using an out-of-the-box baseline model (semi-Markov model TaggerOne (Leaman and Lu, 2016)) for end-to-end biomedical entity/concept recognition and linking using MedMentions.\n\nThe paper describes the data and its curation in great detail. The explicit comparison to related corpora is great. This dataset is substantially larger (hundreds of thousands of annotated mentions vs. ones of thousands) and covers a broader range of concepts (previous works are each limited to a subset of biomedical concepts) than previous manually annotated data resources. MedMentions seems like a high-quality dataset that will accelerate important research in biomedical document retrieval and information extraction.\n\nSince one of the contributions is annotation that is supposed to help retrieval, it would be nice to include a baseline model that uses the data to do retrieval. Also, it looks like the baseline evaluation is only on the retrieval subset of the data. Why only evaluate on the subset and not the full dataset, if not doing retrieval?\n\nThis dataset appears to have been already been used in previous work (Murty et al., ACL 2018), but that work is not cited in this paper. That\'s fine -- I think the dataset deserves its own description paper, and the fact that the data have already been used in an ACL publication is a testament to the potential impact. But it seems like there should be some mention of that previous publication to resolve any confusion about whether it is indeed the same data.\n\nStyle/writing comments:\n- Would be helpful to include more details in the introduction, in particular about your proposed model/metrics. I\'d like to know by the end of the introduction, at a high level, what type of model and metrics you\'re proposing.\n- replace ""~"" with a word (approximately, about, ...) in text\n- Section 2.3: capitalization typo ""IN MEDMENTIONS""\n- Section 2.4, 3: ""Table"" should be capitalized in ""Table 6""\n- Use ""and"" rather than ""/"" in text\n- Section 4: maybe just say ""training"" and ""development"" rather than ""Training"" and ""Dev""\n- 4.1: Markov should be capitalized: semi-Markov\n- 4.1: reconsider use of scare quotes -- are they necessary? \'lexicons\', \'Training\', ""dev\', \'holdout\'\n- 4.1: replace ""aka"" with ""i.e."" or something else more formal. In general this section could use cleanup.\n- 4.1: last paragraph (describing metrics, mention-level vs. document-level) is very confusing, please clarify, especially since you claim that a contribution of the paper is to propose these metrics. Is it the case that mention-level F1 is essentially entity recognition and document-level is entity linking? An example could possibly help here.', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		SylxCx5pTQ	SyejyHp1MN	AKBC.ws/2019/Conference/-/Paper7/Official_Review	['AKBC.ws/2019/Conference/Paper7/Reviewers/Unsubmitted']	2		['everyone']	SylxCx5pTQ	['AKBC.ws/2019/Conference/Paper7/AnonReviewer3']	1546798306685		1547662987214	['AKBC.ws/2019/Conference']
729	1546857222252	{'title': 'MedMentions: A Large Biomedical Corpus Annotated with UMLS Concepts', 'review': 'The paper “MedMentions: A Large Biomedical Corpus Annotated with UMLS Concepts” details the construction of a manually annotated dataset covering biomedical concepts. The novelty of this resource is its size in terms of abstracts and linked mentions as well as the size of the ontology applied (UMLS). \nThe manuscript is clearly written and easy to follow. Although other resources of this type already exist, the authors create a larger dataset covered by a larger ontology. Thus, allowing for the recognition of multiple medical entities at a greater scale than previously created datasets (e.g. CRAFT).\nDespite the clarity, this manuscript can improve the following:\nSection 2.3 – How many annotators were used?\nSection 2.4, point 2 - The process used to determine biomedical relevance is not detailed. Section 4.1 - No reason is given for the choice of TaggerOne. In addition, other datasets could have been tested with TaggerOne for comparison with the MedMentions ST21pv results.\nMisspelling and errors in section 2.3: “Rreviewers”, “IN MEDMENTIONS”\nOverall, this paper fits the conference topics and provides a good contribution in the form of a large annotated biomedical resource.', 'rating': '7: Good paper, accept', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		SylxCx5pTQ	H1gAZoslfN	AKBC.ws/2019/Conference/-/Paper7/Official_Review	['AKBC.ws/2019/Conference/Paper7/Reviewers/Unsubmitted']	3		['everyone']	SylxCx5pTQ	['AKBC.ws/2019/Conference/Paper7/AnonReviewer2']	1546857222252		1547662986961	['AKBC.ws/2019/Conference']
730	1546891187638	"{'title': 'Corpus enrichment based on similarity of annotations', 'review': 'This article describes a methodology to annotate documents using annotations from other related documents. Despite the interesting application, the article suffers from several major flaws or presents issues that require further clarifications\n\nCONS\n- The annotations are linked to named entities. But do all documents contain named entities? Considering the example given in the article: ""iterative algorithms"" and ""EM"", what types of named entities could documents dealing with ""iterative algorithms"" and ""EM"" contain?\n1\n- Why is the cosine similarity measue, considered to be the state of the art, not used for measuring the similarity in the D-measure?\n\n- The G-measure assumes that an annotation is a subject-predict-object triple. I think that this assumption is too strong/restrictive. Furthermore, it\'s hard to imagine anotations that are expressed as triples. In my opinion, and from my own experience, annotations are usually simple terms, which cannot be also be formalized as triples.\n\n- The reasoning underlying the G-measure is not clear. It seems to be a heuristic methods, based on matching subject, predicate and objects. I doubt that this measure generalizes well to other corpora.\n\n- The proposed method should have been compared against other state of the art methods for document similarity. An interesting avenue could be the use of word embeddings to compute document similarity.\n\nPROS:\n- The article is well-written\n- Experiments are well described, sufficiently detailed', 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		HylzClqp67	r1x3h1EbG4	AKBC.ws/2019/Conference/-/Paper8/Official_Review	['AKBC.ws/2019/Conference/Paper8/Reviewers/Unsubmitted']	1		['everyone']	HylzClqp67	['AKBC.ws/2019/Conference/Paper8/AnonReviewer3']	1546891187638		1547662986741	['AKBC.ws/2019/Conference']
731	1546985175897	{'title': 'Good paper', 'review': 'This paper introduces unsupervised learning for corpus-driven annotation enrichment. In particular, the proposed method is different from previous methods by considering the composition of the documents. It has two main contributions — (i) propose two holistic similarity measures of documents, and (ii) propose an EM-like algorithm for semantic annotations of the documents.\n\nPros\n* The paper is self-contained, and written very clearly.\n* The EM algorithm that they propose is novel and original.\n* They included various ablations, including the significance of D- and G-similarity with different hyper parameters. Also, they included some examples for better understanding of readers.\n\nCons\n* There is no clear quantitative empirical results to evaluate the algorithm.', 'rating': '6: Marginally above acceptance threshold', 'confidence': '2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper'}		HylzClqp67	rkle11iMG4	AKBC.ws/2019/Conference/-/Paper8/Official_Review	['AKBC.ws/2019/Conference/Paper8/Reviewers/Unsubmitted']	2		['everyone']	HylzClqp67	['AKBC.ws/2019/Conference/Paper8/AnonReviewer2']	1546985175897		1547662986521	['AKBC.ws/2019/Conference']
732	1547090763515	{'title': 'Learning to extract facts with little prior except a structural prior from a probabilistic program', 'review': 'This paper uses a probabilistic program describing the process by which facts describing entities can be realised in text, and a large number of web pages, to learn to perform fact extraction about people using a single seed fact.\n\nDespite the prodigious computational cost (close to a half million hours of computation to acquire a KB only about people) I found the scale at which this paper applied probalistic programming exciting.  It suggests that providing a structural prior in this form, and then learning the parameters of a model with that structure is a practical technique that could be applied widely.\n\nQuestions that arose whilst reading:  the most direct comparison was with a system using Markov Logic Networks, in which the structural prior takes the form of a FOL theory. A more direct comparison would have been useful - in particular, an estimate of the difficulty of expressing and equivalently powerful model, and the computational cost of trainining that model, in MLN.\n\nQuite a lot of tuning was required to make training tractable (for outlying values of tractable) - this limits the likely applicability of the technique.\n\nThe paper suggests in future work an extension to open domain fact extraction, but it is not clear how complex or tractable the require prob program would be. The one in the paper is in some respects (types mainly) specific to the facts-about-people setting.\n\nIt is unclear why theTAC-KBP Slot Filling track  was mentioned, given that performance on this track does not seem to have been evaluated. An informal evaluation suggesting beyond SoA performance is mentioned, but not useful details given. This significantly weakens what otherwise could be a stand-out paper', 'rating': '8: Top 50% of accepted papers, clear accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		rJgHCgc6pX	ryV8s4VzE	AKBC.ws/2019/Conference/-/Paper9/Official_Review	['AKBC.ws/2019/Conference/Paper9/Reviewers/Unsubmitted']	2		['everyone']	rJgHCgc6pX	['AKBC.ws/2019/Conference/Paper9/AnonReviewer3']	1547090763515		1547662986304	['AKBC.ws/2019/Conference']
733	1544975948488	"{'title': 'Very interesting overview of an existing knowledge base, will inspire interesting discussions', 'review': '\nThe paper is about a knowledge base that is constructed with a probabilistic model (Markov logic network, as implemented in Infer.NET). The system is expansive and covers many important aspects, such as data types and values, templates for extraction and  generation, noise models. The model addresses three kinds of large scale inferences: (1) template learning, (2) schema learning, and (3) fact retrieval.   The knowledge base construction approach is evaluated against other knowledge base approaches, YAGO2, NELL, Knowledge Vault, and DeepDive.\n\nThe paper is a very interesting overview over the knowledge base Alexandia, that would inspire interesting discussions at the AKBC conference. \n\nMy only pet peeve are a set of initial claims, intended to be distinguishing this approach from others, which are simply not true:\n-  ""Alexandia\'s key differences are its unsupervised approach"" -- clearly Alexandria requires prior knowledge on types, and furthermore weakly supervision for template learning etc. Unsupervised means ""not require any clue about what is to be predicted"". \n- ""KnowledgeVault cannot discover new entities"" -- I\'d be surprised.\n- ""DeepDive uses hand-constructed feature extractors"" -- It is a matter of preferences whether one seeds the extraction with patterns or data. This statement does not convince me.\nWhile the discussion of these differences is important, I suggest the authors use more professional language.\n\n\n', 'rating': '9: Top 15% of accepted papers, strong accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		rJgHCgc6pX	Syx48IeNe4	AKBC.ws/2019/Conference/-/Paper9/Official_Review	['AKBC.ws/2019/Conference/Paper9/Reviewers/Unsubmitted']	1		['everyone']	rJgHCgc6pX	['AKBC.ws/2019/Conference/Paper9/AnonReviewer1']	1544975948488		1547662986065	['AKBC.ws/2019/Conference']
734	1546967160894	"{'title': 'Interesting study of simhash for a coreference resolution algorithm', 'review': 'This paper proposes to use a variant of simhash to estimate cosine similarities in a particular coreference resolution algorithm called hierarchical coreference. The original algorithm maintains many different feature sets (for mentions and groups of mentions) subject to union and difference operations, and frequently needs to estimate the cosine similarity between various such sets. To avoid the associated costs, the paper proposes to use sketching techniques instead. The proposed techniques are simple but, as the study shows, can be very effective. I like the paper overall. My main nitpick is that it\'s unclear whether the proposed techniques are useful for other, less specific tasks as well.\n\nPros:\n- Simple techniques\n- Analysis given\n- Convincing experimental results in the considered application\n- Very clear presentation\n\nCons:\n- Quite specific, potentially little impact\n- Somewhat straightforward\n- Relationship to other coreference resolution methods unclear\n\nDetailed comments:\n\nD1. What\'s a ""bag type"" in Sec 4?\n\nD2. On the one hand, I like the tutorial style that the paper is partly written in. On the other hand, large parts of the (initial) discussion are not directly related to the contribution of the paper; this part could be shortened.\n\nD3. The solution to homomorphic simhash is quite obvious. The solution to homomorphic minhash is reminiscent to the AKMV sketch of Beyer et al., ""On Synopses for Distinct-Value Estimation Under Multiset Operations"", SIGMOD 2007. (What\'s different?)\n\nD4. Has the estimator $C_n$ been used before? If not, this might be further highlighted.\n\nD5. It may help give a name to the estimator in 3.2. as well as to spell out its definition.\n\nD6. I found the agree/disagree notation in Fig 1 somewhat misleading. What does it mean for the two models to agree? The decision whether to accept/reject is probabilistic.\n\nD7. What is the total size of all sketches maintained by the algorithm in the various experiments? (It appears 1kb per node [my rough guess] is quite substantial, although it may be less than in the exact method, at least for some nodes.)\n\nD8. It would also be interesting to see the performance w.r.t. number of steps taken.\n\nD9. Is it possible to speed up the exact method to obtain similar performance improvements? Has the method been tuned (e.g., to that fact that most proposals appear to be rejected?)\n\nD10. It remains unclear how the proposed hierarchical coreference model relates to state-of-the-art models, both in terms of cost and in terms of performance. This is a weak point: one may have the impression that this paper speeds up an method that is not state-of-the-art anymore.\n\nTypos:\n\n""We estimate parameters with hyper-parameter search""\n', 'rating': '7: Good paper, accept', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		H1gwRx5T6Q	BylWFdUGGE	AKBC.ws/2019/Conference/-/Paper10/Official_Review	['AKBC.ws/2019/Conference/Paper10/Reviewers/Unsubmitted']	1		['everyone']	H1gwRx5T6Q	['AKBC.ws/2019/Conference/Paper10/AnonReviewer1']	1546967160894		1547662985843	['AKBC.ws/2019/Conference']
735	1545485463957	"{'title': 'Solid technical contribution, related work could be updated', 'review': 'The paper presents a methodology for identifying topical slices of websites that have good cost-benefit ratios for subsequent use as human-annotated ground truth for information extraction, based on initial results achieved by automated extraction techniques.\n\nThe paper presents solid technical and experimental work, and is overall well-written. The proposed methodology, to use identify among automated extraction results those websites and slices most effective for labelling seems a sensible, human-in-the-loop process for information extraction, although this part is not further explored, and the paper focuses on the first part, the selection of sources and slices.\n\nThe main critical issue I see is in the presentation - I have some concerns whether the paper does the state-of-the-art in AKBC justice, and there are a few wording issues. Yet I believe this issue is easy to fix, thus I recommend acceptance.\n\nState-of-the-art and language: The paper argues that the industry standard in AKBC is XPath-based wrapper induction, based on reference to a tutorial and one reference to a 2011 ICDE paper. I believe that especially the NLP community has gone much further since 2011 (e.g., DeepDive, to my knowledge, relies on textual patterns, not document-structure). Maybe these results have not reached industry yet, but then it would be good to support this with more recent references, especially as several of the big players (Google, Baidu, Amazon, Microsoft) have published on their in-house KBs. Maybe it is also difficult to speak of ""the industry standard"" in singular, and there is a multitude. And I certainly wouldn\'t call the industry approach ""broken"" (used twice in intro and abstract).\n\nMinor comments\n - Abstract and introduction: Argument that automatic extraction suffers from low recall is a bit unclear. Classically, low recall is rather a problem of manual selection, while automated selection/using all sources rather has problems with precision?\n - Abstract: ""(RDF triples)"" - technical choice appears unnecessary here\n - Intro: "" Bing [2].The"" - add space\n - 2x: ""conquer"" - awkard wording\n - It seems an assumption in the approach is that the potential of websites is somewhat proportionally represented in the automated initial extraction, i.e., errors are randomly distributed. If so I would be curious to hear how realistic this assumption is, maybe automated extraction, especially based on XPath-structure, could be prone to systematic errors?\n - The course of example 2 was not so clear to me on the initial read, maybe numbering the steps could help?\n - Definition 8: Is there a bound on the length of l (e.g., top-k slices)?\n - What happens if two slices are largely disjunct, but overlap in one attribute, e.g., ""people"" and ""rockets"", both sharing only one property, e.g., ""name""?\n - The paper would benefit from a detailed example (case study), which shows the specific properties and entity counts from a few websites, along with instantiations of expected cost and benefit, especially when comparing different slices within one website. Figure 3 unfortunately skips all these aspects.\n - ""to million of websites"" - ""millions""', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		BketCg9p6X	SJgei2hoxE	AKBC.ws/2019/Conference/-/Paper11/Official_Review	['AKBC.ws/2019/Conference/Paper11/Reviewers/Unsubmitted']	1		['everyone']	BketCg9p6X	['AKBC.ws/2019/Conference/Paper11/AnonReviewer1']	1545485463957		1547662985626	['AKBC.ws/2019/Conference']
736	1546509907302	{'title': 'interesting paper with decent contribution, but doubts about practical viability', 'review': 'The paper presents a method to predict the values\nof numerical properties for entities where these properties\nare missing in the KB (e.g., population of cities or height of athletes).\nTo this end, the paper develops a suite of methods, using\nregression and learned embeddings. Some of the techniques\nresemble those used in state-of-the-art knowledge graph completion,\nbut there is novelty in adapting these techniques\nto the case of numerical properties.\nThe paper includes a comprehensive experimental evaluation\nof a variety of methods.\n\nThis is interesting work on an underexplored problem,\nand it is carried out very neatly.\n\nHowever, I am skeptical that it is practically viable.\nThe prediction errors are such that the predicted values\ncan hardly be entered into a high-quality knowledge base.\nFor example, for city population, the RMSE is in the order\nof millions, and for person height the RMSE is above 0.05\n(i.e., 5cm). So even on average, the predicted values are\nway off. Moreover, average errors are not really the decisive\npoint for KB completion and curation.\nEven if the RMSE were small, say 10K for cities, for some\ncities the predictions could still be embarrassingly off.\nSo a knowledge engineer could not trust them and would \nhardly consider them for completing the missing values.\n\nSpecific comment:\nThe embeddings of two entities may be close for different\nreasons, potentially losing too much information.\nFor example, two cities may have close embeddings because\nthey are in the same geo-region (but could have very different\npopulations) or because they have similar characteristics\n(e.g., both being huge metropolitan areas). Likewise, two\nathletes could be close because of similar origin and\nsimilar success in Olympic Games, or because they played\nin the same teams. \nIt is not clear how the proposed methods can cope with\nthese confusing signals through embeddings or other techniques.', 'rating': '6: Marginally above acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		BJlh0x9ppQ	B1xo8C8iZV	AKBC.ws/2019/Conference/-/Paper12/Official_Review	['AKBC.ws/2019/Conference/Paper12/Reviewers/Unsubmitted']	1		['everyone']	BJlh0x9ppQ	['AKBC.ws/2019/Conference/Paper12/AnonReviewer3']	1546509907302		1547662985373	['AKBC.ws/2019/Conference']
737	1546854143616	"{'title': 'Nice paper on prediction of numeric attributes in KBs.', 'review': 'The paper reports on the prediction of numerical attributes in knowledge bases, a problem that has indeed\nreceived too little attention. It lays out the problem rather clearly and defines datasets, a number of baselines\nas well as a range of embedding-based models that I like because they include both simple pipeline models \n(learn embeddings first, predict numerical attributes later) and models that include the numerical attributes\ninto embedding learning. I also appreciate that the paper makes an attempt to draw together ideas from different\nresearch directions.\n\nOverall, I like the paper. It\'s very solidly done and can serve as excellent base for further studies. Of course, I also have comments/criticisms.\n\nFirst, the authors state that one of the contributions of the paper is to ""introduce the problem of predicting the value \nof entities’ numerical attributes in KB"". This is unfortunately not true. There is relatively old work by Davidov and\nRappoport (ACL 2010) on learning numeric attributes from the web (albeit without a specific reference to KBs), and \na more recent study specifically aimed at attributes from KBs (Gupta et al. EMNLP 2015) which proposed and modelled exactly the same task, including defining freely available FreeBase-derived datasets.\nMore generally speaking, the authors seem to be very up to date regarding approaches that learn embeddings directly\nfrom the KB, but not regarding approaches that use text-based embeddings. This is unfortunate, since the\nmodel that we defined is closely related to the LR model defined in the current paper, but no direct comparison is \npossible due to the differences in embeddings and dataset.\n\nSecond, I feel that not enough motivation is given for some of the models and their design decisions. For example,\nthe choice of linear regression seems rather questionable to me, since the assumptions of linear regression (normal distribution/homoscedasticity) are clearly violated by many KB attributes. If you predict, say, country populations, the \nerror for China and India is orders of magnitude higher then the error for other countries, and the predictions are dominated by the fit to these outliers. This not only concerns the models but also the evaluation, because\nMAE/RMSE also only make sense when you assume that the attributes scale linearly -- Gupta et al. 2015 use this\nas motivation for using logistic regression and a rank-based evaluation. \nI realize that you comment on non-linear regression on p.12 and give a normalized evaluation on p.13: \nI appreciate that, even though I think that it only addresses the problem to an extent.\n\nSimilarly, I like the label propagation idea (p. 7) but I lack the intuition why LP should work on (all) numeric attributes.\nIf, say, two countries border each other, I would expect their lat/long to be similar, but why should their (absolute) GDP be similar? What is lacking here is a somewhat more substantial discussion of the assumptions that this (and the other) \nmodels make about the structure of the knowledge graph and the semantics of the attributes.\n\nSmaller comment:\n* Top of p.5, formalization: This looks like f is a general function, but I assume that one f is supposed to be learned\n   for each attribute? Either it should be f_a, or f should have the signature E x A -> R.\n   (p4: Why is N used as a separate type for numeric attributes if the function f is then supposed to map into reals anyway?)\n\n', 'rating': '8: Top 50% of accepted papers, clear accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		BJlh0x9ppQ	BJeOWyoxGN	AKBC.ws/2019/Conference/-/Paper12/Official_Review	['AKBC.ws/2019/Conference/Paper12/Reviewers/Unsubmitted']	2		['everyone']	BJlh0x9ppQ	['AKBC.ws/2019/Conference/Paper12/AnonReviewer2']	1546854143616		1547662985156	['AKBC.ws/2019/Conference']
738	1546983225794	{'title': 'Solid work, convincing experiments and results', 'review': 'The paper presents innovative work towards learning numerical attributes in a KB, which the authors claim to be the first of its kind. The approach leverages KB embeddings to learn feature representations for predicting missing numerical attribute values. The assumption is that data points that are close to each other in a vector (embeddings) space have the same numerical attribute value. Evaluation of the approach is on a set of highest ranked 10 numerical attributes in a QA setting with questions that require a numerical answer.\n\nThe paper is well written and the approach is explained  in full detail. \n\nMy only concern is the application of the method across numerical attributes of (very) different granularity and context. The authors briefly mention the granularity aspect in section 3 and point to a normalization step. However, this discussion is very brief and leaves perhaps some further questions open on this.', 'rating': '8: Top 50% of accepted papers, clear accept', 'confidence': '2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper'}		BJlh0x9ppQ	rJxMHDcfGN	AKBC.ws/2019/Conference/-/Paper12/Official_Review	['AKBC.ws/2019/Conference/Paper12/Reviewers/Unsubmitted']	3		['everyone']	BJlh0x9ppQ	['AKBC.ws/2019/Conference/Paper12/AnonReviewer1']	1546983225794		1547662984908	['AKBC.ws/2019/Conference']
739	1546996004073	"{'title': ""Little novelty and the evidence doesn't support the claim"", 'review': '\nStrengths -\n1. The paper was overall well written, easy to understand and likely easy to reproduce.\n2. The idea of ""answering structured queries directly over text"" is worth pursuing.\n\nWeaknesses -\n1. The paper contains little modeling novelty. It narrows down the scope of ""structured queries"" to one-hop ""KB triple fragments"" and adopts off-the-shelf neural machine comprehension (MC) models. However, the idea that ""MC models can be used to answer structured KB queries"" has been shown in the CoNLL paper by (Omer et. al 2017), where MC models were used for zero-shot relation extraction by completing triple pattern fragments.\n2. The dataset construction largely replicates the work done by (Welbl et. al. 2018) for WikiHop, while missing several important quality control procedures. For example, (Welbl et. al. 2018) reported significant answer candidate imbalance in the initial WikiHop collection, i.e. some entities are much more likely to appear as an answer than others and a majority class baseline would be very competitive. Similar issues should be studied and controlled for the new dataset used in this paper as well. To this reviewer, a more reasonable approach is to also test the proposed system on existing KB-text benchmark datasets such as WikiHop.\n3. This paper does not provide evidence to support its main claim that ""using text as the database has a number of potential benefits"". For a fair comparison on accuracy, the paper should conduct a head-to-head comparison of the performance of models that use only text vs. those also use a pre-extracted KB. If the ones with access to KBs demonstrate a big performance jump, it is an evidence that pre-extracted KBs are beneficial. (This experiment should be carried out on QA datasets with natural language questions so as not to give models with access to KBs an advantage.) Efficiency-wise, the paper claims that having a separate KB representations incur maintenance cost and synchronization issues. However, query execution on KBs can be highly optimized. On the other hand, text processing and NN execution can sometimes trigger extra overhead. The paper needs to benchmark the runtime efficiency of a text-only system and a system that accesses KBs in a head-to-head fashion, too.\n\n', 'rating': '3: Clear rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		BygfyW5pam	Byg27YpfME	AKBC.ws/2019/Conference/-/Paper13/Official_Review	['AKBC.ws/2019/Conference/Paper13/Reviewers/Unsubmitted']	1		['everyone']	BygfyW5pam	['AKBC.ws/2019/Conference/Paper13/AnonReviewer1']	1546996004073		1547662984691	['AKBC.ws/2019/Conference']
740	1547027129745	"{'title': 'Interesting direction; concerns about the claims. ', 'review': 'This work proposes an approach to learn to answer structured queries over text. The proposed approach first uses Triple Pattern Fragments to decompose complex query to triple patterns. Then the patterns are turned in textual form, and fed into a neural question answering model to extract answer spans. It uses an existing knowledge base (Wikidata) and an accompanying text corpus (Wikipedia) to create the training data. Each training example is created from a triple with an unbound variable using the textual descriptions of the properties and the entities. The approach is evaluated on simple queries corresponding to single triple patterns and shows promising results. \n\nHere\'s some high level questions regarding the main claims: \n\n1. Although this work claims it ""eliminates the need to have an intermediate database in order to answer structured database queries over text"", it actually depends on an existing knowledge base to create the training data. However, if the knowledge base is already there, why don\'t we just use the knowledge base to answer the queries? \n\n2. This work claims that ""Our approach combines distributed query answering (e.g. Triple Pattern Fragments) with models built for extractive question answering"". However, the experiments only evaluate the approach on answering simple queries (single triples). There\'s a discussion about the prototype, but I didn\'t find any quantitative evaluation on complex queries that requires the use of Triple Pattern Fragments. \n\n3. The answers for the structured queries should be entities or predicates instead of strings. The proposed approach seems to be missing one component that maps the answer spans (found by the neural models) to entities or predicates. \n\nSome specific questions: \n\n1.The paper only describes a train/test split. How are the hyperparameters chosen for the neural models? \n\n2. ""For JackQA, the window was increased to 3000 characters, and multiple training sessions were required, reducing the batch size each time to complete the models which not finish from earlier runs, in all three passes were required with 128, 96 and 64 batch size respectively. Total training time was 81 hours."" \n\nWhy are there three passes instead of one pass with one batch size? \n\n3. ""Third, instead of training a joint model for relations, we train a unique model for every relation. This approach fits well to our task which is aiming to bind triple patterns.""\nThis seems a disadvantage comparing to a joint model because the number of models grows with the number relations. \n\nThe main idea is clear, but the writing requires some proof reading. \nTypos or grammatical errors:\n1. ""SQLWikiReading [Hewlett et al., 2016] like our approach extracts a corpus from Wikidata"" => ""WikiReading ""\n2. ""In general, we think our approach in also specifying an extraction procedure is a helpful addition for applying corpus construction in different domains."" seems non-grammatical. \n3. ""[Dirk Weissenborn, 2018]""   => wrong format of citation since this paper have multiple authors.  \n4.""Entities have may have more than one type."" => typo\n5.""It seem that using character level embeddings may worked better in these cases"" => typo\n\n', 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		BygfyW5pam	SkgzazBXMV	AKBC.ws/2019/Conference/-/Paper13/Official_Review	['AKBC.ws/2019/Conference/Paper13/Reviewers/Unsubmitted']	2		['everyone']	BygfyW5pam	['AKBC.ws/2019/Conference/Paper13/AnonReviewer2']	1547027129745		1547662984472	['AKBC.ws/2019/Conference']
741	1547022175804	"{'title': 'Interesting framework for relation extraction', 'review': 'This paper proposes a novel framework for relation extraction by checking whether two entity pairs are analogous in their relation based on the set of sentences which mention them. A hierarchical siamese network (HSN) is presented for this task and trained on distantly supervised data from Wikidata + Wikipedia. The main benefit of this framework is that it can be applied to one-shot relation extraction for unseen relations during training, and the proposed model shows promising results in this setup. Further, entity representations from the siamese network also improve the performance when transferred to an existing state-of-the-art relation extraction model.\n\nThe paper is clearly written and well motivated, though sometimes needlessly verbose. The one-shot experimental setup is carefully designed, and I think should lead to interesting future research as well. However, the baselines compared to HSN in this experiment are rather weak. For example, how does the current method compare to universal schema where certain relations have only 1 example? Or to the methods presented in http://www.akbc.ws/2017/papers/17_paper.pdf?\n\nThe results presented on the transferring entity-pair embeddings to PCNN are more convincing. However, can the authors comment on whether there is any overlap between the training set of T-REX and the test sets of NYT-FB and CC-DBP in this experiment? The underlying KBs have a lot of common facts, hence I am concerned whether there is any leakage here. I have the same question about the 55 and 11 long-tailed relations used form CC-DBP and NYT-FB in the one-shot experiment. Do any of these appear in the training set of T-REX? I will increase my score if convinced by the answers to these questions.\n\nDetailed comments:\n- What is the difference between points (1) and (2) on the first paragraph of page 2?\n- I think it will be interesting to compare this one-shot setup to the zero-shot setup of Levy et al. While they assume natural language queries are available for unseen relations, you assume a single instance is available. Which is a stronger signal for extracting more instances?\n- Section 3, first paragraph ""... co-occur in the same set -> sentence""\n- Does the GRU in HSN treat entity and non-entity words in the same way?\n- Using a fixed context vector for the attention over multiple sentences in a mention set is rather strange. Shouldn\'t this vector somehow depend on the the relation being queried (which is a function of the other entity pair)?\n- Section 5.1, last sentence ""dirtier"" -> ""noisier"".\n- Citation to Liu, 2017 (https://arxiv.org/abs/1705.02426) is appropriate', 'rating': '7: Good paper, accept', 'confidence': '5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		SkxE1b56TQ	BkxdvJNmzE	AKBC.ws/2019/Conference/-/Paper14/Official_Review	['AKBC.ws/2019/Conference/Paper14/Reviewers/Unsubmitted']	1		['everyone']	SkxE1b56TQ	['AKBC.ws/2019/Conference/Paper14/AnonReviewer1']	1547022175804		1547662984219	['AKBC.ws/2019/Conference']
742	1544977631550	"{'title': 'throrough study and a good reminder to think about this important step in the entity linking pipeline', 'review': 'In the area of entity linking, this paper focuses on candidate generation. The author make a valid point, for the importance of this stage: While much work focuses on disambiguation of a candidate set of entities, this only matters if the correct choice is present in the candidate set. This might not be true for simple candidate generation methods. The paper suggests a weighted combination of different candidate generating features, such as exact string match, BM25, character 4-grams or abbreviation matches. These are contrasted with two variations of the DiscK approach.\n\nThe evaluation is based on the Share/Clef eHealth task. It is very exhaustive, evaluating different properties of the candidate set. From an efficiency standpoint, I  would be interested in candidate methods that contain the true candidate in a small pool set i.e., 1000 might not be affordable in a real system. I only have one doubt: The lines do not cross for MRR  (Figure 2), so how is it possible that they cross for  coverage (Figure 1)? Maybe standard error bars would be helpful to understand which differences are due to quantization artifacts.\n\nIt would have been useful to study the effects of the candidate set on the final result when combined with a candidate-based entity linker.\n\nI am not sure I follow the terminology of (DiscK), weighted, combined, binary -- it would help to have an exhaustive list of methods with consistent names.\n\nIf the authors find it useful, here is a related paper that also discusses the effects of the candidate method during entity linking: Dalton, Jeffrey, and Laura Dietz. ""A neighborhood relevance model for entity linking."" Proceedings of the 10th Conference on Open Research Areas in Information Retrieval. 2013.', 'rating': '9: Top 15% of accepted papers, strong accept', 'confidence': '5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		r1xP1W56pQ	S1x_J6gElE	AKBC.ws/2019/Conference/-/Paper15/Official_Review	['AKBC.ws/2019/Conference/Paper15/Reviewers/Unsubmitted']	1		['everyone']	r1xP1W56pQ	['AKBC.ws/2019/Conference/Paper15/AnonReviewer1']	1544977631550		1547662984002	['AKBC.ws/2019/Conference']
743	1545596985045	"{'title': 'Some efficiency gains, but more rigorous evaluation and motivation required', 'review': 'This paper proposes using an existing method, DiscK, to pre-filter candidate concepts for a medical concept linking system. When combined with an existing state-of-the-art linker (DNorm), the joint system is able to consider much fewer candidate concepts, albeit with some drop in accuracy. These results are potentially significant, but assessing the actual impact requires additional measurements and better explanation of the motivation.\n\nPros:\n- DiscK + DNorm achieves concept linking accuracy close to the previous state-of-the-art, while requiring DNorm to process significantly fewer candidates. For example, the system gets up to .759 MRR when considering 2000 candidates, compared to .781 MRR for the full set of ~125K candidates (Table 5).\n\nCons:\n- Evaluation of candidate generation without downstream linking (Section 6) was not compelling. Without information about how the linker might use these candidate sets, it is hard to tell whether these numbers are good or not.\n- The claims of superior computational efficiency in Section 8 are not supported quantitatively. There should be a comparison between DNorm alone and DiscK + DNorm of how many mentions or documents can be processed per second. Otherwise, it is not clear how much overhead DiscK adds compared to the cost of running DNorm on more candidates. Hopefully this issue can be easily addressed.\n- Writing was at times unclear and misleading. Most importantly, the authors write, ""DiscK allows us to efficiently retrieve candidates over a large ontology in linear time."" In fact the point of DiscK is that it enables sublinear queries in the size of the candidate space/ontology. But perhaps the authors are referring to being linear in something else? After all, pairwise scoring models are also linear in the size of the candidate space.\n- Straightforward application of existing approaches, with no new technical contribution\n\nAdditional comments:\n- As an outsider to this medical linking task, I wanted to understand more why speed is important. How expensive is it to run DNorm on a large corpus? Are there online settings in which low latency is desired, or is this only for offline processing?\n- I would like some discussion of two other seemingly more obvious ways to improve runtime:\n    (1) Learn a fast pairwise comparison model and filter using that. The number of candidates is large but not unmanageably large (125K), so it seems possible that you could still get significant wall clock speedups by doing this (especially as this is very parallelizable). Such an approach would also not have to abide by the restrictions imposed by the DiscK framework.\n    (2) Speed up DNorm directly. Based on the original paper (Leaman et al., 2013), it seems that DNorm is slow primarily because of the use of high-dimensional tf-idf vectors. Is this correct? If so, might a simple dimensionality reduction technique, or use of lower-dimensional word vectors (e.g. from word2vec) already make DNorm much more efficient, and thus obviate the need for incorporating DiscK? Relatedly, are other published linkers as slow as DNorm, or much faster?\n- Based on table 5, in practice you would probably choose a relatively high value of K (say ~2000), to maintain near state-of-the-art accuracy. We also know that at high values of K, character 4-gram is competitive with or even better than DiscK. So, what is the runtime profile of the character 4-gram approach? Footnote 8 mentions that it is asymptotically slower, but computing these character overlaps should be very fast, so perhaps speed is not actually a big issue (especially compared to the time it takes DNorm to process the remaining K candidates). This is is related to point (1) above.\n- The authors mention that DNorm is state-of-the-art, but they don\'t provide context for how well other approaches do on this linking task. It would be good to know whether combining DiscK + DNorm is competitive with say, the 2nd or 3rd-best approaches.\n- The organization of this paper was strange. In particular, Section 8 had the most important results, but was put after Discussion (Section 7) and was not integrated into the actual ""Evaluation and Results"" section (Section 6).\n- Measuring MRR in Section 6 is unmotivated if you claim you\'re only evaluating candidate generation, and plan to re-rank all the candidate sets with another system anyways. It is still good to report these MRR numbers, so that they can be compared with the numbers in Section 8, so perhaps all of the MRR\'s should be reported together.', 'rating': '5: Marginally below acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		r1xP1W56pQ	HJebHxu6x4	AKBC.ws/2019/Conference/-/Paper15/Official_Review	['AKBC.ws/2019/Conference/Paper15/Reviewers/Unsubmitted']	2		['everyone']	r1xP1W56pQ	['AKBC.ws/2019/Conference/Paper15/AnonReviewer3']	1545596985045		1547662983787	['AKBC.ws/2019/Conference']
744	1546977059691	{'title': 'Compelling initial result but would benefit from additional experiments, especially direct comparisons with prior work.', 'review': 'This work applies an existing system, DiscK (Chen and Van Durme, 2017) to medical concept linking. They show that DiscK improves the mean reciprocal rank of candidate generation over several simple baselines, and leads to slightly worse but significantly more efficient linking performance in one existing downstream system, DNorm (Leaman et al., 2013). \n\nPro’s:\n-\tClear description of the task and associated challenges. \n-\tThorough explanation of the proposed system, experiments, and results. In particular, the discussion of the performance trade-off between MRR and coverage, and why it is useful that DiscK maintains robust MRR even though it under-performs several baselines w.r.t. coverage, was clear and useful. \n-\tThe downstream concept linking result is compelling: for example, the DNorm+DiscK system with k=5000 and d=1000 only considers 4% of candidates but its coverage declines by just 2% compared to considering all 100% of candidates (as in DNorm alone). \n\nCon’s/Suggestions:\n-\tSection 3/DiscK Overview: Without having read the DiscK paper in full (admittedly), I found Section 3 very hard to follow. How are the feature weights chosen? How are the feature parameters trained? What’s the difference between a feature type and a feature value? Since the DiscK work is integral to the rest of the paper, the authors should spend more time giving a high-level overview of the approach (targeted at “naïve readers”) before delving into the details.\n\n-\tSection 4: The authors note at the end of Section 4 that “While we found that many mentions could be matched to concepts by lexical features, a significant portion required non-lexical features.” It would be helpful if the authors provided a concrete example of such a case. \n\nAlso, they note that “While features from additional mention properties, such as the surrounding sentence, were tested, none provided an improvement over features build from the mention span text alone.” This merits more detailed explication: What features did they try? Since examples requiring non-lexical features were a consistent source of error, why do the authors think that non-span-based features failed to influence model performance on these examples?\n \n-\tData: It is unclear to me whether expanding the ontology to include the additional concepts (e.g. Finding) would prevent comparison with other systems for this dataset, and what is gained from this decision. In addition, it would be helpful if the authors explained what a “preferred entry” is in the context of the SNOWMED-CT ontology. \n\n-\tEvaluation & Results: \n\nThe authors note that “The concept linking systems that were evaluated in the shared task may have included a candidate generation stage, but evaluations of these stages are not provided.” However, it is unclear to me why the systems that do include a candidate generation phase could not either be re-run or re-implemented to get such results, especially since Share/CLEF is a shared task with many entries and corresponding published system descriptions. Since direct comparisons with prior work were omitted, it is hard to gauge the strength of the baselines and proposed system.  \n\nIn addition, it might be useful to test the value of higher MRR in the downstream concept linking task by comparing the DiscK-based candidate generator against one of the baseline approaches with higher coverage but lower MRR. Can DNorm capitalize on a more informative ranking, or are the results similar as long as a basic level of coverage is achieved for a given k?\n\n-\tRelated Work: The authors only briefly mention entity linking work outside of the biomedical domain. Did the authors evaluate any of these approaches, especially in the context of computationally-efficient linking architectures? Also, a more detailed description of DNorm would be useful. \n', 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		r1xP1W56pQ	Bkxhm1YMfN	AKBC.ws/2019/Conference/-/Paper15/Official_Review	['AKBC.ws/2019/Conference/Paper15/Reviewers/Unsubmitted']	3		['everyone']	r1xP1W56pQ	['AKBC.ws/2019/Conference/Paper15/AnonReviewer2']	1546977059691		1547662983535	['AKBC.ws/2019/Conference']
745	1545616191589	"{'title': 'Good work on diverse text sources, but missing some important comparisons', 'review': 'This paper presents a new approach to NER in the botany domain. The authors train an LSTM-CRF  model to copy silver labels generated by a rule-based system. They evaluate and train on a diverse set of text sources, and show that their proposed model is able to generalize reasonably well across text sources and from silver labels to manually-annotated gold labels. The main flaw of this paper is that no previous botanical NER systems, such as a rule-based approach, are evaluated in comparison with the LSTM approach.\n\nPros:\n- Well-written paper with thorough description of system and data collection; good use of illustrative examples (though as an English reader I might appreciate more English examples).\n- Sensible distant supervision approach to get labels for training\n- Text pulled from a variety of sources (Wikipedia, academic articles, blogs) to ensure applicability to a wide variety of documents. Both within-domain and cross-domain accuracy are reported (Table 7), with accuracy remaining quite high in the cross-domain setting (mid 80s to low 90s F1).\n- Includes gold test data annotated by humans, on which the proposed method does reasonably well (82.4 F1)\n\nCons:\n- No comparison to existing botanical NER systems, such as those mentioned in the related work section. In my opinion, this is the main flaw of the paper; if the LSTM does improve over the previous systems on the gold-labeled data, this paper should be a clear accept. At the very least, I would like an evaluation of using the dictionary-based annotations alone (Section 3.3) as test-time predictions. If I understand correctly, this would obtain 100% accuracy on the ""silver-labeled"" data, but there is an open question of how well it generalizes to the gold labels. This would simulate a comparison with SPECIES (Pafilis et al., 2013), but using the same set of gazetteer resources.\n\nOther comments:\n- This type of distant supervision strategy often encourages neural models to memorize the set of strings in the dictionary. In relation extraction, you would commonly mask out entity names (replacing them with a type token) to combat this. Did you observe similar issues? This seems related to your observation that higher dropout helps in a cross-domain setting, as higher dropout makes it harder to memorize the dictionary.\n- I would appreciate a little more discussion of the difference between distant supervision and human-annotated labels, given that the difference in F-scores in Table 9 is very significant. It would be nice to see a systematic categorization of the cases where distant supervision and gold labels differ, plus statistics on how common these different cases are. It could also be interesting to measure the model\'s accuracy on cases where the silver and gold labels disagree, vs. where they agree.', 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		rkeFyWcTTm	S1e_Sjh6xN	AKBC.ws/2019/Conference/-/Paper16/Official_Review	['AKBC.ws/2019/Conference/Paper16/Reviewers/Unsubmitted']	1		['everyone']	rkeFyWcTTm	['AKBC.ws/2019/Conference/Paper16/AnonReviewer3']	1545616191589		1547662983280	['AKBC.ws/2019/Conference']
746	1546975456642	"{'title': 'Useful datasets but insufficient experimental evidence to support proposed approach.', 'review': 'This work targets NER for both scientific and vernacular plant names across languages and domains. Solving this task is important for integrating information from both formal scientific texts and low-resource sources like folk medicine and ethnobotany. The task is difficult because of 1) the paucity of prior work on vernacular plant name recognition and 2) the broader problem of applying NER systems to low-resource settings.\n\nThe authors’ approach is two-pronged. First, they use semi-supervised learning, i.e. they collect a large corpus of noisily-labeled data using an automated system. Second, they train a LSTM-CRF model that integrates both word- and character-level input. They hypothesize that their approach will lead to better generalization across domains without relying on expensive hand-crafted/domain-specific features.\n\nIn order to test their hypothesis, they gather several semi-supervised datasets from four different genres and two languages (English and German), annotating them using several different gazetteers that the authors also collected from various data sources targeting both scientific (Latin) and vernacular names. In their experiments, they compare the LSTM-CRF architecture trained with vs. without initialization to pre-trained FastText word embeddings. \n\nPro’s:\n-\tThe authors collect several different corpora and gazetteers. The former could serve as benchmark datasets for future work on this task. The corpora vary in formality, content (w.r.t. scientific vs. vernacular plant name mentions), and publication time (recent vs. historical). These are all productive axes along which to compare NER system strengths and weaknesses. Similarly, the gazetteers will provide an valuable resource for future system comparisons. The authors note that all of these resources will be publicly available.\n\nCon’s and Suggestions:\n-\tStyle: Much of the writing in this paper is poorly organized. In the Introduction and Related Work sections, the authors tend to jump around to several different topics within the same paragraph. The result is that the information presented lacks cohesion, so it is difficult to elucidate the motivation for this work and, more importantly, its contribution to the task at hand in the context of prior work. Some of the sections are also poorly organized: for example, in the Method section (3), the authors start to summarize how they used their gazetteers to annotate data (3.1.1), then they jump into the specifics of the corpora that they collected (3.1.2), then jump back to describing their annotation methodology (3.3). \n\n-\tRelated Work needs more work: The authors describe many prior results and systems in the Related Work section, but are inconsistent about directly comparing these to facets of their approach. \n\nFor example, though the authors claim that little prior work has been done on extraction of vernacular plant names, they do not address this point cohesively in the Related Work section (e.g. they don’t mention that CoL includes both scientific and vernacular names). Which systems/datasets have attempted to address this issue? If there is truly no prior work in this area, state this fact clearly and talk about the research process.\n\nI recommend organizing the Related Work into several clear sub-sections, for example: “NER” (quickly summarize the task and describe common approaches e.g. rule-based, dictionary-based, first generation ML, neural taggers); “Biological/botanical NER” (talk about problems specific to the domain e.g. it’s low-resource setting, as well as major results in the area); “Vernacular plant name extraction”; and “Semi-supervised learning for NER”. \n\nThe authors should cite the “original” LSTM-CRF paper: Huang, Zhiheng, Wei Xu, and Kai Yu. ""Bidirectional LSTM-CRF models for sequence tagging."" arXiv preprint arXiv:1508.01991 (2015).\n\n-\tExperiments: The authors’ experiments do not show compelling evidence that supports (or negates) their hypotheses. \n\nClaim #1: “This approach is based on both character-level and token-level word representations, which are especially valuable when processing morphologically rich languages like German. Hence, we are able to identify taxonomic mentions not only a scientific (Latin) but also on a vernacular level (German or English).” \n\nTo test this claim, they examine the impact of changing the size of the character embedding dimension from 25 to 29. They conclude that “augmenting the embedding size…has only a very small and not significant positive influence…”. However, in my opinion, a better experiment would have been getting rid of the character-level input entirely and comparing word-only vs. word+character vs. character-only. \n\nAlso, the dimension of the character representation is not the only axis of variation. Presumably, the character-level representations have a large effect on model predictions for out-of-vocabulary words. The authors could test whether character-level representation actually helps generalization to new domains by testing on a set of held-out plant name mentions that are absent from the training data (and thus the learned word-level vocabulary) and see if performance improves over using word-level information alone.\n\nFinally, comparing 25 vs. 29 seems arbitrary in the first place – why did the authors choose 29 over, say, 1000? Why didn’t they run the same experiment for English, which is presumably less morphologically inflected? If varying properties of the character representations have a greater effect for German than English, then at least part of their claim would have supporting experimental evidence.\n\nClaim #2: “ We claim that our approach combining automated data annotation based on dictionary lookups and the subsequent training of neural models can guarantee good performance even without the availability of large precompiled language-specific gazetteers.” \n\nIn other words: domain-specific gazetteers are expensive and must be kept up-to-date. The authors claim that systems only need access to such gazetteers at training time (to get noisy “oracle” labels), because their LSTM-CRF will generalize. \n\nThey attempt to show this by training their model on a Wikipedia-based corpus and testing on the other corpora they collected. However, they don’t guarantee that entities mentioned in the training data are actually absent from any of the test corpora. In other words, while their experiments show that the LSTM-CRF is able to generalize to a variety of unseen entity contexts, they fail to show that their model can also generalize to unseen entities. However, generalization to unseen entities is the crux of the problem: this is why gazetteers need to be updated in the first place.\n\nThey also do not show to what extent gazetteer size and quality affect downstream performance. Briefly, they note that, for German, “we assume model performance is higher due to the larger datasets and cleaner gazetteers”, but they fail to explore this in detail. What would happen if the gazetteers they used for weak supervision only contained a few entries? How noisy can their noisy labels be? \n\n-\tInadequate baselines: The authors fail to compare their LSTM-CRF approach to adequate baselines. In the results section, they focus on comparing only the LSTM-CRF architecture initialized without (baseline) vs. with pre-trained FastText embeddings.\n\nThe most straightforward baseline is their dictionary-based annotation system itself. Briefly, they note that their automatic annotator assigned tags with an F1 of 85.05 on a hand-annotated, gold-standard test set. It is unclear from the context whether this evaluation targeted German, English, or both. Later, in the Results section, their results for their best German LSTM-CRF were substantially worse than this baseline (Table 9 reports an F1 of 82.42). \n\nThe problem is that, while 85.05 F1 is probably the best possible score for the system presented in this work, it is unclear whether 1) 85.05 F1 is a competitive result for this task overall and 2) whether the LSTM-CRF is well-suited to this task. How would a system trained on a smaller but fully-supervised dataset perform on this test set? How does a non-neural architecture with hand-crafted features stack up? \n\nIt is also unclear to me why they do not report corresponding results for a hand-labeled English test set. They also omit details about how many and what variety of entities are included in this test set, and don’t report quality metrics like inter-annotator agreement.\n\nFinally, they fail adequately address prior work in their experiments. In the Discussion, they briefly note that “Habibi et. al. (2017) report a f-score of 83.60% for the species label”, but it is unclear to me whether this is a fair comparison; however, this is the only direct comparison with previously published work that the authors provide. While it is true that the authors created several new datasets for this task, it would have been informative to re-implement and re-run previously-published systems/approaches on these new datasets. \n\nFurthermore, since the authors propose a system for extracting both scientific and vernacular names, and since there seems to be plenty of prior work on scientific name extraction, they could run their system on an existing corpus containing only scientific names and compare to published results along that axis. This would be an informative sanity check for the relative performance of the LSTM-CRF vs. other systems.\n\n-\tOther issues with the experiments: Although they train using 5-fold cross-validation, they fail to report error bars for their results and they do not note whether the results presented represent the average test performance or just the best runs. \n\nWhile they note that they use “default hyperparameters” for their baseline, as far as IO can tell, they do not note whether or not they tuned the hyperparameters for their non-baseline system (and if they did tune, they don’t note what the final hyperparameter settings were, nor how many runs of tuning were required to get the results presented).\n', 'rating': '3: Clear rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		rkeFyWcTTm	S1gK1F_fMN	AKBC.ws/2019/Conference/-/Paper16/Official_Review	['AKBC.ws/2019/Conference/Paper16/Reviewers/Unsubmitted']	2		['everyone']	rkeFyWcTTm	['AKBC.ws/2019/Conference/Paper16/AnonReviewer2']	1546975456642		1547662983063	['AKBC.ws/2019/Conference']
747	1547017526347	"{'title': 'Needs more original content ', 'review': 'The authors aim to build an NER model to label plant names in their scientific (Latin) and vernacular form (English and German). They generate a lexicon of plant names from a variety of sources and use it to create a data set that they then use to run an off the shelf NER model. \n\nUnfortunately this paper presents no new ideas and would maybe be better suited as an exploratory blog post on using Bi-LSTM CRF models [1] for NER in obscure domains. While the authors seem have put in considerable effort into gathering lexicons for plant names in different languages, the paper lacks in any originality of idea or methodology with regards to the NER task that this paper focuses on and the authors do not compare to any previous in-domain baselines.  \n\nIn light of the variety of resources that the authors used to generate the automatically generated dataset and creation of the manually annotated subset, if posed as a paper to introduce this data set along with some baselines on an NER task, it would have made for a more effective paper.\n\nFurther, the authors would be well advised to leave out unnecessary details that add no value to the paper and only serve to disrupt the readability of the paper such as ""a newline is added after each sentence to mark the sentence boundaries"". \n\n[1] Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., & Dyer, C. Neural Architectures for Named Entity Recognition. NAACL-HLT 2016', 'rating': '3: Clear rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		rkeFyWcTTm	SklAEpM7G4	AKBC.ws/2019/Conference/-/Paper16/Official_Review	['AKBC.ws/2019/Conference/Paper16/Reviewers/Unsubmitted']	3		['everyone']	rkeFyWcTTm	['AKBC.ws/2019/Conference/Paper16/AnonReviewer1']	1547017526347		1547662982771	['AKBC.ws/2019/Conference']
748	1546867826808	"{'title': 'Reasonable method, subpar evaluations', 'review': 'The paper suggests a model related to TransE in Knowledge Base Completion where users and items are embedded such that their difference is the embedding of the review of this item by this user.\nIn order to do this, the authors jointly minimize two objectives : a regression objective from a review bag of word embeddings to its score, and the same objective as in TransE to minimize the L2 norm of each ""user + review - item"".\nThe overall objective is a weighted sum of these two objectives and a weight-decay\n\nSection 2 :\n - The exposition for the model is clear, maybe a bit long. I don\'t think both figures 2 and 3 are needed.\n-  The argument made in favor of TransE in the last paragraph of section 2 is a bit weak. In the distmult (or ComplEx, to exploit the advances in Knowledge base completion) setting, it would be enough to find the unit-norm vector that maximizes the objective, then find the review with highest dot product in this direction. Experimental proof that this doesn\'t work as well or better would be interesting.\n\nSection 4: \n- Table 1 should be moved to the appendix. It\'s too big and not super informative. It should be replaced by some interesting metrics like training speed, memory usage on biggest dataset.\n- Table 2 is big and only useful for readers with deep knowledge of these datasets. I would have been satisfied with the last line only, and the entire table moved to the appendix.\n- The use of boldface in table 2 is a bit misleading. Some entries in TransRev are boldened, while the difference is of the same order of magnitude as differences in table 3 which are deemed ""insignificant"" in paragraph 4.5. Please develop more on the significance of these differences. \n\n-Since baselines , aside from HFT are not interesting points of comparison, a proper ablation study of the model would have been great. What happens if we use off-the-shelf word embeddings rather than learning them from this (smaller) dataset ? What does it change in terms of train time ? What about the suggested model change from TransE to State of the Art methods like ComplEx ?\n\n- Why are the performances degrading when rank is increased ? is the weight-decay not doing its job as a regularizer ?\n\n- Please give hyper-parameter ranges and selected values for TransRev. The weighting of the two objectives, as well as the regularizer\'s strength would especially be interesting.\n\n- More training speed comparison between all methods would be of interest.\n\n- Regarding the results, since the MSE is very close for most datasets, which method would you recommend to the reader ? Here again, other metrics such as model size and train time would be useful.\n\nSection 4.7 doesn\'t add much, it\'s more of a sanity check. Maybe a comparison of figure 4 with the same projection applied to off-the-shelf word embeddings to show that joint training is valuable would be better ?\n\nSection 4.8 is honest. The retrieved reviews don\'t seem that useful in practice. \n\n\n\nOverall, I found the model interesting, but would have preferred a more in-depth study of its components.\n- What about the margin in the TransE score ? \n- What about off-the-shelf word embeddings ? Same with off-the-shelf text encoders ?\n- State of the art models in knowledge base completion were dismissed without enough experimental support in my opinion.\n\nComparison with the baselines is also unsatisfying (the current description of the models is too short, or not precise enough). It is said in section 4.6 that T-Nets are related to the model under study. The training objective of T-Nets has never been written anywhere in this paper. Please provide a full comparison between TransRev and the baselines when it\'s meaningful to do so. When it is not, it should be made explicit that the models are their just to compare the performances.\n\nLots of space is wasted in large figures or table that would be more useful in an appendix, reducing the space available for interesting discussions around the suggested model.', 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		HJxikb5T6m	SJejOERlGV	AKBC.ws/2019/Conference/-/Paper17/Official_Review	['AKBC.ws/2019/Conference/Paper17/Reviewers/Unsubmitted']	1		['everyone']	HJxikb5T6m	['AKBC.ws/2019/Conference/Paper17/AnonReviewer2']	1546867826808		1547662982552	['AKBC.ws/2019/Conference']
749	1546997106634	{'title': 'Lack of evidence supports the proposed model', 'review': 'Based upon recent work on embedding based knowledge graph completion methods, the author proposed a new approach to jointly learn user, item and review embeddings. The review embedding represents the difference between users and items and can be used for rating prediction. The author conducted experiments on Amazon user review dataset and demonstrated improved performance compared with other state of the art methods.  The method is straightforward and works better than other methods in most cases on Amazon dataset.\n\nHowever, the proposed method depends heavily on review data, which in real world application is usually very sparse and hard to acquire compared with other user behavior such as clicks and views. The author broke down Amazon dataset into different groups based on item category and ran experiments separately on each group. As a result, all the experiments were essentially done on one dataset, but the author claim “TRANSREV outperforms state of the art recommender systems on a large number of benchmark data sets”. The author should also extend evaluation on other datasets. \n\nAlso, by grouping data based on their category, each subset has only one topic, which makes sense for training the model. However, because review embedding learnt from one product category, which can hardly be used for predicting ratings in the other category. However, this brings out another limitation of the proposed method, cross-domain recommendation. For example, a customer might buy grocery products as well as health products, but they are treated separately in the paper and no cross-domain recommendation can be made in this case. Collaborative filtering methods, on the other hand, can make recommendation based on user behavior regardless of what kind products.', 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		HJxikb5T6m	SJxo_6aMfV	AKBC.ws/2019/Conference/-/Paper17/Official_Review	['AKBC.ws/2019/Conference/Paper17/Reviewers/Unsubmitted']	2		['everyone']	HJxikb5T6m	['AKBC.ws/2019/Conference/Paper17/AnonReviewer1']	1546997106634		1547662982299	['AKBC.ws/2019/Conference']
750	1547109650194	"{'title': 'Please clarify problem setup', 'review': 'In this paper authors suggest a framework for evaluating IE methods by testing the recall of the facts retrieved. As in, does the IE system retrieve all possible facts on a given domain? They motivate their framework with a number of interesting applications.\n\nI thought the idea was really cool, and the motivations were compelling. The paper was well-written. Additionally, authors go to lengths to provide explanations using their interpretable classifier, SVM, which I appreciated. Admittedly, I did not find the results super compelling, but I think that\'s passable as it\'s a novel idea that can be expanded upon.\n\nA primary concern was that, perhaps because this is a new approach as the authors say, the reader would be better served if the problem statement on page 6 were clearer. Despite introducing notation, the authors do not make much use of the notation for developing their problem statement. \n\nAs I understand, the forumation is:\n\nt: input text\nF: facts extracted from input text\nall facts\n\nIn paragraph 2 of page 6, should ""t"" be ""F""? Authors simply say ""all these facts"" in that paragraph, but the preceding paragraph mentioned two sets of facts: ""facts in t, F"" and ""all facts in the world about a topic"".\n\nOn first read, it was totally unclear that the authors are using the text to infer, linguistically, whether a set of facts is complete or not. I urge the authors to put some more time in clarifying this. Maybe one way to do this would be to write a simple equation showing the prediction task?\n\n\nMy next point:, authors define the prediction problem as binary: whether t contains all known facts (1) or not (0). Authors mention later on that this is an imbalanced set. That is an interesting discussion point. I\'m curious also what implications this has for training. Did authors try balancing their data before training? What is the distribution over probabilities that their classifier gives to training examples? Are they OK-looking? Perhaps not necessary for the paper, I\'m just scratching my head over whether there might be pathological training errors here. Some deeper discussion into class imbalance at train-time would be nice.\n\n\nFinally: authors give an interesting linguistic hypothesis for why t should signal towards this problem. They then compare to human annotation. What about comparing to a known closed IE set? Granted, Recall-Assessment classifiers is clearly meant for OpenIE settings, but I\'d be curious to see how was a recall-assessor, f( t, F) performs relative to closed IE settings where recall can be wholly observed. Is it close?\n\nOverall, it\'s an interesting idea but I think it could be developed more. There\'s a lot in this paper that doesn\'t seem to lead anywhere. I\'d like to give this a Revise and Resubmit', 'rating': '6: Marginally above acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		HkgAyb5aaQ	SkxczSYEf4	AKBC.ws/2019/Conference/-/Paper18/Official_Review	['AKBC.ws/2019/Conference/Paper18/Reviewers/Unsubmitted']	2		['everyone']	HkgAyb5aaQ	['AKBC.ws/2019/Conference/Paper18/AnonReviewer2']	1547109650194		1547662982084	['AKBC.ws/2019/Conference']
751	1546608382531	"{'title': 'Doubts in methodology; mainly engineering work', 'review': 'This paper uses a variant of the transformer encoder for relation classification. In particular, entity representations are used to calculate attention weights for the context words. The authors achieve state-of-the-art results on two benchmark datasets.\nDespite that, I have two major concerns: (1) about the methodology (attention using entities only), (2) about the engineering effort. In the following paragraphs, I will describe them in more detail.\n\n(1) methodology: I do not see why entities should be useful for extracting context words that indicate a relation. That would mean that relation-specific information is encoded in the entity names already. In fact, this might be the case for ELMo embeddings since they are context dependent but would not work with other embeddings, especially when an entity is rare and has not been mentioned in the context of a specific relation in the training dataset for the embeddings. This is basically the result the authors get when they compare ELMo embeddings + attention only with GloVe embeddings + attention only on the SemEval dataset. However, entities in SemEval are nominal phrases which might actually be very indicative of their possible relations already. I would expect an even more severe drop in performance for the TACRED dataset which includes persons, organizations and locations as entities. Just the name of a person, for example, does not indicate all possible relations it could participate in. Thus, it would be good if the authors evaluated GloVe + attention only on TACRED as well. In this setting, I would expect that the impact of the position encodings and the named entity embeddings increases a lot and would be much higher than the impact of the actual entity embeddings.\nThe authors repeatedly claim that their results are interpretable and write that they find it ""exciting"" that their results are good but it would be better if the authors could quantify whether their approach only works because ELMo already encodes the relation context within the embeddings for the entities, and if the attention weights are assigned to some words because of the entity embeddings or because of the position or named entity embeddings.\nAnother side note on interpretability: ELMo is a bidirectional RNN model, thus the authors actually use a bidirectional RNN + attention, which is not easier interpretable than other neural networks with attention (since the authors only analyze the attention weights but not the embeddings from the bidirectional RNN model) or even CNNs without attention (from which it is also possible to extract the pooling results).\n\n(2) engineering: The paper does not include any technical contribution. Instead, it seems to be the result of a lot of engineering effort, such as reducing the transformer architecture to something that worked for them, adding and tuning a lot of different regularization techniques, adding named entity information to one of their datasets (without an ablation study), adding class weights also to only one of their datasets (without an ablation study), etc. Moreover, it is not clear how the authors tune the hyperparameters (listed in the appendix) on the SemEval dataset since they mention that they do not use a development set.\n\nAdditional comments:\n- Are the relative positions in Equation 2 fed into an embedding layer? If not, this means that these values can get very large for long contexts which might affect the neural network negatively.\n- ""slot filling"" is NOT ""relation classification"" (see introduction): relation classification is: ""find r in r(e1,e2)"" while slot filling is: ""find e2 in r(e1,e2)""\n- In 2.1 it should be ""defining a relation mention to start at ..."" instead of ""defining an entity mention...""\n- There are several typos, grammar mistakes and format issues: some examples: ""paramteer-free"", ""ensmeble"", figures not readable without color, ""if we would want"", tables too wide, wrong format of words in equations, etc.', 'rating': '3: Clear rejection', 'confidence': '5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		S1xlgbcT6X	SJgDZJ1aZV	AKBC.ws/2019/Conference/-/Paper19/Official_Review	['AKBC.ws/2019/Conference/Paper19/Reviewers/Unsubmitted']	1		['everyone']	S1xlgbcT6X	['AKBC.ws/2019/Conference/Paper19/AnonReviewer1']	1546608382531		1547662981653	['AKBC.ws/2019/Conference']
752	1546970487053	"{'title': 'Very well written and educational paper', 'review': 'In this paper, authors propose a novel approach to relation classification. Given that a relation is determined by two entities and their context, they enrich the entity representations with additional information coming from the context. Such an additional information is extracted via a neural architecture inspired by the transformer, using multihead attention using the two entities as queries and the context as keys and values. The context and entity representations are computed via ELMo embeddings. The proposed approach achieves SOTA results on two relation extraction benchmarks, namely SemEval-2010 and TAC. Furthermore, authors show that the model can provide some sort of  explanations, in terms of attention masks. By leveraging such masks, they find that attention focuses on the shortest dependency path between two entities, providing evidence supporting the Shortest Path Hypothesis.\n\nELMo embeddings seem to have a large impact on the final results, and it\'s great that authors performed an ablation on it using multiple embedding mechanisms. May it be the case that there is an overlap between the ELMo training data and this paper\'s test data?\n\nTypos:\n- pg. 3 ""[..] not reproducable [..]""\n- pg. 4 ""paramteer-free""\n- pg. 10 ""entites""\n- pg. 12, Tab. 4 ""Ensmeble""\n- pg. 12 ""vica versa""\n\n', 'rating': '7: Good paper, accept', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		S1xlgbcT6X	B1xkFHvMz4	AKBC.ws/2019/Conference/-/Paper19/Official_Review	['AKBC.ws/2019/Conference/Paper19/Reviewers/Unsubmitted']	2		['everyone']	S1xlgbcT6X	['AKBC.ws/2019/Conference/Paper19/AnonReviewer2']	1546970487053		1547662981402	['AKBC.ws/2019/Conference']
753	1546988665056	"{'title': 'marginally below acceptance', 'review': 'The authors propose a method for relation classification which uses a simplified transformer architecture together with pretrained Elmo embeddings. The method is tested on the SemEval2018 and TACRED datasets yielding competitive results with prior work. \n\nThe experiments are interesting, though of limited substance and do not fully support the claims made in this manuscript. In particular, the hypothesis of attention mass falling on dependency paths is not quantitatively tested at all. The writing is at points imprecise or unclear -- see comments below.\n\nFurthermore, there appears generally to be only limited novelty in the proposed method. The core contribution, an architecture based on both Elmo and Self-attention can be argued to be fairly standard, given the widespread use of both self-attention and Elmo. From this alone, few new insights emerge to a reader familiar with contemporary work. On the other hand, some valuable ablation experiments have been conducted which confirm and substantiate the individual contribution of both of these model components. \n\n\n\nOther Comments: \n\n- The manuscript would benefit from being more concise at times, concretely at the end of section 2.1, or here: ""Secondly, what we actually do is take the two entities and add information on top of them. ""\n\n- Missing related work: Verga et al. 2018 http://aclweb.org/anthology/N18-1080 have applied self-attention for relation extraction\n\n- Unclear: how are representations formed (equations 1 and 2). Are positional / mention indicator information concatenated to Elmo representations, or added?\n\n- ""because different heads learn to watch out for different features in the sentence."" -- is this actually the case, and why? It is too weak to stand as a justification, and not fully convincing. Has this been shown? A reference would be useful.\n\n- ""We find it exciting that these patterns emerge from learning supervised relation classification."" -- \nIn fact, the second example (Figure 6) you give does not quite support this. I disagree with the claim that ""the two tokens paid attention to the most (”died” and ”February”) describe the relation [”per:date of death""] very accurately"", since a) the token ""dissident"" receives a similar amount of attention to ""February"", and b) ""February"" does not define the relation type.\n\n""In comparison to RNNs, especially their gated, deep and bidirectional version, we do not have long, incomprehensible information flow.""  -- I do not believe this to be quite true, since you are using ELmo representations, which stem from precisely that: bidirectional gated RNNs. Not continuing to train the parameters of these also does not change this.\n\n- Elmo representations do already include context information, so attributing the classification decision to individual tokens via their assigned attention weights has to be taken with a grain of salt.\n\n- Table 1 -- Is this difference significant? 2.7K test samples is only a limited size (although there exist smaller test sets), yet the numbers are relatively close. I suggest running, for example, a t-test.\n\n- Unclear writing: ""We think that a bidirectional LSTM on top of ELMo embeddings (which are produced by bidirectional LSTMs too) can not extract much more information which it could not have learned by training it on GloVe embeddings in the first place.""\n\n- page 10: initially unclear/ambiguous what is meant by ""the heads of the entites"", only becomes clear in the last line before 4.3\n\n- Equation 7: Surely there is a simpler way of stating that mathematical expression with one numerator and one denominator?\n\n- Table 3: Improvement in F1 may be due to better balance of precision and recall. It is speculated that this has to do with the way of setting class weights. Removing this would then be an interesting side experiment, and would help understand if this is where the F1 improvement comes from, or from the other model components.\n\n- ""we completely move away from tedious feature engineering"" -- while I agree with the general point, this statement is not fully true, given that NE information is used (which is even acknowledged).\n\n- Regarding interpretability -- how does the method add something new compared to standard biLSTM+attention? Previous approaches have also used attention weights for interpretation.\n\n- The ""evidence"" for what the model is paying attention to consists in 2 (randomly picked?) examples. This is far from being substantial enough to support the claims made in the paper about the focus on shortest dependency path. I do however believe this to be an interesting hypothesis. Can this be quantified? Can this then be computed for more than 2 examples and put to the test? \n\n- ""we achieve State of the Art results on two established benchmarks in relation classification."" -- This is not immediately obvious. From your the results in table 4, it appears that ""C-GCN + PA-LSTM"" holds SOTA.\n', 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		S1xlgbcT6X	SkxbtnsfzV	AKBC.ws/2019/Conference/-/Paper19/Official_Review	['AKBC.ws/2019/Conference/Paper19/Reviewers/Unsubmitted']	3		['everyone']	S1xlgbcT6X	['AKBC.ws/2019/Conference/Paper19/AnonReviewer3']	1546988665056		1547662981186	['AKBC.ws/2019/Conference']
754	1546510770484	"{'title': 'interesting work, but with a number of weaknesses', 'review': ""The paper addresses the problem of extracting facts\nabout biomedical entities (diseases, symptoms, drugs, etc.)\nfrom text documents.\nTo this end, it presents a pipeline of techniques,\nanchored in semantic role labeling (SRL) and extending SRL\nby specific consideration of copula verbs and noun phrases,\nand also adding a scoring classifier for the final output\nof the extraction process.\n\nThe problem is important and the work is interesting.\nHowever, it has a number of weakness and is not fully convincing.\n\nSpecific comments:\n\n1) The paper merely presents its techniques, without\nmuch discussion about the rationale for making these\ndesign choices.\nFor example, why SRL at all? The resulting frames\ncan capture higher-arity relations, but in the end\nonly triples (i.e. binary relations) are used anyway.\nWhy not start with one of the modern Open IE tools\n(e.g., OpenIE 5.0)?\n\n2) Some components are not sufficiently well explained.\nFor the function srlParse, the method on which all\nsubsequent steps build on, only the interface is described\nin Section 3 (before 3.1 starts).\nAlso, it remains unclear to what extent this first stage\nreflects the modern state of the art?\nWere neural methods like the ones by He, Lee, Lewis\nand Zettlemoyer (ACL 2017, EMNLP 2017, and follow-up\npapers) considered at all?\n\n3) The method for entity linking seems very basic.\nDocument context is not considered (beyond the \nsentence from which the triple of tuple is extracted),\nand there is no consideration of coherence which is\nused by all modern methods.\n\n4) The paper states that SRL is inherently unable\nto produce confidence scores for its outputs.\nIs this a limitation of the specific technique\nemployed, or a claim about all SRL methods in general?\nThis needs further explanation.\n\n5) The numbers of accepted and rejected triples, or positive\nand negative samples, given in the different subsections\nof the experimental section are confusing.\n6.2 talks about 5k triples, out of which 2389 are accepted.\nHowever, 6.4 talks about 1345 accepted and 1945 reject\ntriples from the doctor verification. \n6.5 uses yet another set of numbers for its training.\nThis needs clarification.\n\n6) Table 1 seems to be the end-to-end result. \nThe authors should have added the numbers for the \nevaluated samples for each relation. Accepted numbers\ndon't add up to 5k - so one cannot infer the\ndistribution of the samples over the different relations.\n\n7) The relations given in Table 1 resemble those of\nthe work by Ernst et al. 2015 (included in the paper's\nreferences), and that work also tapped into both PubMed\narticles and biomedical overview pages in Wikipedia,\nMayoClinic etc. Could the current paper's results\nand findings be compared to that prior work?\nOr is there any other baseline for comparison?\n\n8) In Table 1, why was none of the Affects triples\naccepted by the expert judges? Isn't that relation\na relaxed version of Cause or Treat? So it should\nbe simpler than these crisper relations?\n\n9) The accuracy and AUC numbers in 6.4 and 6.5 \nsuggest that the overall pipeline achieves pretty\ngood, that is, mostly correct, outputs.\nHowever, this is not in line with the numbers given\nfor Table 1 where only half of the outputs seem\nto be correct (and informative).\nThis may be my misinterpretation of the experimental\nresults (e.g., different setups in 6.2 and 6.4, 6.5?),\nbut the authors should make these points clearer."", 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		HJxmxbq66Q	Ske93-vjWV	AKBC.ws/2019/Conference/-/Paper20/Official_Review	['AKBC.ws/2019/Conference/Paper20/Reviewers/Unsubmitted']	1		['everyone']	HJxmxbq66Q	['AKBC.ws/2019/Conference/Paper20/AnonReviewer3']	1546510770484		1547662980932	['AKBC.ws/2019/Conference']
755	1547025557441	"{'title': 'Effective but hand-crafted system for constructing KBs from text', 'review': 'This paper describes a system and evaluation methods for constructing a medical KB from unstructured text. The system uses semantic role labeling (SRL) followed by a bunch of heuristics to extract triples from sentences and link them to existing entities in the KB. These triples are then pruned by restricting the types of entities and relations in the triples. To facilitate evaluation, the paper presents an crowd-sourcing verification tool, and also checks the accuracy of KB completion methods to verify the extracted relations. Lastly, a classifier is trained and tested to detect uninformative triples which are not precise enough in their arguments.\n\nFor the extraction system, the main contributions of this paper are heuristics for dealing with the various problems posed by the SRL system, such as copula verbs, noun-phrase based relations and entity linking of long texts. However, it is hard to gauge the effectiveness of these heuristics since they are not evaluated in isolation from the rest of the system. Overall, the system seems effective, however, with about 48% of the extracted facts accepted as correct by experts. On the evaluation side, sections 6.4 and 6.5 present interesting studies for verifying and filtering the extracted triples.\n\nOverall the paper presents a focused contribution on how to construct KBs from text from scratch. This should be interesting to industry practitioners trying to do the same thing in medical or other domains. In terms of research, however, the paper has limited contributions, mostly describing very specific hand-crafted solutions to the problems discussed.\n\nDetailed comments:\n- Page 3, second paragraph ""approached"" -> ""approaches""\n- It is strange to say ""SRL is not based on learning techniques"" when most state-of-the-art SRL models are based on machine learning\n- Section 3.1 last paragraph: ""F1 and F1"" -> ""F1 and F2""\n- Is the SRL system same as that of He et al? Does it use the same general-domain training data?\n- Page 8, last paragraph: ""Subsequently, ,"" -> ""Subsequently, ""\n- Section 6, last paragraph: ""medical"" -> ""medically""\n- More details on the PGM system would be nice.', 'rating': '6: Marginally above acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		HJxmxbq66Q	rJeTq2NXfE	AKBC.ws/2019/Conference/-/Paper20/Official_Review	['AKBC.ws/2019/Conference/Paper20/Reviewers/Unsubmitted']	2		['everyone']	HJxmxbq66Q	['AKBC.ws/2019/Conference/Paper20/AnonReviewer1']	1547025557441		1547662980673	['AKBC.ws/2019/Conference']
756	1547078439499	"{'title': 'Review of Improving Relation Extraction by Pre-trained Language Representations', 'review': ""The paper presents TRE, a Transformer based architecture for relation extraction, evaluating on two datasets - TACRED, and a commonly used Semeval dataset.\n\nOverall the paper seems to have made reasonable choices and figured out some important details on how to get this to work in practice.  While this is a fairly straightforward idea and the paper doesn't make a huge number of innovations on the methodological side, however (it is mostly just adapting existing methods to the task of relation extraction).\n\nOne point that I think is really important to address: the paper really needs to add numbers from the Position-Aware Attention model of Zhang et. al. (e.g. the model used in the original TACRED paper).  It appears that the performance of the proposed model is not significantly better than that model.  I think that is probably fine, since this is a new-ish approach for relation extraction, getting results that are on-par with the state-of-the-art may be sufficient as a first step, but the paper really needs to be more clear about where it stands with respect to the SOTA."", 'rating': '6: Marginally above acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		BJgrxbqp67	Hke1NobVMN	AKBC.ws/2019/Conference/-/Paper21/Official_Review	['AKBC.ws/2019/Conference/Paper21/Reviewers/Unsubmitted']	3		['everyone']	BJgrxbqp67	['AKBC.ws/2019/Conference/Paper21/AnonReviewer2']	1547078439499		1547662980451	['AKBC.ws/2019/Conference']
757	1546891873522	{'title': 'Application of existing method for relation extraction', 'review': 'This article describes a novel application of Transformer networks for relation extraction.\n\nCONS:\n- Method is heavily supervised. It requires plain text sentences as input, but with clearly marked relation arguments. This information might not always be available, and might be too costly to produce manually. \nDoes this mean that special care has to be taken for sentences in the passive and active voice, as the position of the arguments will be interchanged?\n\n- The method assumes the existence of a labelled dataset. However, this may not always be available. \n\n- There are several other methods, which produce state of the art results on relation extraction, which are minimally-supervised. These methods, in my opinion, alleviate the need for huge volumes of annotated data. The added-value of the proposed method vs. minimally-supervised methods is not clear. \n\nPROS:\n- Extensive evaluation\n- Article well-written\n- Contributions clearly articulated\n', 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		BJgrxbqp67	BkxqDfEWfE	AKBC.ws/2019/Conference/-/Paper21/Official_Review	['AKBC.ws/2019/Conference/Paper21/Reviewers/Unsubmitted']	1		['everyone']	BJgrxbqp67	['AKBC.ws/2019/Conference/Paper21/AnonReviewer3']	1546891873522		1547662980235	['AKBC.ws/2019/Conference']
758	1547060621409	{'title': 'Incremental but solid contribution', 'review': 'This paper presents a transformer-based relation extraction model that leverages pre-training on unlabeled text with a language modeling objective.\n\nThe proposed approach is essentially an application of the OpenAI GPT to relation extraction. Although this work is rather incremental, the experiments and analysis are thorough, making it a solid contribution.\n\nGiven that the authors have already set up the entire TRE framework, it should be rather easy to adapt the same approach to BERT, and potentially raise the state of the art even further.\n\nIn terms of writing, I think the authors should reframe the paper as a direct adaptation of OpenAI GPT. In its current form, the paper implies much more novelty than it actually has, especially in the abstract and intro; I think the whole story about latent embeddings replacing manually-engineered features is quite obvious in 2019. I think the adaptation story will make the paper shorter and significantly clearer.\n', 'rating': '7: Good paper, accept', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		BJgrxbqp67	S1ercHpmMV	AKBC.ws/2019/Conference/-/Paper21/Official_Review	['AKBC.ws/2019/Conference/Paper21/Reviewers/Unsubmitted']	2		['everyone']	BJgrxbqp67	['AKBC.ws/2019/Conference/Paper21/AnonReviewer1']	1547060621409		1547662980026	['AKBC.ws/2019/Conference']
759	1547112472087	"{'title': ""An interesting paper in methodology, but I'm not entirely convinced by the experiments"", 'review': ""This paper proposes an interesting combination of semi-supervised learning and ensemble learning for information extraction, with experiments conducted on a biomedical relation extraction task. The proposed method is novel (to certain degree) and intuitively plausible, but I'm not entirely convinced of its effectiveness. \n\nPros:\n\n- Novel combination of semi-supervised learning and ensemble learning for information extraction\n- Good discussion of literature\n- Paper is well written and easy to follow\n\nCons:\n\n- Experimental design and results are not supportive enough of the effectiveness of the proposed method.\n\nSpecifically, I think the following problems undermine the claims:\n\n- The experiments are conducted on a single dataset. I understand it may be hard to get more biomedical RE datasets, but the proposed method is in principle not limited to biomedical RE (even though the authors limited their claim to biomedical RE). In general I think it needs multiple datasets to test a generic learning methodology.\n\n- Only one split of training data is used, and the unlabeled data to labeled data are close in scale. It would be really informative if there are experiments and performance curves with different labeled subsets of different sizes. Also, (after under-sampling) 4500 unlabeled vs. 2000 labeled seems less impressive and may be not very supportive of the usefulness of the proposed method. What will happen if there are only 200 labeled examples? 400?\n\n- No comparison with other semi-supervised learning methods for information extraction, so it's not clear how competent the proposed method is compared with other alternatives. \n\n- The fact that the mean base learner performance is often on par with the meta-learner, and simple majority vote of the weak labels can largely outperform the meta-learner may suggest a better meta-learner other than LSTM should have been used."", 'rating': '5: Marginally below acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		rygDeZqap7	BJgeQgqVzV	AKBC.ws/2019/Conference/-/Paper22/Official_Review	['AKBC.ws/2019/Conference/Paper22/Reviewers/Unsubmitted']	3		['everyone']	rygDeZqap7	['AKBC.ws/2019/Conference/Paper22/AnonReviewer3']	1547112472087		1547662979809	['AKBC.ws/2019/Conference']
760	1546803216447	{'title': 'review ', 'review': '-summary\nThis paper addresses the problem of generating training data for biological relation extraction, looking specifically at the biocreative chemprot dataset. The authors weakly label data by using a set of weak classifiers, then use those predictions as additional training data for a meta learning algorithm.\n\n-pros\n- nicely written paper with good exposition on related works\n- good analysis experiments\n\n-cons\n- experiments all on a single dataset\n- methodology isn’t very novel\n\n-questions\n- In table 3, majority vote of weak classifiers outperforms the meta learning. Are these two numbers comparable and if so, does this mean an ensemble of weak classifiers is actually the best performing method in your experiments.\n\n', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		rygDeZqap7	ryedGdR1GE	AKBC.ws/2019/Conference/-/Paper22/Official_Review	['AKBC.ws/2019/Conference/Paper22/Reviewers/Unsubmitted']	1		['everyone']	rygDeZqap7	['AKBC.ws/2019/Conference/Paper22/AnonReviewer2']	1546803216447		1547662979590	['AKBC.ws/2019/Conference']
761	1547006650341	"{'title': 'The paper proposes a potentially interesting direction, though needs to improve in many aspects.', 'review': 'The authors propose to semi-supervised method for relation classification, which trains multiple base learners using a small labeled dataset, and applies an ensemble of them to annotate unlabeled examples for semi-supervised learning. They conducted experiments on a BioCreative shared task for detecting chemical-protein interactions from biomedical abstracts. The experimental results suggest that ensemble could help denoise the self supervised labels. Overall, this is an interesting direction to explore, but the paper can also improve significantly in several aspects.\n\nFirst, the discussion about related work is a bit lacking and at place slightly misguided. The authors seem to be unaware of a growing body of recent work in biomedical machine reading using indirect supervision, e.g., Deep Probabilistic Logic (EMNLP-18). These methods have demonstrated success in extracting biomedical relations across sentences and beyond abstracts, using zero labeled examples. Likewise, in discussing semi-supervised approaches and cross-view learning, the authors should discuss their obvious connections to recent progress such as Semi-Supervised Sequential Modeling with Cross-View Learning (EMNLP-18) and EZLearn (IJCAI-18).\n\nAdditionally, the authors seem to pitch the proposed method against standard weak supervision approaches. E.g., the paper stated ""For the creation of the weak labels, we use classifiers pretrained in a small labeled dataset, instead of large Knowledge Bases which might be unavailable."" Effectively, the authors implied that distant supervision is not applicable here because it requires ""large knowledge bases"". In fact, the precise attraction of distant supervision is that knowledge bases are generally available for the relations of value, even though their coverage is sparse and not up-to-date. And past work has shown success with small KBs using distant supervision, such as recent work in precision oncology:\n\t- Distant Supervision for Relation Extraction beyond the Sentence Boundary (EACL-17)\n\t- Cross-Sentence N-ary Relation Extraction with Graph LSTMs (TACL-17)\n\nA labeled dataset contains strictly more information than the annotated relations, and arguably is harder to obtain than the corresponding knowledge base. Some prior work even simulated distant supervision scenarios from labeled datasets, e.g., DISTANT SUPERVISION FOR CANCER PATHWAY EXTRACTION FROM TEXT (PSB-15). These won\'t detract significance in exploring ensemble learning, but the proposed direction should be considered as complementary rather than competing with standard weak supervision approaches.\n\nThe paper should also discuss related datasets and resources other than BioCreative. E.g., BioNLP Shared Task on event extraction (Kim et al. 2009) is influential in biomedical relation extraction, followed by a number of shared tasks in the same space.\n\nAnother major area that can be improved is the technical and experimental details. E.g.:\n\nThe ensemble of base learners is key to the proposed method. But it\'s unclear from the paper how many base learners have been considered, what are their types, etc. At the high level, the paper should specify the total number of candidate base learners, distribution over major types (SVM, NN, ...), the types of chosen centroids, etc. For each type, the paper should include details about the variation, perhaps in supplement. E.g., SVM kernels, NN architectures. \n\nTable 3 shows the mean F1 score of base learners, which raises a number of questions. E.g., what\'re the top and bottom performers and their scores? If the mean is 53, that means some base learner learning from partial training set performs similar to LSTM learning from the whole training set, so it seems that out right some learner (supposedly not LSTM) is more suited for this domain? It\'s unclear whether LSTM is included in the base learner. If it was, then its low performance would mean that some top performers are even higher, so the ensemble gain could simply stem from their superiority. In any case, the lack of details makes it very hard to assess what\'s really going on in the experiment.\n\nA key point in the paper is that high-capacity methods such as LSTM suffers from small dataset. While in general this could be true, the LSTM in use seems to be severely handicapped. For one, the paper suggested that they undersampled in training LSTM, which means that LSTM was actually trained using less data compared to others. They did this for imbalance, but it is probably unnecessary. LSTMs are generally less sensitive to label imbalance. And if one really has to correct for that, reweighting instances is a better option given the concern about small training set to begin with.\n\nThe paper also didn\'t mention how the word embedding was initialized in LSTM. If they\'re randomly initialized, that\'s another obvious handicap the LSTM suffers from. There are publicly available PubMed word2vec embedding, not to more recent approaches like Elmo/Bert. \n\nMinor comments:\n\nIn Table 3, what does it mean by ""F1 of weak labels""? Does it mean that at test time, one compute the ensemble of predictions from base learners?\n\nThe paper uses D_G in one place and D_B in another. Might be useful to be consistent.\n\n""Text trimming"" sounds quite odd. What the paragraph describes is just standard feature engineering (i.e., taking n-grams from whole text vs. in between two entities).\n\nFig 3: it\'s unclear which line is which from the graph/caption. One has to guess from the analysis, \n\nFig 5: what was used in tSNE? I.e., how is each instance represented? It\'s also hard to make any conclusion from the graph, as the authors\' tried to make the point about distributions.\n', 'rating': '5: Marginally below acceptance threshold', 'confidence': '5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}"		rygDeZqap7	B1lGTzl7fV	AKBC.ws/2019/Conference/-/Paper22/Official_Review	['AKBC.ws/2019/Conference/Paper22/Reviewers/Unsubmitted']	2		['everyone']	rygDeZqap7	['AKBC.ws/2019/Conference/Paper22/AnonReviewer1']	1547006650341		1547662979375	['AKBC.ws/2019/Conference']
762	1545265844045	{'title': 'A clearly written paper, but the evaluation is hard to interpret', 'review': 'This paper proposes a method for detecting the occurrences of events in news articles.  Essentially, the assumption is that if the mentions of certain entities (people, location, organization) change sharply over time, then there should be some event happens.  The paper then suggests to build an entity graph where a node is an entity and an edge indicates the connected two entities cooccur in the same document, with weights indicating on how important those entities in the document are.  Events are detected by monitoring the weighted degree of each node through a sliding window, over a series of news articles.\n\nWhile the methods used to construct the graph and also to determine the weights are simple, the idea does seem interesting.  The results are also presented in various plots, which provide some insight of the data.  However, the main question left really unanswered is whether the proposed method is indeed better than other alternatives, as well as whether the method really advances the state of the art.  Currently, such comparisons only exist in Table 1 and Figure 11. How to interpret them remains generally unclear.\n\nPros:\n\t• Interesting idea and simple method\n\t• Writing is clear\nCons:\n\t• Hard to interpret the comparisons to baselines; not clear whether it advances the state of the art\n', 'rating': '3: Clear rejection', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		HylqxZqppX	SJx22zvdgE	AKBC.ws/2019/Conference/-/Paper23/Official_Review	['AKBC.ws/2019/Conference/Paper23/Reviewers/Unsubmitted']	1		['everyone']	HylqxZqppX	['AKBC.ws/2019/Conference/Paper23/AnonReviewer2']	1545265844045		1547662979121	['AKBC.ws/2019/Conference']
763	1546807786445	"{'title': 'review', 'review': 'The authors propose a method for detecting ""important"" events (i.e. those containing named entities) in streaming news data. Their technique identifies named entities, then constructs time-dependent weighted graphs between entities where weights are determined by entity co-occurrences. Important events are identified using change-point detection over these graphs combined with community detection to identify distinct events. The authors compare their method to a simple tf-idf clustering baseline and human annotations, finding that their technique discovers more event clusters than the simple baseline, though not as many as the human annotators.\n\nA lot of related work is listed, but the authors don\'t compare to it by running previous models on their own data, or running their models on data used by prior work. Is there a rationale for this? In particular, Sayyadi et al. (2009) and Melvin et al. (2017) both use very similar techniques (weighted networks between noun phrases and sometimes named entities w/ weightings determined by co-occurrence, community detection to discover events, and time series analysis to identify important events. It seems like the main contribution of this paper is the exact form of change-point detection and the use of just named entities, but the authors are not explicit about this so I\'m not sure (without reading the prior work). And, these works seem so similar I expect the authors to compare to them.\n\nOverall, this paper addresses an interesting problem with a reasonable approach, but as the paper is currently written it is unclear exactly how this work compares to previous work, both in terms of techniques and empirical results. The paper fails to answer the following important questions: Why should the proposed method out-perform previous related works? How does the method in fact compare to previous related works, methodologically and empirically? In the introduction the authors state that this paper aims simply to demonstrate the feasibility of the approach, but since it\'s so similar to previous work it seems unnecessary to devote an entire paper to this.\n\nStyle/writing comments:\n\nWhat does the color indicate in Figure 4? I guess each entity has a color, you should indicate this in the caption. \np. 11, I think the correct capitalization is: tf-idf (and there\'s some weird spacing going on in the tf-idf vectorization parenthetical)\nSection 4.1 would benefit from some subsections/signposting, it\'s easy to get lost in this long section with many large figures\nTable 1 is very confusing, though it seems important as it\'s the closest thing to a quantitative evaluation, if I understand correctly. I\'m not sure why the KeyGraph column uses percentages while the other columns use counts. And though I expect the rows to line up across columns, they do not. I feel these results could be presented much more clearly.', 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		HylqxZqppX	Skxzxc1gGN	AKBC.ws/2019/Conference/-/Paper23/Official_Review	['AKBC.ws/2019/Conference/Paper23/Reviewers/Unsubmitted']	2		['everyone']	HylqxZqppX	['AKBC.ws/2019/Conference/Paper23/AnonReviewer3']	1546807786445		1547662978904	['AKBC.ws/2019/Conference']
764	1547006750094	"{'title': ""The paper needs to clarify why it's relevant for AKBC."", 'review': ""This paper explores using entity co-occurrence networks to detect news events. The authors first extract named entities from news articles using publicly available tools like SpaCy, then tally entity co-occurrence, identify entities of centrality, and find clusters of entities as event topics.\n\nIt's a bit unclear about the paper's relevance to AKBC. The proposed method would identify candidate topics as bags of entities, which are a far cry from event detection. Additionally, the paper demonstrates the method on a few hot topics such as Brexit. How would identification of such well known topics help? Can the proposed method help detect events that are less well known?\n\nThe research contributions are also a bit unclear from the presentation. The proposed system essentially combines a few available tools (NER, community detection, ...). It's unclear what's the research challenge the authors set out to address, and what innovation was proposed. The evaluation is limited and qualitative, which didn't shed much light to the significance of the proposed system. "", 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		HylqxZqppX	SygIQQxmfN	AKBC.ws/2019/Conference/-/Paper23/Official_Review	['AKBC.ws/2019/Conference/Paper23/Reviewers/Unsubmitted']	3		['everyone']	HylqxZqppX	['AKBC.ws/2019/Conference/Paper23/AnonReviewer1']	1547006750094		1547662978682	['AKBC.ws/2019/Conference']
765	1547070786527	{'title': 'Interesting approach to taxonomy extension', 'review': 'The paper presents an interesting approach to taxonomy extension that is based on identifying synonyms for component words of multi-word terms in the taxonomy. The approach seems to rely very much on WordNet, which may be a weakness. Coverage of WordNet is rather limited and the approach may therefore be limited in application. Also, word sense disambiguation (selecting the appropriate sense for a component word) is a challenge that has not been addressed in full detail, although this will be dealt with by the classification step in filtering, if I understand correctly. Overall, the paper is well-written and clear in ambitions and achieved results. The experiments use an extensive crowdsourced gold standard, which is a valuable research outcome on its own if it will be released publicly.', 'rating': '7: Good paper, accept', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		rJx2g-qaTm	SyljBpJVM4	AKBC.ws/2019/Conference/-/Paper24/Official_Review	['AKBC.ws/2019/Conference/Paper24/Reviewers/Unsubmitted']	2		['everyone']	rJx2g-qaTm	['AKBC.ws/2019/Conference/Paper24/AnonReviewer1']	1547070786527		1547662978463	['AKBC.ws/2019/Conference']
766	1546705210004	{'title': 'Simple approach for a practical use case', 'review': 'The authors describe and evaluate two approaches to collecting alternative aliases (synonyms) for entities in a taxonomy: expansion from WordNet synsets and from search queries followed by a binary classification to refine the generated candidate sets. Mitigating vocabulary mismatch in search applications provides a good motivating use case for ontology/taxonomy construction and is an important research direction.\n\nQuestions:\n* How were the negative samples for training the classifier selected in 4.3?\n* What is the overlap between the synonym sets generated using WordNet and the search queries?\n* Can the WordNet-generated candidates improve performance for aligning synonyms collected from search queries? i.e. output of the first method as input to the second synonym selection method.\n* Are there other evaluation results that can show improvement from implementing the proposed approaches on the target tasks, e.g. search or information extraction?\n\nRemarks:\n* Semantic Network seems to be a synonym for a Knowledge Graph, which is a more frequently used term. The relation has to be made explicit.\n* The structure of the paper is confusing: only one of the candidate selection methods is described in the Section 3 but experimental results for two approaches are reported in Section 4.', 'rating': '6: Marginally above acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		rJx2g-qaTm	HkefHtIA-4	AKBC.ws/2019/Conference/-/Paper24/Official_Review	['AKBC.ws/2019/Conference/Paper24/Reviewers/Unsubmitted']	1		['everyone']	rJx2g-qaTm	['AKBC.ws/2019/Conference/Paper24/AnonReviewer2']	1546705210004		1547662978244	['AKBC.ws/2019/Conference']
767	1547084560971	"{'title': 'Review of Combining Long Short Term Memory and Convolutional Neural Network for Cross-Sentence n-ary Relation Extraction', 'review': ""The paper presents an approach to cross-sentence relation extraction that combines LSTMs and convolutional neural network layers with word and position features.  Overall the choices made seem reasonable, and the paper includes some interesting analysis / variations (e.g., showing that an LSTM layer followed by a CNN is a better choice than the other way around).\n\nEvaluation is performed on two datasets, Quirk and Poon (2016) and a chemical induced disease dataset.  The paper compares a number of model variations, but there don't appear to be any comparisons to state-of-the-art results on these datasets.  The paper could benefit from comparisons to SOTA on these or other datasets."", 'rating': '6: Marginally above acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		Sye0lZqp6Q	ByxYGXmVME	AKBC.ws/2019/Conference/-/Paper25/Official_Review	['AKBC.ws/2019/Conference/Paper25/Reviewers/Unsubmitted']	3		['everyone']	Sye0lZqp6Q	['AKBC.ws/2019/Conference/Paper25/AnonReviewer3']	1547084560971		1547662978025	['AKBC.ws/2019/Conference']
768	1545349436935	{'title': 'Good paper with small modeling improvements but thorough evaluations', 'review': 'The paper presents a method for n-ary cross sentence relation extraction.\nGiven a list of entities, and a list of sentences the task is to identify which relation (from a predefined list) is described between the entities in the given sentences.\nThe proposed model stacks CNN on LSTM to get long range dependencies in the text, and shows to be effective, either beating or equalling the state-of-the-art on two datasets for the task.\n\nOverall, I enjoyed reading the paper, and would like to see it appear in the conference.\nWhile the proposed model is not very novel, and was shown effective on other tasks such as text classification or sentiment analysis, this is the first time it was applied for this specific task.\nIn addition, I appreciate the additional evaluations, which ablate the different parts of the model, analyze its performance by length between entities, compare it with many variations as baselines and against state-of-the-art for the task.\n\nMy main comments are mostly in terms of presentation - see below.\n\n\nDetailed comments:\n\nIn general, I think that wording can be tighter, and some repetitive information can be omitted. For example, Section 4 could be condensed to highlight the main findings, instead of splitting them across subsections.\nI think that Section 3.1.2 (“Position Features”) would benefit from an example showing an input encoding.\nTable 5 shows up in the references. \n\nMinor comments and typos:\n\nText on P. 9 overflows the page margins.\nI think that Table 3 would be a little easier to read if the best performance in each column were highlighted in some manner.\nSection 2, p. 3: “mpdel” -> model.\nPerhaps using “Figure 1” instead of “Listing 1” is more consistent with *ACL-like papers?\n', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		Sye0lZqp6Q	B1eBHYjFgV	AKBC.ws/2019/Conference/-/Paper25/Official_Review	['AKBC.ws/2019/Conference/Paper25/Reviewers/Unsubmitted']	1		['everyone']	Sye0lZqp6Q	['AKBC.ws/2019/Conference/Paper25/AnonReviewer1']	1545349436935		1547662977800	['AKBC.ws/2019/Conference']
769	1546608499915	"{'title': 'Nice experimental study', 'review': 'The paper addresses cross-sentence n-ary relation extraction. The authors propose a model consisting of an LSTM layer followed by an CNN layer and show that it outperforms other model choices. The experiments are sound and complete and the presented results look convincing. The paper is well written and easy to follow. \nIn total, it presents a nice experimental study.\n\nSome unclear issues / questions for the authors:\n- ""The use of multiple filters facilitates selection of the most important feature for each feature map"": What do you mean with this sentence? Don\'t you get another feature map for each filter? Isn\'t the use of multiple filters rather to capture different semantics within the sentence?\n- ""The task of predicting n-ary relations is modeled both as a binary and multi-class classification problem"": How do you do that? Are there different softmax layers? And if yes, how do you decide which one to use?\n- Table 1/2: How can you draw conclusions about the performance on binary and ternary relations from these tables? I can only see the distinction of single sentence and cross sentence there.\n- Table 3: The numbers for short distance spans (mostly 20.0) look suspicious to me. What is the frequency of short/medium/long distance spans in the datasets? Are they big enough to be able to draw any conclusions from them?\n- You say that CNN_LSTM does not work because after applying the CNN all sequential information is lost. But how can you apply an LSTM afterwards then? Is there any recurrence at all? (The sequential information would not be lost after the CNN if you didn\'t apply pooling. Have you tried that?)\n- Your observation that more than two positional embeddings decrease the performance is interesting (and unexpected). Do you have any insights on this? Does the model pay attention at all to the second of three entities? What would happen if you simply deleted this entity or even some context around this entity (i.e., perform an adversarial attack on your model)?\n\nOther things that should be improved:\n- sometimes words are in the margin\n- there are some typos, e.g., ""mpdel"", ""the dimensions ... was set... and were initialised"", ""it interesting""', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		Sye0lZqp6Q	SJxnuy1TZV	AKBC.ws/2019/Conference/-/Paper25/Official_Review	['AKBC.ws/2019/Conference/Paper25/Reviewers/Unsubmitted']	2		['everyone']	Sye0lZqp6Q	['AKBC.ws/2019/Conference/Paper25/AnonReviewer2']	1546608499915		1547662977577	['AKBC.ws/2019/Conference']
770	1547133594116	"{'title': 'Recent relevant work not adequately discussed or compared.  ', 'review': 'Paper summary:  This paper presents a method of learning word embeddings for the purpose of representing hypernym relations.  The learning objective is the sum of (a) a measure of the “distributional inclusion” difference vector magnitude and (b) the GloVE objective.  Experiments on four benchmark datasets are mostly (but not entirely positive) versus some other methods.\n\nThe introduction emphasizes the need for a representation that ""able to encode not only the direct hypernymy relations between the hypernym and hyponym words, but also the indirect and the full hierarchical hypernym path.”  There has been significant interest in recent work on representations aiming for exactly this goal, including Poincare Embeddings [Nikel and Kiela], Order Embeddings [Vendrov et al], Probabilistic Order Embeddings [Lai and Hockenmaier], Box embeddings [Vilnis et al].  It seems that there should be empirical comparisons to these methods.\n\nI found the order of presentation awkward, and sometimes hard to follow.  For example, I would have liked to see a clear explanation of test-time inference before the learning objective was presented, and I’m still left wondering why there is not a closer correspondence between the multiple inference methods described (in Table 3) and the learning objective.\n\nI would also have liked to see a clear motivation for why the GloVE embedding is compatible with and beneficial for the hypernym task.  “Relatedness” is different than “hypernymy.”', 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		S1xf-W5paX	ByxfoMkHM4	AKBC.ws/2019/Conference/-/Paper26/Official_Review	['AKBC.ws/2019/Conference/Paper26/Reviewers/Unsubmitted']	3		['everyone']	S1xf-W5paX	['AKBC.ws/2019/Conference/Paper26/AnonReviewer3']	1547133594116		1547662977358	['AKBC.ws/2019/Conference']
771	1547069213590	"{'title': 'Well motivated approach but some concerns', 'review': 'This paper proposed a joint learning method of hypernym from both raw text and supervised taxonomy data.  The innovation is that the model is not only modeling hypernym pairs but also the whole taxonomy. The experiments demonstrate better or similar performance on hypernym pair detection task and much better performance on a new task called ""hierarchical path completion"". The method is good motivated and intuitive. Lots of analysis on the results are done which I liked a lot. But I have some questions for the authors.\n\n1) One major question I have is for the taxonomy evaluation part, I think there are works trying to do taxonomy evaluation by using node-level and edge-level evaluation. \'A Short Survey on Taxonomy Learning from Text Corpora:\nIssues, Resources, and Recent Advances\' from NAACL 2017 did a nice summarization for this. Is there any reason why this evaluation is not applicable here?\n\n2) At the end of section 4.2, the author mentioned Retrofit, JointReps and HyperVec are using the original author prepared wordnet data. Then the supervised training data is different for different methods? Is there a more controlled experiment where all experiments are using the same training data?\n\n3) In section 4.4, there are three prediction methods are introduced including ADD, SUM, and DH. The score is calculated using cosine similarity. But the loss function used in the model is by minimizing the L2 distance between word embeddings? Is there any reason why not use L2 but cosine similarity in this setting? Also, I\'m assuming SUM and DH are using cosine similarity as well? It might be useful to add that bit of information.\n\n4) The motivation for this paper is to using taxonomy instead of just hypernym pairs? Another line of research trying to encode the taxonomy structure into the geometry space such that the taxonomy will be automatically captured due to the self-organized geometry space. Some papers including but not restricted \'Order-Embeddings of Images and Language\', \n\'Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures\'. Probably this line of work is not directly comparable, but it might be useful to add to the related work session.\n\nA few minor points: \n1) In equation four of section 3, t_max appears for the first time. This equation maybe part of the GLOVE objective, but a one-sentence explanation of t_max might be needed here.\n2) at the end of section 3, the calculation of gradients for different parameters are given, but the optimization is actually performed by AdaGrad. Maybe it would be good to move these equations to the appendix.\n3) In section 4.1 experiment set up, the wordnet training data is generated by performing transitive closure I assume? How does the wordnet synsets get mapped to its surface form in order to do further training and evaluation?\n', 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		S1xf-W5paX	ryg8XPy4fE	AKBC.ws/2019/Conference/-/Paper26/Official_Review	['AKBC.ws/2019/Conference/Paper26/Reviewers/Unsubmitted']	2		['everyone']	S1xf-W5paX	['AKBC.ws/2019/Conference/Paper26/AnonReviewer1']	1547069213590		1547662977136	['AKBC.ws/2019/Conference']
772	1547016513446	"{'title': 'Needs better evaluation ', 'review': 'This paper presents a method to jointly learn word embeddings using co-occurrence statistics as well as by incorporating hierarchical information from semantic networks like WordNet. \n\nIn terms of novelty, this work only provides a simple extension to earlier papers [1,2] by changing the objective function to instead make the word embeddings of a hypernym pair similar but with a scaling factor that depends on the distance of the words in the hierarchy.\n\nWhile the method seems to learn some amount of semantic properties, most of the baselines reported seem either outdated or ill fitted to the task and do not serve well to evaluate the value of the proposed method for the given task. \nFor example the JointRep baseline is based on a semantic similarity task which primarily learns word embeddings based on synonym relations and seems to not be an appropriate baseline to compare the current approach to.\nFurther, there are two primary methods of incorporating semantic knowledge into word embeddings - by incorporating them during the training procedure or by post processing the vectors to include this knowledge. While I understand that this method falls into the first category, it is still important and essential to compare to both types of strategies of word vector specialization. In this regard [3]  has been shown to beat HyperVec and other methods on hypernym detection and directionality benchmarks and should be included in the results. It would be also interesting to see how the current approach fares on graded hypernym benchmarks such as Hyperlex. \n\nMinor comments : Section 4.2 there is a word extending out of the column boundaries. \n\n\n[1] Alsuhaibani, Mohammed, et al. ""Jointly learning word embeddings using a corpus and a knowledge base."" PloS one (2018)\n[2] Bollegala, Danushka, et al. ""Joint Word Representation Learning Using a Corpus and a Semantic Lexicon."" AAAI. 2016.\n[3] Vulić, Ivan, and Nikola Mrkšić. ""Specialising Word Vectors for Lexical Entailment."" NAACL-HLT 2018.', 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		S1xf-W5paX	B1eYBtGmG4	AKBC.ws/2019/Conference/-/Paper26/Official_Review	['AKBC.ws/2019/Conference/Paper26/Reviewers/Unsubmitted']	1		['everyone']	S1xf-W5paX	['AKBC.ws/2019/Conference/Paper26/AnonReviewer2']	1547016513446		1547662976915	['AKBC.ws/2019/Conference']
773	1546876857016	"{'title': 'Baselines on completion insufficient', 'review': 'This work introduces two methods : KBC-UWS and KBC-UWS-Constraints. Both based on ProjE. KBC-UWS is ProjE with a cross-entropy loss to exploit the confidence scores in NELL. The second method adds 3 losses to be minimized jointly, added to satisfy 3 ontological constraints. The method is validated in two settings : knowledge base refinement on NELL and link prediction on an augmented version on FB15K. \n\nsince it also introduced type constraints for knowledge base completion. \n\nSection 1:\n- Define T-Box and A-Box.\n\nSection 2:\n- [1] could be mentioned since it studied type constraints for knowledge base completion. \n\nSection 3.2 :\n- Why not allow a small margin for Sub-Property as in Inverse ? This way, all constraints are represented as hinge-losses on differences.\n- All losses should be normalized to be comparable in terms of scale, and for the values of lambda{1|2|3} to make sense.\n\nSection 4 :\n- It would have been interesting to compare other link prediction methods with the same cross-entropy loss as in KBC-UWS. As is, the comparison with TransH, HolE and TransE seems a bit unfair (trained on a smaller dataset...) and isn\'t really that informative.\n- Table 3 would be better in an appendix, as it is not discussed in the text and no conclusions are drawn from it.\n- I didn\'t understand why KBC approaches need manually built KBs and this approach did not ?  (last paragraph of section 4.1)\n\nSection 4.2 :\n- The methods used on the FB15K Datasets are far from state of the art. As such, it is difficult to assess the improvement given by the added ""inverse"" objective as well as type constraints. ComplEx [2] reaches the state of the art on FB15K with a MRR of 86 [3], without any additional knowledge on the dataset.  As such, the results in Table 5 are not good enough to bear any conclusions.\n\n\nOverall, I think the idea behind the paper is not necessarily bad. But the interest of such ideas would mostly be experimental. As such, I would ask for a more in depth experimental section, with real effort made to compare methods in a similar setting. Here, improvements due to ProjE are indistinguishable from improvements due to using a larger dataset and the cross-entropy as a loss for NELL. For FB15K, no conclusions can be taken due to the results being too far from the state of the art.\n\n\n\n[1] Denis Krompas Type-Constrained Representation Learning in Knowledge Graphs\n[2] Trouillon et al. Complex embeddings for simple link prediction\n[3] Lacroix et al. Canonical Tensor Decomposition for Knowledge Base Completion', 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		rklHbZqaTQ	rkeZ6PeZfE	AKBC.ws/2019/Conference/-/Paper27/Official_Review	['AKBC.ws/2019/Conference/Paper27/Reviewers/Unsubmitted']	1		['everyone']	rklHbZqaTQ	['AKBC.ws/2019/Conference/Paper27/AnonReviewer3']	1546876857016		1547662976698	['AKBC.ws/2019/Conference']
774	1546935015941	"{'title': 'Interesting but unclear connection to related work', 'review': 'The paper considers how to encode ontological knowledge into deep networks\nfor knowledge graph embedding. To this end, it encodes inequality constraints on the \nloss induced by ontological horn clauses (T-Boxes) and uses the ""distance to satisfiablity"" as a loss. \n\nOverall, the paper is ok written. The idea of using general world knowledge \nto guide learning methods for knowledge graphs is also interesting, the idea of constraining the output layer of a neural network in a ""distance to satisfiablity""\nmanner, is also interesting and the empirical results to show an interesting \nimprovement. However, the presentation of the paper should be improved. In particular, \nthe discussion of related work should be improved.  \n\nFor instance, the authors should discuss the semantic loss introduced in\n\nJingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, Guy Van den Broeck:\nA Semantic Loss Function for Deep Learning with Symbolic Knowledge. \nICML 2018: 5498-5507\n\nas it also measures how close a neural network is to satisfying constraints \non its output. \n\nAlso I wondering about the connection to \n\nZhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard H. Hovy, Eric P. Xing:\nHarnessing Deep Neural Networks with Logic Rules. ACL (1) 2016\n\nwhen encoding T-Box statements as first order logical rules. This should also \nlink to \n\nThomas Demeester, Tim Rocktäschel, Sebastian Riedel:\nLifted Rule Injection for Relation Embeddings. EMNLP 2016: 1389-1399\n\nAdditionally,  it would be great if the authors could relate/compare to methods\non deep learning for onotology learning/reasoning. \n\nGiulio Petrucci, Marco Rospocher, Chiara Ghidini:\nExpressive ontology learning as neural machine translation. \nJ. Web Sem. 52-53: 66-82 (2018)\n\nPatrick Hohenecker, Thomas Lukasiewicz:\nOntology Reasoning with Deep Neural Networks. \nCoRR abs/1808.07980 (2018)\n', 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		rklHbZqaTQ	r1egxsRbGE	AKBC.ws/2019/Conference/-/Paper27/Official_Review	['AKBC.ws/2019/Conference/Paper27/Reviewers/Unsubmitted']	2		['everyone']	rklHbZqaTQ	['AKBC.ws/2019/Conference/Paper27/AnonReviewer1']	1546935015941		1547662976475	['AKBC.ws/2019/Conference']
775	1546956354474	"{'title': 'A KG embedding model with penalty terms for rule violations that is reminiscent of prior work', 'review': 'This paper proposes a model for knowledge bases that combines KG embeddings (KGE) and rules. The key idea is to (1) use an existing KG embedding model and (2) incorporate a set of rules via penalty terms for rule violations. Such approaches have been explored before, but the paper ignores or does not correctly attribute related work in the area. In fact, it remains unclear what is novel (if anything) and how the proposed method compares to prior work experimentally.\n\nPros:\nS1. Simple approach\nS2. Decent results\n\nCons:\nW1. Unclear novelty\nW2. Does not compare to alternative KGE+rule methods\nW3. Penelty terms partly not convincing\nW4. Grounding unclear\n\nDetailed comments:\n\nD1. Instead of listing all relevant work, I\'ll just give one example. [A] makes use of soft labels (akin to the ""confidence"" used here) and a penalty term for rule violations to learn a model for FB15k. [A] is neither cited nor compared to, and its numbers are better than what is presented here.\n\n[A] Guo et al. Knowledge Graph Embedding with Iterative Guidance from Soft Rules. AAAI18.\n\nD2. The authors claim that existing KGE methods cannot make use of noisy information from information extraction systems. The paper does not really state why not, but I guess what is meant is that they cannot use confidence scores. This is not true; it\'s straightforward to incorporate confidences (e.g. via likelihoods as done in [A]). The setup for the KB refinement experiments, which does not use all the soft triples to train competing KBE models, is highly flawed. This also explains why KBC-UWS appears to give such a boost, even though it\'s the plain ProjE model.\n\nD3. I found party of the presentation to be dismissive of prior work.\n\nD4. The proposed model does not appear to be ""deep"" as claimed in the title.\n\nD5. The related work section is unfocused and, as indicated above, misses related work in the area.\n\nD6. Fig. 1 needs an explanation.\n\nD7. Eq. (2) should be motivated better (KL divergence). Also, the same data points flow in many times into the loss (via different I); is this intended and, if so, why? I also fail to see why this is ""weak supervision"".\n\nD8. I don\'t buy the argument for disjointWith. The discussion of Inverse seems flawed, since $P(x,y)\\to Q(y,x)$ does not means that $Q$ implies $P$. The discussion of Domain/Range is incomplete (stops mid-sentence).\n\nD9. I do not understand the grounding part. What is grounded, exactly, and why doesn\'t this lead to a blow up? What\'s different from prior approaches? Related: x and y are unbound in Eq (7).\n\nD10. The experiments should compare to alternative models using exactly the same set of rules. Right now, there is not a single comparison to alternative KGE+rules models.\n\nD11. It\'s unclear to me how the models have been trained. Has any form of sampling been employed? What optimizer has been used? Was there regularization? Etc.\n', 'rating': '2: Strong rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		rklHbZqaTQ	Sye9SCmzfV	AKBC.ws/2019/Conference/-/Paper27/Official_Review	['AKBC.ws/2019/Conference/Paper27/Reviewers/Unsubmitted']	3		['everyone']	rklHbZqaTQ	['AKBC.ws/2019/Conference/Paper27/AnonReviewer2']	1546956354474		1547662976255	['AKBC.ws/2019/Conference']
776	1547111515054	"{'title': 'Add more methods in the experiment', 'review': ""Authors here discuss a method to test the quality of tags given to scientific papers by seeing how well they are able\nto use these tags to predict citations. Authors make the point that article-tagging systems are often wrong, and are\nalso often not validated in any way besides human validation. The authors' problem statement and potentially very useful\nto practitioners, and the paper is well-written and easy to understand.\n\nI was a bit underwhelmed with the algorithm and the experiments. I think this paper ultimately needs some more meat\nbefore it is accepted. The most obvious place for that would be to compare more Algorithms against each other in the \nExperiments section.\n\nI thought there were many directions that authors could have gone in in extending this algorithm and I think that without\nmultiple algorithms and comparisons this paper is thin. There are also problems that stood out in my mind.\n\nAuthors did not address what effect the potentially high cardinality of the tag space has on their matching counts. This may\nhave been solved with a more careful look at correlated groups of tags. \n\nI'd like to see the distribution over matching counts. What is the distribution for y=1 samples? What is the distribution for\ny=0? I suspect both have very low means as this is a very sparse problem. I urge authors to look at this. \n\nWhat is the distribution over tag counts per tag in the corpus? I suspect that it is very left-skewed.. several tags are\nused often while many are barely used. This also might pose a problem for the algorithm.\n They did not consider upweighting matches on less frequently used tags. A match for a tag that is used all the time should\ncount less than a match for a tag that is barely used at all. A TF-IDF style approach might be useful in this regard."", 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		SyeD-b9T6m	SygQP3tNzE	AKBC.ws/2019/Conference/-/Paper28/Official_Review	['AKBC.ws/2019/Conference/Paper28/Reviewers/Unsubmitted']	3		['everyone']	SyeD-b9T6m	['AKBC.ws/2019/Conference/Paper28/AnonReviewer3']	1547111515054		1547662975996	['AKBC.ws/2019/Conference']
777	1545254206177	"{'title': 'Interesting idea but lack of technical content', 'review': 'This paper proposes an interesting idea on how to evaluate the quality of the ""tags"" of a scientific paper.  Essentially, the authors argue that good tags should help predict the citation graph.  While I may not agree with this argument completely, it seems a reasonable proposal.  However, beyond this proposal, the paper doesn\'t seem to have enough technical content.  Essentially, what it has is to have some sampled papers (with positive and negative labels based on the citations), and use the overlap of keyword tags as the scoring function to calculate AUC.  It is thus difficult to derive useful conclusions from this work.\n\nPros:\n\t• Interesting proposal on measuring the quality of the keyword tags of a paper\nCons:\n\t• Lack of technical content\n\t• Some parts of the paper are not written clearly\n', 'rating': '3: Clear rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		SyeD-b9T6m	B1gISBVuxV	AKBC.ws/2019/Conference/-/Paper28/Official_Review	['AKBC.ws/2019/Conference/Paper28/Reviewers/Unsubmitted']	1		['everyone']	SyeD-b9T6m	['AKBC.ws/2019/Conference/Paper28/AnonReviewer2']	1545254206177		1547662975781	['AKBC.ws/2019/Conference']
778	1547039182005	{'title': 'need stronger experiments and better presentation', 'review': 'This work proposes a simple measure of the consistency of the tagging of scientific papers: whether these tags are predictive for the citation graph links.   An algorithm is proposed to calculate consistency, and experiments with human- and machine-generated tags.  The paper also introduces cross-consistency , the ability to predict citation links between papers tagged by different taggers.\n\n* The problem is not well motivated. A more clear introduction would be great for readers to understand the importance of the proposed measure.\n* (fig 2) The real tags are compared with random tags. Stronger experiments are needed to support the proposed method.\n* What are the key choices/hyperparameters of the measure if we would like to use it? How should we choose them?\n* The area under curve (AUC) metric is used in the paper. How about other metrics?\n* How do we find the set of seed papers?\n* Some real examples would be helpful.', 'rating': '3: Clear rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		SyeD-b9T6m	SkxLAZdXz4	AKBC.ws/2019/Conference/-/Paper28/Official_Review	['AKBC.ws/2019/Conference/Paper28/Reviewers/Unsubmitted']	2		['everyone']	SyeD-b9T6m	['AKBC.ws/2019/Conference/Paper28/AnonReviewer1']	1547039182005		1547662975550	['AKBC.ws/2019/Conference']
779	1546773590438	"{'title': 'good qa paper', 'review': 'This paper focuses on the recently introduced ARC Challenge dataset, which contains 2,590 multiple choice questions authored for grade-school science exams. The paper presents a system that reformulates a given question into queries that are used to retrieve supporting text from a large corpus of science-related text. The rewriter is able to incorporate background knowledge from ConceptNet. A textual entailment system trained on SciTail that identifies support in the retrieved results.  Experiments show that the proposed system is able to outperform several baselines on ARC.\n\n* (Sec 2.2)  ""[acl-2013] Paraphrase-driven learning for open question answering"" and ""[emnlp-2017] Learning to Paraphrase for Question Answering"" can be added in the related work section.\n* (Sec 3.1) Seq2seq predicts 0 and 1 to indicate whether the word is salient. A more straightforward method is using a pointer network for the decoder, which directly selects words from the input. This method should be more effective than seq2seq used in Sec 3.1.1.\n* (Sec 3.1) How about the performance of removing the top crf layer? The LSTM layer and the classifier should play the most important role.\n* How to better utilize external resources is an interesting topic and is potentially helpful to improve the results of answering science exam questions. For example, the entailment module described in Sec 5.1 can be trained on other larger data, which in turn helps the problem with smaller data. I would like to see more details about this.\n* Are the improvements significant compared to the baseline methods? Significance test is necessary because the dataset is quite small.\n* Experiments on large-scale datasets are encouraged.', 'rating': '6: Marginally above acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		HJxYZ-5paX	H1eRU4D1GN	AKBC.ws/2019/Conference/-/Paper29/Official_Review	['AKBC.ws/2019/Conference/Paper29/Reviewers/Unsubmitted']	2		['everyone']	HJxYZ-5paX	['AKBC.ws/2019/Conference/Paper29/AnonReviewer3']	1546773590438		1547662975109	['AKBC.ws/2019/Conference']
780	1546984641485	"{'title': 'Good paper, some clarifications needed', 'review': 'Summary\n\nThis paper addresses the ARC dataset by reformulating the question using embeddings from ConceptNet. Their model selects a few terms from the question using the embeddings from ConceptNet, rewrites the query based on the selected terms, retrieves the documents and solves the query. The empirical result shows that embeddings from ConceptNet is beneficial, and the overall result is comparable to recent performance on ARC dataset.\n\nQuality\npros\n1) This paper contains a thorough study of recent QA models and datasets.\n2) This paper describes the model architecture, conducts ablation studies of different Essential terms classification, and includes thorough comparisons with recent models on ARC challenges.\n\ncons\n- Although the paper includes recent works on QA models/datasets, it doesn’t contain much studies on query reformulations. For example,  ""Ask the Right Questions: Active Question Reformulation with Reinforcement Learning” (Buck et al., ICLR 2018) is one of the related works that the paper didn’t cite.\n- The paper does not have any example of reformulated queries or error analysis.\n\nClarity\n\npros\n1) The paper describes the framework and model architecture carefully.\n\ncons\n1) It is hard to understand how exactly they reformulate the query based on selected terms. (I think examples would help) For example, in Fig 2, after “activities”, “used”, “conserve” and “water” were selected, how does rewriter write the query? The examples will help.\n2) Similar to the above, it would be helpful to see the examples of decision rules in Section 5.2.\n3) It is hard to understand how exactly each component of the model was trained. First of all, is rewrite module only trained on Essential Terms dataset (as mentioned in Section 3.1.3) and never fine-tuned on ARC dataset? Same question for entailment modules: is it only trained on SciTail, not fine-tuned on ARC dataset? How did decision rules trained? Are all the modules trained separately, and haven’t been trained jointly? What modules were trained on ARC dataset? All of these are a bit confusing since there’re many components and many datasets were used.\n\nOriginality & significance\n\npros\n* Query reformulation methods have been used on several QA tasks (like Buck et al 2018 above), and incorporating background knowledge has been used before too (as described in the paper), but I think it’s fairly original to do both in the same time.\n\ncons\n* It is a bit disappointing that the only part using background knowledge is selecting essential terms using ConceptNet embedding. I think the term “using background knowledge” is too general term for this specific idea.\n\nIn general, I think the paper has enough contribution to be accepted, if some descriptions are better clarified.', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		HJxYZ-5paX	S1gYa35MzN	AKBC.ws/2019/Conference/-/Paper29/Official_Review	['AKBC.ws/2019/Conference/Paper29/Reviewers/Unsubmitted']	3		['everyone']	HJxYZ-5paX	['AKBC.ws/2019/Conference/Paper29/AnonReviewer2']	1546984641485		1547662974880	['AKBC.ws/2019/Conference']
781	1547018267379	"{'title': 'Elegant Method, Neat Result, Manuscript Needs Major Reorganization ', 'review': 'This paper presents a simple model for representing lexical relations as vectors given pre-trained word embeddings.\n\nAlthough the paper only evaluates on out-of-context lexical benchmarks, as do several other papers in lexical-semantics, the empirical results are very encouraging. The proposed method achieves a substantial gain over existing supervised and unsupervised methods of the same family, i.e. that rely only on pre-trained word embeddings as inputs.\n\nIn my view, the main innovation behind this method is in the novel loss function. Rather than using just a single word pair and training it to predict the relation label, the authors propose using *pairs of word pairs* as the instance, and predicting whether both pairs are of the same relation or not. This creates a quadratic amount of examples, and also decouples the model from any schema of pre-defined relations; the model is basically forced to learn a general notion of similarity between relation vectors. I think it is a shame that the loss function is described as a bit of an afterthought in Section 4.4. I urge the authors to lead with a clear and well-motivated description of the loss function in Section 3, and highlight it as the main modeling contribution.\n\nI think it would greatly strengthen the paper to go beyond the lexical benchmarks and show whether the learned relation vectors can help in downstream tasks, such as QA/NLI, as done in pair2vec (https://arxiv.org/abs/1810.08854). This comment is true for every paper in lexical semantics, not only this one in particular.\n\nIt would also be nice to have an empirical comparison to pattern-based methods, e.g. Vered Shwartz\'s line of work, the recent papers by Washio and Kato from NAACL 2018 and EMNLP 2018, or the recently-proposed pair2vec by Joshi et al (although this last one was probably published at the same time that this paper was submitted). The proposed method doesn\'t need to be necessarily better than pattern-based methods, as long as the fundamental differences between the methods are clearly explained. I think it would still be a really exciting result to show that you can get close to pattern-based performance without pattern information.\n\nMy main concern with the current form of the paper is that it is written in an extremely convoluted and verbose manner, whereas the underlying idea is actually really simple and elegant. For example, in Section 3, there\'s really no reason to use so many words to describe something as standard as an MLP. I think that if the authors try to rewrite the paper with the equivalent space as an ACL short (around 6-7 pages in AKBC format), it would make the paper much more readable and to-the-point. As mentioned earlier, I strongly advise placing more emphasis on the new loss function and presenting it as the core contribution.\n\nMinor comment: Section 2.2 describes in great detail the limitation of unsupervised approaches for analogies. While this explanation is good, it does not properly credit ""Linguistic Regularities in Sparse and Explicit Word Representations"" (Levy and Goldberg, 2014) for identifying the connection between vector differences and similarity differences. For example, the term 3CosAdd was actually coined in that paper, and not in the original (Mikolov, Yih, and Zweig; 2013) paper, in order to explain the connection between adding/subtracting vectors and adding/subtracting cosine similarities. The interpretation of PairDiff as a function of word similarities (as presented in 2.2) is very natural given Levy and Goldberg\'s observation.\n', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		r1e3WW5aTX	rklXmgmXMV	AKBC.ws/2019/Conference/-/Paper30/Official_Review	['AKBC.ws/2019/Conference/Paper30/Reviewers/Unsubmitted']	2		['everyone']	r1e3WW5aTX	['AKBC.ws/2019/Conference/Paper30/AnonReviewer1']	1547018267379		1547662974440	['AKBC.ws/2019/Conference']
782	1547098684262	{'title': 'This paper contains a substantial amount of engineering work; the descriptions of the task and the method are unclear; and novelty of the research is limited.', 'review': 'This paper applies knowledge graph features to predict compound repositioning. The task is not very well defined, e.g., the inputs and outputs. Many terminologies are used without introduced or described, e.g. MeSH, SNOMED, compound repurposing, z-sore.  \n\nThe method is not novel. An existing knowledge graph MEDLINE is used for feature extraction, and an existing algorithm is used for relation prediction. In general, the method description is very vague and hard to understand.\n\nThe idea of evaluating relation prediction performance along the time dimension is interesting. However, the paper does not go very deep on this issue. For example, it does not discuss how to deal with emerging nodes and edges in the knowledge networks; how different time intervals affect the prediction performance.', 'rating': '5: Marginally below acceptance threshold', 'confidence': '2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper'}		BkeCW-q6aQ	rJlNS5UVG4	AKBC.ws/2019/Conference/-/Paper31/Official_Review	['AKBC.ws/2019/Conference/Paper31/Reviewers/Unsubmitted']	2		['everyone']	BkeCW-q6aQ	['AKBC.ws/2019/Conference/Paper31/AnonReviewer2']	1547098684262		1547662974180	['AKBC.ws/2019/Conference']
783	1547069069307	{'title': 'Interesting task but unclear writing/analysis', 'review': 'This paper tackles the problem of compound repositioning. They applied an existing method to a text-mined/noisy dataset, i.e SemMedDB. In addition to this, they also proposed a new learning/evaluated framework which is the time-resolved learning task. The framework is well motivated and more close to real life usage. Additional analysis is done for the time-resolved learning methods. But more carefully explanation needs to be provided. I have several questions.\n\n1) In the Repurposing Algorithm of Method section, the authors mentioned ElasticNet regression is used for the final classification task. But the original Repehtio project used logistic regression as the classifier. Is there a reason why changing of classifier here? Is logistic regression not working as good? If so, any reason for that?\n\n2) In the last paragraph of Building time-resolved networks in the Results section, a ratio of around 80% current and 20% split is done for each network year. But in the appendix graph, the train/test number is very different such as 1950. Is there a random selection involving when choosing the train/test data? If there is, it is a useful piece of information need to be added to the paper.\n\n3) Figure3 is a very interesting graph, especially graph C. But which year is chosen is not clear. If there is no specific year is chosen for this analysis, how the average is performed? Does the -20 in x-axis include fewer data points than the 0 along x-axis? If I understand correctly, the left part of the Zero Point is the part of the training data. Then why the average precision is still below 0.5? The drop from -5 to 0 is especially interesting. Is there any insight the author can add to the paper?\n\nOverall, this is an interesting paper trying to tackle a time-involved problem, careful analysis is done but more details need to be provided.', 'rating': '6: Marginally above acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		BkeCW-q6aQ	Bylr58yVzE	AKBC.ws/2019/Conference/-/Paper31/Official_Review	['AKBC.ws/2019/Conference/Paper31/Reviewers/Unsubmitted']	1		['everyone']	BkeCW-q6aQ	['AKBC.ws/2019/Conference/Paper31/AnonReviewer1']	1547069069307		1547662973959	['AKBC.ws/2019/Conference']
784	1545092210143	"{'title': 'Interesting analysis, but may not be enough content', 'review': 'The paper describes the creation of OPIEC -- an Open IE corpus over English Wikipedia.\nThe corpus is created in a completely automatic manner, by running an off-the-shelf OIE system (MinIE), which yields 341M SVO tuples. Following, this resource is automatically filtered to identify triples over named entities (using an automatic NER system, yielding a corpus of 104M tuples), and only entities which match entries in Wikipedia (5.8M tuples).\n\nOn the positive side, I think that resources for Open IE are useful, and can help spur more research and analyses.\n\nOn the other hand, however, I worry that OPIEC may be too skewed towards the predictions of a specific OIE system, and that the work presented here consists mainly of running off-the-shelf can be extended to contain more novel substance, such as a new Open IE system and its evaluation against this corpus, or more dedicated manual analysis. For example, I believe that most similar resources (e.g., ReVerb tuples) were created as a part of a larger research effort.\n\nThe crux of the matter here I think, is the accuracy of the dataset, reported tersely in Section 5.3, in which a manual analysis (who annotated? what were their guidelines? what was their agreement?) finds that the dataset is estimated to have 60% correct tuples. Can this be improved? Somehow automatically verified?\n\nDetailed comments:\n\n- I think that the paper should make it clear in the title or at least in the abstract that the corpus is created automatically by running an OIE system on a large scale. From current title and abstract I was wrongfully expecting a gold human-annotated dataset.\n\n- Following on previous points, I think the paper misses a discussion on gold vs. predicted datasets for OIE, and their different uses. Some missing gold OIE references:\nWu and Weld (2010),  Akbik and Loser (2012), Stanovsky and Dagan (2016).\n\n- Following this line, I don\'t think I agree with the claim in Section 4.3 that ""it is the largest corpus with golden annotations to date"". As far as I understand, the presented corpus is created in a completely automated manner and bound to contain prediction errors.\n\n- I think that some of the implementation decisions seem sometimes a little arbitrary. For instance, for the post-processing example which modifies \n(Peter Brooke; was a member of; Parliament) to (Peter Brooke; was ; a member of Parliament), I think I would\'ve preferred the original relation, imagining a scenario where you look for all members of parliament (X; was a member of; Parliament), or all of the things Peter Brooke was a member of (Peter Brooke; was a member of; Y) seems more convenient to me.\n\nMinor comments & typos:\n\n- I assume that in Table 1, unique relations and arguments are also in millions? I think this could be clearer, if that\'s the indeed the case.\n\n- I think it\'d be nice to add dataset sizes to each of the OPIEC variants in Fig 1.\n\n- End of Section 3.1 ""To avoid loosing this relationship"" -> ""losing this relationship""\n\n- Top of P. 6: ""what follows, we [describe] briefly discuss these""\n\n- Section 4.5 (bottom of p. 9) ""NET type"" -> ""NER type""?', 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		HJxeGb5pTm	rylqOh2HgE	AKBC.ws/2019/Conference/-/Paper32/Official_Review	['AKBC.ws/2019/Conference/Paper32/Reviewers/Unsubmitted']	1		['everyone']	HJxeGb5pTm	['AKBC.ws/2019/Conference/Paper32/AnonReviewer1']	1545092210143		1547662973734	['AKBC.ws/2019/Conference']
785	1545406136128	{'title': 'Good paper on producing a triple store from Wikipedia articles.', 'review': 'This paper presents a dataset of open-IE triples that were collected from Wikipedia with the help of a recent extraction system. \n\nThis venue seems like an ideal fit for this paper and I think it would make a good addition to the conference program. While there is little technical originality, the overall execution of the experimental part is quite good and I like that the authors focused in their report on describing how they filtered the output of the employed IE system and that they present interesting examples from the conducted filtering steps.\n\nI particularly liked the section on comparing the new resource to the existing knowledge bases from the same source (YAGO, DBpedia), I think it makes a lot of sense to pick resources that leverage other parts of Wikipedia (category system, ...) and not the main article text, and to look into how coverage of the these different approaches relates.\n\nIt would have been nice to also compare against other datasets/triple stores/... that used open-IE to extract from Wikipedia. A couple of references discussing such are listed in the paper, e.g., DefIE or KB-Unify seem like good candidates.\n', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		HJxeGb5pTm	Hkxg6IKqgV	AKBC.ws/2019/Conference/-/Paper32/Official_Review	['AKBC.ws/2019/Conference/Paper32/Reviewers/Unsubmitted']	2		['everyone']	HJxeGb5pTm	['AKBC.ws/2019/Conference/Paper32/AnonReviewer3']	1545406136128		1547662973468	['AKBC.ws/2019/Conference']
786	1547058996860	{'title': 'OPIEC: An Open Information Extraction Corpus', 'review': 'In this paper, the authors build a new corpus for information extraction which is larger comparing to the prior public corpora and contains information not existing in current corpora. The dataset can be useful in other applications. The paper is well written and easy to follow. It also provides details about the corpus. However, there are some questions for the authors: 1) It uses the NLP pipeline and the MinIE-SpaTe system. When you get the results, do you evaluate to what extent that the results are correct? 2) In Section 3.4, the author mentioned the correctness is around 65%, what do you do for those incorrect tuples? 3) Have you tried any task-based evaluation on your dataset?', 'rating': '7: Good paper, accept', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		HJxeGb5pTm	B1eTN16XfN	AKBC.ws/2019/Conference/-/Paper32/Official_Review	['AKBC.ws/2019/Conference/Paper32/Reviewers/Unsubmitted']	3		['everyone']	HJxeGb5pTm	['AKBC.ws/2019/Conference/Paper32/AnonReviewer2']	1547058996860		1547662973251	['AKBC.ws/2019/Conference']
787	1545249747581	"{'title': 'A nice model that extends SeqIE to incorporate long-distance relations', 'review': 'This paper proposes an IE model that leverages the additional relational connections on the input text (e.g., co-ref chains, sentence dependencies, etc.) to improve an LSTM-CRF IE model.  In particular, the long-distance connections are represented as a graph, where the additional embeddings (i.e., features) are created using graph convolution.  This additional set of features are then combined with the regular embeddings used in the sequence prediction model.  Experiments on three tasks, show small, but consistent gains over the original sequential IE model.\n\nThe paper is clearly written and is easy to follow.  The basic idea is simple but sensible.  As the authors accurately discussed in the related work, while there are other existing models that take advantage of the graph structure in the text, the proposed solution is different.  The GraphIE model proposed in this paper can be viewed as a natural generalization of the SeqIE model, by augmenting the original features with the long-distance information propagated through the graph structure.\n\nOn the other hand, while the paper demonstrates the superiority of GraphIE over SeqIE, it nevertheless lacks some analysis to present solid evidence on whether the particular graph structures are truly the main source of improvement.  Specifically, since GraphIE generalizes SeqIE, it is not surprising that the performance is better.  However, because of differences are not huge, there could be possibly different explanations on why the proposed method ""works"" and whether some model design choices are actually important or needed.  Some examples of analysis include:\n\n\t• Trivial feature augmentation\n\t\n\tBecause in the experiments, GCN only uses 1 or 2 layers, which means that only information from the linked node, or its immediate neighbors are passed. This suggests a simple baseline, which is just to use the original embeddings of these nodes (without the GCN layers) in the SeqIE model.  The results can shed some light on whether a formal graph convolution and the additional parameters are required, or it\'s more important to pass some information from the legitimate long-distance chains.\n\t\n\t• Random connections\n\n\tA common question on the performance gain from a more complex neural model is whether it\'s primely due to a bigger model.  One baseline to test this hypothesis is to use a somewhat random graph structure.  For instance, instead of following the obvious connections, try to construct the graph with roughly the same density, but randomly.  While personally I don\'t expect it to work, it will be nevertheless interesting to see the results.\n\nOther comments:\n\n\t1. Some of the model decision choices are not justified\n\t\t• At the end of Sec. 4.1, the sentence embedding is created using the average hidden vectors.  Why didn\'t you use the concatenation of the vectors from both ends of BiLSTM as commonly done?\n\t\t• Sec. 4.3, why did you split the GCN vectors instead of providing it directly as input for both forward and backword LSTMs?\n\n\t2. Sec. 6.3 Generalization, I feel this part is not described clearly.  What do you mean by template exactly?\n\n\t3. ""i.e."" -> ""i.e.,""\n', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		Sye7fZcTTm	rkg3R7XOe4	AKBC.ws/2019/Conference/-/Paper33/Official_Review	['AKBC.ws/2019/Conference/Paper33/Reviewers/Unsubmitted']	1		['everyone']	Sye7fZcTTm	['AKBC.ws/2019/Conference/Paper33/AnonReviewer2']	1545249747581		1547662973028	['AKBC.ws/2019/Conference']
788	1546952333519	{'title': 'An Interesting Application of graph NNs to IE', 'review': 'This is an interesting and well-written paper applying graph neural networks to a range of (mostly non-standard) information extraction problems. The word-level model used in this work is very similar to GCNs of Marcheggiani and Titov (EMNLP 2017), where it was applied to semantic role labelling task, a problem closely related to information extraction. Though there have been a number of papers applying GCNs and other forms of graph neural networks to NLP (not so much to information extraction though), I find a number of aspects of this paper interesting and novel:\n— the use of combination of world level and sentence level graphs (i.e. connected sentences rather than only words, as in much of the previous work)\n- the use of non-linguistic graphs (specifically graphs encoding social networks and visual layout).\n— GCNs are used not on top of RNNs, as in much previous work, but rather plugged in between layers.\n\nSome questions:\n\n(1) What kind of non-linearity is used? (i.e. sigma in (4))\n(2) Some previous work reported that gates or attention have been very useful in graph neural networks (incl in NLP applications), have the authors experimented with using them?\n(3) Have the authors experimented with self-attention as an alternative to using predefined graphs? (e.g., seems natural for visual graphs)\n\n\nI think the related work section needs to be expanded. I realize that the authors chose to cite only work on relation extraction, but since the paper proposes general techniques for integrating graph neural networks in encoders, they need to go further than that:\n\n\n> “all these studies achieved improvements only when using dependency trees\n\nI believe that Peng et al. (TACL 2017) used co-reference information (though maybe have not observed much of improvement from using them). Moreover, in NLP there have been many applications of GCNs beyond using syntactic dependency trees: e.g., combinations of knowledge graphs and text were used in (CMU), Radev used sentence level graphs to encode discourse relations between sentences, Marcheggiani et al (2018) used combinations of semantic and syntactic graphs in NMT, etc.\n\nIn fact, the model used for Task 2 is quite similar to that used for multi-document / multi-hop QA by De Cao et al (2018) and … Both use graphs encoding co-reference information.\n\n\n\n', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		Sye7fZcTTm	HJlIqRffz4	AKBC.ws/2019/Conference/-/Paper33/Official_Review	['AKBC.ws/2019/Conference/Paper33/Reviewers/Unsubmitted']	2		['everyone']	Sye7fZcTTm	['AKBC.ws/2019/Conference/Paper33/AnonReviewer1']	1546952333519		1547662972777	['AKBC.ws/2019/Conference']
789	1547017022296	"{'title': 'An interesting paper integrating non-local contextual information for IE via graph neural networks', 'review': 'This paper proposes a framework, GraphIE, to integrate graph-structured non-local contextual information into sentence-level information extraction. The main idea is to inject a graph module, which is essentially a graph convolutional network (GCN), between a BiLSTM encoder and a BiLSTM-CRF decoder. Experiments on three diverse datasets demonstrate that the proposed framework, integrating non-local contextual information, consistently outperforms the local baseline. \n\nPros:\n\n1. The proposed method, though not very novel, is well thought-out and intuitively plausible\n2. The writing is good and easy to follow\n3. Comprehensive experiments on diverse applications with good results.\n\nCons:\n\n1. The motivation (in particular the motivating example in Figure 1) is somewhat weak.\n2. Related to 1, I\'m concerned that the applicability of the proposed framework may not be as good as claimed\n\nOverall I found this an interesting and well-written paper that makes a good amount of contribution in methodology and experimentation. It fits nicely in the recent trend of exploring non-local context in information extraction. My major concern is that I\'m not entirely convinced by the motivation, in particular the motivating example in Figure 1. A question that persists in mind when reading through this paper is ""what non-local context information can be used and how to get that"". It\'s true that in principle many kinds of non-local information can be helpful for IE, but it could often be the case that acquiring such information is an even harder task than the target IE task itself. For example, cross-sentence co-reference resolution is itself a very challenging task. This would probably limit the application of the proposed framework, and it\'s worth more in-depth discussion.\n\nFootnote 10: Inaccurate claim. Isn\'t GloVe used in your method also pre-trained on large external corpora? It\'s just not that contextual.\n\nFootnote 5: It\'s confusing why sampling is not necessary for feature engineering approaches but is necessary for neural models.', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		Sye7fZcTTm	ryeLSiz7fN	AKBC.ws/2019/Conference/-/Paper33/Official_Review	['AKBC.ws/2019/Conference/Paper33/Reviewers/Unsubmitted']	3		['everyone']	Sye7fZcTTm	['AKBC.ws/2019/Conference/Paper33/AnonReviewer3']	1547017022296		1547662972544	['AKBC.ws/2019/Conference']
790	1547131097759	"{'title': 'A nice attempt, but problematic presentation', 'review': 'The manuscript presents LEAPFROG, a probabilistic framework for automated KB construction. Its distinguishing feature is to maintain alternative interpretations and certainty of information. For LEAPFROG, a KG is represented as a belief graph (BG).\n\nI liked the motivation and general idea of the framework. However, the manuscript has a serious problem in terms of presentation. Despite of my effort, it was very hard to understand the mechanism of LEAPFROG in detail, which is mostly written in Section 2 and 3.  Sections 2, 3, and 4 need to be rewritten substantially. Otherwise, I am wondering how many audience can fully understand the presented framework. In many cases, explanations are  only partial, relying on a small number of fragmentary examples.\n\nOverall, I think the manuscript is a case with good idea but missing important details.\n\nMy detailed comments and questions below are mostly in that regard.\n\n- In Figure 1, the alternatives are really alternative interpretations or simply alternative mentions? ""Military jet"" and ""SU25"" sound like referring to the same entity.\n- In the beginning of section 2, it says ""the belief propagation engine performs inference ..."". What kind of inference do you mean?\n- Section 2.1 describes of only the case of SM-KBP. It seems LEAPFROG needs an ontology as its initial input, which is not explained in any of the manuscript and in Figure 2.\n- Section 2.2 explains about evidence graph construction. However, the explanation is too much partial. Also, this part is not evaluated at all, as the evaluation relied on annotations provided by LDC. I understand that this part is not the main focus of the manuscript. However, it is a problem that a thorough explanation is missing and also evaluation is also totally missing.\n- Section 2.3, what do you mean with this comment: we ""leverage"" relation mentions. I could not understand the mechanism (even after reading section 3.3).\n- Section 2.3 explains how an evidence is ground to existing KEs. However, it does not explain when and how a new KE is created.\n- I could not understand the ""parent-child analogy"".\n- Section 2.3 is the most important part to understand the mechanism of LEAPFROG, but it is extremely hard to follow. The explanations are highly fragmentary, and are not well connected with each other.\n- How the posterior of Obs(O2) is computed?\n- In the last sentence of Section 2.3, it says ""Factor f1 captures the relation between the four parts of the observation"". Do you mean three parts?\n- In section 3.2, how certainty is represented at an event level? Is it explained somewhere in the manuscript?\n- After reading section 4, honestly, I do not feel the proposed framework is really evaluated. Please explain in more explicit way: which part of the framework is evaluated by which part of the experiment?\n- The discussion in section 4.4 is not very interesting. I do not think the case shows the benefit of using the proposed framework very well.\n\nOverall I think it is a good attempt, but it is not very well presented in the manuscript.', 'rating': '5: Marginally below acceptance threshold', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		SJeHzZqapm	HJxG1FR4zN	AKBC.ws/2019/Conference/-/Paper34/Official_Review	['AKBC.ws/2019/Conference/Paper34/Reviewers/Unsubmitted']	3		['everyone']	SJeHzZqapm	['AKBC.ws/2019/Conference/Paper34/AnonReviewer3']	1547131097759		1547662972317	['AKBC.ws/2019/Conference']
791	1547073550856	"{'title': 'This paper proposes a belief propagation approach for knowledge graph construction. Specifically, the paper models event/entity extraction via a factor graph and applied belief propagation for inference. The approach is able to model uncertainty in grounding by considering distributions as variables. Most interestingly, the approach can bias extraction results by assigning higher confidence to some documents than others.', 'review': 'The motivation for the paper is well articulated: in scenarios where we extract knowledge from different documents with multiple possibly conflicting interpretations, we would like to propagate the uncertainty to the end-user applications instead of  performing maximum a posteriori inference selecting only one interpretation. \n\nThe problem definition is important and I believe I have not seen much work along these lines, so the problem statement is novel IMHO.\n\nThe ""trick"" applied that took me some while to understand is that variables in the factor graph are actually distributions rather than variables ranging over sets of entities. The authors could have made this much clearer in my opinion. I think this very basic idea is hidden behind the ""en passant"" note on page 4 that they use open sets as values for variables. So variables encode distributions and thus uncertainty. This is quite an important point that I practically missed in the first reading of the paper. \n\nIf this understanding is correct, then I find Figure 4 misleading. In particular, I do not understand why observation 1 is connected via the factor to three variables where each of these variables corresponds to a distribution. This is really unclear to me. I would have expected that the observation is connected to one variables that encodes the distribution over entities. But the figure seems to suggest otherwise. Figure 4 seems to suggest that P1 stands for Truck 1, P2 for truck 2 and T for a maximal uncertainty in which truck if could be. T seems to represent thus the uniform distribution, the others, P1 and P2 represent specific choices of truck 1 and truck 2. \n\nIf variables represent distributions, how many variables are there actually for each observation? Is the number of variable bound.\n\nThe approach works as follows: it uses preprocessing to determine the relevant candidates in a sentence and then instantiates a factor graph with corresponding variables where each of the variables stands for a distribution over entities of the corresponding type. This process is what the authors call ""grounding"". Actually, it is inference, unless with grounding they mean actually ""rolling"" out the factor graph. The terminology is quite unclear and misleading I find here. \n\nI understand that inference is performed using belief propagation. First of all, the authors mention that they graphs are loop-free. However, I do not see really why this would be the case. This would served some further explanation. Further, I do not see how exact inference can be tractable as there is an infinite number of possible distributions. There is some discretization involved, but this is not mentioned. \n\nGiven the doubts I have about how the model works, it was difficult for me to understand how the source reliability and mention uncertainty are really modeled, so I have to pass here and hope that other reviewers got this right. \n\nThe evaluation shows that the approach delivers good results, but does not compare the results to other state-of-the-art approaches. I was puzzled by the fact that the authors only provide Precision and Recall figures, but no F-Measures. Why is this the case? The results simulating the biases in Table 4 are interesting, but I wondered why the results do not come even close to the results in Table 2. \n\nIn general, the approach is interesting and promising, but large parts of the paper are imprecise, do not introduce properly the relevant notions, use non-standard terminology and obfuscate/omit important details. A factor graph is not described properly. The type of factors is not defined. One can assume that standard log-linear factors are used as in most works. But it is not made clear.\n\nOne example, on page 6 the authors say ""LEAPFROG has several common observation models with pre-built factors and local functions"". This means that LEAPFROG uses standard models that are independent of the actual model. Fair enough. But the models need to be described properly. \n\nWhat I found inappropriate are the unwarranted and unjustified claims about how people infer interpretations, e.g.  ""People often tend to stick to their existing belief regarding an interpretation unless a compelling evidence causes them to change it"". There are other unjustified claims along these lines in the people about human inference and interpretation abilities.\n\nOverall, the paper needs to be substantially improved along the lines mentioned above before it can be accepted at any conference.', 'rating': '5: Marginally below acceptance threshold', 'confidence': '2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper'}"		SJeHzZqapm	BkeDfugVMN	AKBC.ws/2019/Conference/-/Paper34/Official_Review	['AKBC.ws/2019/Conference/Paper34/Reviewers/Unsubmitted']	2		['everyone']	SJeHzZqapm	['AKBC.ws/2019/Conference/Paper34/AnonReviewer2']	1547073550856		1547662972058	['AKBC.ws/2019/Conference']
792	1546255990501	{'title': 'Interesting Paper for AKBC', 'review': 'Summary:\nThis work proposes to automatically construct knowledge base that maintains alternative probabilistic interpretation for KG elements via a probabilistic framework. This framework also allows for the injection of prior knowledge and generates corresponding interpretation. The experimental results and case study reveal the effectiveness of the proposed method.\n\nStrong points:\n1. This work is well motivated and structured.\n2. The probabilistic framework of constructing KG from scratch is novel as it allows for different interpretations and alters according to human prior belief.\n\nWeak points:\n1. Technique-wise, this work is a bit hard to follow. For instance, the basic definitions, e.g., the definitions of “mentions”, “events”, “attributes” and “relations”, are not clearly given. As a result, it can become confusing when the construction details are elaborated. More confusions can be found in the detailed points.\n2. This work lacks the references to some works that also centers on constructing KG from a probabilistic perspective. (1) \tJay Pujara, Probabilistic Models for Scalable Knowledge Graph Construction. University of Maryland, College Park, MD, USA 2016. (2) Knowledge Graph Identification. Jay Pujara, Hui Miao, Lise Getoor, William Cohen. 2013 International Semantic Web Conference (ISWC). What is the difference between this work and Pujara’s works?\n3. The experiment is comparatively weak since there is no baseline or competitors. Plus, the scale of the dataset is relatively small as far as a normal KG is concerned. In consequence, it is hard to judge whether this method can achieve superior results on this problem.\n\nDetailed Points:\n1. What is the exact difference between “relation” and “event” in this work? It seems that much more emphasis is laid on “events”, while “relations” are barely mentioned.\n2. In the subsection “Event and Relation Grounding”, it is said that “Across multiple documents, such a heuristic adds irrelevant candidates so we add a more restrictive constraint by having candidates match at least the patient argument”. Why is this approach effective and why should “patient argument”, instead of “instrument argument” be required?\n3. In the subsection “Event and Relation Grounding”, it is said that “In our domain, relations usually encode affiliations and location hierarchy, so the arguments can be considered to have a parent-child analogy”. What about broader scenarios?\n4. In the “Factors” subsection of Section 2.3, the given example seems not to be matched with Fig.4. In the description, the “variables” connected by the factor are obs.name, obs.affiliation and obs.target, while they become T, P1, P2 in the Figure. In addition, what does the right part of Fig.4 try to express?\n5. Some use of terms are too casual. For instance, in the second line of page 10, we “don’t” might as well be changed to “we do not”.\n6. Are there any measures to control the error propagations? For instance, the result of “Entity Grounding” can influence the “Event and Relation Grounding”.', 'rating': '6: Marginally above acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		SJeHzZqapm	rye0dAOPZN	AKBC.ws/2019/Conference/-/Paper34/Official_Review	['AKBC.ws/2019/Conference/Paper34/Reviewers/Unsubmitted']	1		['everyone']	SJeHzZqapm	['AKBC.ws/2019/Conference/Paper34/AnonReviewer1']	1546255990501		1547662971798	['AKBC.ws/2019/Conference']
793	1547062511033	"{'title': 'Winograd Schema Challenge: Is it the Reporting Bias that Makes it Difficult or the Demanding Nature of the Reasoning Systems?', 'review': 'This paper targets at Winograd schema challenge which requires world knowledge to make the correct prediction. For machine learning models, researchers have tried to incorporate knowledge base to help solve this problem. In this paper, the authors extract a knowledge sentence for search engine to help solve the challenge. The motivation is clear and interesting and the paper is well written. But the paper still lacks some explanations: \n1) From the title, it seems the authors want to compare the reporting bias with the reasoning system to see which one could help this challenge. But there is on such comparison. \n2) Do you have any constraint about the knowledge sentence extraction? In Section 4, step 3 says you will manually classify the sentence to be useful or not. What is the justification criteria? \n3) On page 5 the last paragraph, you said if (p1,p2) belongs to the list, they refer to the same entity. Is this true? For example, ""I saw the doctor with his son yesterday. He is 5 years old now."" In this case, ""he"" and ""his"" do not refer to the same person. \n4) For the alignment algorithm, the case TTT or TTF, is there any theory to guarantee that the claim is correct?\n5) When you form the two sentences to the textual entailment problem, how do you get the ground truth label?\n', 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		H1xdMZ566m	SJxweT6XfN	AKBC.ws/2019/Conference/-/Paper35/Official_Review	['AKBC.ws/2019/Conference/Paper35/Reviewers/Unsubmitted']	1		['everyone']	H1xdMZ566m	['AKBC.ws/2019/Conference/Paper35/AnonReviewer1']	1547062511033		1547662971541	['AKBC.ws/2019/Conference']
794	1546804493549	{'title': 'review', 'review': 'In this paper, the authors propose methods for automatically generating animations from the text of a screenplay. The authors create a pipeline to extract relevant sections of text which is then simplified into simple actions. These actions are then given to an existing animation engine making calls to one of 52 prebuilt animations.  They also generated a new corpus to train and evaluate their models. The authors evaluated their rule based sentence simplification module and additionally had users rate how reasonable the generated animations were given the text - though no alternative methods were investigated. \n\nThe task itself is an interesting one but the methods themselves are less so. I also do not find the evaluations particularly compelling. My biggest concerns overall is that this paper does not seem like a good fit for AKBC and references to knowledge bases seem like a post-hoc addition. If the authors or other reviewers can convince me otherwise I’d be happy to reevaluate my review, otherwise I can not currently recommend this paper for this particular venue. \n', 'rating': '3: Clear rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		Hkg5zW96p7	SylUGTRkG4	AKBC.ws/2019/Conference/-/Paper36/Official_Review	['AKBC.ws/2019/Conference/Paper36/Reviewers/Unsubmitted']	2		['everyone']	Hkg5zW96p7	['AKBC.ws/2019/Conference/Paper36/AnonReviewer2']	1546804493549		1547662971101	['AKBC.ws/2019/Conference']
795	1547006879105	"{'title': 'Text-to-animation is an interesting topic to explore, but the paper needs a bit more work to demonstrate its relevance and contributions.', 'review': ""The authors consider the task to generate animation from text description. Overall, this is a novel topic that seems quite interesting, which should be encouraged. Regarding the proposed work in particular, there are a few aspects that could be improved.\n\nFirst, the paper can benefit from highlighting the relevance to the AKBC community. E.g., the authors seem to treat the problem as text simplification, and evaluate it using BLEU. Can't we view the end-to-end problem as information extraction instead (text -> sequence of animation events)? It seems that the animation rendering engine accepts formal spec rather than NL description, as the ARF evaluation suggests. Would be great if the authors can clarify why IE is not a good end-to-end formulation.\n\nThe paper can benefit by highlighting the research contributions w.r.t. AKBC. As it stands, the proposed system simply applied a few available NLP tools in sequence. So methodologically, it's unclear how significant the contribution is. The authors might want to discuss how the problem formulation and the curated dataset could be of interest to AKBC, the statistics of the dataset, etc. Empirically, the paper also lacks experimental comparison and example analysis. \n"", 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		Hkg5zW96p7	SJxDo7emfV	AKBC.ws/2019/Conference/-/Paper36/Official_Review	['AKBC.ws/2019/Conference/Paper36/Reviewers/Unsubmitted']	3		['everyone']	Hkg5zW96p7	['AKBC.ws/2019/Conference/Paper36/AnonReviewer1']	1547006879105		1547662970887	['AKBC.ws/2019/Conference']
796	1546944376424	"{'title': 'Review', 'review': 'This paper presents a method for extracting relations from chemical texts by means of a shallow parsing approach. The task described in the paper is interesting and the goal of understanding scientific literature would have quite some impact. However, the method presented here seems to be mostly a fairly straightforward shallow parsing technique based on a fairly standard deep neural network. While the method does improve over the baseline, the baseline is rather trivial so the improvement is not surprising. Given that there is already much work on semantic parsing, I wonder if another approach might have performed better even given the ""resource-poor"" setting in this paper. \n\nThe paper also describes a dataset, that given the value of this task may be a strong contribution and the authors report an encouraging IAA score. However, this description is quite short and it is unclear how much is manually annotated and how much is automatically annotated.', 'rating': '6: Marginally above acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		S1eg7-9pp7	SyxeFy-fG4	AKBC.ws/2019/Conference/-/Paper37/Official_Review	['AKBC.ws/2019/Conference/Paper37/Reviewers/Unsubmitted']	2		['everyone']	S1eg7-9pp7	['AKBC.ws/2019/Conference/Paper37/AnonReviewer3']	1546944376424		1547662970444	['AKBC.ws/2019/Conference']
797	1547006627420	"{'title': 'Interesting application domain but some concerns', 'review': 'This paper introduces an approach for extracting synthesis procedures from scientific literature. The main focus of the work is on unlabeled edge place, for which the authors propose to use neural networks to model operation-argument affinity and a pairwise ranking function as the objective function for model training. \n\nThe targeted domain, i.e., material science, looks new and interesting; and the problem of extracting structured knowledge is a relevant and complex problem. \n\nThe methodological contribution is, however, a bit fragmented and unbalanced: the annotated dataset, the entity extraction, and the unlabeled edge placement all seem to be relevant from the perspective of the application domain, yet the paper discusses entity extraction very briefly and mainly concentrates on unlabeled edge placement. Similarly, the authors only report a few numbers as the experimental result for entity extraction without any analysis or discussion on the challenges, insights, etc. \n\nThen, for the unlabeled edge placement, while the model looks suitable for the problem, the contribution is unclear. Is it the first neural model for such a problem? If so, how does the performance compare with non-neural methods? \n\nThe experiment section needs clarification and discussions. How are the hyperparameters of the comparative methods determined? Is the performance of the different variants of the neural model significantly different? Why is the DeepSet-Type-VS model outperformed by DeepSet-VS and what does this indicate?\n\nMinor:\n- what does it mean by ""outperforms ... by 4 points precision and 2 points F1""\n- in Eq. 4, why W_l is a 3D tensor if s_i and e_i are vectors?', 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		S1eg7-9pp7	rygisfgmME	AKBC.ws/2019/Conference/-/Paper37/Official_Review	['AKBC.ws/2019/Conference/Paper37/Reviewers/Unsubmitted']	3		['everyone']	S1eg7-9pp7	['AKBC.ws/2019/Conference/Paper37/AnonReviewer1']	1547006627420		1547662970234	['AKBC.ws/2019/Conference']
798	1546125490325	{'title': 'The paper reports a summary of the SHINRA project for structuring Wikipedia collaborative construction scheme.', 'review': 'The paper described SHINRA2018 task that construct knowledge base rather than evaluating only limited test data. The paper repeat the task with larger and better training data from the output of the previous task. \n\nThe paper is well written in general, though there are some redundancies between abstract and introduction with exactly the same content. The SHINRA share task provide a good resource and platform for evaluating knowledge graph construction task on Japanese Wikipedia. \n\nOne of the concerns is that the paper did not really solve the first statement in abstract that it still evaluates on limited test data with 100 samples. The main contribution of the paper seems to be ensemble learning which has been proved efficient in many previous work.', 'rating': '6: Marginally above acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		HygfXWqTpm	r1gc2eYBWV	AKBC.ws/2019/Conference/-/Paper38/Official_Review	['AKBC.ws/2019/Conference/Paper38/Reviewers/Unsubmitted']	1		['everyone']	HygfXWqTpm	['AKBC.ws/2019/Conference/Paper38/AnonReviewer2']	1546125490325		1547662970015	['AKBC.ws/2019/Conference']
799	1547037459795	"{'title': 'A practical work but lacks methodological contribution', 'review': 'This paper introduces a project for structuring Wikipedia by aggregating the outputs from different systems through ensemble learning. It presents a case study of entity and attribute extraction from Japanese Wikipedia. \n\nMy major concern is the lack of methodological contribution. \n- Ensemble learning, which seems most like the methodological contribution, is applied in a straightforward way. The finding that ensemble learning gives better results than individual learners is trivial.\n- Authors state that a key feature of the project is using bootstrapping or active learning. This, however, is not explained in the paper nor supported by experimental results.\n\nClarification or details are needed for steps introduced by section 3-6:\n- In ""Extended Named Entity"", why would the top-down ontology ENE better than the inferred or crowd created ones? I think each of them has pros and cons.\n- In ""Categorization of Wikipedia Entities"", is training data created by multiple annotators? what is the agreement between the multiple annotators for the test (and the training) data? How much error of the machine learning model is caused by incorrect human annotations?\n- In ""Share-Task Definition"", ""We give out 600 training data for each category."" does it mean 600 entities?\n- In ""Building the Data"", what is the performance of experts and crowds in the different stages?\n\nWriting should be improved. Some examples:\n- what does it mean by ""15 F1 score improvement on a category"".\n- a lot of text in the abstract is repeated in the introduction.\n- ""For example, ”Shinjuku Station” is a kind of railway station is a type of ..."": not a sentence.\n- ""4 show the most frequent categories"": should be Table 1.\n- page 8, ""n ¿ t""L corrupted symbol.\n\nAs the last comment, I wonder how (much) this ensemble learning method can be better than crowd based KBC methods, as motivated by abstract and introduction. I would assume that machine learning has similar reliability issue as crowdsourcing even when ensemble learning is used. ', 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		HygfXWqTpm	S1xnGsw7fN	AKBC.ws/2019/Conference/-/Paper38/Official_Review	['AKBC.ws/2019/Conference/Paper38/Reviewers/Unsubmitted']	2		['everyone']	HygfXWqTpm	['AKBC.ws/2019/Conference/Paper38/AnonReviewer1']	1547037459795		1547662969799	['AKBC.ws/2019/Conference']
800	1546510087042	"{'title': 'very competent work on an important problem', 'review': ""The paper presents a method for named entity disambiguation\ntailored to the important case of medical entities,\nspecifically diseases with MeSH and OMIM\nas the canonicalized entity repository.\nThe method, coined NormCo, is based on a cleverly designed\nneural network with distant supervision from MeSH tags of\nPubMed abstracts and an additional heuristic for estimating\nco-occurrence frequencies for long-tail entities.\n\nThis is very competent work on an important and challenging\nproblem. The method is presented clearly, so it would be easy\nto reproduce its findings and adopt the method for further\nresearch in this area.\nOverall a very good paper.\n\nSome minor comments:\n\n1) The paper's statement that coherence models have\nbeen introduced only recently is exaggerated. \nFor general-purpose named entity disambiguation, coherence\nhas been prominently used already by the works of\nRatinov et al. (ACL 2011), Hoffart et al. (EMNLP 2011)\nand Ferragina et al. (CIKM 2010); and the classical\nworks of Cucerzan (EMNLP 2007) and Milne/Witten (CIKM 2008)\nimplicitly included considerations on coherence as well.\nThis does not reduce the merits of the current paper,\nbut should be properly stated when discussing prior works.\n\n2) The experiments report only micro-F1. Why is macro-F1 \n(averaged over all documents) not considered at all?\nWouldn't this better reflect particularly difficult cases\nwith texts that mention only long-tail entities,\nor with unusual combinations of entities?\n"", 'rating': '8: Top 50% of accepted papers, clear accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		BJerQWcp6Q	H1g1fkDsWE	AKBC.ws/2019/Conference/-/Paper39/Official_Review	['AKBC.ws/2019/Conference/Paper39/Reviewers/Unsubmitted']	1		['everyone']	BJerQWcp6Q	['AKBC.ws/2019/Conference/Paper39/AnonReviewer3']	1546510087042		1547662969586	['AKBC.ws/2019/Conference']
801	1546801930823	{'title': 'Simple, fast method with decent results on disease normalization (linking)', 'review': 'Summary:\nThe authors address the problem of disease normalization (i.e. linking). They propose a neural model with submodules for mention similarity and for entity coherence. They also propose methods for generating additional training data. Overall the paper is nicely written with nice results from simple, efficient methods.\n\nPros:\n- Paper is nicely written with good coverage of related work\n- LCA analysis is a useful metric for severity of errors\n- strong results on the NCBI corpus\n- methods are significantly faster and require far fewer parameters than TaggerOne while yielding comparable results\n\nCons:\n- Results on BC5 are mixed. Why?\n- Data augmentation not applied to baselines\n- Methods are not very novel\n\nQuestions:\n- Were the AwA results applied only at test time or were the models (including baselines) re-trained using un-resolved abbreviation training data?', 'rating': '7: Good paper, accept', 'confidence': '5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		BJerQWcp6Q	BJlXzXCyfN	AKBC.ws/2019/Conference/-/Paper39/Official_Review	['AKBC.ws/2019/Conference/Paper39/Reviewers/Unsubmitted']	2		['everyone']	BJerQWcp6Q	['AKBC.ws/2019/Conference/Paper39/AnonReviewer2']	1546801930823		1547662969366	['AKBC.ws/2019/Conference']
802	1546965709675	"{'title': 'Very interesting paper that describes a positive contribution to the state of the art in BioNLP', 'review': 'This paper proposes a deep-learning-based method to solve the known BioNLP task of disease normalization on the NCBI disease benchmark (where disease named entities are normalized/disambiguated against the MeSH and OMIM disease controlled vocabularies and taxonomies). The best known methods (DNorm, TaggerOne) are based on a pipeline combination of sequence models (conditional random fields) for disease recognition, and (re)ranking models for linking/normalization.\n\nThe current paper proposes instead an end-to-end entity recognition and normalization system relying on word-embeddings, siamese architecture and recursive neural networks to improve significantly (4%, 84 vs 80% F1-score, T. 3). A key feature is the use of a GRU autoencoder to encode or represent the ""context"" (related entities of a given disease within the span of a sentence), as a way of approximating or simulating collective normalization (in graph-based entity linking methods), which they term ""coherence model"". This model is combined (weighted linear combination) with a model of the entity itself.\nFinally, the complete model is trained to maximize similarity between MeSH/OMIM and this combined representation.\nThe model is enriched further with additional techniques (e.g., distant supervision). \n\nThe paper is well written, generally speaking. The evaluation is exhaustive. In addition to the NCBI corpus, the BioCreative5 CDR (chemical-disease relationship) corpus is used. Ablation tests are carried out to test for the contribution of each module to global performance. Examples are discussed.\n\nThere are a few minor issues that it would help to clarify:\n\n(1) Why GRU cells instead of LSTM cells?\n(2) Could you please explain/recall why (as implied) traditional models are NP-hard? I didn\'t get it. Do you refer to the theoretical complexity of Markov random fields/probabilistic graphical models? Maybe you should speak of combinatorial explosion instead and give some combinatorial figure (and link this to traditional methods). My guess is that this is important, as the gain in runtime performance (e.g., training time - F. 4) might be linked to this.\n(3) A link should be added to the GitHub repository archiving the model/code, to ensure reproducibility of results.\n(4) Could you please check for *statistical significance* for T. 3, 5, 6 and 7? At least for the full system (before ablations). You could use cross-validation. ', 'rating': '9: Top 15% of accepted papers, strong accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		BJerQWcp6Q	Skg8RfIzfN	AKBC.ws/2019/Conference/-/Paper39/Official_Review	['AKBC.ws/2019/Conference/Paper39/Reviewers/Unsubmitted']	3		['everyone']	BJerQWcp6Q	['AKBC.ws/2019/Conference/Paper39/AnonReviewer1']	1546965709675		1547662969145	['AKBC.ws/2019/Conference']
803	1544494026432	{'title': 'An adaptive hyperparameter tuning method for knowledge graph embeddings', 'review': 'This paper proposes a novel adaptive hyperparameter tuning method for knowledge graph embeddings. Variational inference is used to induce the hyperparameters of the prior multivariate normal distribution, from which the entity and relation embeddings are sampled. Improvement has been obtained over DistMult and ComplEx on the link prediction task. This work is technically sound. The paper is well written and easy to follow, despite of one or two typos.\n\nReasons to accept:\n1. Applying variational inference is new to knowledge graph embedding learning.\n2. The method shows improvement over DistMult and ComplEx.\n3. Balanced MRR is proposed towards a better metric on the skewed distribution of data.\n\nReasons to reject:\n1. The proposed method is not compared with better existing works, such as RGCN, ConvE and SimplE.\n\nMinor:\nThe authors might also consider discussing the differences of their work and [1] in the related work section.\n\n[1] Chen et al. Embedding Uncertain Knowledge Graphs. AAAI-19. https://arxiv.org/pdf/1811.10667.pdf', 'rating': '6: Marginally above acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		rylPm-5a67	BklM0iqh1E	AKBC.ws/2019/Conference/-/Paper40/Official_Review	['AKBC.ws/2019/Conference/Paper40/Reviewers/Unsubmitted']	1		['everyone']	rylPm-5a67	['AKBC.ws/2019/Conference/Paper40/AnonReviewer1']	1544494026432		1547662968925	['AKBC.ws/2019/Conference']
804	1546935118905	"{'title': 'Shows that Bayesian learning can speed up computing embeddings of knowledge graühs', 'review': 'The paper proposes a Bayesian extension to existing knowledge base embedding methods.\nSpecifically, it presents density-based DistMult and ComplEx variants with diagonal Gaussians, which are then used for hyperparameter tuning of the embedding methods\nusing variational EM. As a consequence, the (hyperparameter) optimization \nis more efficient than the traditional grid search approach.\n\nOverall, the paper is very well written. And I like the motivation of a the Bayesian\nframework due to low frequencies of some facts. The generative model of KG embeddings\nis natural, although it might be interesting to discuss in more details the assumptions\nmade by using a softmax to normalize the scores over the tail indices as well as of the multinomial. This seems to be related to the discussion of the weak performance of the \nvariational approach for high-dimensional embeddings. The rest, however, follows well from standard arguments of variational inference. \n\nThe main downside are the empirical results. Or to be more precise, the presentation \nof the empirical results. Table 1 shows the performance. While there seems to be an advantage of the presented Bayesian approach, the benefit is not very high (are the differences significant?). Indeed, it is interesting that the MAP approach presented in the paper performs better than the original methods, nevertheless, the authors argue in the intro that the speed up is one of the main advantages. The benefit of avoiding to spend resources on hyperparameter tuning is not illustrated. Empirical results on the\nrunning time should be added.  \n\nAdditionally, the paper should discuss in more details the parameter uncertainty argument made\nin the beginning. While the authors touch upon this when deriving the algorithm as the Bayesian treatment of the embeddings is required to guarantee convergence, the conclusions  also argue that variational inference many not be the right thing to do for high-dimensional embeddings. This leaves the reader somewhat alone. Should I use the approach or not?\n\nminor:\n\n(page 4) ""Hitchcock [1927] assume*s* ....""\n', 'rating': '6: Marginally above acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		rylPm-5a67	BJewUs0-fV	AKBC.ws/2019/Conference/-/Paper40/Official_Review	['AKBC.ws/2019/Conference/Paper40/Reviewers/Unsubmitted']	2		['everyone']	rylPm-5a67	['AKBC.ws/2019/Conference/Paper40/AnonReviewer2']	1546935118905		1547662968706	['AKBC.ws/2019/Conference']
805	1546952201205	{'title': 'interesting direction but both the results and motivation are not very convincing', 'review': 'The general idea of using Bayesian modelling for link prediction is not as new, and the discussion of related work does not seem to be quite adequate.\nThe authors should contrast their approach with previous work rather than simply mentioning a small subset of the work and saying that their proposal is different. Variatonal mean field methods haven been studied actively in this context (e.g., Nakajima and Sugiyama, 2011). Blackbox VI / reparametrization trick has also been used in the related word embedding context (see, e.g., Brazinskas et al (2017)).  Bayesian versions of tensor factorization have also been proposed, e.g., Nakajima (2012) and  Zhang, Salwen, Glass, Giles, (EMNLP 2014). It needs to be clear what the differences between then proposal and the previous work are.\n \n\nUnfortunately, the paper and the approach is in many respects counter-intuitive / ad-hoc. E.g., an embedding for each entity is drawn from its own prior (i.e. from a distribution with hyper-parameters specific to each entity or each relation), the corresponding hyperparameters are estimated from data (i.e. empirical Bayes). E.g., the prior distribution for the entity USA will end up different from that of UK and then only a single vector will be drawn from each prior (e.g., an embedding for the USA will be drawn from the prior for the USA and the embedding for UK from the prior for UK). Using EM with single-sample-per-paramterized-prior is an unorthodox choice which would generally result in degenerate solutions (the prior will simply match the posterior). It does not happen here as they use a specific restricted type of priors. These priors are not flexible enough as their mode is fixed (centred Gaussians), so they could not only match if the posterior modes are left at 0. With more flexible priors (interestingly more appropriate from the modelling perspective!) the method would have failed.  This construction is different from what has been done in BMFs / TMFs where the priors are normally shared across entities (or at least entities of the same type).\n\nThe actually used method is a pipeline: MAP (stage 1) -> VMF to determine hyperparamters (stage 2) -> MAP with new hyperparamters (stage 3). It is not clear how the hyperparameters are set in the first MAP iteration.  Given that Bayesian modelling is not used, I do not understand why the authors claim that they “keep track of parameter uncertainty”. In the end their approach is not Bayesian.  (See “we argue that the parameter uncertainty in knowledge graph embedding models can be significant, calling for a Bayesian approach”). \n\nThe empirical results are somewhat mixed as the improvements from using the method are not particularly impressive. However, the improvement for DistMult seem more convincing. However, I was confused by the statement (in Table 1) that the parameters of the ComplEx baseline are tasks from the Lacroix paper, whereas nothing was said about DistMult. Does it imply that the DistMult model is less tuned? What hyperparameters do the authors refer to? Does it include frequency based weighting of the regularizer? Has frequency based weighting been used for Complex only, not DistMult? \n\nI am not entirely sure that their claim that their approach is a simpler alternative to previous parameter selection methods is well supported. E.g., the cited method of Lacroix et al. relies on frequency and does not seem any harder to tune.\n\nIt may seem surprising that the VMF method alone (i.e. stage 2) is less effective than the pipeline. The authors mention it only in conclusions (should really be mentioned earlier, together with extra discussion / analysis). Whereas one may have expected that the improvements from Bayesian modelling should be greater for less frequent entities / relations, a degradation in performance (rather than the lack of significant improvement) for frequent entities probably points to some problems of the model. Why would it then work within a pipeline? I guess the reason is that the posteriors for infrequent entities will have large variances (essentially the effect of the entropy term) -> hence the prior dispersion parameters will also decrease with frequency (in average) -> hence the terms with higher frequency will be more heavily regularised. This matches the recipe of Lacroix et al / Serbs & Salakutdinov et al. The hope is that there is more to it than simply following this recipe. As far as I understand, the ComplEx baseline has already used the frequency-based weights so some improvements over this baseline may suggested that the induced weighting is more appropriate. However, I am not sure if the DistMult baseline has already used them (unclear).\n\nThe initialisation for stage 1 (the first iteration MAP) is not described. How sensitive the model was to the initialization? If the initialization of the dispersion parameters is uniform (and the regularisation is sufficiently strong), I could image that it would result in small variance for the variational paraders for infrequent items and potentially break the method? Or, does it only slows down inference?\n\nWhat are termination conditions? How is T chosen? Has the method actually converged?\n\nOverall, the paper has not convinced me that this approach is well motivated and less brittle than previous alternatives. There are just too many aspects which seem ad-hoc, and not properly discussed / motivated in the submission.\n\n\n\n\n\n\n', 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		rylPm-5a67	Byl-M0GfzV	AKBC.ws/2019/Conference/-/Paper40/Official_Review	['AKBC.ws/2019/Conference/Paper40/Reviewers/Unsubmitted']	3		['everyone']	rylPm-5a67	['AKBC.ws/2019/Conference/Paper40/AnonReviewer3']	1546952201205		1547662968483	['AKBC.ws/2019/Conference']
806	1547066983891	{'title': 'Weakly Supervised Web-Table Classification Using Table Embeddings', 'review': 'Tables in the webpage or paper contains certain information.  However the variety of structures and formats make it difficult for us to utilize such data. In this paper, the authors aim at training embeddings for table entities which can be used to do table clustering in some specific domain. They propose methods to generate sentences using words in tables and use the table vectors they generated to do the clustering. Comparing with other baseline methods, their vector can provide better results in some domains. The paper is well written and easy to follow. But there are a few questions for the authors:\n- When you choose the T sentences, how many sentences will you select? How is the number of words in T sentences comparing to the number of table words? If we change the number of T sentences, will it also affect the performance in Table 4?\n- Have you tried other ways to generate the table vector other than using the mean and mediation?\n', 'rating': '7: Good paper, accept', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		Hye97Zcaam	Hylgd0CQfV	AKBC.ws/2019/Conference/-/Paper41/Official_Review	['AKBC.ws/2019/Conference/Paper41/Reviewers/Unsubmitted']	2		['everyone']	Hye97Zcaam	['AKBC.ws/2019/Conference/Paper41/AnonReviewer1']	1547066983891		1547662968229	['AKBC.ws/2019/Conference']
807	1546717471100	"{'title': 'Interesting problem but contributions overstated', 'review': 'This paper proposes and approach for classifying tables found on webpages according to their structure into 5 categories. The main idea is to first cluster them and then ask users to label the clusters. While the problme is interesting and approach taken is sensible, I have the followinc concerns:\n\n- While it is the case that a lot of useful information in the web appears in tables, it is not clear to me whether the particular classification task dealt with is useful to do some other task. Has the classification scheme proposed been used in some other work to accomplish some kind of information extraction task? Lumping all tables that don\'t fit in the 4 categories defined into a non-data category sounds rather arbitrary.\n\n- The three ""unusual"" domains studied while interesting, I don\'t see why they are different from other domains. Noise in tables is nothing new in the web, and often the tables implicitly define the schema. Also I don\'t see why they pages these tables appear ""look very different"". I appreciate that these domains are important for societal reasons, but their differences from other domains needs to be quantified.\n\n- In annotating the clusters, how were ties dealt with?\n\n- It was a bit odd to see that the ground truth was not a random sample from the data labeled, but adjusted in order to have a relatively uniform label distribution. Also non-data seems to be rather populous to be ignored, especially when compared against the list.\n\n- It is also odd to fix the number of clusters before deciding the ""sentences"" that are essentially what the tables are clustered by. As it stands, any conclusions drawn about the ""sentence"" construction can only be considered valid for the number of clusters chosen, which likely to change when different quantities are considered.\n\n- Also, the human effort required for labeling is linearly dependent on the number of clusters obtained, thus it is not fixed. A small number of clusters is more likely to result in impure clusters, thus it is not possible to know in advance the cost of applying it to a new domain. A pre-trained model, or a supervised training model with a limited number of training datapoints has a fixed cost to be applied or ported to a new domain.\n\n- For a valid comparison against fully supervised methods, there should be the same number of training data points as those labeled by the annotators in cluster labeling.', 'rating': '5: Marginally below acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		Hye97Zcaam	HJgPQYKAWN	AKBC.ws/2019/Conference/-/Paper41/Official_Review	['AKBC.ws/2019/Conference/Paper41/Reviewers/Unsubmitted']	1		['everyone']	Hye97Zcaam	['AKBC.ws/2019/Conference/Paper41/AnonReviewer2']	1546717471100		1547662968007	['AKBC.ws/2019/Conference']
808	1547117480314	"{'title': 'A meaningful step towards open-world probabilistic databases', 'review': ""This paper studies the problem of probabilistic databases (PDBs) under the open-world assumption, a promising direction for open-world probabilistic knowledge bases. This work builds on [Ceylan et al., 2016] and extends it to allow for constraints at the schema level. It alleviates a problem of the previous work that tends to give close-to-1 probability upper bounds for binary queries, which are not very informative. Query (safe union of conjunctive query only) efficiency within the constrained PDBs is analyzed, which shows that now a smaller set of queries can be efficiently answered. An approximate algorithm is then proposed to compute bounds for the probability upper bound of binary queries.\n\nThe paper is very well written. I'm actually amazed by how the authors successfully managed to write a piece of theoretic work like this in such an easy-to-understand way. It's truly a pleasure to read. \n\nIn sum, I think this work proposes a meaningful extension of an existing PDB model and is a solid step towards practical open-world PDB.\n\nIn Section 6, it would be great if, in addition to the open problems specific to the proposed method, there are discussion of open problems of open-world PDBs/KBs in general."", 'rating': '8: Top 50% of accepted papers, clear accept', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		r1xTXZ9p6Q	Hkgx2mjNME	AKBC.ws/2019/Conference/-/Paper42/Official_Review	['AKBC.ws/2019/Conference/Paper42/Reviewers/Unsubmitted']	2		['everyone']	r1xTXZ9p6Q	['AKBC.ws/2019/Conference/Paper42/AnonReviewer3']	1547117480314		1547662967789	['AKBC.ws/2019/Conference']
809	1546935187174	{'title': 'Novel and strong results on open probabilistic databases', 'review': 'The paper extends the open world probabilistic databases due to Ceylan et al. by constraining the mean probability allowed for a relation. For example, as \nused as an example in the paper, while we may not know the probability of each \nperson on earth being a scientist individually, we might know that 1% of the \ngeneral population are scientists. The paper now formalizes what this means \nmathematically and investigates the computational complexity of query answering in\nthe resulting constrained open-world probabilistic databases. In general, we cannot hope to find efficient query answering approaches. How, based on a submodular approximation,\none can 1-eps^-1 approximation.  \n\nOverall, this is a very solid paper. It introduces an interesting and novel extension of open-world probabilistic databases and presents a rigorous complexity analysis. This shows that adding mean probability constraints requires approximative inference, which can be established using submodularity.  ', 'rating': '8: Top 50% of accepted papers, clear accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		r1xTXZ9p6Q	rkxj9sRWMN	AKBC.ws/2019/Conference/-/Paper42/Official_Review	['AKBC.ws/2019/Conference/Paper42/Reviewers/Unsubmitted']	1		['everyone']	r1xTXZ9p6Q	['AKBC.ws/2019/Conference/Paper42/AnonReviewer2']	1546935187174		1547662967573	['AKBC.ws/2019/Conference']
810	1547102768857	{'title': 'Review', 'review': 'The paper tries to identify and classify the background/commonsense knowledge required to answer the questions from the Winograd Schema Challenge (WSC). It also proposes a deterministic algorithm for answering WSC problems based on the representation chosen for the statement and knowledge. For background, the WSC contains statements such as “The man could not lift his son because he was so weak.” with the question being to resolve the pronoun to its correct reference. Correctly resolving the pronoun requires world knowledge; to resolve ‘he’ in the example above, one needs to know that if a person is weak they might not be able to lift something. Wrongly resolving the pronoun to the son in the example would entail that things that are weak cannot be lifted. \n\nThe paper uses the representation produced by the K-parser [Sharma et al. 2015] to represent the input statement, which parses the statement into a directed acyclic graph which action (verbs), entity (nouns), property (adjectives), etc. nodes. Based on this representation, the paper identifies 12 different categories of knowledge required to solve the WSC problems. For example, A knowledge category is “Property prevents Action” whose instantiation is weak person ‘prevents’ person lifts. Apart from similar categories to these, correlation amongst actions and properties is needed, and knowing whether a statement is more likely than another statement. \nThe paper proposes a deterministic algorithm to answer WSC problems by using the K-parser representation and the required knowledge. \n\nOf the 12 different categories of knowledge required identified in the paper, the proposed method can only handle 10 of those. It cannot handle problems that require knowing when a statement is more likely than another statement, or when a problem requires multiple different kinds of knowledge. Another limitation of the proposed approach is, of the 10 types that can be handled, all of them require knowledge in similar propositional form, ‘sentence1 predicate sentence2’. For example, ‘action causes property’, ‘action1 follows action2’. These limitations seem prohibitive. Empirically the paper finds that only 69% of the problems in the Definite Pronoun Resolution (DPR) Dataset can be solved if only the top categories of knowledge are used. It would have been useful if other question-answering datasets, or coreference resolution datasets are analyzed together to form a comprehensive categorization of required knowledge to improve coverage across problems and datasets.\n\nAnother major limitation of the proposed approach is the semantic representation used (K-parser). The semantic representation produced by the K-parser is not error-free and requires identifying non-trivial semantic relations. For example, identifying ‘weak’ is a ‘trait’ of ‘he’. The limitations of the closed class of semantic relations used is not discussed in the paper. Empirically, 18% of the problems solvable in the DPR are incorrectly answered due to parser errors. This number is worrying to use this representation for a large-scale purpose.\n', 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		r1gyNZ9Tam	rJeKV9DNMV	AKBC.ws/2019/Conference/-/Paper43/Official_Review	['AKBC.ws/2019/Conference/Paper43/Reviewers/Unsubmitted']	3		['everyone']	r1gyNZ9Tam	['AKBC.ws/2019/Conference/Paper43/AnonReviewer2']	1547102768857		1547662967348	['AKBC.ws/2019/Conference']
811	1547071984867	"{'title': 'This title examines the types of knowledge that are required by a system to solve the Winograd Schema Challenge. It classifies the knowledge patterns needed into 12 knowledge types. It proposes an algorithm to leverage this knowledge for inference and proposes a first approach to extract such knowledge from a corpus.', 'review': 'This title examines the types of knowledge that are required by a system to solve the Winograd Schema Challenge. It classifies the knowledge patterns needed into 12 knowledge types. It proposes an algorithm to leverage this knowledge for inference and proposes a first approach to extract such knowledge from a corpus.\n\nThe approach is interesting and I certainly welcome this type of work a lot as it sheds light on how knowledge-agnostic machine reading approaches can be when solving complex reading tasks. I value the effort done by the authors to work through the examples of the Winograd challenge and systematize the needed knowledge into 12 knowledge types. This is an important contribution. The knowledge in these patterns is represented as a (dependency) graph, relying on the same representation used for representing the sentence and question.\n\nIt is also good to see an algorithm proposed to ingest these knowledge pieces to determine the answer to a problem from the Winograd challenge. The algorithm essentially merges a dependency / graph representation of a sentence and question with the background knowledge in the form of patterns and performs a number of merging operations until the question pronoun in the question is resolved to some entity in the text. The answer retrieval is implemented using Answer Set Programming. \nK-Parser is used to translate natural language text into graphical representations.\n\nThe evaluation shows that the 12 knowledge types can ""cover"" all examples from the Winograd challenge and 82% in the DPR dataset. It is unclear to me here if the authors really mean here that their algorithm found the correct solution in 100% of the cases for the Winograd dataset given the hard-coded knowledge.\n\nWhile the main focus of the paper is not on automatically inducing the knowledge in question, they perform a small experiment testing in how many cases they find sentences containing the knowledge in a larger corpus. They success to find relevant sentences in 100 out of 161 problems for the Winograd dataset; this is a promising result.\n\nWhat I found quite cumbersome and in need for improvement is the description of the ""reasoning algorithm"" that essentially merges representation. The procedure as it is describe looks a bit adhoc. I was wondering if the authors instead could describe the underlying principles that are applied for merging. As it stands, the procedure is hard to trace and to understand. \n\nOverall, this is an interesting paper sheding light on types of knowledge that could play a role in more challenging machine reading tasks.\n', 'rating': '6: Marginally above acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		r1gyNZ9Tam	ryxFxMlEGE	AKBC.ws/2019/Conference/-/Paper43/Official_Review	['AKBC.ws/2019/Conference/Paper43/Reviewers/Unsubmitted']	2		['everyone']	r1gyNZ9Tam	['AKBC.ws/2019/Conference/Paper43/AnonReviewer3']	1547071984867		1547662967093	['AKBC.ws/2019/Conference']
812	1547069302372	"{'title': 'Detailed analysis of the WSC task but limited application to other tasks', 'review': ""This paper aims to do a deep analysis of Winograd Schema Challenge and solve them by categorized different kind of knowledge needed to answer the questions.  A semantic graph-based approach is proposed for 10/12 types of questions in WSC. By using manually extracted knowledge, the proposed approach has really good performance on the WSC data.  The paper is well written and easy to follow. The definitions are clear and the result analysis further confirmed the proposed approach's performance gain.\n\nOne concern is the current work as the author mentioned, is still using manually extracted knowledge which makes the model super hard to scale, even the paper experiments are performing on the limited subset of the WSC data. How to further extend the current analysis of WSC to other tasks remains unclear too. It would be good if the authors could add those.\n\nA few minor points, \n\n1) The last column of Table 1 is named as # prob-lems, is that the number problems fall into this category? Might need a short caption on that. \n3) In section 4.3.2, the authors mentioned step 1, 2, 3 of the knowledge extraction pipeline. But it's not clearly defined anywhere before. It would be nice to add those details."", 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		r1gyNZ9Tam	r1lRdvJ4M4	AKBC.ws/2019/Conference/-/Paper43/Official_Review	['AKBC.ws/2019/Conference/Paper43/Reviewers/Unsubmitted']	1		['everyone']	r1gyNZ9Tam	['AKBC.ws/2019/Conference/Paper43/AnonReviewer1']	1547069302372		1547662966870	['AKBC.ws/2019/Conference']
813	1547072807184	"{'title': 'Mermaid integrates multiple existing techniques to achieve commercial viability in KB construction', 'review': 'The paper descibes a system that integrates existing or slightly modified techiques for entity identification and web-scale knowledge mining to create JKB, a ""Knowledge Graph"" like resource for Japanese content.\n\nAlthough the techniques used are not particularly novel, the description of how they can be integrated and adapted to reach commercial quality, and of when human curation of the results is and is not needed, will be helpful to others producing entity-property bases in the future.\n\nA particular barier to more general application lies in the use of path-based information extraction, which is likely to be effective only for a relatively small class of entity types. That the system is limited in this respect is also reflected in the manual construction of the type mappings.\n\nThere are numerous small grammar and usage errors, although the general expostion is clear and helpful.\nIt might be advisable to chose an illustrative example that does not require the inclusion of illustrations of partially naked (cartoon) humans.', 'rating': '6: Marginally above acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		SJxfV-q6Tm	B1gy4BgNfN	AKBC.ws/2019/Conference/-/Paper44/Official_Review	['AKBC.ws/2019/Conference/Paper44/Reviewers/Unsubmitted']	2		['everyone']	SJxfV-q6Tm	['AKBC.ws/2019/Conference/Paper44/AnonReviewer3']	1547072807184		1547662966650	['AKBC.ws/2019/Conference']
814	1546706278792	{'title': 'Subtle evaluation results', 'review': 'The authors describe an information extraction system architecture. The Related Work section is missing. The evaluation procedure is not clear, there is no benchmark dataset and experiments are not reproducible.', 'rating': '2: Strong rejection', 'confidence': '5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'}		SJxfV-q6Tm	S1lJ_aIRb4	AKBC.ws/2019/Conference/-/Paper44/Official_Review	['AKBC.ws/2019/Conference/Paper44/Reviewers/Unsubmitted']	1		['everyone']	SJxfV-q6Tm	['AKBC.ws/2019/Conference/Paper44/AnonReviewer2']	1546706278792		1547662966433	['AKBC.ws/2019/Conference']
815	1545405495735	"{'title': 'Good paper with two caveats', 'review': ""This paper presents a methodology for automatically annotating texts from Wikipedia with entity spans and types. The intended use is the generation of training sets for NER systems with a type hierarchy going beyond the common four types per, loc, org, misc.\n\nI think this paper is a good fit for AKBC, given that entity recognition and linking systems are an essential part of knowledge-base population. This should be good for interesting discussions.\n\nA big caveat of this paper is that, while the approach discussed by the authors is generally sound, it is very tailored towards Wikipedia texts, and it is unclear how it can be generalized beyond. Since this approach is meant for downstream training of NER systems, these systems will be limited to Wikipedia-style texts. A statement from the authors regarding this limitation would have been nice. Or maybe it's not a limitation at all? If the authors could show that NER systems trained on this Wikipedia-style texts do perform well on, say, scientific texts, that would be already good.\n\nA second issue I see is that Stage III of the algorithm filters out sentences that might have missing entity labels. This makes sense, provided that the discarded sentences are not fundamentally different from the sentences retained in the NER tagger training set. For example, they could be longer on average, or could have more subordinate clauses, or just more entities on average, etc. This is something the authors should look into and should report in their paper. If they find that there are differences, an experiment would be nice that applies an NER tagger trained on the produced data to these sentences and verifies its performance there.\n\nA minor point: The authors claim their approach scales to thousands of entity types, which I find a bit of an overstatement, given that the dataset produced by the authors has <1.5k different types (which is already quite a lot)."", 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		HylHE-9p6m	HyelSNY5gE	AKBC.ws/2019/Conference/-/Paper45/Official_Review	['AKBC.ws/2019/Conference/Paper45/Reviewers/Unsubmitted']	1		['everyone']	HylHE-9p6m	['AKBC.ws/2019/Conference/Paper45/AnonReviewer2']	1545405495735		1547662966203	['AKBC.ws/2019/Conference']
816	1545067901175	"{'title': 'Very interesting problem, presentation can be improved.', 'review': 'This paper proposes the problem of link prediction in open KBs, aims to introduce benchmark datasets, and evaluates baseline methods.\n\nThe problem appears charming and novel, and is an ideal fit for the topics of the AKBC conference. In particular, given the unavoidable information loss when transforming text into fixed-relation triples, keeping KBs open is often desirable - predicting links in such open KBs then is an interesting challenge.\n\nThe paper provides a solid contribution in terms of discussion of related work, 2 benchmark datasets, and the evaluation of several baselines. It is also generally well-written and free of typos.\n\nMy concerns are mainly on three points: 1) I have issues understanding the evaluation methodology, 2) a technical concern about the datasets, and 3) the qualitative evaluation of the results is very brief, limiting insights on whether the datasets and results are useful or not. I believe that addressing 2) and 3) could greatly increase the impact of the paper.\n\n1) Methodology:\n(Disclaimer: I am no expert in embedding models) It is not clear to me how the link prediction is performed technically - can the embeddings in Figure 1 be inverted (from embeddings back to surface text)? As I naively understand classical embedding models, one would take embeddings of subject and predicate, apply a transformation, then output those entities as candidate objects whose embeddings are closest in embedding space to subject+predicate (or more complex transformations). Given the unrestricted nature of open surface forms, I don\'t see how now to ""search"" for embeddings of entities near subject+object. Or are you iteratively testing the distance to the embeddings of all entities of the open KB (but in this way, there would be no way to predict new entities/surface forms as objects)? I would appreciate a clarification/walk-trhough how top-k predictions are computed for a given test triple.\n\n2) The dataset description is a bit unclear, what exactly does each dataset consist of, and what is extracted from it? In some way a set of synonymous entity mentions (paraphrases) is computed? I would suggest to make the components of the datasets more formal (e.g., ""each dataset is a tuple of ...""). Also, treating different mentions as paraphrases appears somewhat in conflict with an argument made on the top of page 7 - that different mentions carry semantically interesting information. I imagine that could also play a role at the prediction stage - if (Senator Obama, worked for Bill Clinton), but ""President Obama"" did not, can this be modelled?\n\n3) The qualitative evaluation of the results is rather unsatisfactory and does not allow me to judge what happens. There are still 2.5 pages of space so that should be easy to address. The only example, professors at Princeton, cannot be intuitively understood (probably not even by people at Princeton). I\'d suggest to add several examples about known topics, e.g., movies by Tarantino, states of the US, Italian dishes, or something similar that allows an intuitive understanding.\nAlso, I would greatly appreciate examples that show which influence variations in subjects or predicates have - are predictions for (Trump, visited, x) different from (President Trump, often visited), (Donald Trump, has been to), and so on. \n\nMinor comments\n - Capitalization of systems please check, e.g., ""WikiPedia"", ""DBPedia"", ""freebase""\n - Notation is a bit cumbersome. Why are there letters i, j, k, additionally head entities, tail entities? Why not intuitive s, p, o? Being in the head of a triple is not a property of an entity, just a role - entities can appear both in the subject and object position, so the distinction between ""head entities"" and ""tail entities"" appears incorrect.\n - last paragraph of Section 2: Add ""and k is a paraphrase of r""\n - ""as representative of such models"" - before 3 categories are introduced, clarify whether this is a representative of all 3 categories?\n - ""an KG"" -> ""a""\n - ""filtered setting"" is unclear to me\n - what is an ""oracle link function""?\n - ""a state of the art model which ..."" - state of the art for which task?\n - ""gold annotation for entity links"" - what are ""entity links""?', 'rating': '7: Good paper, accept', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		HJl_NZ5T67	HJxSY6ISgE	AKBC.ws/2019/Conference/-/Paper46/Official_Review	['AKBC.ws/2019/Conference/Paper46/Reviewers/Unsubmitted']	1		['everyone']	HJl_NZ5T67	['AKBC.ws/2019/Conference/Paper46/AnonReviewer2']	1545067901175		1547662965984	['AKBC.ws/2019/Conference']
817	1546979154717	"{'title': 'Interesting problem, but challenging to learn something new from this paper', 'review': 'This paper investigates Open Link Prediction - the tasks consists in, given the surface forms of a source entity and an open relation, predict the surface form of the target entity using a set of Open Information Extraction (OIE) triples. Authors introduce the task, propose a set of baselines (mainly inspired to the link prediction literature), a dataset, and an evaluation protocol (also inspired from the same literature). This problem is very related to e.g. language modelling, but nothing is mentioned about it in the paper. Indeed the main contribution in this paper seems to be the dataset, but authors do not really provide any assessment on its quality, nor make it available for inspection (they only mention some overlap between train and test data).\n\nTypos:\n- abstract: ""sucessfully""\n- pg. 2: ""predifined set of entites""\n- pg. 2: ""extremly""\n- pg. 3: ""entites""\n- pg. 5: ""entitites""\n- pg. 7: ""correponds to a continous""', 'rating': '5: Marginally below acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		HJl_NZ5T67	rJeiIPKzMN	AKBC.ws/2019/Conference/-/Paper46/Official_Review	['AKBC.ws/2019/Conference/Paper46/Reviewers/Unsubmitted']	2		['everyone']	HJl_NZ5T67	['AKBC.ws/2019/Conference/Paper46/AnonReviewer3']	1546979154717		1547662965725	['AKBC.ws/2019/Conference']
818	1546713184371	"{'title': 'Strong results reported but very confusing description', 'review': ""This paper proposes an event extraction approach based using generative adversarial imitation learning. The results on TAC dataset are very strong, in most cases improving on the state of the art. However, the positioning of the proposed approach with respect to the literature and the description of the algorithm is rather confusing, thus it is difficult to assess the novelty and contributions made. Here are a few points for the authors to consider in revising their work:\n\n- generative adversartial imitation learning is a framework proposed by Ho and Ermon, as cited. They propose an algorithm to optimize the objective defined using trust region policy optimization, however the approach proposed  in this paper does not follow that algorithm. Is there a reason for this? Assuming the approach was named as it followed the work of Ho and Ermon, it would make sense to explain and justify the deviations from it. GAIL in the original paper does not assume access to an expert for supervision, but the algorithm proposed here does, so there are substantial differences.\n\n- the approach proposed is also referred to in the title as end to end. However, there are two separate training steps: one with Q learning for trigger extraction and one for argument detection trained with policy gradient.\n\n- in GAN training, the generator creates datapoints and the discriminator classifies them. However here we have a classifier (a set of them to be precise) that predict the labels for entities and triggers and the arguments, and an extractor that assigns rewards, a regressor. I think this is much more similar to the actor-critic approaches, see here for example: https://arxiv.org/abs/1607.07086\n\n- the argument prediction is modeled as independent classification steps. What's the reason to use an RL algorithm such as policy gradient? Cost sensitive classification is well studied, see: https://cling.csd.uwo.ca/papers/cost_sensitive.pdf and the motivation is often to address class imbalance\n\n- for the entity and trigger tagging, the natural baseline is to apply CRF-biLSTM style approaches, e.g. : https://www.aclweb.org/anthology/N16-1030\nIt takes into account context from the whole sentence as well as the actions themselves. As far as I understand the paper proposes a different training objective from cross entropy in equation 5, however the loss there is not defined with respect to the gold standard so it hard to follow.\n\nOn the whole, while the results presented are very strong, at the same time I can't understand the approach even though I went through the relevant literature and I consider myself rather familiar with the topic."", 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		BJlsVZ966m	HJx_vdORbN	AKBC.ws/2019/Conference/-/Paper47/Official_Review	['AKBC.ws/2019/Conference/Paper47/Reviewers/Unsubmitted']	1		['everyone']	BJlsVZ966m	['AKBC.ws/2019/Conference/Paper47/AnonReviewer3']	1546713184371		1547662965503	['AKBC.ws/2019/Conference']
819	1546874334292	"{'title': 'Slightly flawed idea with confusing descriptions. ', 'review': 'The paper introduces generative adversarial imitation learning (GAIL) for event and argument extraction. A discriminator is trained to distinguish between gold labels and system predicted labels to estimate rewards given to the extractor for its predictions during training. The general idea of the paper is unfortunately flawed as the task of event extraction is not a suitable setup for GAIL, at least how the paper presents it. Please find detailed comments in the following. Furthermore, the writing is quite confusing and it is hard to follow especially the methods section. The results are surprisingly good, however, the paper lacks theoretical justification for why this is the case. Finally, a proper baseline is missing, that is using the same model with standard supervised learning and teacher forcing. Therefore, I cannot recommend this paper for acceptance. \n\n==Concerning RL==\nI have trouble with how the paper presents the used techniques. This is not a classical RL setup and I am not sure that using Q-Learning for sequence labeling is all that useful if not compared to the standard way of doing things which is to use the gold labels during training as input to the unidirecitonal LSTM and train using cross-entropy (teacher forcing). Argument role prediction is also not really an RL problem. Calling this policy gradient is confusing because in RL there is usually no direct supervision over the policy but only a reward the comes from an environment. The formulation here is thus very artificial.\n\n==Concerning GAIL==\nI like the idea of having dynamic rewards or losses. However, there is something very strange going on in this setup. If the discriminator can better estimate what a good reward is than the extractor given a state and a label, why can we not use the discriminator to predict the correct label in the first place? In fact the extractor will learn exactly the same behaviour as the discriminator which is trained on the actual task as it learns to score gold labels higher than labels predicted by the system, that is, if the system is wrong. This would only make sense if computing D(a,s) for all a is not feasible, ie. the action space is too large. However, it is not too large in this case. I am quite concerned by this but the paper doesn\'t offer any answers as to why the extractor should be better than the discriminator and why such a training should be at all better than normal supervised learning. I believe that this might be due to the ranking loss of D and the way the extractor is used to sample negative examples for ranking. A possible scenario where it makes sense would be if the extractor was trained on unlabeled data. Maybe it also makes sense in case past actions are actually used to compute future states and affect does future actions to discount for the fact that past actions may have caused false future actions. But there is a clear lack of evidence in the paper that this is really the case.\n\nSuggestions:\nIn my opinion the paper needs to tone down the RL part and instead focus on adopting ideas from inverse RL and GAIL to scale standard supervised losses dynamically. This, however, requires a theoretical justification why it should be better than teacher forcing and unscaled cross-entropy loss, which needs to be the baseline.\n\n\nComments and questions:\n\n- notation could be clearer, e.g., v_t_tr v_t_ar, the extra \'t\' is confusing\n- unecessarily complicated notation and description of method\n- many confusing formulations: e.g.: \n-- ""In event extraction task, entity, event and argument role labels yield to a complex structure with variant difficulties.""\n-- ""Equation 12 reveals a scenario of adversary between ground truth and extractor and Generative Adversarial Imitation Learning (GAIL) [Ho and Ermon, 2016], which is based on GAN [Goodfellow et al., 2014], fits such adversarial nature.""\n- why scaling the reward with alpha? Why use sigmoid and not tanh for the reward coming from function D(s,a)?\n- why using 5 and -5 as reward? When you use -0.1 and 0.1 as thresholds for the dynamic reward model, why don\'t you use this as a fixed threshold?\n- how are the ""actions"" used in the future? Are they fed into the uni-directional LSTM at the top of Figure 2? This is not clear from text or image. If actions are not used as additional input, then this is not RL because the actions taken wouldn\'t affect the future states.\n-- If actions are taken into consideration I would like to see a baseline with teacher forcing, where the model is always given the correct label as input during training time in the uni-directional LSTM, the way it is usually done.', 'rating': '3: Clear rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		BJlsVZ966m	Bye81CJbfN	AKBC.ws/2019/Conference/-/Paper47/Official_Review	['AKBC.ws/2019/Conference/Paper47/Reviewers/Unsubmitted']	2		['everyone']	BJlsVZ966m	['AKBC.ws/2019/Conference/Paper47/AnonReviewer1']	1546874334292		1547662965278	['AKBC.ws/2019/Conference']
820	1546980729910	"{'title': ""Interesting paper and model, but I'm wondering if the same can be achieved with something simpler"", 'review': 'In this paper, authors propose a framework for entity and event extraction based on inverse reinforcement learning. The underlying idea in the proposed method is to use the discriminator in a generative adversarial network for estimating the difficulty of an instance, and thus having more meaningful rewards. This is very interesting - one concern I have is whether the same can be achieved with a classic density estimation model, instead of using a GAN (which can be challenging to train for symbolic data).\n\nTypos:\n- pg. 4 ""ebmeddings""\n\n', 'rating': '6: Marginally above acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		BJlsVZ966m	B1efYpFzGE	AKBC.ws/2019/Conference/-/Paper47/Official_Review	['AKBC.ws/2019/Conference/Paper47/Reviewers/Unsubmitted']	3		['everyone']	BJlsVZ966m	['AKBC.ws/2019/Conference/Paper47/AnonReviewer2']	1546980729910		1547662965018	['AKBC.ws/2019/Conference']
821	1546764605350	"{'title': 'easy to follow and includes sufficient references', 'review': 'The survey paper examines the various components of semantic parsing and discusses previous work. The semantic parsing models are categorized into different types according to the supervision forms and the modeling techniques. Overall, the survey is easy to follow and includes sufficient references. The following points can be improved:\n\n* Training a semantic parser involves NL, MR, context, data, model, and learning algorithms. A summarization and examples of popular datasets are helpful.\n* Sec 3.1  Rule based systems can be expanded. The current section is too brief.\n* Sec 2.1 Language for Meaning Representation and Sec 2.2 Grammar should be merged. \n* P9: ""Machine translation techniques"" is not a method instead of `` 5. Alternate forms of supervision""\n* Sec 8 is too brief now. More discussions on future work are welcome.\n\nminor:\n*  Combinatory categorial Grammar ->  Combinatory Categorial Grammar', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		HylaEWcTT7	rJgSB-BJMN	AKBC.ws/2019/Conference/-/Paper48/Official_Review	['AKBC.ws/2019/Conference/Paper48/Reviewers/Unsubmitted']	1		['everyone']	HylaEWcTT7	['AKBC.ws/2019/Conference/Paper48/AnonReviewer3']	1546764605350		1547662964798	['AKBC.ws/2019/Conference']
822	1546984739654	{'title': 'Good survey paper, but no originality', 'review': 'Summary:\nThis paper conducts a thorough survey on semantic parsing, as the title suggested. This paper introduces the formal definition of the semantic parsing, categorized them, describes the development of the system from 1970s to very recent in (fairly) chronological order.\n\nQuality & clarity:\nThe survey is very thorough and self-contained, and the descriptions are all very clear and well-written.\n\nOriginality & significance:\nSince it is the survey paper, it’s hard to say it has originality. \n\nGeneral comment:\nAlthough the survey is very thorough, the paper does not have an original contribution, which conference paper should have.', 'rating': '5: Marginally below acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		HylaEWcTT7	r1x2mTqzf4	AKBC.ws/2019/Conference/-/Paper48/Official_Review	['AKBC.ws/2019/Conference/Paper48/Reviewers/Unsubmitted']	2		['everyone']	HylaEWcTT7	['AKBC.ws/2019/Conference/Paper48/AnonReviewer2']	1546984739654		1547662964579	['AKBC.ws/2019/Conference']
823	1547070011417	{'title': 'Concerns about evaluation and strength of model', 'review': 'Summary:\nThe paper proposes a system to identify and correct errors in knowledge graphs. For a given relation in the knowledge graph, the system first finds text that is relevant to either the entities involved or the relation type. It then encodes the text by averaging the pretrained vectors for each token, and concatenates this average with a small set of binary features indicating which portions of the relation are present in the text. This encoding is then passed through a two-layer MLP, and the result of that is passed separately through a sigmoid layer to predict credibility of the relation, and a softmax layer to predict a correction for the relation type.\n\nPros:\n- The system appears to achieve good performance, but the systems used for comparisons don’t seem very strong. The numbers would be more convincing with a stronger baseline.\n\n- The task/approach is somewhat novel. Jointly modeling credibility and corrections on a standalone knowledge graph is new. However, just modeling them jointly doesn’t have much of an improvement over the independent training in the evaluation. It’s not clear to me that the approach is significantly different from a pipeline in which an information retrieval system is used to find relevant sentences which are then passed through an IE system. \n\nConcerns:\n1) Did you consider other approaches for combining the sentences? Given that a single good sentence is sufficient to indicate a relation, and the only requirements for the selected sentences are that they contain a single aspect of the relation, it seems like averaging the sentences potentially just adds noise. This appears to be supported by the “number of sentences” ablation.\n\n2) The negative sampling for wikidata could be improved. Given that the model is based on the provenance, and the provenance is randomly assigned to negative examples, it seems like the negative examples in wikidata will be easy to identify.\n\n3) The model description is not completely clear to me:\n- How are the binary features from different sentences combined?\n- Is the credibility prediction aware of the proposed relation beyond the binary features? As it’s described, the text encoding seems like it would be a very noisy signal given that it might credibly describe a relation different from the proposed one.\n\n4) I would like to see more analysis as to why shared gumbel noise has such a large impact? What kinds of problems does it fix?\n\n5) Given that the model relies pretty heavily on it, I would like to see some analysis of how strong the sentence retrieval/featurization is. Some helpful things to include might be:\n- Statistics of how many sentences are retrieved on average, and some statistics about which features are triggered on them (ie how often do you see a sentence containing both entities and triggering the relation).\n- One baseline that would be helpful would be to run a state-of-the-art IE system on the retrieved sentences (or provided provenance for TAC), and compare the output to the proposed relation.\n\n\nMinor things:\n- It’s not entirely clear to me what the outer product ablation is, or why it’s included given that it doesn’t seem to have any effect.\n- The writing is not very polished. There are a good number of typos, as well as a duplicate discussion paragraph on page 11.\n', 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}		ryleB-56pQ	ryxmHcy4GV	AKBC.ws/2019/Conference/-/Paper49/Official_Review	['AKBC.ws/2019/Conference/Paper49/Reviewers/Unsubmitted']	2		['everyone']	ryleB-56pQ	['AKBC.ws/2019/Conference/Paper49/AnonReviewer2']	1547070011417		1547662964355	['AKBC.ws/2019/Conference']
824	1546782115661	{'title': 'Interesting problem definition, but unclear model description', 'review': 'This paper proposes the problem of identifying whether a fact triple (extracted by a system) is correct or not, and to further repair it so that if possble. A model is proposed based on a multi layer perceptron to perform these two tasks jointly with result reported on two datasets, one from Wikipedia and one from TAC-KBP.\n\nI like the idea of having a system to check the correctness and possibly repair the output of an IE system, and the paper does a good job describing the task and setting the framework in which this can operate.\n\nHowever I found the modeling section troublesome to follow. Here are my concerns:- The equations represent the basic multilayer perceptron, with the graph stating that a sentence is the input, but the text stating that the input is a triple, with a sentence optional. Given that the sentences are important for the task as they provide the source information, it is necessary to detail this explicitly. \n\n-Even less understandable to me was the use of the Gumbel-Softmax trick. There were no equations to describe it, and the justification that it is necessary due to the discrete classification output does not seem appropriate, as training classifiers with back propagation is not a problem. Given the text description is seems to be more like a form of dropout.\n\n- The manual annotation and feature extraction in section 4.1 needs to be explained more fully. Also, were the competing systems, i.e. the LR approach, given the same information?', 'rating': '5: Marginally below acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		ryleB-56pQ	BJghiSFyGE	AKBC.ws/2019/Conference/-/Paper49/Official_Review	['AKBC.ws/2019/Conference/Paper49/Reviewers/Unsubmitted']	1		['everyone']	ryleB-56pQ	['AKBC.ws/2019/Conference/Paper49/AnonReviewer1']	1546782115661		1547662964099	['AKBC.ws/2019/Conference']
825	1546948004070	"{'title': 'Nice initial take on adversarial attacks on KG embedding models, but fuzzy and incomplete presentation', 'review': 'The paper introduces adversarial attacks on KG embedding models for link\nprediction models (AALG). The goal of AALG is to lower the model confidence in a\ngiven target triple as much as possible by either adding or removing a single\ntriple to the KG and retraining the model. The paper presents heuristic methods,\na set of applications, and initial experiments. Overall, I like the approach\ntaken in this paper; it\'s a nice initial step towards AALG. However, the paper\nsuffers from a fuzzy presentation and a highly incomplete discussion of the\nexperimental study. As a consequence, key points of the paper remain somewhat\nmysterious to me and I am hesitant to recommend acceptance.\n\nPros:\nS1. Introduces adversarial attacks to KG models\nS2. Proposes heuristic algorithms to execute attack efficiently\nS3. Large number of experiments\n\nCons:\nW1. Presentation needs work, esp. around main technical contributions\nW2. Experimental setup largely unclear\nW3. Only quite limited form of AALP considered\nW4. Releationship to prior work partly unclear\n\nDetailed comments:\n\nD1. The definition of the loss function is imprecise. What is summed over? What\nabout negative sampling? This is important because the loss function is used\nlater to find attack triples.\n\nD2. The optimization problems of Sec. 3 do not aim for maximum change of the\ntarget triples as claimed. Instead, they aim to lower the score of the target\ntriple as much as possible. This is a mismatch to the motivation given in the\npaper (which should be appropriately adapted) and makes formulas unnecessarily\ncomplicated: the paper seems to solve $\\argmin \\bar \\psi(s,r,o)$.\n\nD3. The focus on fixed-object attacks appears reasonable, but is mentioned\nsomewhat late. Moreover, the ""easily expandable"" claim needs justification.\n\nD4. I found Sec. 4.1. very hard to follow. This is partly due to suboptimal\npresentation; e.g., the notation also mixes constants (i.e., optimum solutions)\nand variables (in derivatives) and is inconsistent (e.g., $\\cdot$ vs $\\times$ for dot\nproduct). The decomposition of the loss function is unclear (partly because it\'s\nnot well-defined) and needs to be spelled out. More importantly, it remains\nunclear what assumptions are really made, which surrogate function is optimized\nand how, and why this is a good choice. This crucial section needs revision and\nperhaps additional material in the appendix.\n\nD5. I do not buy the argument that the most adversarial triple for (s,r,o) is\nlikely to have the same object. I prefered if the authors just state that they\nfocus on this case or provide further justification.\n\nD6. The idea to find z_{s\',r\'} of an added triple is appealing. Due to the issues\nin 4.1, it\'s not clear how this is found though and which assumptions are made.\n\nD7. I suggest to add a study on the accuracy of the inverter network. For\nDistMult, the network probably isn\'t needed and one can use maximum\ninner-product search instead.\n\nD8. Is the effect of normalization (e.g., to unit norm) of the embeddings\nconsidered in any way?\n\nD9. The description of the experimental setup is largely missing. How have the\nmodels been trained? What was the serach grid, which hyperparameters have been\nselected and based on what criteria? How have models been retrained? From\nscratch? For how long? For how long have the AALP-methods (which are also\ngradient based) been run? Were there any hyperparameters (such as step size or\noptimizer)? If so, wow were they selected? Are the metrics being used filtered\nor unfiltered?\n\nD10. A natural baseline for AALP-Remove is to remove the neighor where\n$f(s\',r\')$ is ""closest"" to $f(s,r)$ (similarly AALP-Add).\n\nD11. The influence function as well as the relationship to Koh and Liang [2017]\nshould be discussed in more detail in the main part of the paper (Sec. 4.1).\n\nD12. In Tab 6.1, what are the scores correlated to? Why are there sometimes\nsmaller and sometimes larger differences between AALP and the influence\nfunction? What are the targets? Have results been averaged in any way? Is it\nfeasible to compute the best adversarial triple on these datasets without the\nfixed-object limitation (see D5)?\n\nD13. I found the discussion in the experimental study not particularly\ninsightful and somewhat hand-wavy. E.g., why do attacks perform better on some\nrelations than on others?\n\nD14. Tab 4 indicates that the attack triples can be insightful (ConvE and\nremoval), but quite often they are not. This questions how much interpretability\nwe can get from such attacks.\n\nD15. It\'s unclear to me how rules have been extracted.\n\nD16. In the experiments on error detection in which only the total number of\nerrors is known: Is it assumed that the target test triple is given? If so, how\ncould such an approach be used to actually detect erroneous triples? Is it\npossible or conceivable to find the added triples fully automatically?\n\nD17. The relationship to [Minervini et al., 2017, Cai and Wang, 2017] should be\nfurther clarified. Do these methods provide suitable baselines?', 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		Hkg7rbcp67	H1enoaZGfV	AKBC.ws/2019/Conference/-/Paper50/Official_Review	['AKBC.ws/2019/Conference/Paper50/Reviewers/Unsubmitted']	2		['everyone']	Hkg7rbcp67	['AKBC.ws/2019/Conference/Paper50/AnonReviewer2']	1546948004070		1547662963875	['AKBC.ws/2019/Conference']
826	1546852449037	"{'title': 'Important study on adversarial attacks on link prediction models', 'review': 'The paper presents two adversarial attacks for link prediction models: one adding a (wrong) fact to the knowledge base and one removing a (correct) fact from the knowledge base. Structure and writing style are good.\nThe method is well motivated and well described. The experiments demonstrate its effectiveness.\n\nI think the paper is an important step towards more robust models and would therefore be of general interest for not only link prediction research but a wider community.\nEspecially, I like the applications of using the approach for interpreting models and for detecting wrong facts in knowledge bases. It would be nice if the authors could also provide some ideas for future research directions, such as the prospects of using their approach for improving link prediction models (Table 5 suggests that there is room for improvement, not only regarding robustness but also regarding soundness of learned implications).\n\nSome issues:\n- Figure 1 (especially the red color in 1b) is hard to read and it took me a while to see the color and understand it. Without colors it is not readable at all. It would be easier if the authors chose a different presentation, e.g., a dashed line for the removed fact.\n- Why did the authors choose only multiplicative models? There approach should also work for additive models, or is there a limitation? It would be good if the authors could discuss that.\n- Table 3: The ""Uncertain Test"" looks interesting but it is model dependent, right? It would be good if the authors could provide more details here, i.e., how large is the test set for DistMult vs. ConvE, how is the distribution of relations, etc.\n- Table 3: The alignment of the numbers in the Table should be improved', 'rating': '7: Good paper, accept', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		Hkg7rbcp67	BylFwOqgGV	AKBC.ws/2019/Conference/-/Paper50/Official_Review	['AKBC.ws/2019/Conference/Paper50/Reviewers/Unsubmitted']	1		['everyone']	Hkg7rbcp67	['AKBC.ws/2019/Conference/Paper50/AnonReviewer1']	1546852449037		1547662963619	['AKBC.ws/2019/Conference']
827	1547004662037	{'title': 'A nice algorithmic contribution for integrating user feedback for KBC', 'review': 'This paper introduces a method to integrate user feedback into KBs in the presence of identity uncertainty, a problem that arises in the integration of new data in knowledge base construction. The proposed method represents user feedback as feedback mentions and uses an online algorithm for integrating these mentions into the KB.\n\nThe paper targets an important problem in knowledge base construction, i.e., integrating user feedback in the online setting. The proposed hierarchical model looks reasonable and effective. And overall, the work is well presented. \n\nThe paper makes an algorithmic contribution. The contribution is, however, limited from the perspective of human computation. The experiment uses simulated user feedback for evaluating the method. In real-world settings, user feedback can be skewed to certain types (e.g., negative feedback) or be noisy (so the feedback is not reliable). How would these affect the result?', 'rating': '7: Good paper, accept', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		SygLHbcapm	Bye0ls1mfV	AKBC.ws/2019/Conference/-/Paper51/Official_Review	['AKBC.ws/2019/Conference/Paper51/Reviewers/Unsubmitted']	1		['everyone']	SygLHbcapm	['AKBC.ws/2019/Conference/Paper51/AnonReviewer1']	1547004662037		1547662963402	['AKBC.ws/2019/Conference']
828	1544487028868	{'title': 'A good contribution of learning resources, but technically it is more like a practice of Transformer rather than proposing new techniques', 'review': 'This paper proposes the use of the multi-head attention encoder of the Transformer for neural relation extraction (NRE) from text. A large learning resource for NRE is introduced along with the proposed model, which is a great contribution. The proposed model performs impressively on FB15k-237 for KG completion.\nHowever, this work lacks technical contributions, and does not obtain a very satisfying result on the NRE task conducted on the introduced dataset.\n\nReasons to accept:\n1. A large learning resource for NRE is created based on ClueWeb, which contains more than 5 million cases.\n2. This paper is strong at proposing a large learning resource for NRE, and has conducted thorough experiments on two the two tasks of NRE and KB completion. Case study and analysis of distance metrics are also provided.\n3. The performance of the proposed model on FB15k-237 is impressive.\n\nReasons to reject:\n1. I feel that this paper lacks novel technical contributions. The learning architecture is the same as other works on NRE. The only difference is that the sentence encoder is substituted with Transformer, which has however already been a well-recognized technique.\n2. The proposed model does not obtain better performance than GloRE on the relation extraction task. ', 'rating': '5: Marginally below acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}		SklKr-qaT7	H1eadgY3J4	AKBC.ws/2019/Conference/-/Paper52/Official_Review	['AKBC.ws/2019/Conference/Paper52/Reviewers/Unsubmitted']	1		['everyone']	SklKr-qaT7	['AKBC.ws/2019/Conference/Paper52/AnonReviewer2']	1544487028868		1547662963176	['AKBC.ws/2019/Conference']
829	1546984504394	"{'title': 'Is the pre-training worth the effort?', 'review': 'This paper proposes a method for pre-training vector representations of textual relations, and then applying them to relational inference tasks.\n\nMy main concern with this work is that the results show a marginal improvement over the baselines, if at all. This is surprising, because one would expect pre-training on a massive corpus to yield substantial improvements, as seen in ELMo and BERT. Assuming that the model is indeed learning important information during pre-training, I think that adding perhaps more difficult benchmarks, such as zero-shot relation extraction (e.g. https://arxiv.org/abs/1706.04115), could help convince the reader that this kind of pre-training is worth the effort.\n\nI also think there is a missing experiment in 4.2 to support any claims that the proposed approach improves performance: if you combine labeled data (e.g. the kind used to train GloRE) with the proposed unsupervised approach, can we get better results than GloRE? If not, wouldn\'t that undermine the whole idea of pretraining?\n\nIn the related work, the authors write: ""To summarize, no previous work has targeted general embedding of textual relations by learning on large scale corpora.""\nI believe the authors are unaware of the work on hypernymy detection (e.g. https://arxiv.org/abs/1603.06076), and especially the more recent work on relation representation by Washio and Kato (https://arxiv.org/abs/1809.03411 & https://arxiv.org/abs/1809.03401), as well as some more contemporary work (https://arxiv.org/abs/1810.08854). It would be great to see an empirical comparison with these papers and a discussion of their differences from the proposed approach.\n\nI also found the loss function a bit weird. In reality, there is exactly one relation that each instance refers to, and it can (theoretically) be inferred from looking at the entities as well. The model is given these entities as input, but the loss is telling the model to ignore that information, and essentially misleading it to always predict some kind of entity-oblivious average. If you have (from distant supervision) the specific Freebase relation that this particular instance refers to, why not use it as the target?\n\nMy other concerns are mainly about writing. The paper is extremely verbose, and I think much of the content could be conveyed in half the page count. There are also some specific points that need to be clarified:\n- Intro: ""However, neither of these objectives is very meaningful for textual relation embedding."" - that\'s an unbased hypothesis, which I don\'t believe is true.\n- What is GloRE? Both a citation and a quick explanation are missing.\n- What is the difference from Su et al., 2017? This needs to be made crystal clear. Under my current interpretation, it\'s (a) using transformers instead of RNNs, (b) training on ClueWeb. These differences need to be properly ablated in the paper as well.\n- What exactly are the global co-occurrence statistics? From the textual description it seems like you\'re calculating P(r|t) = #(r, t) / #(*, t). Please be explicit; the paper needs to be self-contained without having to read Su et al., 2017.\n', 'rating': '4: Ok but not good enough - rejection', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		SklKr-qaT7	Bkgxrh9zfN	AKBC.ws/2019/Conference/-/Paper52/Official_Review	['AKBC.ws/2019/Conference/Paper52/Reviewers/Unsubmitted']	2		['everyone']	SklKr-qaT7	['AKBC.ws/2019/Conference/Paper52/AnonReviewer1']	1546984504394		1547662962964	['AKBC.ws/2019/Conference']
830	1547015060655	"{'title': 'Written well; not convinced about the effect of pre-training.', 'review': ""This paper builds on the work of Su et al., 2017 and applies it to learn embeddings of textual relations on a large scale web corpus (Clueweb09). By aligning, this large scale corpus with Freebase knowledge base, they obtain a relation graph containing over 5.5M textual relations aligned to around 1,925KB relation. They show very marginal improvements over two tasks (relation extraction over NYT dataset and KB Completion over FB15k-237 dataset). \n\n(+) The paper is executed well and is clearly written for the most part.\n\n(-) The main contribution of the paper is applying the work of Su et al., 2017 i.e. learning textual embeddings via global statistics, but on a larger dataset and use the pre-trained embeddings in related downstream task. This is a little weak in terms of novelty and makes it less interesting. \n(-) Even after training on the large corpus, the model only performs comparably or lesser to the original baseline model. For example, on the NYT dataset, PCNN+ATT+GloRE  still slightly performs better than the result of the paper (even though it is only trained on the NYT dataset) and on FB15k-237, the results are only marginally better. Thus the empirical results are a little weak.\n(-) I encountered problems understanding Sec 4.2. I am not sure why averaging would be the best thing to do in this case. Also do you fine tune the textual embeddings on the NYT dataset using objective in equation 8? This was not clear. Overall this section should be clearly written.\n\nMinor comments:\n\n(-) There should be a discussion regarding why the shortest dependency paths between two entities is the best way to represent textual relations.\n(-) In section 3.1, considering swapping the notation ‘r' and ’t’, ’t’ for textual relations would make it more readable in my opinion\n\nOther Question:\nWould there be any effort to make the gathered aligned dataset public or at least the relation graph. I think that could be a solid contribution of the paper if that would be possible."", 'rating': '5: Marginally below acceptance threshold', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		SklKr-qaT7	SyeT5Xz7zV	AKBC.ws/2019/Conference/-/Paper52/Official_Review	['AKBC.ws/2019/Conference/Paper52/Reviewers/Unsubmitted']	3		['everyone']	SklKr-qaT7	['AKBC.ws/2019/Conference/Paper52/AnonReviewer3']	1547015060655		1547662962703	['AKBC.ws/2019/Conference']
831	1547085341870	"{'title': 'Interesting direction, but no empirical evaluation.', 'review': ""The paper proposes to build a knowledge graph of food-related information with the motivation to help build systems to help generate food recommendations.  I think this is actually a really interesting direction at a high level, and I found the paper interesting to read, but I don't think this is a good fit for this venue as the paper doesn't appear to contain any empirical results.  The paper does describe embeddings that are learned and can provide a way to measure the similarity between different ingredients, however, the paper doesn't contain experiments to demonstrate the value of the embeddings."", 'rating': '4: Ok but not good enough - rejection', 'confidence': '4: The reviewer is confident but not absolutely certain that the evaluation is correct'}"		B1gjSZ9TpQ	HyxUQU7EM4	AKBC.ws/2019/Conference/-/Paper53/Official_Review	['AKBC.ws/2019/Conference/Paper53/Reviewers/Unsubmitted']	3		['everyone']	B1gjSZ9TpQ	['AKBC.ws/2019/Conference/Paper53/AnonReviewer2']	1547085341870		1547662962481	['AKBC.ws/2019/Conference']
832	1547074302623	"{'title': 'Interesting work but should be explained better', 'review': ""\nThe paper describes an approach for building a food knowledge graph starting from web materials about food and recipes in order to be used to generate food recommendations tailored to an individual’s dietary habits and preferences and answering questions on the composition of dishes, nutritional content of food items, substitutions, etc. The knowledge graph is also augmented by incorporating  entity resolution and relationship discovery. \n\n\nThe topic is interesting and  has many potentials.  The approach is rigorous and clearly explained in the various step. \nHowever, I have some concerns about the paper. The main one is related to the availability of the knowledge graph: I did not find a link in the paper, it is essential that it is shared and public, in order to be inspected. \n\nSecond, all the methodological choices made at each step should be motivated better (for example, why the use of cosine similarity and not other similarity or other mapping methods?).\n\nOne of the principle of semantic web is the reuse of knowledge.  The need of creating a food ontology (Figure 1) without reusing existing ones  should be motivated more. Again it should be public. Morever, I don't understand how it is used  (or is this the Foodon ontology listed on page 7? It is not clear to me).\n\nI'd like to see more explanations in relation to the implications of the work in term of possibile use for generating recommendations, that is one of the goal of the approach as stated in the abstract and in the introduction but not explained later on in the paper. \n\nThe evaluation of the relationship discovery it is very clear to me: how is it performed? how are the results? \n\nFinally, the structured of the paper can be improved, for example clarifying better in the introduction the starting problems, the goals and the contributions of the papers and putting all the related work in the related work section.\nMoreover, all the recipes' examples used should be explained better, not only in the figures but also in the text. "", 'rating': '6: Marginally above acceptance threshold', 'confidence': '3: The reviewer is fairly confident that the evaluation is correct'}"		B1gjSZ9TpQ	ryeD-ogVGV	AKBC.ws/2019/Conference/-/Paper53/Official_Review	['AKBC.ws/2019/Conference/Paper53/Reviewers/Unsubmitted']	2		['everyone']	B1gjSZ9TpQ	['AKBC.ws/2019/Conference/Paper53/AnonReviewer3']	1547074302623		1547662962267	['AKBC.ws/2019/Conference']
833	1546952201901	"{'title': 'Good but not surprising', 'review': 'In general, the paper presents a routing practice, that is, apply lifted probabilistic inference to rule learning over probabilistic KBs, such that the scalability of the system is enhanced but being applicable to a limited scope of rules only. I would not vote for reject if other reviewers agree to acceptance.\n\nSpecifically, the proposed algorithm SafeLearner extends ProbFOIL+ by using lifted probabilistic inference (instead of using grounding), which first applies AMIE+ to find candidate deterministic rules, and then jointly learns probabilities of the rules using lifted inference.\n\nThe paper is structured well, and most part of the paper is easy to follow.\n\nI have two major concerns with the motivation. It reads that there are two challenges associated with rule learning from probabilistic KBs, i.e., sparse and probabilistic nature. \n1) While two challenges are identified by the authors, but the paper deals with the latter issue only? How does sparsity affect the algorithm design?\n\n2) The paper can be better motivated, although there is one piece of existing work for learning probabilistic rules from KBs (De Raedt et al. [2015]). Somehow, I am not convinced by the potential application of the methods; that is, after generating the probabilistic rules, how can I apply the probabilistic rules? It will be appreciated if the authors can present some examples of the use of probabilistic rules. Moreover, if it is mainly to complete probabilistic KBs, how does this probabilistic logics based approach compare against embedding based approach?\n', 'rating': '6: Marginally above acceptance threshold', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		HkyI-5667	HJxMMAfMGV	AKBC.ws/2019/Conference/-/Paper54/Official_Review	['AKBC.ws/2019/Conference/Paper54/Reviewers/Unsubmitted']	2		['everyone']	HkyI-5667	['AKBC.ws/2019/Conference/Paper54/AnonReviewer1']	1546952201901		1547662961831	['AKBC.ws/2019/Conference']
834	1546882344207	"{'title': 'review', 'review': 'This paper was interesting and rather clearly written, as someone who didn\'t have much background in rule learning.\n\nSection 5.1.1.1 : ""(ii) tuples contained in the answer of Q\' where Q\' is the same as Q but without the rule with empty body, but not in the training set""  is unclear.\n\nIn the algorithm, line 22, aren\'t rules removed until H leads to a ""safe"" UCQ ?\n\nSection 6.1 : ""In line 7 we formulate a UCQ Q from all the candidate rules in H (explained in 5.2 with an example)"". I was unable to find the example in section 5.2 \n\nIt would be interesting to have an idea of the maximum scale that ProbFoil+ can handle, since it seems to be the only competitor to the suggested method. \n\nIn section 7.2 does the ""learning time"" include the call to AMIE+ ? if not, it would be interesting to break down the time into its deterministic and learning components, since the former is only necessary to retrieve the correct probabilities.\n\nBeing new to this subject, I found the paper to be somewhat clear. However, I found that it was sometimes hard to understand what was a part of the proposed system, and what was done in Amie+ or Slimshot.\n\nFor example, ""But, before calling Slimshot\nfor lifted inference on the whole query, we first break down the query to independent subqueries such that no variable is common in more than one sub-query. Then, we perform\ninference separately over it and later unify the sub-queries to get the desired result.""\n\nThis is described as important to the speedup over ProbFoil+ in the conclusion, yet doesn\'t appear in Algorithm 1. \n\nSimilarly, ""it caches the structure of queries before doing inference "" is mentioned in the conclusion but I couldn\'t map it to anything in Algorithm 1 or in the paper.\n\n\nI lean toward an accept because the work seems solid, but I feel like I don\'t have the background required to judge on the contributions of this paper, which seems to me like a good use of Amie+/Slimshot with a reasonable addition of SGD to learn rule weights. Some of the components which are sold as important for the speed-up in the conclusion aren\'t clear enough in the main text. Some numbers to experimentally back-up how important these additions are to the algorithms would be welcome. \n', 'rating': '6: Marginally above acceptance threshold', 'confidence': ""1: The reviewer's evaluation is an educated guess""}"		HkyI-5667	r1eeNpZbzV	AKBC.ws/2019/Conference/-/Paper54/Official_Review	['AKBC.ws/2019/Conference/Paper54/Reviewers/Unsubmitted']	1		['everyone']	HkyI-5667	['AKBC.ws/2019/Conference/Paper54/AnonReviewer3']	1546882344207		1547662961605	['AKBC.ws/2019/Conference']
835	1538087883067	{'title': 'Complement Objective Training', 'abstract': 'Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to adversarial attacks.\n', 'keywords': ['optimization', 'entropy', 'image recognition', 'natural language understanding', 'adversarial attacks', 'deep learning'], 'authorids': ['haoyunchen@gapp.nthu.edu.tw', 'peihsin@gapp.nthu.edu.tw', 'newgod1992@gapp.nthu.edu.tw', 'scchang@cs.nthu.edu.tw', 'jypan@google.com', 'yutingchen@google.com', 'wewei@google.com', 'dacheng@google.com'], 'authors': ['Hao-Yun Chen', 'Pei-Hsin Wang', 'Chun-Hao Liu', 'Shih-Chieh Chang', 'Jia-Yu Pan', 'Yu-Ting Chen', 'Wei Wei', 'Da-Cheng Juan'], 'TL;DR': 'We propose Complement Objective Training (COT), a new training paradigm that optimizes both the primary and complement objectives for effectively learning the parameters of neural networks.', 'pdf': '/pdf/d775d0529739857b1307f413cc8580f995fd24c7.pdf', 'paperhash': 'chen|complement_objective_training', '_bibtex': '@inproceedings{\nchen2018complement,\ntitle={Complement Objective Training},\nauthor={Hao-Yun Chen and Pei-Hsin Wang and Chun-Hao Liu and Shih-Chieh Chang and Jia-Yu Pan and Yu-Ting Chen and Wei Wei and Da-Cheng Juan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyM7AiA5YX},\n}'}		HyM7AiA5YX	HyM7AiA5YX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	880	HJgb2VkcK7	['everyone']		['ICLR.cc/2019/Conference']	1538087883067		1547644776886	['ICLR.cc/2019/Conference']
836	1547634853436	{'title': 'g(z)=0 alternatives!', 'comment': 'You can choose g(z) to be a neural network or a pre-fixed analytical function.\n\nBy the way, feel free to contact us directly via email\n'}		SkgiX2Aqtm	rJlas_tnzN	ICLR.cc/2019/Conference/-/Paper1388/Official_Comment	['ICLR.cc/2019/Conference/Paper1388/Reviewers/Unsubmitted']	9		['everyone']	S1xGzSv2G4	['ICLR.cc/2019/Conference/Paper1388/Authors']	1547634853436		1547634853436	['ICLR.cc/2019/Conference/Paper1388/Authors', 'ICLR.cc/2019/Conference']
837	1547625737814	{'comment': 'setting g(z)=0 means PIE is doing denoising variational pseudo inverse\nis there better alternative other than setting g(z)=0?\n', 'title': 'g(z)=0 alternatives?'}		SkgiX2Aqtm	S1xGzSv2G4	ICLR.cc/2019/Conference/-/Paper1388/Public_Comment	[]	6		['everyone']	Hyxvo6Osf4	['(anonymous)']	1547625737814		1547625737814	['(anonymous)', 'ICLR.cc/2019/Conference']
838	1538087931828	"{'title': 'Minimal Images in Deep Neural Networks: Fragile Object Recognition in Natural Images', 'abstract': 'The human ability to recognize objects is impaired when the object is not shown in full. ""Minimal images"" are the smallest regions of an image that remain recognizable for humans. Ullman et al. (2016) show that a slight modification of the location and size of the visible region of the minimal image produces a sharp drop in human recognition accuracy. In this paper, we demonstrate that such drops in accuracy due to changes of the visible region are a common phenomenon between humans and existing state-of-the-art deep neural networks (DNNs), and are much more prominent in DNNs. We found many cases where DNNs classified one region correctly and the other incorrectly, though they only differed by one row or column of pixels, and were often bigger than the average human minimal image size. We show that this phenomenon is independent from previous works that have reported lack of invariance to minor modifications in object location in DNNs. Our results thus reveal a new failure mode of DNNs that also affects humans to a much lesser degree. They expose how fragile DNN recognition ability is in natural images even without adversarial patterns being introduced. Bringing the robustness of DNNs in natural images to the human level remains an open challenge for the community. ', 'keywords': [], 'authorids': ['sanjanas@mit.edu', 'gby@csail.mit.edu', 'xboix@mit.edu'], 'authors': ['Sanjana Srivastava', 'Guy Ben-Yosef', 'Xavier Boix'], 'pdf': '/pdf/b2361cc9688c8ac5cbfeb9756d3244a4c1d30c2b.pdf', 'paperhash': 'srivastava|minimal_images_in_deep_neural_networks_fragile_object_recognition_in_natural_images', '_bibtex': '@inproceedings{\nsrivastava2018minimal,\ntitle={Minimal Images in Deep Neural Networks: Fragile Object Recognition in Natural Images},\nauthor={Sanjana Srivastava and Guy Ben-Yosef and Xavier Boix},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xNb2A9YX},\n}'}"		S1xNb2A9YX	S1xNb2A9YX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1161	HJgDw6icFQ	['everyone']		['ICLR.cc/2019/Conference']	1538087931828		1547615969851	['ICLR.cc/2019/Conference']
839	1538087905357	{'title': 'ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech', 'abstract': 'In this work, we propose a new solution for parallel wave generation by WaveNet.  In contrast to parallel WaveNet (van Oord et al., 2018), we distill a Gaussian inverse autoregressive flow from the autoregressive WaveNet by minimizing a regularized KL divergence between their highly-peaked output distributions. Our method computes the KL divergence in closed-form, which simplifies the training algorithm and provides very efficient distillation. In addition, we introduce the first text-to-wave neural architecture for speech synthesis, which is fully convolutional and enables fast end-to-end training from scratch. It significantly outperforms the previous pipeline that connects a text-to-spectrogram model to a separately trained WaveNet (Ping et al., 2018). We also successfully distill a parallel waveform synthesizer conditioned on the hidden representation in this end-to-end model.', 'keywords': ['text-to-speech', 'deep generative models', 'end-to-end', 'text to waveform'], 'authorids': ['pingwei01@baidu.com', 'pengkainan@baidu.com', 'jitongc@gmail.com'], 'authors': ['Wei Ping', 'Kainan Peng', 'Jitong Chen'], 'pdf': '/pdf/9c44be7b1472021adcfd03b47114d3a47ff7951b.pdf', 'paperhash': 'ping|clarinet_parallel_wave_generation_in_endtoend_texttospeech', '_bibtex': '@inproceedings{\nping2018clarinet,\ntitle={ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech},\nauthor={Wei Ping and Kainan Peng and Jitong Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HklY120cYm},\n}'}		HklY120cYm	HklY120cYm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1006	S1gBV3LFYX	['everyone']		['ICLR.cc/2019/Conference']	1538087905357		1547613923593	['ICLR.cc/2019/Conference']
840	1538087746319	"{'title': 'Hierarchical interpretations for neural network predictions', 'abstract': ""Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method: agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. We introduce ACD using examples from Stanford Sentiment Treebank and ImageNet, in order to diagnose incorrect predictions, identify dataset bias, and extract polarizing phrases of varying lengths. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise."", 'keywords': ['interpretability', 'natural language processing', 'computer vision'], 'authorids': ['chandan_singh@berkeley.edu', 'jmurdoch@berkeley.edu', 'binyu@berkeley.edu'], 'authors': ['Chandan Singh', 'W. James Murdoch', 'Bin Yu'], 'TL;DR': 'We introduce and validate hierarchical local interpretations, the first technique to automatically search for and display important interactions for individual predictions made by LSTMs and CNNs.', 'pdf': '/pdf/edd9240b121ad8ce494d42bb4ce31f44f4d8d045.pdf', 'paperhash': 'singh|hierarchical_interpretations_for_neural_network_predictions', '_bibtex': '@inproceedings{\nsingh2018hierarchical,\ntitle={Hierarchical interpretations for neural network predictions},\nauthor={Chandan Singh and W. James Murdoch and Bin Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkEqro0ctQ},\n}'}"		SkEqro0ctQ	SkEqro0ctQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	113	rkeJSn9KFm	['everyone']		['ICLR.cc/2019/Conference']	1538087746319		1547604597679	['ICLR.cc/2019/Conference']
841	1547602516905	"{'title': 'Other components besides the convolutions and self-attentions are also important', 'comment': 'One of our contributions is to better understand the importance of self-attention which is often perceived as the most important design choice in the architecture of Vaswani et al. \nTable 3 of our paper shows that self-attention alone only accounts for a small portion of the improvement of Vaswani et al. over previous work, e.g., row 6 in Table 3 ""CNN Depthwise + Increasing kernel"" uses the depthwise convolution of Kaiser et al. (2017) and it is only 0.5 BLEU behind our reimplementation of Vaswani et al.\nTherefore, modeling choices other than self-attention contribute a very large fraction of the improvement of Vaswani et al. over other work. In early experiments, we found that FFN blocks between the self-attention module in the Transformer are very important. '}"		SkVhlh09tX	Bkxa89b2G4	ICLR.cc/2019/Conference/-/Paper1115/Official_Comment	['ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted']	12		['everyone']	H1gWfxVozE	['ICLR.cc/2019/Conference/Paper1115/Authors']	1547602516905		1547602516905	['ICLR.cc/2019/Conference/Paper1115/Authors', 'ICLR.cc/2019/Conference']
842	1538087745357	{'title': 'DPSNet: End-to-end Deep Plane Sweep Stereo', 'abstract': 'Multiview stereo aims to reconstruct scene depth from images acquired by a camera under arbitrary motion. Recent methods address this problem through deep learning, which can utilize semantic cues to deal with challenges such as textureless and reflective regions. In this paper, we present a convolutional neural network called DPSNet (Deep Plane Sweep Network) whose design is inspired by best practices of traditional geometry-based approaches. Rather than directly estimating depth and/or optical flow correspondence from image pairs as done in many previous deep learning methods, DPSNet takes a plane sweep approach that involves building a cost volume from deep features using the plane sweep algorithm, regularizing the cost volume via a context-aware cost aggregation, and regressing the depth map from the cost volume. The cost volume is constructed using a differentiable warping process that allows for end-to-end training of the network. Through the effective incorporation of conventional multiview stereo concepts within a deep learning framework, DPSNet achieves state-of-the-art reconstruction results on a variety of challenging datasets.', 'keywords': ['Deep Learning', 'Stereo', 'Depth', 'Geometry'], 'authorids': ['dlarl8927@kaist.ac.kr', 'haegonj@andrew.cmu.edu', 'stevelin@microsoft.com', 'iskweon77@kaist.ac.kr'], 'authors': ['Sunghoon Im', 'Hae-Gon Jeon', 'Stephen Lin', 'In So Kweon'], 'TL;DR': 'A convolution neural network for multi-view stereo matching whose design is inspired by best practices of traditional geometry-based approaches', 'pdf': '/pdf/62e139f362828405640e9afdf740eb468debad26.pdf', 'paperhash': 'im|dpsnet_endtoend_deep_plane_sweep_stereo', '_bibtex': '@inproceedings{\nim2018dpsnet,\ntitle={{DPSN}et: End-to-end Deep Plane Sweep Stereo},\nauthor={Sunghoon Im and Hae-Gon Jeon and Stephen Lin and In So Kweon},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeYHi0ctQ},\n}'}		ryeYHi0ctQ	ryeYHi0ctQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	108	BylpuqDvKX	['everyone']		['ICLR.cc/2019/Conference']	1538087745357		1547600609882	['ICLR.cc/2019/Conference']
843	1547592708890	{'comment': 'Hi, \n\nThis is indeed a well written paper and gives an interesting perspective on low distortion embeddings into Wasserstein space.\n\nI wanted to point out that our work https://arxiv.org/abs/1808.09663 (and the openreview version from june last year https://openreview.net/pdf?id=Bkx2jd4Nx7), has considered representing entities (such as words, sentences, etc) as (discrete) distribution over their contexts, where the contexts are essentially point embeddings in a low-dimensional space. We use optimal transport/Wasserstein distance to compare & compose entities and this is approximated via the entropic regularization/Sinkhorn divergence (Cuturi, 2013). \n\nIt would be great if you could maybe update your paper to reflect this and include a reference to our work.  \n\nThanks for your time. \n\nBest,\nSidak', 'title': 'Regarding work on embedding with discrete distributions in Wasserstein space'}		rJg4J3CqFm	Hkx6WVknz4	ICLR.cc/2019/Conference/-/Paper978/Public_Comment	[]	3		['everyone']	rJg4J3CqFm	['~Sidak_Pal_Singh1']	1547592708890		1547592751733	['~Sidak_Pal_Singh1', 'ICLR.cc/2019/Conference']
844	1547589545577	{'comment': 'Congrats for the acceptance! According to the description in paper, I assume when applying AES on an ensemble of 5 with k=10, every member in the ensemble has 10 snapshots and 50 forward passes are needed in total? Is this correct? Thanks!', 'title': 'Question on the implementation of AES with ensemble'}		SJfb5jCqKm	SJeMnP0ozN	ICLR.cc/2019/Conference/-/Paper510/Public_Comment	[]	2		['everyone']	Hyed9FrMg4	['(anonymous)']	1547589545577		1547589545577	['(anonymous)', 'ICLR.cc/2019/Conference']
845	1538087835079	{'title': 'h-detach: Modifying the LSTM Gradient Towards Better Optimization', 'abstract': 'Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP). This problem becomes more evident in tasks where the information needed to correctly solve them exist over long time scales, because EVGP prevents important gradient components from being back-propagated adequately over a large number of steps. We introduce a simple stochastic algorithm (\\textit{h}-detach) that is specific to LSTM optimization and targeted towards addressing this problem. Specifically, we show that when the LSTM weights are large, the gradient components through the linear path (cell state) in the LSTM computational graph get suppressed. Based on the hypothesis that these components carry information about long term dependencies (which we show empirically), their suppression can prevent LSTMs from capturing them. Our algorithm\\footnote{Our code is available at https://github.com/bhargav104/h-detach.} prevents gradients flowing through this path from getting suppressed, thus allowing the LSTM to capture such dependencies better. We show significant improvements over vanilla LSTM gradient based training in terms of convergence speed, robustness to seed and learning rate, and generalization using our modification of LSTM gradient on various benchmark datasets.', 'keywords': ['LSTM', 'Optimization', 'Long term dependencies', 'Back-propagation through time'], 'authorids': ['bhargavkanuparthi25@gmail.com', 'devansharpit@gmail.com', 'giancarlo.kerg@gmail.com', 'rosemary.nan.ke@gmail.com', 'ioannis@iro.umontreal.ca', 'yoshua.umontreal@gmail.com'], 'authors': ['Bhargav Kanuparthi', 'Devansh Arpit', 'Giancarlo Kerg', 'Nan Rosemary Ke', 'Ioannis Mitliagkas', 'Yoshua Bengio'], 'TL;DR': 'A simple algorithm to improve optimization and handling of long term dependencies in LSTM', 'pdf': '/pdf/12ad6196127f084a8e473d83361f0697b9673d4b.pdf', 'paperhash': 'kanuparthi|hdetach_modifying_the_lstm_gradient_towards_better_optimization', '_bibtex': '@inproceedings{\nkanuparthi2018hdetach,\ntitle={h-detach: Modifying the {LSTM} Gradient Towards Better Optimization},\nauthor={Bhargav Kanuparthi and Devansh Arpit and Giancarlo Kerg and Nan Rosemary Ke and Ioannis Mitliagkas and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryf7ioRqFX},\n}'}		ryf7ioRqFX	ryf7ioRqFX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	609	SylUa_35tQ	['everyone']		['ICLR.cc/2019/Conference']	1538087835079		1547579054329	['ICLR.cc/2019/Conference']
846	1538087876585	{'title': 'Neural Probabilistic Motor Primitives for Humanoid Control', 'abstract': 'We focus on the problem of learning a single motor module that can flexibly express a range of behaviors for the control of high-dimensional physically simulated humanoids. To do this, we propose a motor architecture that has the general structure of an inverse model with a latent-variable bottleneck. We show that it is possible to train this model entirely offline to compress thousands of expert policies and learn a motor primitive embedding space. The trained neural probabilistic motor primitive system can perform one-shot imitation of whole-body humanoid behaviors, robustly mimicking unseen trajectories. Additionally, we demonstrate that it is also straightforward to train controllers to reuse the learned motor primitive space to solve tasks, and the resulting movements are relatively naturalistic. To support the training of our model, we compare two approaches for offline policy cloning, including an experience efficient method which we call linear feedback policy cloning. We encourage readers to view a supplementary video (https://youtu.be/CaDEf-QcKwA ) summarizing our results.', 'keywords': ['Motor Primitives', 'Distillation', 'Reinforcement Learning', 'Continuous Control', 'Humanoid Control', 'Motion Capture', 'One-Shot Imitation'], 'authorids': ['jsmerel@google.com', 'leonardh@google.com', 'agalashov@google.com', 'arahuja@google.com', 'vuph@google.com', 'gregwayne@google.com', 'ywteh@google.com', 'heess@google.com'], 'authors': ['Josh Merel', 'Leonard Hasenclever', 'Alexandre Galashov', 'Arun Ahuja', 'Vu Pham', 'Greg Wayne', 'Yee Whye Teh', 'Nicolas Heess'], 'TL;DR': 'Neural Probabilistic Motor Primitives compress motion capture tracking policies into one flexible model capable of one-shot imitation and reuse as a low-level controller.', 'pdf': '/pdf/bbc8c6572187d3850df32c814646ceaa8d5d09db.pdf', 'paperhash': 'merel|neural_probabilistic_motor_primitives_for_humanoid_control', '_bibtex': '@inproceedings{\nmerel2018neural,\ntitle={Neural Probabilistic Motor Primitives for Humanoid Control},\nauthor={Josh Merel and Leonard Hasenclever and Alexandre Galashov and Arun Ahuja and Vu Pham and Greg Wayne and Yee Whye Teh and Nicolas Heess},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJl6TjRcY7},\n}'}		BJl6TjRcY7	BJl6TjRcY7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	843	Hyl_T8KcF7	['everyone']		['ICLR.cc/2019/Conference']	1538087876585		1547574434058	['ICLR.cc/2019/Conference']
847	1538087777041	{'title': 'Hierarchical Visuomotor Control of Humanoids', 'abstract': 'We aim to build complex humanoid agents that integrate perception, motor control, and memory. In this work, we partly factor this problem into low-level motor control from proprioception and high-level coordination of the low-level skills informed by vision. We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies. The resulting system is able to control a physically-simulated humanoid body to solve tasks that require coupling visual perception from an unstabilized egocentric RGB camera during locomotion in the environment. Supplementary video link:  https://youtu.be/fBoir7PNxPk', 'keywords': ['hierarchical reinforcement learning', 'motor control', 'motion capture'], 'authorids': ['jsmerel@google.com', 'arahuja@google.com', 'vuph@google.com', 'stunya@google.com', 'liusiqi@google.com', 'dhruvat@google.com', 'heess@google.com', 'gregwayne@google.com'], 'authors': ['Josh Merel', 'Arun Ahuja', 'Vu Pham', 'Saran Tunyasuvunakool', 'Siqi Liu', 'Dhruva Tirumala', 'Nicolas Heess', 'Greg Wayne'], 'pdf': '/pdf/c64a5a1a7586ab5357231dd332bb03f3b34fc184.pdf', 'paperhash': 'merel|hierarchical_visuomotor_control_of_humanoids', 'TL;DR': 'Solve tasks involving vision-guided humanoid locomotion, reusing locomotion behavior from motion capture data.', '_bibtex': '@inproceedings{\nmerel2018hierarchical,\ntitle={Hierarchical Visuomotor Control of Humanoids},\nauthor={Josh Merel and Arun Ahuja and Vu Pham and Saran Tunyasuvunakool and Siqi Liu and Dhruva Tirumala and Nicolas Heess and Greg Wayne},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJfYvo09Y7},\n}'}		BJfYvo09Y7	BJfYvo09Y7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	283	SylwuHOcFX	['everyone']		['ICLR.cc/2019/Conference']	1538087777041		1547572962626	['ICLR.cc/2019/Conference']
848	1538087749281	{'title': 'Adversarial Domain Adaptation for Stable Brain-Machine Interfaces', 'abstract': 'Brain-Machine Interfaces (BMIs) have recently emerged as a clinically viable option\nto restore voluntary movements after paralysis. These devices are based on the\nability to extract information about movement intent from neural signals recorded\nusing multi-electrode arrays chronically implanted in the motor cortices of the\nbrain. However, the inherent loss and turnover of recorded neurons requires repeated\nrecalibrations of the interface, which can potentially alter the day-to-day\nuser experience. The resulting need for continued user adaptation interferes with\nthe natural, subconscious use of the BMI. Here, we introduce a new computational\napproach that decodes movement intent from a low-dimensional latent representation\nof the neural data. We implement various domain adaptation methods\nto stabilize the interface over significantly long times. This includes Canonical\nCorrelation Analysis used to align the latent variables across days; this method\nrequires prior point-to-point correspondence of the time series across domains.\nAlternatively, we match the empirical probability distributions of the latent variables\nacross days through the minimization of their Kullback-Leibler divergence.\nThese two methods provide a significant and comparable improvement in the performance\nof the interface. However, implementation of an Adversarial Domain\nAdaptation Network trained to match the empirical probability distribution of the\nresiduals of the reconstructed neural signals outperforms the two methods based\non latent variables, while requiring remarkably few data points to solve the domain\nadaptation problem.', 'keywords': ['Brain-Machine Interfaces', 'Domain Adaptation', 'Adversarial Networks'], 'authorids': ['a-farshchiansadegh@northwestern.edu', 'juan.gallego@northwestern.edu', 'joseph@josephpcohen.com', 'yoshua.bengio@umontreal.ca', 'lm@northwestern.edu', 'solla@northwestern.edu'], 'authors': ['Ali Farshchian', 'Juan A. Gallego', 'Joseph P. Cohen', 'Yoshua Bengio', 'Lee E. Miller', 'Sara A. Solla'], 'TL;DR': 'We implement an adversarial domain adaptation network to stabilize a fixed Brain-Machine Interface against gradual changes in the recorded neural signals.', 'pdf': '/pdf/42b1af648df0cfebe9b454256f895b390736f949.pdf', 'paperhash': 'farshchian|adversarial_domain_adaptation_for_stable_brainmachine_interfaces', '_bibtex': '@inproceedings{\nfarshchian2018adversarial,\ntitle={{ADVERSARIAL} {DOMAIN} {ADAPTATION} {FOR} {STABLE} {BRAIN}-{MACHINE} {INTERFACES}},\nauthor={Ali Farshchian and Juan A. Gallego and Joseph P. Cohen and Yoshua Bengio and Lee E. Miller and Sara A. Solla},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyx6Bi0qYm},\n}'}		Hyx6Bi0qYm	Hyx6Bi0qYm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	129	rJexke5FYX	['everyone']		['ICLR.cc/2019/Conference']	1538087749281		1547572913872	['ICLR.cc/2019/Conference']
849	1547571825336	{'title': 'Re: Still some issues', 'comment': '\nWe thank the reviewer for pointing out the notational mistake, wherein e() was maintained in several in-text instances even after it was purged from the equations. We will correct this in the final version.\n\nRegarding the confusion about Q(lambda) and the IMPALA architecture, we draw distinctions between the type of policy (deterministic vs. stochastic), the reinforcement learning algorithm, the data-gathering strategy, and the particular architecture of function approximators used to represent the policy.\n\nWe train a greedy, deterministic agent which represents its Q function using a feedforward network that drives a recurrent memory. While the extension of Q-learning agents to the recurrent case is straightforward (especially in the case of relatively short episodes such as ours, where the LSTM can be fully unrolled in time), we note now that it was previously explored by Hausknecht & Stone (2015). We will cite this work in the final version for the sake of fair attribution and added clarity.\n\nThere are two orthogonal similarities between the agents used in our experiments and IMPALA. The first is that we gather experience for our replay buffer in a distributed manner with a centralized learner, resembling IMPALA’s approach at a high level. The second is with regards to the function approximators chosen: our network architectures are identical to those employed in the IMPALA work, except that rather than policy and state-value output layers, an output layer based on equation (6) computes action-values (the previous reward and action are also omitted as inputs). Q(lambda) is then used to compute the targets for this layer.\n\nWe stress that the choice of experience-gathering strategy, the network architecture (including the use of an LSTM), and even the use of Q(lambda) targets are implementation choices that are not central to DISCERN’s contribution.'}		r1eVMnA9K7	ByxK_M5iMV	ICLR.cc/2019/Conference/-/Paper1256/Official_Comment	['ICLR.cc/2019/Conference/Paper1256/Reviewers/Unsubmitted']	36		['everyone']	rkxLYyujMN	['ICLR.cc/2019/Conference/Paper1256/Authors']	1547571825336		1547571825336	['ICLR.cc/2019/Conference/Paper1256/Authors', 'ICLR.cc/2019/Conference']
850	1547566495383	{'title': 'Good!', 'comment': 'You are welcome!'}		SkgiX2Aqtm	Hyxvo6Osf4	ICLR.cc/2019/Conference/-/Paper1388/Official_Comment	['ICLR.cc/2019/Conference/Paper1388/Reviewers/Unsubmitted']	8		['everyone']	B1gy49uiMN	['ICLR.cc/2019/Conference/Paper1388/Authors']	1547566495383		1547566495383	['ICLR.cc/2019/Conference/Paper1388/Authors', 'ICLR.cc/2019/Conference']
851	1547565606613	"{'comment': 'I tested on CIFAR-10, it works like denoising:\nIn early training:\nthe recovered (inverted feature) images have black parts (the residual, torch.zeros_like(r), not fully trained)\n\nIn later epochs:\nreplacing residual output with torch.zeros_like(r), then invert back to image, does image completion (black parts before are filled with some pixels), not 100% accurate, but plausible! hence ""pseudo inverse""\n\nThank you for your time to explain!', 'title': 'It works!'}"		SkgiX2Aqtm	B1gy49uiMN	ICLR.cc/2019/Conference/-/Paper1388/Public_Comment	[]	5		['everyone']	rJgFlgSKGE	['(anonymous)']	1547565606613		1547565681436	['(anonymous)', 'ICLR.cc/2019/Conference']
852	1547562877720	"{'title': 'Still some issues', 'comment': ""I'm currently reading the paper and found a few points that deserve clarification.\n\n* In Section 4, the relationship between $e()$ and $\\xhi_\\phi$ is not explicitly written. The authors probably mean $e(s) = \\xi_\\phi(h(s))^T\\xi_\\phi(h(s))$.\n\nIf this is so, rather than defining $l_g$ as they do, they could rewrite (5) as\n\n$$... = log \\frac{exp(\\beta e(s_g))}{exp(\\beta e(s_g)) + \\sum^K exp(e(d_k))}$$.\n\n* In Section 4 the authors say that Q is trained with $Q(\\lambda)$, but in Appendix A2 they describe something more complicated related to IMPALA and using an LSTM. Where is the truth?\n\nI would be glad to see these points fixed in the final version of the paper (or the arxiv one)""}"		r1eVMnA9K7	rkxLYyujMN	ICLR.cc/2019/Conference/-/Paper1256/Official_Comment	['ICLR.cc/2019/Conference/Paper1256/Reviewers/Unsubmitted']	35		['everyone']	r1eVMnA9K7	['ICLR.cc/2019/Conference/Paper1256/AnonReviewer3']	1547562877720		1547562877720	['ICLR.cc/2019/Conference/Paper1256/AnonReviewer3', 'ICLR.cc/2019/Conference']
853	1538087900448	{'title': 'Emerging Disentanglement in Auto-Encoder Based Unsupervised Image Content Transfer', 'abstract': 'We study the problem of learning to map, in an unsupervised way, between domains $A$ and $B$, such that the samples $\\vb \\in B$ contain all the information that exists in samples $\\va\\in A$ and some additional information. For example, ignoring occlusions, $B$ can be people with glasses, $A$ people without, and the glasses, would be the added information. When mapping a sample $\\va$ from the first domain to the other domain, the missing information is replicated from an independent reference sample $\\vb\\in B$. Thus, in the above example, we can create, for every person without glasses a version with the glasses observed in any face image. \n\nOur solution employs a single two-pathway encoder and a single decoder for both domains. The common part of the two domains and the separate part are encoded as two vectors, and the separate part is fixed at zero for domain $A$. The loss terms are minimal and involve reconstruction losses for the two domains and a domain confusion term. Our analysis shows that under mild assumptions, this architecture, which is much simpler than the literature guided-translation methods, is enough to ensure disentanglement between the two domains. We present convincing results in a few visual domains, such as no-glasses to glasses, adding facial hair based on a reference image, etc.', 'keywords': ['Image-to-image Translation', 'Disentanglement', 'Autoencoders', 'Faces'], 'authorids': ['theoripress@gmail.com', 'tomer22g@gmail.com', 'sagiebenaim@gmail.com', 'wolf@fb.com'], 'authors': ['Ori Press', 'Tomer Galanti', 'Sagie Benaim', 'Lior Wolf'], 'TL;DR': 'An image to image translation method which adds to one image the content of another thereby creating a new image.', 'pdf': '/pdf/ea305d5777dc2a4201ed7e386cd0a37a9bc5c586.pdf', 'paperhash': 'press|emerging_disentanglement_in_autoencoder_based_unsupervised_image_content_transfer', '_bibtex': '@inproceedings{\npress2018emerging,\ntitle={Emerging Disentanglement in Auto-Encoder Based Unsupervised Image Content Transfer},\nauthor={Ori Press and Tomer Galanti and Sagie Benaim and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BylE1205Fm},\n}'}		BylE1205Fm	BylE1205Fm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	980	SJgV3JL5Km	['everyone']		['ICLR.cc/2019/Conference']	1538087900448		1547560383310	['ICLR.cc/2019/Conference']
854	1547556479032	{'title': 'A quick reply. ', 'comment': 'In our experiments, we have also found that GP sometimes leads to training divergence, we suggest using MaxGP, which we have found usually can solve this problem. \n\nFixing discriminator and training generator is a well-defined optimization problem, but it is less interesting we think. Because it is merely finding the optimal x, the x that holds the maximum D(x). Or more strictly, optimizing all x towards the local optimal. This is kind of simulating the typical cause of mode collapse problem in GANs.  \n\nGiven fixed P_r and P_g and training the discriminator is estimating the given distance metric, say Wasserstein distance, between P_r and P_g, which is a sound optimization problem we believe. And we suspect that being hard to get the optimal discriminative function is one important cause of why GAN is currently not easy to train. This is also what motivates the authors to study optimizer. \n\nIf the memory is not wrong, we have achieved Inception Score around 8.0 with AdaShift (comparable with Adam). For more experiment details, we will post another response in a few weeks. \n'}		HkgTkhRcKQ	rygwYIIiGV	ICLR.cc/2019/Conference/-/Paper1028/Official_Comment	['ICLR.cc/2019/Conference/Paper1028/Reviewers/Unsubmitted']	10		['everyone']	rkgAZSb9ME	['ICLR.cc/2019/Conference/Paper1028/Authors']	1547556479032		1547556830562	['ICLR.cc/2019/Conference/Paper1028/Authors', 'ICLR.cc/2019/Conference']
855	1538087938212	"{'title': 'Stochastic Gradient/Mirror Descent: Minimax Optimality and Implicit Regularization', 'abstract': 'Stochastic descent methods (of the gradient and mirror varieties) have become increasingly popular in optimization. In fact, it is now widely recognized that the success of deep learning is not only due to the special deep architecture of the models, but also due to the behavior of the stochastic descent methods used, which play a key role in reaching ""good"" solutions that generalize well to unseen data. In an attempt to shed some light on why this is the case, we revisit some minimax properties of stochastic gradient descent (SGD) for the square loss of linear models---originally developed in the 1990\'s---and extend them to \\emph{general} stochastic mirror descent (SMD) algorithms for \\emph{general} loss functions and \\emph{nonlinear} models. \nIn particular, we show that there is a fundamental identity which holds for SMD (and SGD) under very general conditions, and which implies the minimax optimality of SMD (and SGD) for sufficiently small step size, and for a general class of loss functions and general nonlinear models.\nWe further show that this identity can be used to naturally establish other properties of SMD (and SGD), namely convergence and \\emph{implicit regularization} for over-parameterized linear models (in what is now being called the ""interpolating regime""), some of which have been shown in certain cases in prior literature. We also argue how this identity can be used in the so-called ""highly over-parameterized"" nonlinear setting (where the number of parameters far exceeds the number of data points) to provide insights into why SMD (and SGD) may have similar convergence and implicit regularization properties for deep learning. ', 'keywords': ['optimization', 'stochastic gradient descent', 'mirror descent', 'implicit regularization', 'deep learning theory'], 'authorids': ['azizan@caltech.edu', 'hassibi@caltech.edu'], 'authors': ['Navid Azizan', 'Babak Hassibi'], 'pdf': '/pdf/75986eb7da72eb30cb4574e9521d629cdf1efef1.pdf', 'paperhash': 'azizan|stochastic_gradientmirror_descent_minimax_optimality_and_implicit_regularization', '_bibtex': '@inproceedings{\nazizan2018stochastic,\ntitle={Stochastic Gradient/Mirror Descent: Minimax Optimality and Implicit Regularization},\nauthor={Navid Azizan and Babak Hassibi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJf9ZhC9FX},\n}'}"		HJf9ZhC9FX	HJf9ZhC9FX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1199	rkgx2_pcYX	['everyone']		['ICLR.cc/2019/Conference']	1538087938212		1547550859035	['ICLR.cc/2019/Conference']
856	1547546632659	"{'comment': ""I'm not sure if I have fully-understand your great work.\nIn my opinion, the difference between you and (Kaiser et al., 2017) is the softmax-normalized and share weights over the channel dimension. And you use these two mechanism not only reduce the number of parameter, but increase the result greatly(from 26.1 to 28.9). \nSo can you explain why these two mechanism is so useful?\nThanks"", 'title': 'Problem about lightconv'}"		SkVhlh09tX	H1gWfxVozE	ICLR.cc/2019/Conference/-/Paper1115/Public_Comment	[]	10		['everyone']	SkVhlh09tX	['(anonymous)']	1547546632659		1547546632659	['(anonymous)', 'ICLR.cc/2019/Conference']
857	1538088004311	{'title': 'Global-to-local Memory Pointer Networks for Task-Oriented Dialogue', 'abstract': 'End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.', 'keywords': ['pointer networks', 'memory networks', 'task-oriented dialogue systems', 'natural language processing'], 'authorids': ['jason.wu@connect.ust.hk', 'rsocher@salesforce.com', 'cxiong@salesforce.com'], 'authors': ['Chien-Sheng Wu', 'Richard Socher', 'Caiming Xiong'], 'TL;DR': 'GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.', 'pdf': '/pdf/db8acf203bd7c5a73dd104a07690d8f6f8d85757.pdf', 'paperhash': 'wu|globaltolocal_memory_pointer_networks_for_taskoriented_dialogue', '_bibtex': '@inproceedings{\nwu2018globaltolocal,\ntitle={Global-to-local Memory Pointer Networks for Task-Oriented Dialogue},\nauthor={Chien-Sheng Wu and Richard Socher and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxnHhRqFm},\n}'}		ryxnHhRqFm	ryxnHhRqFm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1581	ryl8EsY9tm	['everyone']		['ICLR.cc/2019/Conference']	1538088004311		1547543031921	['ICLR.cc/2019/Conference']
858	1538087948383	"{'title': 'Improving Differentiable Neural Computers Through Memory Masking, De-allocation, and Link Distribution Sharpness Control', 'abstract': ""The Differentiable Neural Computer (DNC) can learn algorithmic and question answering tasks. An analysis of its internal activation patterns reveals three problems: Most importantly, the lack of key-value separation makes the address distribution resulting from content-based look-up noisy and flat, since the value influences the score calculation, although only the key should. Second, DNC's de-allocation of memory results in aliasing, which is a problem for content-based look-up. Thirdly, chaining memory reads with the temporal linkage matrix exponentially degrades the quality of the address distribution. Our proposed fixes of these problems yield improved performance on arithmetic tasks, and also improve the mean error rate on the bAbI question answering dataset by 43%."", 'keywords': ['rnn', 'dnc', 'memory augmented neural networks', 'mann'], 'authorids': ['robert@idsia.ch', 'juergen@idsia.ch'], 'authors': ['Robert Csordas', 'Juergen Schmidhuber'], 'pdf': '/pdf/05c63512d8140d51e1c26b35f3c823afcaeef7e3.pdf', 'paperhash': 'csordas|improving_differentiable_neural_computers_through_memory_masking_deallocation_and_link_distribution_sharpness_control', '_bibtex': '@inproceedings{\ncsordas2018improving,\ntitle={Improving Differentiable Neural Computers Through Memory Masking, De-allocation, and Link Distribution Sharpness Control},\nauthor={Robert Csordas and Juergen Schmidhuber},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGEM3C9KQ},\n}'}"		HyGEM3C9KQ	HyGEM3C9KQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1258	rkgF-0HqK7	['everyone']		['ICLR.cc/2019/Conference']	1538087948383		1547540259530	['ICLR.cc/2019/Conference']
859	1538087829940	"{'title': 'Learning Representations of Sets through Optimized Permutations', 'abstract': ""Representations of sets are challenging to learn because operations on sets should be permutation-invariant. To this end, we propose a Permutation-Optimisation module that learns how to permute a set end-to-end. The permuted set can be further processed to learn a permutation-invariant representation of that set, avoiding a bottleneck in traditional set models. We demonstrate our model's ability to learn permutations and set representations with either explicit or implicit supervision on four datasets, on which we achieve state-of-the-art results: number sorting, image mosaics, classification from image mosaics, and visual question answering.\n"", 'keywords': ['sets', 'representation learning', 'permutation invariance'], 'authorids': ['yz5n12@ecs.soton.ac.uk', 'jsh2@ecs.soton.ac.uk', 'apb@ecs.soton.ac.uk'], 'authors': ['Yan Zhang', 'Jonathon Hare', 'Adam Prügel-Bennett'], 'TL;DR': 'Learn how to permute a set, then encode permuted set with RNN to obtain a set representation.', 'pdf': '/pdf/1a94ebcef7eb6e2e6a107ecf8174ab229d9d6655.pdf', 'paperhash': 'zhang|learning_representations_of_sets_through_optimized_permutations', '_bibtex': '@inproceedings{\nzhang2018learning,\ntitle={Learning Representations of Sets through Optimized Permutations},\nauthor={Yan Zhang and Jonathon Hare and Adam Prügel-Bennett},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJMCcjAcYX},\n}'}"		HJMCcjAcYX	HJMCcjAcYX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	580	rklemVjYYm	['everyone']		['ICLR.cc/2019/Conference']	1538087829940		1547522039473	['ICLR.cc/2019/Conference']
860	1538087945109	{'title': 'Kernel Change-point Detection with Auxiliary Deep Generative Models', 'abstract': 'Detecting the emergence of abrupt property changes in time series is a challenging problem. Kernel two-sample test has been studied for this task which makes fewer assumptions on the distributions than traditional parametric approaches. However, selecting kernels is non-trivial in practice. Although kernel selection for the two-sample test has been studied, the insufficient samples in change point detection problem hinder the success of those developed kernel selection algorithms. In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model. With deep kernel parameterization, KL-CPD endows kernel two-sample test with the data-driven kernel to detect different types of change-points in real-world applications. The proposed approach significantly outperformed other state-of-the-art methods in our comparative evaluation of benchmark datasets and simulation studies.', 'keywords': ['deep kernel learning', 'generative models', 'kernel two-sample test', 'time series change-point detection'], 'authorids': ['wchang2@cs.cmu.edu', 'chunlial@cs.cmu.edu', 'yiming@cs.cmu.edu', 'bapoczos@cs.cmu.edu'], 'authors': ['Wei-Cheng Chang', 'Chun-Liang Li', 'Yiming Yang', 'Barnabás Póczos'], 'TL;DR': 'In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model as a surrogate to the abnormal distribution. ', 'pdf': '/pdf/e96b952c18a888cce887a9ca5f19d108a6730b45.pdf', 'paperhash': 'chang|kernel_changepoint_detection_with_auxiliary_deep_generative_models', '_bibtex': '@inproceedings{\nchang2018kernel,\ntitle={Kernel Change-point Detection with Auxiliary Deep Generative Models},\nauthor={Wei-Cheng Chang and Chun-Liang Li and Yiming Yang and Barnabás Póczos},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GbfhRqF7},\n}'}		r1GbfhRqF7	r1GbfhRqF7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1239	B1lhu3wUFm	['everyone']		['ICLR.cc/2019/Conference']	1538087945109		1547510795060	['ICLR.cc/2019/Conference']
861	1538087891880	{'title': 'Unsupervised Domain Adaptation for Distance Metric Learning', 'abstract': 'Unsupervised domain adaptation is a promising avenue to enhance the performance of deep neural networks on a target domain, using labels only from a source domain. However, the two predominant methods, domain discrepancy reduction learning and semi-supervised learning, are not readily applicable when source and target domains do not share a common label space. This paper addresses the above scenario by learning a representation space that retains discriminative power on both the (labeled) source and (unlabeled) target domains while keeping representations for the two domains well-separated. Inspired by a theoretical analysis, we first reformulate the disjoint classification task, where the source and target domains correspond to non-overlapping class labels, to a verification one. To handle both within and cross domain verifications, we propose a Feature Transfer Network (FTN) to separate the target feature space from the original source space while aligned with a transformed source space. Moreover, we present a non-parametric multi-class entropy minimization loss to further boost the discriminative power of FTNs on the target domain. In experiments, we first illustrate how FTN works in a controlled setting of adapting from MNIST-M to MNIST with disjoint digit classes between the two domains and then demonstrate the effectiveness of FTNs through state-of-the-art performances on a cross-ethnicity face recognition problem.\n', 'keywords': ['domain adaptation', 'distance metric learning', 'face recognition'], 'authorids': ['kihyuk.sohn@gmail.com', 'wendyshang1208@gmail.com', 'xiangyu@nec-labs.com', 'manu@nec-labs.com'], 'authors': ['Kihyuk Sohn', 'Wenling Shang', 'Xiang Yu', 'Manmohan Chandraker'], 'TL;DR': 'A new theory of unsupervised domain adaptation for distance metric learning and its application to face recognition across diverse ethnicity variations.', 'pdf': '/pdf/e74ca52b8d3c2088099523253d1f999a64ae2e93.pdf', 'paperhash': 'sohn|unsupervised_domain_adaptation_for_distance_metric_learning', '_bibtex': '@inproceedings{\nsohn2018unsupervised,\ntitle={Unsupervised Domain Adaptation for Distance Metric Learning},\nauthor={Kihyuk Sohn and Wenling Shang and Xiang Yu and Manmohan Chandraker},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BklhAj09K7},\n}'}		BklhAj09K7	BklhAj09K7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	931	BJeLJ6KUFm	['everyone']		['ICLR.cc/2019/Conference']	1538087891880		1547506299278	['ICLR.cc/2019/Conference']
862	1538087993764	"{'title': 'Contingency-Aware Exploration in Reinforcement Learning', 'abstract': ""This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning. To investigate this question, we consider an instantiation of this hypothesis evaluated on the Arcade Learning Element (ALE). In this study, we develop an attentive dynamics model (ADM) that discovers controllable elements of the observations, which are often associated with the location of the character in Atari games. The ADM is trained in a self-supervised fashion to predict the actions taken by the agent. The learned contingency information is used as a part of the state representation for exploration purposes. We demonstrate that combining actor-critic algorithm with count-based exploration using our representation achieves impressive results on a set of notoriously challenging Atari games due to sparse rewards. For example, we report a state-of-the-art score of >11,000 points on Montezuma's Revenge without using expert demonstrations, explicit high-level information (e.g., RAM states), or supervised data. Our experiments confirm that contingency-awareness is indeed an extremely powerful concept for tackling exploration problems in reinforcement learning and opens up interesting research questions for further investigations."", 'keywords': ['Reinforcement Learning', 'Exploration', 'Contingency-Awareness'], 'authorids': ['jwook@umich.edu', 'guoyijie@umich.edu', 'marcin.lukasz.moczulski@gmail.com', 'junhyuk@umich.edu', 'neal@nealwu.com', 'mnorouzi@google.com', 'honglak@eecs.umich.edu'], 'authors': ['Jongwook Choi', 'Yijie Guo', 'Marcin Moczulski', 'Junhyuk Oh', 'Neal Wu', 'Mohammad Norouzi', 'Honglak Lee'], 'TL;DR': ""We investigate contingency-awareness and controllable aspects in exploration and achieve state-of-the-art performance on Montezuma's Revenge without expert demonstrations."", 'pdf': '/pdf/36557a45875125a2708435d13531e01c415c1190.pdf', 'paperhash': 'choi|contingencyaware_exploration_in_reinforcement_learning', '_bibtex': '@inproceedings{\nchoi2018contingencyaware,\ntitle={Contingency-Aware Exploration in Reinforcement Learning},\nauthor={Jongwook Choi and Yijie Guo and Marcin Moczulski and Junhyuk Oh and Neal Wu and Mohammad Norouzi and Honglak Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxGB2AcY7},\n}'}"		HyxGB2AcY7	HyxGB2AcY7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1520	BkgJH2p5FX	['everyone']		['ICLR.cc/2019/Conference']	1538087993764		1547501853725	['ICLR.cc/2019/Conference']
863	1538087863562	{'title': 'AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks', 'abstract': 'Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead.  In comparison, AntisymmetricRNN achieves the same goal by design.  We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler.', 'keywords': [], 'authorids': ['bchang@stat.ubc.ca', 'minminc@google.com', 'haber@math.ubc.ca', 'edchi@google.com'], 'authors': ['Bo Chang', 'Minmin Chen', 'Eldad Haber', 'Ed H. Chi'], 'pdf': '/pdf/2a9115d8d0b9b45d5f95cccb989b565712d95e20.pdf', 'paperhash': 'chang|antisymmetricrnn_a_dynamical_system_view_on_recurrent_neural_networks', '_bibtex': '@inproceedings{\nchang2018antisymmetricrnn,\ntitle={Antisymmetric{RNN}: A Dynamical System View on Recurrent Neural Networks},\nauthor={Bo Chang and Minmin Chen and Eldad Haber and Ed H. Chi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxepo0cFX},\n}'}		ryxepo0cFX	ryxepo0cFX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	768	r1gtQhZ5tX	['everyone']		['ICLR.cc/2019/Conference']	1538087863562		1547490872386	['ICLR.cc/2019/Conference']
864	1547488630957	"{'title': 'Sorry! Did not intend to make statement about your paper', 'comment': 'Thanks for raising this issue. My comment was not intended to be a statement about your paper, but I understand your concern. So to clarify, I was not saying that your paper was a ""rough draft"", etc.; that was meant in reference to what the anonymous commenter\'s proposed policy would incentivize. I agree with you that it would be inappropriate for an area chair to refer to a submission in that way (especially without providing concrete details).\n\nI do think it is absolutely appropriate (and perhaps even necessary) to explain the reasoning behind an accept/reject decision, and so I stand by my decision to respond to the anonymous commenter. For instance, I have now explicitly stated a policy that I follow regarding papers that make substantial changes during the revision phase, and why I follow that policy. This allows the community to discuss the policy and judge whether it is good or bad, which will hopefully allow me and others to improve the review process in the future. The main change I would make in retrospect is to explicitly clarify that my comment was not directed at the present submission.\n\nI also do acknowledge that you got less engagement from the reviewers on your submission than would be ideal, and hope that you fare better in the next round of submission. I personally found the paper to be an interesting read.'}"		ryxY73AcK7	H1x1KpBqfN	ICLR.cc/2019/Conference/-/Paper1380/Official_Comment	['ICLR.cc/2019/Conference/Paper1380/Reviewers/Unsubmitted']	14		['everyone']	SkxeNOzqfV	['ICLR.cc/2019/Conference/Paper1380/Area_Chair1']	1547488630957		1547488807962	['ICLR.cc/2019/Conference/Paper1380/Area_Chair1', 'ICLR.cc/2019/Conference']
865	1547474984211	"{'title': 'OpenReview is not the place to debate hypotheticals', 'comment': 'Posting publicly because the AC has not responded to our private comment.\n\nI get that it\'s fun to engage in debates about our community\'s publication standards, and that you want to defend your decision against criticism. But I wish you would be a bit more careful in this context. OpenReview is archival, and you are posting in your official capacity as AC, so any comment you make will be interpreted as referring to our submission. Your (I assume inadvertent) implication that our submission was the sort of thing the process needs to disincentivize is careless and misleading.\n\nOur original submission was a finished paper, and all of the algorithms and mathematical results were already in more or less their final form. In the revision, we added a bunch of new experiments, and did a global rewrite for clarity (which I think is what the revision period is intended for). Our original submission was not perfect by any means, and two of the three reviewers gave us insightful and constructive feedback that helped us improve the paper. But my students and I simply do not submit half-baked work.\n\nThe anonymous commenter does raise an important issue with the review process, namely that R1 did not take the time to read the paper even once. This wasn\'t because it was ""incomplete"", but because it had some typos scattered throughout, such as \\citet vs. \\citep. I would never, as an AC, endorse this as a legitimate reason not to write a proper review. I did have some papers in my AC batch which were genuinely incomplete (e.g. 6 pages), and even then I still insisted the reviewers read the paper and write real reviews.\n\n- Roger\n'}"		ryxY73AcK7	SkxeNOzqfV	ICLR.cc/2019/Conference/-/Paper1380/Official_Comment	['ICLR.cc/2019/Conference/Paper1380/Reviewers/Unsubmitted']	13		['everyone']	B1geukTDGV	['ICLR.cc/2019/Conference/Paper1380/Authors']	1547474984211		1547474984211	['ICLR.cc/2019/Conference/Paper1380/Authors', 'ICLR.cc/2019/Conference']
866	1547470086161	{'comment': 'Dear authors,\nWe agree that when training GANs a lot of issues may arise such as generator discriminator disbalance, the requirement of a discriminator to be k-Lipschitz (in the case of WGAN). However, at this point we were not able to achieve results with AdaShift that are comparable to or better than the results with Adam or AMSGrad and given that this is an interesting problem that is much more realistic than training a generator against a fixed discriminator that could be used to study the behavior of an optimizer, we find this experiment more useful. We note that training a discriminator against a fixed generator essentially leads to training a neural network to distinguish dataset images from noise which is a peculiar setting. We conducted experiments in two settings when training both discriminator and generator. First is the typical setting WGAN-GP with penalty coefficient equal to 10 and the second is WGAN max-GP with penalty coefficient equal to 0.1. While in the first setting we were able to train the generative model with all optimizers (albeit with AdaShift performing worse than both Adam and AMSGrad), we found that in the second setting the results are much worse for all optimizers and the generative model does not train well (we were not able to achieve Inception Score of at least 4 after 100 epochs of training). In our case making learning rate for AdaShift 10 times bigger than the learning rates of Adam and AMSGrad lead to divergence in the first setting. We also tried a number of other learning rates and again were not able to achieve results for AdaShift that are comparable to the other optimizers in the first setting. We would greatly appreciate it if you could provide further details that may lead to comparable results between different optimizers.\n', 'title': 'WGAN experiment'}		HkgTkhRcKQ	rkgAZSb9ME	ICLR.cc/2019/Conference/-/Paper1028/Public_Comment	[]	4		['everyone']	BygUdM1fGE	['~Mikhail_Konobeev1']	1547470086161		1547470086161	['~Mikhail_Konobeev1', 'ICLR.cc/2019/Conference']
867	1538087827521	"{'title': 'ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness', 'abstract': ""Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation."", 'keywords': ['deep learning', 'psychophysics', 'representation learning', 'object recognition', 'robustness', 'neural networks', 'data augmentation'], 'authorids': ['robert@geirhos.de', 'patricia@rubisch.net', 'claudio.michaelis@bethgelab.org', 'matthias.bethge@uni-tuebingen.de', 'felix.wichmann@uni-tuebingen.de', 'wieland.brendel@bethgelab.org'], 'authors': ['Robert Geirhos', 'Patricia Rubisch', 'Claudio Michaelis', 'Matthias Bethge', 'Felix A. Wichmann', 'Wieland Brendel'], 'TL;DR': 'ImageNet-trained CNNs are biased towards object texture (instead of shape like humans). Overcoming this major difference between human and machine vision yields improved detection performance and previously unseen robustness to image distortions.', 'pdf': '/pdf/52b01e9642a6b02996501674ae619371f8b6e721.pdf', 'paperhash': 'geirhos|imagenettrained_cnns_are_biased_towards_texture_increasing_shape_bias_improves_accuracy_and_robustness', '_bibtex': '@inproceedings{\ngeirhos2018imagenettrained,\ntitle={ImageNet-trained {CNN}s are biased towards texture; increasing shape bias improves accuracy and robustness.},\nauthor={Robert Geirhos and Patricia Rubisch and Claudio Michaelis and Felix Wichmann and Wieland Brendel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bygh9j09KX},\n}'}"		Bygh9j09KX	Bygh9j09KX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	566	Byelrp_9tX	['everyone']		['ICLR.cc/2019/Conference']	1538087827521		1547465365138	['ICLR.cc/2019/Conference']
868	1538087897134	{'title': 'Feature Intertwiner for Object Detection', 'abstract': 'A well-trained model should classify objects with unanimous score for every category. This requires the high-level semantic features should be alike among samples, despite a wide span in resolution, texture, deformation, etc. Previous works focus on re-designing the loss function or proposing new regularization constraints on the loss. In this paper, we address this problem via a new perspective. For each category, it is assumed that there are two sets in the feature space: one with more reliable information and the other with less reliable source. We argue that the reliable set could guide the feature learning of the less reliable set during training - in spirit of student mimicking teacher’s behavior and thus pushing towards a more compact class centroid in the high-dimensional space. Such a scheme also benefits the reliable set since samples become more closer within the same category - implying that it is easilier for the classifier to identify. We refer to this mutual learning process as feature intertwiner and embed the spirit into object detection. It is well-known that objects of low resolution are more difficult to detect due to the loss of detailed information during network forward pass. We thus regard objects of high resolution as the reliable set and objects of low resolution as the less reliable set. Specifically, an intertwiner is achieved by minimizing the distribution divergence between two sets. We design a historical buffer to represent all previous samples in the reliable set and utilize them to guide the feature learning of the less reliable set. The design of obtaining an effective feature representation for the reliable set is further investigated, where we introduce the optimal transport (OT) algorithm into the framework. Samples in the less reliable set are better aligned with the reliable set with aid of OT metric. Incorporated with such a plug-and-play intertwiner, we achieve an evident improvement over previous state-of-the-arts on the COCO object detection benchmark.', 'keywords': ['feature learning', 'computer vision', 'deep learning'], 'authorids': ['yangli@ee.cuhk.edu.hk', 'db014@ie.cuhk.edu.hk', 'shaoss@link.cuhk.edu.hk', 'wanli.ouyang@gmail.com', 'xgwang@ee.cuhk.edu.hk'], 'authors': ['Hongyang Li', 'Bo Dai', 'Shaoshuai Shi', 'Wanli Ouyang', 'Xiaogang Wang'], 'TL;DR': '(Camera-ready version) A feature intertwiner module to leverage features from one accurate set to help the learning of another less reliable set.', 'pdf': '/pdf/84ec6cb778a53957314e00cf2d51733adcaff22d.pdf', 'paperhash': 'li|feature_intertwiner_for_object_detection', '_bibtex': '@inproceedings{\nli2018feature,\ntitle={Feature Intertwiners},\nauthor={Hongyang Li and Bo Dai and Shaoshuai Shi and Wanli Ouyang and Xiaogang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxZJn05YX},\n}'}		SyxZJn05YX	SyxZJn05YX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	961	SylsaGzUY7	['everyone']		['ICLR.cc/2019/Conference']	1538087897134		1547462291855	['ICLR.cc/2019/Conference']
869	1547419633507	{'title': 'eps is a fixed constant!', 'comment': 'eps is a constant during optimization.\ndim(g(z)) = dim(r), \nso g(z) = torch.zeros_like(r)\n'}		SkgiX2Aqtm	rJgFlgSKGE	ICLR.cc/2019/Conference/-/Paper1388/Official_Comment	['ICLR.cc/2019/Conference/Paper1388/Reviewers/Unsubmitted']	7		['everyone']	HkgfHR4Yz4	['ICLR.cc/2019/Conference/Paper1388/Authors']	1547419633507		1547419633507	['ICLR.cc/2019/Conference/Paper1388/Authors', 'ICLR.cc/2019/Conference']
870	1547419193601	{'comment': 'So eps = constant?\nAlso since g(z)=0, is it g(z)=torch.zeros_like(z) or g(z)=z-z\nThanks again!', 'title': 'So eps is a fixed constant?'}		SkgiX2Aqtm	HkgfHR4Yz4	ICLR.cc/2019/Conference/-/Paper1388/Public_Comment	[]	4		['everyone']	H1lir6NKMV	['(anonymous)']	1547419193601		1547419193601	['(anonymous)', 'ICLR.cc/2019/Conference']
871	1547418541331	{'comment': 'how do I find\neps?', 'title': 'eps and g(z)?'}		SkgiX2Aqtm	r1grhsEFGE	ICLR.cc/2019/Conference/-/Paper1388/Public_Comment	[]	3		['everyone']	H1lFblNFMN	['(anonymous)']	1547418541331		1547419030566	['(anonymous)', 'ICLR.cc/2019/Conference']
872	1547418946577	{'title': 'eps!', 'comment': 'eps is a scalar hyper-parameter of the method. \nIs it introduced in Eq. 10\nIts role is discussed after Eq. 17\nIn experiment 5.1 we demonstrate how it affects the process of the encoding.'}		SkgiX2Aqtm	H1lir6NKMV	ICLR.cc/2019/Conference/-/Paper1388/Official_Comment	['ICLR.cc/2019/Conference/Paper1388/Reviewers/Unsubmitted']	6		['everyone']	r1grhsEFGE	['ICLR.cc/2019/Conference/Paper1388/Authors']	1547418946577		1547418946577	['ICLR.cc/2019/Conference/Paper1388/Authors', 'ICLR.cc/2019/Conference']
873	1547415553051	{'title': 'Implementation!', 'comment': 'Hello\n\n>> How do I implement r mapping to Normal?\nIn order to train PIE, one maximises the function in Eq. 13\nSo r ~ N(g(z), eps^2) if |r - g(z)|_2^2 = 0\n\n>> What is function g(z) to parameterise mu, is it just a Linear layer from d dim to D-d dim?\ng(z) could be any differentiable function from d to D-d\nIn our experiments we use g(z) = 0\n\n>> If g(z) is Linear layer with input_dim =d and output_dim=D-d ,\n>> is the objective to minimize is r_loss = 0.5* -(g(z).mean() - 0.001)).mean()\nNo. It is r_loss = - ((r - g(z))**2).sum()  / (2 * eps**2)'}		SkgiX2Aqtm	H1lFblNFMN	ICLR.cc/2019/Conference/-/Paper1388/Official_Comment	['ICLR.cc/2019/Conference/Paper1388/Reviewers/Unsubmitted']	5		['everyone']	SJlNGufOfN	['ICLR.cc/2019/Conference/Paper1388/Authors']	1547415553051		1547415553051	['ICLR.cc/2019/Conference/Paper1388/Authors', 'ICLR.cc/2019/Conference']
874	1547399497555	"{'comment': 'I would like to recommend the original GAN paper [1] to resolve your misunderstanding on adversarial fake sample generation. Sampling (equivalent to sample from a fake ""distribution"" here, not only the most misleading sample) is the key to the success in GAN.\n\nIf the generator only generates the most misleading sample, it\'s easy to see that the  Global Optimality is not \n\np_g = p_data. \n\nBecause the optimal discriminator D now is \n\nD∗_G(x) = 0 if (p_g(x) >= p_g(*)) else 1\n\n[1] Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A, Bengio Y. Generative adversarial nets. NIPS 2014', 'title': 'Recommending a paper'}"		HkgEQnRqYQ	r1eMUblYGN	ICLR.cc/2019/Conference/-/Paper1347/Public_Comment	[]	19		['everyone']	HyxmHhCOMV	['(anonymous)']	1547399497555		1547399497555	['(anonymous)', 'ICLR.cc/2019/Conference']
875	1547394106949	"{'comment': ""I think you may have misread or misunderstood the above comment and the Wieting et al. paper. Wieting et al.'s method is self-adversarial negative sampling. It seems to me that their method is more advanced because they identified and addressed the false negative problem. I would be happy to see a constructive comparison. \n\nOn the other hand, like other comments said, reported results are buried under many optimization techniques and tunings. As a fellow researcher I would like to see more direct results. "", 'title': 'Acknowledging and comparison with related work'}"		HkgEQnRqYQ	HyxmHhCOMV	ICLR.cc/2019/Conference/-/Paper1347/Public_Comment	[]	18		['everyone']	S1lX0Hps-V	['(anonymous)']	1547394106949		1547398082870	['(anonymous)', 'ICLR.cc/2019/Conference']
876	1545375799205	"{'comment': 'I am concerned with the style of presentation in this paper.\n\n1. ""rotation in complex plane"": in knowledge graph embedding community, the ComplEx model [1] is very well established. It involves complex number product, which is ""rotation in complex plane"". The authors failed to compare to existing work.\n\n2. ""self-adversarial negative sampling"": this technique was used at least in 2016 to train sentence embedding [2]. The authors also failed to compare to existing work.\n\n3. Reporting of result: for fair comparison to future work (and also past work), the paper should include results of RotatE on standard setting.\n\nThe result is worthy, nevertheless the writing\'s getting on my nerve. This is only my ranting, but that\'s what gets to people working closely with this topic. This is in a comment to AC because the conference can choose which practice to endorse. I urge the authors to rewrite in a more straightforward and fair style.\n\n[1] Trouillon, Théo, et al. ""Complex embeddings for simple link prediction."" ICML \'16.\n[2] Wieting, John, et al. ""Towards universal paraphrastic sentence embeddings."" ICLR \'16.', 'title': 'Thanks for your hard work, and a few comments'}"		HkgEQnRqYQ	ryxJBxM5xE	ICLR.cc/2019/Conference/-/Paper1347/Public_Comment	[]	13		['everyone']	HJeMTnkBe4	['(anonymous)']	1545375799205		1547397907523	['(anonymous)', 'ICLR.cc/2019/Conference']
877	1547343883559	{'comment': '\nOriginal bijective net is\nF(x)=z\nF_inv(z)=x\n\nPIE net is:\nF(x)=[z;r] where r~Normal(mu=g(z), sd= <<1 )\nF_inv(F(x)) = [z;g(z)]\n\nHow do I implement r mapping to Normal?\n\nWhat is function g(z) to parameterise mu, is it just a Linear layer from d dim to D-d dim?\n\nIf g(z) is Linear layer with input_dim =d and output_dim=D-d ,\nis the objective to minimize is r_loss = 0.5* -(g(z).mean() - 0.001)).mean()\nThanks in advance\n', 'title': 'Implementation?'}		SkgiX2Aqtm	SJlNGufOfN	ICLR.cc/2019/Conference/-/Paper1388/Public_Comment	[]	2		['everyone']	SkgiX2Aqtm	['(anonymous)']	1547343883559		1547394835952	['(anonymous)', 'ICLR.cc/2019/Conference']
878	1543580508308	{'comment': 'This issue with ranking in evaluation seems important. I think most open sources rank same-score triples the *same lowest rank*. Did you rank differently in RotatE (random/ordinal/max rank)?\n\nThis issue should be irrelevant if the model give different scores for almost all triples. Is there a type of model that gives same score for many triples?', 'title': 'Intriguing evaluation issue'}		HkgEQnRqYQ	B1lVvosACX	ICLR.cc/2019/Conference/-/Paper1347/Public_Comment	[]	9		['everyone']	rkgm5j1aAQ	['(anonymous)']	1543580508308		1547392596728	['(anonymous)', 'ICLR.cc/2019/Conference']
879	1547332073890	{'title': 'Informing the Design of Spoken Conversational Search', 'authors': ['Johanne R. Trippas', 'Damiano Spina', 'Lawrence Cavedon', 'Hideo Joho', 'Mark Sanderson'], 'authorids': ['johanne.trippas@rmit.edu.au', 'damiano.spina@rmit.edu.au', 'lawrence.cavedon@rmit.edu.au', 'hideo@slis.tsukuba.ac.jp', 'mark.sanderson@rmit.edu.au'], 'abstract': 'We conducted a laboratory-based observational study where pairs of people performed search tasks communicating verbally. Examination of the discourse allowed commonly used interactions to be identified for Spoken Conversational Search (SCS). We compared the interactions to existing models of search behaviour. We find that SCS is more complex and interactive than traditional search. This work enhances our understanding of different search behaviours and proposes research opportunities for an audio-only search system. Future work will focus on creating models of search behaviour for SCS and evaluating these against actual SCS systems.', 'artifact type': ['Dataset'], 'requested badges': ['Artifacts Available'], 'html': 'https://jtrippas.github.io/Spoken-Conversational-Search/', 'paperhash': 'trippas|informing_the_design_of_spoken_conversational_search'}		rJgGxq1_z4	rJgGxq1_z4	ACM.org/SIGIR/Badging/-/Submission	[]	4		['everyone']		['~Johanne_Trippas1']	1547332073890		1547332073890	['~Johanne_Trippas1', 'ACM.org/SIGIR/Badging', 'johanne.trippas@rmit.edu.au', 'damiano.spina@rmit.edu.au', 'lawrence.cavedon@rmit.edu.au', 'hideo@slis.tsukuba.ac.jp', 'mark.sanderson@rmit.edu.au']
880	1538088001887	{'title': 'On the Universal Approximability and Complexity Bounds of Quantized ReLU Neural Networks', 'abstract': 'Compression is a key step to deploy large neural networks on resource-constrained platforms. As a popular compression technique, quantization constrains the number of distinct weight values and thus reducing the number of bits required to represent and store each weight. In this paper, we study the representation power of quantized neural networks. First, we prove the universal approximability of quantized ReLU networks on a wide class of functions. Then we provide upper bounds on the number of weights and the memory size for a given approximation error bound and the bit-width of weights for function-independent and function-dependent structures. Our results reveal that, to attain an approximation error bound of $\\epsilon$, the number of weights needed by a quantized network is no more than $\\mathcal{O}\\left(\\log^5(1/\\epsilon)\\right)$ times that of an unquantized network. This overhead is of much lower order than the lower bound of the number of weights needed for the error bound, supporting the empirical success of various quantization techniques. To the best of our knowledge, this is the first in-depth study on the complexity bounds of quantized neural networks.', 'keywords': ['Quantized Neural Networks', 'Universial Approximability', 'Complexity Bounds', 'Optimal Bit-width'], 'authorids': ['yding5@nd.edu', 'jliu16@nd.edu', 'jinjun@us.ibm.com', 'yshi4@nd.edu'], 'authors': ['Yukun Ding', 'Jinglan Liu', 'Jinjun Xiong', 'Yiyu Shi'], 'TL;DR': 'This paper proves the universal  approximability of quantized ReLU neural networks and puts forward the complexity bound given arbitrary error.', 'pdf': '/pdf/b19c7c43db718baa927048e94b97f624fd0b35d1.pdf', 'paperhash': 'ding|on_the_universal_approximability_and_complexity_bounds_of_quantized_relu_neural_networks', '_bibtex': '@inproceedings{\nding2018on,\ntitle={On the Universal Approximability and Complexity Bounds of Quantized Re{LU} Neural Networks},\nauthor={Yukun Ding and Jinglan Liu and Jinjun Xiong and Yiyu Shi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJe9rh0cFX},\n}'}		SJe9rh0cFX	SJe9rh0cFX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1567	HJgBavactX	['everyone']		['ICLR.cc/2019/Conference']	1538088001887		1547327839875	['ICLR.cc/2019/Conference']
881	1547321191756	"{'title': 'Need to think about incentives', 'comment': ""The problem with the policy that you suggest is that it creates poor incentives that would further strain an already strained reviewing system. If we allowed substantial revisions past the reviewing deadline, then everyone would be incentivized to submit rough drafts at time of submission and then revise later. In such a world, would there even be a point in reviewers looking at papers at time of submission? This would be equivalent to pushing the submission deadline a month later (the notification deadline would also have to be pushed back since reviewers would need time to review all the revisions). I understand wanting to propagate ideas sooner but it is the author's responsibility to have a finished paper by the submission deadline.\n\nIn addition, there are many aspects of reviewing a paper other than deciding whether the paper is interesting (e.g. does the idea actually make sense, are all key claims substantiated, etc.). My judgment was that reviewers would not be able to adequately judge these without essentially performing a second set of reviews.""}"		ryxY73AcK7	B1geukTDGV	ICLR.cc/2019/Conference/-/Paper1380/Official_Comment	['ICLR.cc/2019/Conference/Paper1380/Reviewers/Unsubmitted']	11		['everyone']	BJxzaUjDfE	['ICLR.cc/2019/Conference/Paper1380/Area_Chair1']	1547321191756		1547321191756	['ICLR.cc/2019/Conference/Paper1380/Area_Chair1', 'ICLR.cc/2019/Conference']
882	1538087842221	{'title': 'SNAS: stochastic neural architecture search', 'abstract': 'We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.', 'keywords': ['Neural Architecture Search'], 'authorids': ['xiesirui@sensetime.com', 'zhenghehui@sensetime.com', 'liuchunxiao@sensetime.com', 'linliang@ieee.org'], 'authors': ['Sirui Xie', 'Hehui Zheng', 'Chunxiao Liu', 'Liang Lin'], 'pdf': '/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf', 'paperhash': 'xie|snas_stochastic_neural_architecture_search', '_bibtex': '@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}'}		rylqooRqK7	rylqooRqK7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	648	r1x_4XTwtX	['everyone']		['ICLR.cc/2019/Conference']	1538087842221		1547310028965	['ICLR.cc/2019/Conference']
883	1547301551954	{'title': 'Thanks for the pointers', 'comment': 'Hi Roman,\n\nThanks for the pointers! I just went through these papers and, indeed, it was an enjoyable read. We will add citations in the camera-ready version. \n\nBest,\nZiyu Wang'}		BkgtDsCcKQ	B1gdhGdvME	ICLR.cc/2019/Conference/-/Paper282/Official_Comment	['ICLR.cc/2019/Conference/Paper282/Reviewers/Unsubmitted']	12		['everyone']	S1epmLv8z4	['ICLR.cc/2019/Conference/Paper282/Authors']	1547301551954		1547301551954	['ICLR.cc/2019/Conference/Paper282/Authors', 'ICLR.cc/2019/Conference']
884	1538087821370	{'title': 'LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators', 'abstract': 'Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation and tangram graphic design.', 'keywords': [], 'authorids': ['lijianan15@gmail.com', 'jimyang@adobe.com', 'hertzman@adobe.com', 'jianmzha@adobe.com', 'ciom_xtf1@bit.edu.cn'], 'authors': ['Jianan Li', 'Jimei Yang', 'Aaron Hertzmann', 'Jianming Zhang', 'Tingfa Xu'], 'pdf': '/pdf/6bea64344b5868c50f04a043422f06436b39c251.pdf', 'paperhash': 'li|layoutgan_generating_graphic_layouts_with_wireframe_discriminators', '_bibtex': '@inproceedings{\nli2018layoutgan,\ntitle={Layout{GAN}: Generating Graphic Layouts with Wireframe Discriminator},\nauthor={Jianan Li and Tingfa Xu and Jianming Zhang and Aaron Hertzmann and Jimei Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxB5sRcFQ},\n}'}		HJxB5sRcFQ	HJxB5sRcFQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	533	H1xdAnEXKX	['everyone']		['ICLR.cc/2019/Conference']	1538087821370		1547282017501	['ICLR.cc/2019/Conference']
885	1538087958462	{'title': 'Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion', 'abstract': 'This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.', 'keywords': [], 'authorids': ['ruiqigao@ucla.edu', 'jianwen@ucla.edu', 'sczhu@stat.ucla.edu', 'ywu@stat.ucla.edu'], 'authors': ['Ruiqi Gao', 'Jianwen Xie', 'Song-Chun Zhu', 'Ying Nian Wu'], 'pdf': '/pdf/090e558affbc18158574ab1135d9b1fbcc9b6b56.pdf', 'paperhash': 'gao|learning_grid_cells_as_vector_representation_of_selfposition_coupled_with_matrix_representation_of_selfmotion', '_bibtex': '@inproceedings{\ngao2018learning,\ntitle={Learning Grid-like Units with Vector Representation of Self-Position and Matrix Representation of Self-Motion},\nauthor={Ruiqi Gao and Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx0Mh05YQ},\n}'}		Syx0Mh05YQ	Syx0Mh05YQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1317	S1e47365Y7	['everyone']		['ICLR.cc/2019/Conference']	1538087958462		1547279523408	['ICLR.cc/2019/Conference']
886	1547232805326	{'comment': 'Dear Colleagues,\n\nCongratulations on having your impressive work accepted! \n\nWe similarly consider BNNs to be a very important and exciting field of study, and investigate them in the wide regime, where equivalence to the GP prior arises. As you reference this line of work in section 3.2, we would like to point out relevant concurrent work:\n\n1) https://openreview.net/forum?id=B1EA-M-0Z derived the NN-GP correspondence concurrently with Matthews et al, 2018, and\n2) https://openreview.net/forum?id=B1g30j0qF7 similarly derived the CNN-GP correspondence concurrently with Garriga-Alonso et al, 2018.\n\nIn addition to being concurrent derivations, both works provide a lot of complementary findings. I hope you find these references useful!\n\nBest,\nRoman.', 'title': 'On function-space priors for wide BNNs'}		BkgtDsCcKQ	S1epmLv8z4	ICLR.cc/2019/Conference/-/Paper282/Public_Comment	[]	1		['everyone']	BkgtDsCcKQ	['~Roman_Novak2']	1547232805326		1547232875248	['~Roman_Novak2', 'ICLR.cc/2019/Conference']
887	1538087891708	{'title': 'Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes', 'abstract': 'There is a previously identified equivalence between wide fully connected neural networks (FCNs) and Gaussian processes (GPs). This equivalence enables, for instance, test set predictions that would have resulted from a fully Bayesian, infinitely wide trained FCN to be computed without ever instantiating the FCN, but by instead evaluating the corresponding GP. In this work, we derive an analogous equivalence for multi-layer convolutional neural networks (CNNs) both with and without pooling layers, and achieve state of the art results on CIFAR10 for GPs without trainable kernels. We also introduce a Monte Carlo method to estimate the GP corresponding to a given neural network architecture, even in cases where the analytic form has too many terms to be computationally feasible. \n\nSurprisingly, in the absence of pooling layers, the GPs corresponding to CNNs with and without weight sharing are identical. As a consequence, translation equivariance in finite channel CNNs trained with stochastic gradient descent (SGD) has no corresponding property in the Bayesian treatment of the infinite channel limit - a qualitative difference between the two regimes that is not present in the FCN case. We confirm experimentally, that while in some scenarios the performance of SGD-trained finite CNNs approaches that of the corresponding GPs as the channel count increases, with careful tuning SGD-trained CNNs can significantly outperform their corresponding GPs, suggesting advantages from SGD training compared to fully Bayesian parameter estimation.\n', 'keywords': ['Deep Convolutional Neural Networks', 'Gaussian Processes'], 'authorids': ['romann@google.com', 'xlc@google.com', 'yasamanb@google.com', 'jaehlee@google.com', 'gregyang@microsoft.com', 'danabo@google.com', 'jpennin@google.com', 'jaschasd@google.com'], 'authors': ['Roman Novak', 'Lechao Xiao', 'Yasaman Bahri', 'Jaehoon Lee', 'Greg Yang', 'Daniel A. Abolafia', 'Jeffrey Pennington', 'Jascha Sohl-dickstein'], 'TL;DR': 'Finite-width SGD trained CNNs vs. infinitely wide fully Bayesian CNNs. Who wins?', 'pdf': '/pdf/0352803b1f6f5721e1921c8a77970a8104aa87c5.pdf', 'paperhash': 'novak|bayesian_deep_convolutional_networks_with_many_channels_are_gaussian_processes', '_bibtex': '@inproceedings{\nnovak2019bayesian,\ntitle={Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes},\nauthor={Roman Novak and Lechao Xiao and Yasaman Bahri and Jaehoon Lee and Greg Yang and Daniel A. Abolafia and Jeffrey Pennington and Jascha Sohl-dickstein},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1g30j0qF7},\n}'}		B1g30j0qF7	B1g30j0qF7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	930	BygW0PTqtm	['everyone']		['ICLR.cc/2019/Conference']	1538087891708		1547232291124	['ICLR.cc/2019/Conference']
888	1538087922194	{'title': 'Random mesh projectors for inverse problems', 'abstract': 'We propose a new learning-based approach to solve ill-posed inverse problems in imaging. We address the case where ground truth training samples are rare and the problem is severely ill-posed---both because of the underlying physics and because we can only get few measurements. This setting is common in geophysical imaging and remote sensing. We show that in this case the common approach to directly learn the mapping from the measured data to the reconstruction becomes unstable. Instead, we propose to first learn an ensemble of simpler mappings from the data to projections of the unknown image into random piecewise-constant subspaces. We then combine the projections to form a final reconstruction by solving a deconvolution-like problem. We show experimentally that the proposed method is more robust to measurement noise and corruptions not seen during training than a directly learned inverse.', 'keywords': ['imaging', 'inverse problems', 'subspace projections', 'random Delaunay triangulations', 'CNN', 'geophysics', 'regularization'], 'authorids': ['kkothar3@illinois.edu', 'gupta67@illinois.edu', 'mdehoop@rice.edu', 'dokmanic@illinois.edu'], 'authors': ['Konik Kothari*', 'Sidharth Gupta*', 'Maarten v. de Hoop', 'Ivan Dokmanic'], 'TL;DR': 'We solve ill-posed inverse problems with scarce ground truth examples by estimating an ensemble of random projections of the model instead of the model itself.', 'pdf': '/pdf/f0c5769d733f712784e7591ed5334346de30eec5.pdf', 'paperhash': 'kothari|random_mesh_projectors_for_inverse_problems', '_bibtex': '@inproceedings{\nkothari2018random,\ntitle={Random mesh projectors for inverse problems},\nauthor={Konik Kothari and Sidharth Gupta and Maarten v. de Hoop and Ivan Dokmanic},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGcghRct7},\n}'}		HyGcghRct7	HyGcghRct7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1103	HJeFZPJ9Ym	['everyone']		['ICLR.cc/2019/Conference']	1538087922194		1547231725647	['ICLR.cc/2019/Conference']
889	1547206568732	{'title': 'Camera ready version', 'comment': 'Dear Area Chair, Reviewers and Readers,\n\nWe were delighted to learn that our work has been recommended for acceptance and are looking forward to presenting it at the conference. We have now uploaded the camera ready version of the manuscript and linked the source code repository.\n\nWe were absolutely thrilled by the amount of positive feedback our work has received and would like to thank everyone who participated in this forum.\n\n\nBest wishes,\n\n\nICLR 2019 Conference Paper1058 Authors'}		SkxXg2C5FX	Byeb2yZ8fV	ICLR.cc/2019/Conference/-/Paper1058/Official_Comment	['ICLR.cc/2019/Conference/Paper1058/Reviewers/Unsubmitted']	16		['everyone']	SkxXg2C5FX	['ICLR.cc/2019/Conference/Paper1058/Authors']	1547206568732		1547206639368	['ICLR.cc/2019/Conference/Paper1058/Authors', 'ICLR.cc/2019/Conference']
890	1538087914556	"{'title': ""Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word Vectors"", 'abstract': 'Recent literature suggests that averaged word vectors followed by simple post-processing outperform many deep learning methods on semantic textual similarity tasks. Furthermore, when averaged word vectors are trained supervised on large corpora of paraphrases, they achieve state-of-the-art results on standard STS benchmarks. Inspired by these insights, we push the limits of word embeddings even further. We propose a novel fuzzy bag-of-words (FBoW) representation for text that contains all the words in the vocabulary simultaneously but with different degrees of membership, which are derived from similarities between word vectors. We show that max-pooled word vectors are only a special case of fuzzy BoW and should be compared via fuzzy Jaccard index rather than cosine similarity. Finally, we propose DynaMax, a completely unsupervised and non-parametric similarity measure that dynamically extracts and max-pools good features depending on the sentence pair. This method is both efficient and easy to implement, yet outperforms current baselines on STS tasks by a large margin and is even competitive with supervised word vectors trained to directly optimise cosine similarity.', 'keywords': ['word vectors', 'sentence representations', 'distributed representations', 'fuzzy sets', 'bag-of-words', 'unsupervised learning', 'word vector compositionality', 'max-pooling', 'Jaccard index'], 'authorids': ['vitali.zhelezniak@babylonhealth.com', 'sasho.savkov@babylonhealth.com', 'april.shen@babylonhealth.com', 'francesco.moramarco@babylonhealth.com', 'jack.flann@babylonhealth.com', 'nils.hammerla@babylonhealth.com'], 'authors': ['Vitalii Zhelezniak', 'Aleksandar Savkov', 'April Shen', 'Francesco Moramarco', 'Jack Flann', 'Nils Y. Hammerla'], 'TL;DR': 'Max-pooled word vectors with fuzzy Jaccard set similarity are an extremely competitive baseline for semantic similarity; we propose a simple dynamic variant that performs even better.', 'pdf': '/pdf/9cffc8ba59ad271525f94f86911b97cf855bb54d.pdf', 'paperhash': 'zhelezniak|dont_settle_for_average_go_for_the_max_fuzzy_sets_and_maxpooled_word_vectors', '_bibtex': ""@inproceedings{\nzhelezniak2018dont,\ntitle={Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word Vectors},\nauthor={Vitalii Zhelezniak and Aleksandar Savkov and April Shen and Francesco Moramarco and Jack Flann and Nils Y. Hammerla},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkxXg2C5FX},\n}""}"		SkxXg2C5FX	SkxXg2C5FX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1058	HJlltxn5YX	['everyone']		['ICLR.cc/2019/Conference']	1538087914556		1547205066856	['ICLR.cc/2019/Conference']
891	1538087808359	{'title': 'ADef: an Iterative Algorithm to Construct Adversarial Deformations', 'abstract': 'While deep neural networks have proven to be a powerful tool for many recognition and classification tasks, their stability properties are still not well understood. In the past, image classifiers have been shown to be vulnerable to so-called adversarial attacks, which are created by additively perturbing the correctly classified image. In this paper, we propose the ADef algorithm to construct a different kind of adversarial attack created by iteratively applying small deformations to the image, found through a gradient descent step. We demonstrate our results on MNIST with convolutional neural networks and on ImageNet with Inception-v3 and ResNet-101.', 'keywords': ['Adversarial examples', 'deformations', 'deep neural networks', 'computer vision'], 'authorids': ['rima.alaifari@sam.math.ethz.ch', 'alberti@dima.unige.it', 'tandrig@sam.math.ethz.ch'], 'authors': ['Rima Alaifari', 'Giovanni S. Alberti', 'Tandri Gauksson'], 'TL;DR': 'We propose a new, efficient algorithm to construct adversarial examples by means of deformations, rather than additive perturbations.', 'pdf': '/pdf/901b877b7dda447ba534f8edb8fc5cf7d28f20ce.pdf', 'paperhash': 'alaifari|adef_an_iterative_algorithm_to_construct_adversarial_deformations', '_bibtex': '@inproceedings{\nalaifari2018adef,\ntitle={{AD}ef: an Iterative Algorithm to Construct Adversarial Deformations},\nauthor={Rima Alaifari and Giovanni S. Alberti and Tandri Gauksson},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hk4dFjR5K7},\n}'}		Hk4dFjR5K7	Hk4dFjR5K7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	461	r1lXLv4FKm	['everyone']		['ICLR.cc/2019/Conference']	1538087808359		1547202826021	['ICLR.cc/2019/Conference']
892	1538087788941	{'title': 'On the Turing Completeness of Modern Neural Network Architectures', 'abstract': 'Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.', 'keywords': ['Transformer', 'NeuralGPU', 'Turing completeness'], 'authorids': ['jperez@dcc.uchile.cl', 'javier.marinkovic95@gmail.com', 'pbarcelo@dcc.uchile.cl'], 'authors': ['Jorge Pérez', 'Javier Marinković', 'Pablo Barceló'], 'TL;DR': 'We show that the Transformer architecture and the Neural GPU are Turing complete.', 'pdf': '/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf', 'paperhash': 'pérez|on_the_turing_completeness_of_modern_neural_network_architectures', '_bibtex': '@inproceedings{\npérez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge Pérez and Javier Marinković and Pablo Barceló},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}'}		HyGBdo0qFm	HyGBdo0qFm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	351	rkld3_qFtQ	['everyone']		['ICLR.cc/2019/Conference']	1538087788941		1547162771177	['ICLR.cc/2019/Conference']
893	1538087976563	{'title': 'Spherical CNNs on Unstructured Grids', 'abstract': 'We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals. \nTo this end, we replace conventional convolution kernels with linear combinations of differential operators that are weighted by learnable parameters. Differential operators can be efficiently estimated on unstructured grids using one-ring neighbors, and learnable parameters can be optimized through standard back-propagation. As a result, we obtain extremely efficient neural networks that match or outperform state-of-the-art network architectures in terms of performance but with a significantly lower number of network parameters. We evaluate our algorithm in an extensive series of experiments on a variety of computer vision and climate science tasks, including shape classification, climate pattern segmentation, and omnidirectional image semantic segmentation. Overall, we present (1) a novel CNN approach on unstructured grids using parameterized differential operators for spherical signals, and (2) we show that our unique kernel parameterization allows our model to achieve the same or higher accuracy with significantly fewer network parameters.', 'keywords': ['Spherical CNN', 'unstructured grid', 'panoramic', 'semantic segmentation', 'parameter efficiency'], 'authorids': ['chiyu.jiang@berkeley.edu', 'jingweih@stanford.edu', 'kkashinath@lbl.gov', 'prabhat@lbl.gov', 'pmarcus@me.berkeley.edu', 'niessner@tum.de'], 'authors': ['Chiyu Max Jiang', 'Jingwei Huang', 'Karthik Kashinath', 'Prabhat', 'Philip Marcus', 'Matthias Niessner'], 'TL;DR': 'We present a new CNN kernel for unstructured grids for spherical signals, and show significant accuracy and parameter efficiency gain on tasks such as 3D classfication and omnidirectional image segmentation.', 'pdf': '/pdf/74de4e6b515041d42b8292ad78da6a8d0c4c39a5.pdf', 'paperhash': 'jiang|spherical_cnns_on_unstructured_grids', '_bibtex': '@inproceedings{\njiang2018spherical,\ntitle={Spherical {CNN}s on Unstructured Grids},\nauthor={Chiyu Max Jiang and Jingwei Huang and Karthik Kashinath and Prabhat and Philip Marcus and Matthias Niessner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkl-43C9FQ},\n}'}		Bkl-43C9FQ	Bkl-43C9FQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1422	r1gdrYT5F7	['everyone']		['ICLR.cc/2019/Conference']	1538087976563		1547154135101	['ICLR.cc/2019/Conference']
894	1538087730393	{'title': 'Convolutional Neural Networks on Non-uniform Geometrical Signals Using Euclidean Spectral Transformation', 'abstract': 'Convolutional Neural Networks (CNN) have been successful in processing data signals that are uniformly sampled in the spatial domain (e.g., images). However, most data signals do not natively exist on a grid, and in the process of being sampled onto a uniform physical grid suffer significant aliasing error and information loss. Moreover, signals can exist in different topological structures as, for example, points, lines, surfaces and volumes. It has been challenging to analyze signals with mixed topologies (for example, point cloud with surface mesh). To this end, we develop mathematical formulations for Non-Uniform Fourier Transforms (NUFT) to directly, and optimally, sample nonuniform data signals of different topologies defined on a simplex mesh into the spectral domain with no spatial sampling error. The spectral transform is performed in the Euclidean space, which removes the translation ambiguity from works on the graph spectrum. Our representation has four distinct advantages: (1) the process causes no spatial sampling error during initial sampling, (2) the generality of this approach provides a unified framework for using CNNs to analyze signals of mixed topologies, (3) it allows us to leverage state-of-the-art backbone CNN architectures for effective learning without having to design a particular architecture for a particular data structure in an ad-hoc fashion, and (4) the representation allows weighted meshes where each element has a different weight (i.e., texture) indicating local properties. We achieve good results on-par with state-of-the-art for 3D shape retrieval task, and new state-of-the-art for point cloud to surface reconstruction task.', 'paperhash': 'jiang|convolutional_neural_networks_on_nonuniform_geometrical_signals_using_euclidean_spectral_transformation', 'keywords': ['Non-uniform Fourier Transform', '3D Learning', 'CNN', 'surface reconstruction'], 'authorids': ['chiyu.jiang@berkeley.edu', 'dqw@berkeley.edu', 'jingweih@stanford.edu', 'pmarcus@me.berkeley.edu', 'niessner@tum.de'], 'authors': ['Chiyu Max Jiang', 'Dequan Wang', 'Jingwei Huang', 'Philip Marcus', 'Matthias Niessner'], 'TL;DR': 'We use non-Euclidean Fourier Transformation of shapes defined by a simplicial complex for deep learning, achieving significantly better results than point-based sampling techiques used in current 3D learning literature.', 'pdf': '/pdf/77e67f8e10b7d6ebf9b6c70c117154a6ede31b56.pdf', '_bibtex': '@inproceedings{\njiang2018convolutional,\ntitle={Convolutional Neural Networks on Non-uniform Geometrical Signals Using Euclidean Spectral Transformation},\nauthor={Chiyu Max Jiang and Dequan Wang and Jingwei Huang and Philip Marcus and Matthias Niessner},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1G5ViAqFm},\n}'}		B1G5ViAqFm	B1G5ViAqFm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	27	S1eqoiK_tQ	['everyone']		['ICLR.cc/2019/Conference']	1538087730393		1547154106581	['ICLR.cc/2019/Conference']
895	1538087791005	{'title': 'Graph Wavelet Neural Network', 'abstract': 'We present graph wavelet neural network (GWNN), a novel graph convolutional neural network (CNN), leveraging graph wavelet transform to address the shortcomings of previous spectral graph CNN methods that depend on graph Fourier transform. Different from graph Fourier transform, graph wavelet transform can be obtained via a fast algorithm without requiring matrix eigendecomposition with high computational cost. Moreover, graph wavelets are sparse and localized in vertex domain, offering high efficiency and good interpretability for graph convolution. The proposed GWNN significantly outperforms previous spectral graph CNNs in the task of graph-based semi-supervised classification on three benchmark datasets: Cora, Citeseer and Pubmed.', 'keywords': ['graph convolution', 'graph wavelet transform', 'graph Fourier transform', 'semi-supervised learning'], 'authorids': ['xubingbing@ict.ac.cn', 'shenhuawei@ict.ac.cn', 'caoqi@ict.ac.cn', 'qiuyunqi@ict.ac.cn', 'cxq@ict.ac.cn'], 'authors': ['Bingbing Xu', 'Huawei Shen', 'Qi Cao', 'Yunqi Qiu', 'Xueqi Cheng'], 'TL;DR': 'We present graph wavelet neural network (GWNN), a novel graph convolutional neural network (CNN), leveraging graph wavelet transform to address the shortcoming of previous spectral graph CNN methods that depend on graph Fourier transform.', 'pdf': '/pdf/3298c2f91e55e505a6cb5cc98588ba2b5a76ad5a.pdf', 'paperhash': 'xu|graph_wavelet_neural_network', '_bibtex': '@inproceedings{\nxu2018graph,\ntitle={Graph Wavelet Neural Network},\nauthor={Bingbing Xu and Huawei Shen and Qi Cao and Yunqi Qiu and Xueqi Cheng},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1ewdiR5tQ},\n}'}		H1ewdiR5tQ	H1ewdiR5tQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	363	HkeuD5y9Ym	['everyone']		['ICLR.cc/2019/Conference']	1538087791005		1547117314839	['ICLR.cc/2019/Conference']
896	1538087788235	{'title': 'A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation', 'abstract': 'The convergence rate and final performance of common deep learning models have significantly benefited from recently proposed heuristics such as learning rate schedules, knowledge distillation, skip connections and normalization layers. In the absence of theoretical underpinnings, controlled experiments aimed at explaining the efficacy of these strategies can aid our understanding of deep learning landscapes and the training dynamics. Existing approaches for empirical analysis rely on tools of linear interpolation and visualizations with dimensionality reduction, each with their limitations. Instead, we revisit the empirical analysis of heuristics through the lens of recently proposed methods for loss surface and representation analysis, viz. mode connectivity and canonical correlation analysis (CCA), and hypothesize reasons why the heuristics succeed. In particular, we explore knowledge distillation and learning rate heuristics of (cosine) restarts and warmup using mode connectivity and CCA.  Our empirical analysis suggests that: (a) the reasons often quoted for the success of cosine annealing are not evidenced in practice; (b) that the effect of learning rate warmup is to prevent the deeper layers from creating training instability; and (c) that the latent knowledge shared by the teacher is primarily disbursed in the deeper layers.', 'keywords': ['deep learning heuristics', 'learning rate restarts', 'learning rate warmup', 'knowledge distillation', 'mode connectivity', 'SVCCA'], 'authorids': ['akhilesh.gotmare@epfl.ch', 'nkeskar@salesforce.com', 'cxiong@salesforce.com', 'rsocher@salesforce.com'], 'authors': ['Akhilesh Gotmare', 'Nitish Shirish Keskar', 'Caiming Xiong', 'Richard Socher'], 'TL;DR': 'We use empirical tools of mode connectivity and SVCCA to investigate neural network training heuristics of learning rate restarts, warmup and knowledge distillation.', 'pdf': '/pdf/e9289f90a3af3f458b6b8331b8080c3e5793812b.pdf', 'paperhash': 'gotmare|a_closer_look_at_deep_learning_heuristics_learning_rate_restarts_warmup_and_distillation', '_bibtex': '@inproceedings{\ngotmare2018a,\ntitle={A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation},\nauthor={Akhilesh Gotmare and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r14EOsCqKX},\n}'}		r14EOsCqKX	r14EOsCqKX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	347	SkxY3CPKFX	['everyone']		['ICLR.cc/2019/Conference']	1538087788235		1547112924218	['ICLR.cc/2019/Conference']
897	1547086623371	{'title': '[DEPRECATED] This version is outdated', 'comment': 'We have released a new version of this paper on arxiv https://arxiv.org/abs/1901.02860\nalong with code, pretrained models, hyperparameters, as well as new (even better) results.\n\nPlease refer to our arxiv version in your future work.'}		HJePno0cYm	BJxw7iX4fN	ICLR.cc/2019/Conference/-/Paper717/Official_Comment	['ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted']	12		['everyone']	HJePno0cYm	['ICLR.cc/2019/Conference/Paper717/Authors']	1547086623371		1547086623371	['ICLR.cc/2019/Conference/Paper717/Authors', 'ICLR.cc/2019/Conference']
898	1538087996564	{'title': 'Revealing interpretable object representations from human behavior', 'abstract': 'To study how mental object representations are related to behavior, we estimated sparse, non-negative representations of objects using human behavioral judgments on images representative of 1,854 object categories. These representations predicted a latent similarity structure between objects, which captured most of the explainable variance in human behavioral judgments. Individual dimensions in the low-dimensional embedding were found to be highly reproducible and interpretable as conveying degrees of taxonomic membership, functionality, and perceptual attributes. We further demonstrated the predictive power of the embeddings for explaining other forms of human behavior, including categorization, typicality judgments, and feature ratings, suggesting that the dimensions reflect human conceptual representations of objects beyond the specific task.', 'keywords': ['category representation', 'sparse coding', 'representation learning', 'interpretable representations'], 'authorids': ['charles.zheng@nih.gov', 'francisco.pereira@nih.gov', 'bakerchris@mail.nih.gov', 'martin.hebart@nih.gov'], 'authors': ['Charles Y. Zheng', 'Francisco Pereira', 'Chris I. Baker', 'Martin N. Hebart'], 'TL;DR': 'Human behavioral judgments are used to obtain sparse and interpretable representations of objects that generalize to other tasks', 'pdf': '/pdf/a46e0f965eaf30a542a1193a26ad70f9d036060f.pdf', 'paperhash': 'zheng|revealing_interpretable_object_representations_from_human_behavior', '_bibtex': '@inproceedings{\nzheng2018revealing,\ntitle={Revealing interpretable object representations from human behavior},\nauthor={Charles Y. Zheng and Francisco Pereira and Chris I. Baker and Martin N. Hebart},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxSrhC9KX},\n}'}		ryxSrhC9KX	ryxSrhC9KX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1536	Bke09e6cKQ	['everyone']		['ICLR.cc/2019/Conference']	1538087996564		1547064009144	['ICLR.cc/2019/Conference']
899	1538087786653	{'title': 'Near-Optimal Representation Learning for Hierarchical Reinforcement Learning', 'abstract': 'We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation -- the mapping of observation space to goal space -- is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods.', 'keywords': ['representation hierarchy reinforcement learning'], 'authorids': ['ofirnachum@google.com', 'shanegu@google.com', 'honglak@google.com', 'svlevine@eecs.berkeley.edu'], 'authors': ['Ofir Nachum', 'Shixiang Gu', 'Honglak Lee', 'Sergey Levine'], 'TL;DR': 'We translate a bound on sub-optimality of representations to a practical training objective in the context of hierarchical reinforcement learning.', 'pdf': '/pdf/1bb1b57d88a73d614370d0d3fb6cd75e6c5cab0e.pdf', 'paperhash': 'nachum|nearoptimal_representation_learning_for_hierarchical_reinforcement_learning', '_bibtex': '@inproceedings{\nnachum2018nearoptimal,\ntitle={Near-Optimal Representation Learning for Hierarchical Reinforcement Learning},\nauthor={Ofir Nachum and Shixiang Gu and Honglak Lee and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1emus0qF7},\n}'}		H1emus0qF7	H1emus0qF7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	338	r1x-x3OqFX	['everyone']		['ICLR.cc/2019/Conference']	1538087786653		1547049709330	['ICLR.cc/2019/Conference']
900	1538087773098	{'title': 'Quaternion Recurrent Neural Networks', 'abstract': 'Recurrent neural networks (RNNs) are powerful architectures to model sequential data, due to their capability to learn short and long-term dependencies between the basic elements of a sequence. Nonetheless, popular tasks such as speech or images recognition, involve multi-dimensional input features that are characterized by strong internal dependencies between the dimensions of the input vector. We propose a novel quaternion recurrent neural network (QRNN), alongside with a quaternion long-short term memory neural network (QLSTM), that take into account both the external relations and these internal structural dependencies with the quaternion algebra. Similarly to capsules, quaternions allow the QRNN to code internal dependencies by composing and processing multidimensional features as single entities, while the recurrent operation reveals correlations between the elements composing the sequence. We show that both QRNN and QLSTM achieve better performances than RNN and LSTM in a realistic application of automatic speech recognition. Finally, we show that QRNN and QLSTM reduce by a maximum factor of 3.3x the number of free parameters needed, compared to real-valued RNNs and LSTMs to reach better results, leading to a more compact representation of the relevant information.', 'keywords': ['Quaternion recurrent neural networks', 'quaternion numbers', 'recurrent neural networks', 'speech recognition'], 'authorids': ['titouan.parcollet@alumni.univ-avignon.fr', 'mirco.ravanelli@gmail.com', 'mohamed.morchid@univ-avignon.fr', 'georges.linares@univ-avignon.fr', 'chiheb.trabelsi@polymtl.ca', 'rdemori@cs.mcgill.ca', 'yoshua.bengio@mila.quebec'], 'authors': ['Titouan Parcollet', 'Mirco Ravanelli', 'Mohamed Morchid', 'Georges Linarès', 'Chiheb Trabelsi', 'Renato De Mori', 'Yoshua Bengio'], 'pdf': '/pdf/61bfe1883871c0199c6c47572ed9c8f6de9ebe3c.pdf', 'paperhash': 'parcollet|quaternion_recurrent_neural_networks', '_bibtex': '@inproceedings{\nparcollet2018quaternion,\ntitle={Quaternion Recurrent Neural Networks},\nauthor={Titouan Parcollet and Mirco Ravanelli and Mohamed Morchid and Georges Linarès and Chiheb Trabelsi and Renato De Mori and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByMHvs0cFQ},\n}'}		ByMHvs0cFQ	ByMHvs0cFQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	261	rJl2swNFKm	['everyone']		['ICLR.cc/2019/Conference']	1538087773098		1547048813162	['ICLR.cc/2019/Conference']
901	1538087796305	{'title': 'Efficient Lifelong Learning with A-GEM', 'abstract': 'In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency', 'keywords': ['Lifelong Learning', 'Continual Learning', 'Catastrophic Forgetting', 'Few-shot Transfer'], 'authorids': ['arslan.chaudhry@eng.ox.ac.uk', 'ranzato@fb.com', 'mrf@fb.com', 'elhoseiny@fb.com'], 'authors': ['Arslan Chaudhry', 'Marc’Aurelio Ranzato', 'Marcus Rohrbach', 'Mohamed Elhoseiny'], 'TL;DR': 'An efficient lifelong learning algorithm that provides a better trade-off between accuracy and time/ memory complexity compared to other algorithms. ', 'pdf': '/pdf/68b34388f383ea8919ffc158ee58a351da811add.pdf', 'paperhash': 'chaudhry|efficient_lifelong_learning_with_agem', '_bibtex': '@inproceedings{\nchaudhry2018efficient,\ntitle={Efficient Lifelong Learning with A-{GEM}},\nauthor={Arslan Chaudhry and Marc’Aurelio Ranzato and Marcus Rohrbach and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkf2_sC5FX},\n}'}		Hkf2_sC5FX	Hkf2_sC5FX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	392	r1lYDHPctm	['everyone']		['ICLR.cc/2019/Conference']	1538087796305		1547031779185	['ICLR.cc/2019/Conference']
902	1547017069994	"{'comment': ""The paper proposes an exploratory algorithm which replaces exploration approach such as e-greedy, which only relies on random walks, in favor of a goal oriented Reinforcement Learning (RL) approach. The authors propose Q-map, a convolutional autoencoder-like architecture that is used to simultaneously produce value estimates for all possible goals in compatible environments i.e environments that support spatial coordinates as goals. Finally, the authors report the results of a RL agent that explores using Q-map and exploits using a DQN on Montezuma’s Revenge and Super Mario All-Stars environment. \n\nWe tried to reproduce the authors' result on the Super Mario All-Stars environment and sought to extend the scope of the experimentation by testing the generalization of the agent as well. The authors have made public their code and we ported it to the Pytorch framework. While the authors have mentioned the details of most of the hyperparameters being used, the details regarding how the authors deal with a sliding window present in the environment are a little unclear. \n\nThe results regarding the comparison of the performance of the proposed algorithm and the baseline that we generated differ from those in the paper. Our results indicate a better performance of the baseline algorithm than the proposed algorithm. We tried to recreate the exact conditions under which the results in the paper were observed but the paper mentions an averaging of multiple runs of the algorithm with different seeds. As the algorithm and the baseline are required to be run for 5 million timesteps, which together end up taking 6 days to complete, we had to report the results for only a single run. Without any training or finetuning, the proposed agent generalizes poorly on unseen level, though this can be explained by the very different backgrounds of the levels."", 'title': 'Findings of the ICLR 2019 reproducibility challenge'}"		rye7XnRqFm	rke8uoz7MV	ICLR.cc/2019/Conference/-/Paper1346/Public_Comment	[]	1		['everyone']	rye7XnRqFm	['~Shishir_Sharma1']	1547017069994		1547017134890	['~Shishir_Sharma1', 'ICLR.cc/2019/Conference']
903	1546936941561	"{'title': 'Thanks a lot!', 'comment': ""Thanks a lot for your interest in our paper and your reproduction of our results. \n\n* GANs is currently a mystery. When you joint training the generator and discriminator, there exists a lot of issues on the minimax conflict and generator-discriminator balance/tradeoff in GANs, even using the relatively mature WGAN. So, we choose to train the discriminator only, i.e.., fixing the generator. \n\n* WGAN requires the discriminative function to be k-Lipschitz. However, how to enforce the Lipschitz is currently a not well-addressed problem. Though WGAN-GP provides a solution which is well accepted, we believe it is not an accurate implementation of Lipschitz. We thus choose MaxGP which we believe is a more accurate implementation. See Appendix E in [1] for details. \n\n* The penalty coefficient is set to 0.1 because we empirically found that no matter for MaxGP or the original GP, it is better than 10.0 in most cases. So, we use 0.1 as the default. \n\n* Thanks for point out the discrepancy between the implementation and the description in the paper. We think we have miswritten it in the paper, though in the current form of the paper, to match the implementation, we only need reformulation the function phi to be sqrt(max x^2). We choose the square the gradient first because we'd like $v$ to reflect the gradient scale, otherwise, the max operation will ignore the negative ones. We will revise the paper to make it more clear. Thanks. \n\n[1] Understanding the Effectiveness of Lipschitz-Continuity in Generative Adversarial Nets: https://openreview.net/pdf?id=r1zOg309tX \n\nBTW: did you try different learning rate for AdaShift in the WGAN experiments? Its best learning rate is usually around ten times the learning rate of Adam. According to our experiences, they should be comparable. \n""}"		HkgTkhRcKQ	BygUdM1fGE	ICLR.cc/2019/Conference/-/Paper1028/Official_Comment	['ICLR.cc/2019/Conference/Paper1028/Reviewers/Unsubmitted']	9		['everyone']	BJglD8rbzV	['ICLR.cc/2019/Conference/Paper1028/Authors']	1546936941561		1547001786544	['ICLR.cc/2019/Conference/Paper1028/Authors', 'ICLR.cc/2019/Conference']
904	1538087870016	{'title': 'Quasi-hyperbolic momentum and Adam for deep learning', 'abstract': 'Momentum-based acceleration of stochastic gradient descent (SGD) is widely used in deep learning. We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step. We describe numerous connections to and identities with other algorithms, and we characterize the set of two-state optimization algorithms that QHM can recover. Finally, we propose a QH variant of Adam called QHAdam, and we empirically demonstrate that our algorithms lead to significantly improved training in a variety of settings, including a new state-of-the-art result on WMT16 EN-DE. We hope that these empirical results, combined with the conceptual and practical simplicity of QHM and QHAdam, will spur interest from both practitioners and researchers. Code is immediately available.', 'keywords': ['sgd', 'momentum', 'nesterov', 'adam', 'qhm', 'qhadam', 'optimization'], 'authorids': ['maj@fb.com', 'denisy@fb.com'], 'authors': ['Jerry Ma', 'Denis Yarats'], 'TL;DR': 'Mix plain SGD and momentum (or do something similar with Adam) for great profit.', 'pdf': '/pdf/d2ea4b4ed5c48630b8c757bb107e9d470a5957ff.pdf', 'paperhash': 'ma|quasihyperbolic_momentum_and_adam_for_deep_learning', '_bibtex': '@inproceedings{\nma2018quasihyperbolic,\ntitle={Quasi-hyperbolic momentum and Adam for deep learning},\nauthor={Jerry Ma and Denis Yarats},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1fUpoR5FQ},\n}'}		S1fUpoR5FQ	S1fUpoR5FQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	805	S1etVYa5YQ	['everyone']		['ICLR.cc/2019/Conference']	1538087870016		1546995089212	['ICLR.cc/2019/Conference']
905	1546895069752	{'title': 'Re: Problems on the implementation of LightConv', 'comment': '1. Thank you for pointing out this typo.  It should reduce the number of parameters by a factor of “d/H” rather than “H”, so 112 is still the correct number.\n\n2. The matrix would be a band matrix, i.e. entries outside of the kernel are zeros. As you noticed, this is similar to the matrix multiplication in the self-attention, which has O(n^2) time complexity. However, we observe that when the sequence is short (< 1000), this implementation is practically faster.\n'}		SkVhlh09tX	SJg8yJrbG4	ICLR.cc/2019/Conference/-/Paper1115/Official_Comment	['ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted']	11		['everyone']	H1xwAVoPb4	['ICLR.cc/2019/Conference/Paper1115/Authors']	1546895069752		1546993110380	['ICLR.cc/2019/Conference/Paper1115/Authors', 'ICLR.cc/2019/Conference']
906	1538087745964	{'title': 'Post Selection Inference with Incomplete Maximum Mean Discrepancy Estimator', 'abstract': 'Measuring divergence between two distributions is essential in machine learning and statistics and has various applications including binary classification, change point detection, and two-sample test. Furthermore, in the era of big data, designing divergence measure that is interpretable and can handle high-dimensional and complex data becomes extremely important. In this paper, we propose a post selection inference (PSI) framework for divergence measure, which can select a set of statistically significant features that discriminate two distributions. Specifically, we employ an additive variant of maximum mean discrepancy (MMD) for features and introduce a general hypothesis test for PSI. A novel MMD estimator using the incomplete U-statistics, which has an asymptotically normal distribution (under mild assumptions) and gives high detection power in PSI, is also proposed and analyzed theoretically. Through synthetic and real-world feature selection experiments, we show that the proposed framework can successfully detect statistically significant features. Last, we propose a sample selection framework for analyzing different members in the Generative Adversarial Networks (GANs) family. ', 'keywords': ['Maximum Mean Discrepancy', 'Selective Inference', 'Feature Selection', 'GAN'], 'authorids': ['makoto.yamada@riken.jp', 'yiwu1@andrew.cmu.edu', 'yaohungt@cs.cmu.edu', 'hirofumi-ohta@g.ecc.u-tokyo.ac.jp', 'rsalakhu@cs.cmu.edu', 'takeuchi.ichiro@nitech.ac.jp', 'fukumizu@ism.ac.jp'], 'authors': ['Makoto Yamada', 'Denny Wu', 'Yao-Hung Hubert Tsai', 'Hirofumi Ohta', 'Ruslan Salakhutdinov', 'Ichiro Takeuchi', 'Kenji Fukumizu'], 'pdf': '/pdf/58247a068f5dc435934b5b11e910ef0316338b4b.pdf', 'paperhash': 'yamada|post_selection_inference_with_incomplete_maximum_mean_discrepancy_estimator', '_bibtex': '@inproceedings{\nyamada2018post,\ntitle={Post Selection Inference with Incomplete Maximum Mean Discrepancy Estimator},\nauthor={Makoto Yamada and Denny Wu and Yao-Hung Hubert Tsai and Hirofumi Ohta and Ruslan Salakhutdinov and Ichiro Takeuchi and Kenji Fukumizu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkG5SjR5YQ},\n}'}		BkG5SjR5YQ	BkG5SjR5YQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	111	rJxhUGDUKX	['everyone']		['ICLR.cc/2019/Conference']	1538087745964		1546991454289	['ICLR.cc/2019/Conference']
907	1546981334097	{'title': 'Siamese architecture', 'comment': 'Thanks for the comment.\n\n- The input to each branch of our Siamese architecture is a one-hot encoding of a word from the vocabulary. Next, each of the branches goes through a fully-connected linear layer, followed by the point cloud layer, which is a fully-connected linear layer whose output is normalized to lie within the unit ball. The Sinkhorn distance is then applied to the outputs of the two branches to compute the distances used within the contrastive loss.\n\n- You’re right, thanks!'}		rJg4J3CqFm	S1xCCk9GfN	ICLR.cc/2019/Conference/-/Paper978/Official_Comment	['ICLR.cc/2019/Conference/Paper978/Reviewers/Unsubmitted']	16		['everyone']	HJe5LctXW4	['ICLR.cc/2019/Conference/Paper978/Authors']	1546981334097		1546981334097	['ICLR.cc/2019/Conference/Paper978/Authors', 'ICLR.cc/2019/Conference']
908	1538087909254	{'title': 'AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods', 'abstract': 'Adam is shown not being able to converge to the optimal solution in certain cases. Researchers recently propose several algorithms to avoid the issue of non-convergence of Adam, but their efficiency turns out to be unsatisfactory in practice. In this paper, we provide a new insight into the non-convergence issue of Adam as well as other adaptive learning rate methods. We argue that there exists an inappropriate correlation between gradient $g_t$ and the second moment term $v_t$ in Adam ($t$ is the timestep), which results in that a large gradient is likely to have small step size while a small gradient may have a large step size. We demonstrate that such unbalanced step sizes are the fundamental cause of non-convergence of Adam, and we further prove that decorrelating $v_t$ and $g_t$ will lead to unbiased step size for each gradient, thus solving the non-convergence problem of Adam. Finally, we propose AdaShift, a novel adaptive learning rate method that decorrelates $v_t$ and $g_t$ by temporal shifting, i.e., using temporally shifted gradient $g_{t-n}$ to calculate $v_t$. The experiment results demonstrate that AdaShift is able to address the non-convergence issue of Adam, while still maintaining a competitive performance with Adam in terms of both training speed and generalization. ', 'keywords': ['optimizer', 'Adam', 'convergence', 'decorrelation'], 'authorids': ['heyohai@apex.sjtu.edu.cn', 'neverquit@sjtu.edu.cn', 'gslu@apex.sjtu.edu.cn', 'wanghongwei55@gmail.com', 'wnzhang@sjtu.edu.cn', 'yyu@apex.sjtu.edu.cn'], 'authors': ['Zhiming Zhou*', 'Qingru Zhang*', 'Guansong Lu', 'Hongwei Wang', 'Weinan Zhang', 'Yong Yu'], 'TL;DR': 'We analysis and solve the non-convergence issue of Adam.', 'pdf': '/pdf/6e6f6adc0995fefe1616ec27b4c33639b4c7abdd.pdf', 'paperhash': 'zhou|adashift_decorrelation_and_convergence_of_adaptive_learning_rate_methods', '_bibtex': '@inproceedings{\nzhou2018adashift,\ntitle={AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods},\nauthor={Zhiming Zhou and Qingru Zhang and Guansong Lu and Hongwei Wang and Weinan Zhang and Yong Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkgTkhRcKQ},\n}'}		HkgTkhRcKQ	HkgTkhRcKQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1028	r1e-e-M5t7	['everyone']		['ICLR.cc/2019/Conference']	1538087909254		1546938380799	['ICLR.cc/2019/Conference']
909	1546768217394	{'comment': 'We reproduce the results for this 2019 ICLR submission under the ICLR reproducibility challenge 2019. We also review the claims and findings of the paper carefully. We study the probability distributions which the algorithm converges to during different runs, and try to explain the behaviour. We point out some limitations in their work, provide a discussion of their results. We also try to verify the extent of their claims, and actual effect of their algorithm. Precisely, we try to find how much tuning does it require and how exactly is it benefiting the test accuracy. This underscores some limitations in their work. Finally, we offer some suggestions to improve their research. Our full report can be found at https://openreview.net/forum?id=SJlGaRBJMN . Our reproducibility code can be found at https://github.com/AnayMehrotra/Reproducibility_Challenge_ICLR_2019 .', 'title': 'Summary of submission to ICLR reproducibility challenge 2019'}		H1lGHsA9KX	SklbPkUkzE	ICLR.cc/2019/Conference/-/Paper68/Public_Comment	[]	1		['everyone']	H1lGHsA9KX	['(anonymous)']	1546768217394		1546937366496	['(anonymous)', 'ICLR.cc/2019/Conference']
910	1546906719039	"{'comment': 'We have carried out a  reproducibility analysis of this interesting paper on meta-learning. Some parameters and training methodologies, which would be required for full reproducibility, are not present in the manuscript at the time of writing:\n- stride of the convolutional filters\n- padding of the convolutional filters\n- a clear stopping criterion (<-> ""the error on the meta-validation set does not decrease meaningfully for 20k episodes""),\n\nHowever, making reasonable assumptions, we were able to reproduce the most important part of the paper (R2D2) in TensorFlow and achieve similar results. We did not reproduce the LRD2 part of the paper, as we wanted to focus on the truly differentiable closed-form solver (R2D2). Most importantly, we were able to reproduce the increase in performance of the proposed method (with the given architecture) over some reproduced baseline results, which supports the conclusions in the original paper.\n\nThe different neural network architectures should be taken into consideration when comparing results. For example the MAML baseline of Finn et al. (2017) uses four convolutional blocks with [32, 32, 32, 32] filters, whereas this paper\'s four blocks employ a [96, 192,384, 512] scheme. Because of this we implemented R2D2 with both the architecture mentioned in the paper and the MAML baseline architecture. In our reproducibility report we show that when using the exact same baseline architecture as MAML, and standard training procedure, the improvement in performance of the proposed method is not clear.\n\nOur full reproducibility report is available at: https://github.com/reproducibility-challenge/iclr_2019/blob/c53e6c1ea8d0e158f66b7d70681fa6ecde6a4f2b/papers/LCAX-HyxnZh0ct7/LCAX.pdf\nOur codebase: https://github.com/ArnoutDevos/r2d2', 'title': 'ICLR 2019 Reproducibility Challenge key findings'}"		HyxnZh0ct7	BkxDPnDZMV	ICLR.cc/2019/Conference/-/Paper1211/Public_Comment	[]	8		['everyone']	HyxnZh0ct7	['(anonymous)']	1546906719039		1546906719039	['(anonymous)', 'ICLR.cc/2019/Conference']
911	1546906028305	{'comment': 'As part of the ICLR 2019 Reproducibility Challenge, we (Sheldon Benard, Vincent Luczkow, & Samin Yeasar Arnob) attempted to replicate the results of Discriminator-Actor-Critic:  Addressing Sample Inefficiency and Reward Bias in Inverse Reinforcement Learning.  Discriminator-Actor-Critic (DAC) is an adversarial imitation learning algorithm. It uses an off-policy reinforcement learning algorithm to improve upon the sampling efficiency of existing methods, and it extends the learning environment with absorbing states and uses a new reward function in order to achieve unbiased rewards.  We were able to achieve comparable rewards and sample efficiency on two of the four environments.  For the environments in which we were unable to reproduce the original results, we will continue to perform experiments and converse with the authors.   All  of  our  code  is  available  at: \n\nhttps://github.com/vluzko/dac-iclr-reproducibility', 'title': 'ICLR 2019 Reproducibility Challenge Description'}		Hk4fpoA5Km	S1xE3KP-fE	ICLR.cc/2019/Conference/-/Paper784/Public_Comment	[]	8		['everyone']	Hk4fpoA5Km	['~Sheldon_Benard2']	1546906028305		1546906519743	['~Sheldon_Benard2', 'ICLR.cc/2019/Conference']
912	1546901092118	{'title': 'Segmentation label propagation using deep convolutional neural networks and dense conditional random field', 'authors': ['Mingchen Gao', 'Ziyue Xu', 'Le Lu', 'Aaron Wu', 'Isabella Nogues', 'Ronald M Summers', 'Daniel J Mollura'], 'authorids': ['ziyue.xu@gmail.com'], 'pdf': '/pdf/68912041bdf11ff7972897d0c0aed67482cbf0d8.pdf', 'paperhash': 'gao|segmentation_label_propagation_using_deep_convolutional_neural_networks_and_dense_conditional_random_field'}		Bkg2wLUWME	Bkg2wLUWME	OpenReview.net/Archive/-/Direct_Upload	[]	202		['everyone']		['~Ziyue_Xu1']	1546901092118		1546901092118	['~Ziyue_Xu1']
913	1546901030085	{'title': 'Deep vessel tracking: a generalized probabilistic approach via deep learning', 'authors': ['Aaron Wu', 'Ziyue Xu', 'Mingchen Gao', 'Mario Buty', 'Daniel J Mollura'], 'authorids': ['ziyue.xu@gmail.com'], 'pdf': '/pdf/5e0043241d967e61ba2bd6a10b2bcf5675886a4d.pdf', 'paperhash': 'wu|deep_vessel_tracking_a_generalized_probabilistic_approach_via_deep_learning'}		B1eCQULbG4	B1eCQULbG4	OpenReview.net/Archive/-/Direct_Upload	[]	201		['everyone']		['~Ziyue_Xu1']	1546901030085		1546901030085	['~Ziyue_Xu1']
914	1546900968491	{'title': 'Characterization of lung nodule malignancy using hybrid shape and appearance features', 'authors': ['Mario Buty', 'Ziyue Xu', 'Mingchen Gao', 'Ulas Bagci', 'Aaron Wu', 'Daniel J Mollura'], 'authorids': ['ziyue.xu@gmail.com'], 'pdf': 'https://arxiv.org/pdf/1609.06668.pdf', 'paperhash': 'buty|characterization_of_lung_nodule_malignancy_using_hybrid_shape_and_appearance_features'}		HJgxxIU-fN	HJgxxIU-fN	OpenReview.net/Archive/-/Direct_Upload	[]	200		['everyone']		['~Ziyue_Xu1']	1546900968491		1546900968491	['~Ziyue_Xu1']
915	1546900893040	{'title': 'Multi-label deep regression and unordered pooling for holistic interstitial lung disease pattern detection', 'authors': ['Mingchen Gao', 'Ziyue Xu', 'Le Lu', 'Adam P Harrison', 'Ronald M Summers', 'Daniel J Mollura'], 'authorids': ['ziyue.xu@gmail.com'], 'pdf': 'https://cse.buffalo.edu/~mgao8/files/2016_MLMI_ILD_Detection.pdf', 'paperhash': 'gao|multilabel_deep_regression_and_unordered_pooling_for_holistic_interstitial_lung_disease_pattern_detection'}		H1eHjrLWzV	H1eHjrLWzV	OpenReview.net/Archive/-/Direct_Upload	[]	199		['everyone']		['~Ziyue_Xu1']	1546900893040		1546900893040	['~Ziyue_Xu1']
916	1546900792516	{'title': 'A multichannel block‐matching denoising algorithm for spectral photon‐counting CT images', 'authors': ['Adam P. Harrison', 'Ziyue Xu', 'Amir Pourmorteza', 'David A. Bluemke', 'and Daniel J. Mollura'], 'authorids': ['ziyue.xu@gmail.com'], 'pdf': '/pdf/4e3521da7151019f66e709251f0e6850053e5453.pdf', 'paperhash': 'harrison|a_multichannel_blockmatching_denoising_algorithm_for_spectral_photoncounting_ct_images'}		HJWrB8WGV	HJWrB8WGV	OpenReview.net/Archive/-/Direct_Upload	[]	198		['everyone']		['~Ziyue_Xu1']	1546900792516		1546900792516	['~Ziyue_Xu1']
917	1546900642862	{'title': 'Progressive and multi-path holistically nested neural networks for pathological lung segmentation from CT images', 'authors': ['Adam P Harrison', 'Ziyue Xu', 'Kevin George', 'Le Lu', 'Ronald M Summers', 'Daniel J Mollura'], 'authorids': ['ziyue.xu@gmail.com'], 'pdf': 'https://arxiv.org/pdf/1706.03702.pdf', 'paperhash': 'harrison|progressive_and_multipath_holistically_nested_neural_networks_for_pathological_lung_segmentation_from_ct_images'}		Byesi4UbGV	Byesi4UbGV	OpenReview.net/Archive/-/Direct_Upload	[]	197		['everyone']		['~Ziyue_Xu1']	1546900642862		1546900642862	['~Ziyue_Xu1']
918	1546900589470	{'title': '3D Convolutional Neural Networks with Graph Refinement for Airway Segmentation Using Incomplete Data Labels', 'authors': ['Dakai Jin', 'Ziyue Xu', 'Adam P Harrison', 'Kevin George', 'Daniel J Mollura'], 'authorids': ['ziyue.xu@gmail.com'], 'pdf': 'https://www.researchgate.net/profile/Dakai_Jin/publication/319578238_3D_Convolutional_Neural_Networks_with_Graph_Refinement_for_Airway_Segmentation_Using_Incomplete_Data_Labels/links/59c867d9a6fdccc71923feab/3D-Convolutional-Neural-Networks-with-Graph-Refinement-for-Airway-Segmentation-Using-Incomplete-Data-Labels.pdf', 'paperhash': 'jin|3d_convolutional_neural_networks_with_graph_refinement_for_airway_segmentation_using_incomplete_data_labels'}		Bkerd4IbG4	Bkerd4IbG4	OpenReview.net/Archive/-/Direct_Upload	[]	196		['everyone']		['~Ziyue_Xu1']	1546900589470		1546900589470	['~Ziyue_Xu1']
919	1546900531066	{'title': 'Pathological Pulmonary Lobe Segmentation from CT Images using Progressive Holistically Nested Neural Networks and Random Walker', 'authors': ['Kevin George', 'Adam P Harrison', 'Dakai Jin', 'Ziyue Xu', 'Daniel J Mollura'], 'authorids': ['ziyue.xu@gmail.com'], 'pdf': 'https://arxiv.org/pdf/1708.04503.pdf', 'paperhash': 'george|pathological_pulmonary_lobe_segmentation_from_ct_images_using_progressive_holistically_nested_neural_networks_and_random_walker'}		BygsNNL-zV	BygsNNL-zV	OpenReview.net/Archive/-/Direct_Upload	[]	195		['everyone']		['~Ziyue_Xu1']	1546900531066		1546900531066	['~Ziyue_Xu1']
920	1546900425997	{'title': 'Holistic classification of CT attenuation patterns for interstitial lung diseases via deep convolutional neural networks', 'authors': ['Mingchen Gao', 'Ulas Bagci', 'Le Lu', 'Aaron Wu', 'Mario Buty', 'Hoo-Chang Shin', 'Holger Roth', 'Georgios Z Papadakis', 'Adrien Depeursinge', 'Ronald M Summers', 'Ziyue Xu', 'Daniel J Mollura'], 'authorids': ['ziyue.xu@gmail.com'], 'pdf': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5881940/pdf/nihms801793.pdf', 'paperhash': 'gao|holistic_classification_of_ct_attenuation_patterns_for_interstitial_lung_diseases_via_deep_convolutional_neural_networks'}		SyxMA7I-ME	SyxMA7I-ME	OpenReview.net/Archive/-/Direct_Upload	[]	194		['everyone']		['~Ziyue_Xu1']	1546900425997		1546900425997	['~Ziyue_Xu1']
921	1546900330561	{'title': 'White matter hyperintensity segmentation from T1 and FLAIR images using fully convolutional neural networks enhanced with residual connections', 'authors': ['Dakai Jin', 'Ziyue Xu', 'Adam P Harrison', 'Daniel J Mollura'], 'authorids': ['ziyue.xu@gmail.com'], 'pdf': 'https://arxiv.org/pdf/1803.06782.pdf', 'paperhash': 'jin|white_matter_hyperintensity_segmentation_from_t1_and_flair_images_using_fully_convolutional_neural_networks_enhanced_with_residual_connections'}		HJemuX8-MN	HJemuX8-MN	OpenReview.net/Archive/-/Direct_Upload	[]	193		['everyone']		['~Ziyue_Xu1']	1546900330561		1546900330561	['~Ziyue_Xu1']
922	1546900283192	{'title': 'Joint solution for PET image segmentation, denoising, and partial volume correction', 'authors': ['Ziyue Xu', 'Mingchen Gao', 'Georgios Z Papadakis', 'Brian Luna', 'Sanjay Jain', 'Daniel J Mollura', 'Ulas Bagci'], 'authorids': ['ziyue.xu@gmail.com'], 'pdf': 'http://www.cs.ucf.edu/~bagci/publications/jointsolution.pdf', 'paperhash': 'xu|joint_solution_for_pet_image_segmentation_denoising_and_partial_volume_correction'}		Hyx7rQ8ZGV	Hyx7rQ8ZGV	OpenReview.net/Archive/-/Direct_Upload	[]	192		['everyone']		['~Ziyue_Xu1']	1546900283192		1546900283192	['~Ziyue_Xu1']
923	1546898681786	{'pdf': 'https://arxiv.org/pdf/1806.04051.pdf', 'title': 'CT-Realistic Lung Nodule Simulation from 3D Conditional Generative Adversarial Networks for Robust Lung Segmentation', 'authors': ['Dakai Jin', 'Ziyue Xu', 'Youbao Tang', 'Adam P Harrison', 'Daniel J Mollura'], 'authorids': ['ziyue.xu@gmail.com'], 'paperhash': 'jin|ctrealistic_lung_nodule_simulation_from_3d_conditional_generative_adversarial_networks_for_robust_lung_segmentation'}		rygzbaBWGV	rygzbaBWGV	OpenReview.net/Archive/-/Direct_Upload	[]	191		['everyone']		['~Ziyue_Xu1']	1546898681786		1546900214998	['~Ziyue_Xu1']
924	1546897114158	{'comment': 'As a part of the ICLR 2019 reproducibility challenge, we worked to reproduce the results of this paper (h-detach). The code for reproducing the results was provided by the authors. We perform some of the experiments mentioned in the paper. A link to the full report and the codebase can be found at the end of this message.\n\nThe authors tackle the exploding and vanishing gradient problem(EVGP)  in the LSTM. The authors say that when the weights of the LSTM are large, due to their repeated multiplication, the gradients through the cell state path get suppressed. The authors empirically prove that this path carries information about long-term dependencies. The authors have also provided theoretical proof of their claim. \n\nThe authors have proposed a stochastic algorithm to mitigate the above mentioned problems. The main motive of the authors is to prove that training an LSTM with h-detach results in faster convergence and more stability during training. We performed experiments for the copying task, sequential mnist task and our results were able to confirm the claims made by the author. We also performed the experiments mentioned in the ablation study and were able to get similar results as the authors.\n\nThe authors have given information on how they chose the detach probability for the sequential mnist task. They have also mentioned, in a reply to AnonReviewer3 below, that they tried different values to of the probability and found the values between 0.25 and 0.50 work best. They have sufficiently experimented with their algorithm on different learning rates and seeds. It would also be interesting to see how the performance of h-detach is affected by a change in batch size.\n\nIn conclusion, we have validated the claims of the authors through our experiments that using h-detach stabilizes training, leads to faster convergence and is robust to different seeds and learning rates.\n\ncodebase: https://github.com/dido1998/h-detach\nreport: https://drive.google.com/drive/folders/1EtEYBS0LRRGsCwhxiXwfDY2ndsgCJPpc?usp=sharing\n\n', 'title': 'Reproducibility study of h-detach:Modifying the LSTM Gradient Towards Better Optimization'}		ryf7ioRqFX	rJlMyDSbGV	ICLR.cc/2019/Conference/-/Paper609/Public_Comment	[]	2		['everyone']	ryf7ioRqFX	['~Aniket_Rajiv_Didolkar1']	1546897114158		1546897114158	['~Aniket_Rajiv_Didolkar1', 'ICLR.cc/2019/Conference']
925	1546896984506	{'comment': 'We reproduced some of the experiments from this paper as a part of the ICLR reproducibility challenge. While we replicated the optimizer and the experiments in PyTorch, we would like to thank the authors for sharing their implementation in TensorFlow as that made verification of the results easier. Overall, our findings are close to the results from the paper and we summarize them as follows:\n\n* While the authors have not experimented with the sequential online optimization problem proposed by Reddi et al. (2018) since it did not fit their assumptions, we found that the proposed AdaShift optimizer is able to converge but is slower than AMSGrad. At the same time we also confirm the results of the experiment with a stochastic counterexample on which Adam fails.\n\n* In the logistic regression and MLP experiments on MNIST the results of all optimizers are quite similar in our experiments as well as in the paper.\n\n* We also tried to reproduce more complex experiments such as WGAN-GP and NMT. In WGAN-GP we note the atypical way in which the authors conducted their experiments with a generative model, specifically, by fixing generator and training only discriminator. We trained both discriminator and generator and in our experiments using AdaShift lead to worse Inception Score than using either Adam or AMSGrad which in turn had quite similar results. Additionally, we note that the authors set a penalty coefficient to 0.1 which is significantly different from its value in the paper that proposed this method where it was equal to 10. We also note that authors use maxGP version of the loss function in their code. In our opinion, motivation and discussion of these choices would be beneficial. At the same time for the translation model, AdaShift showed the best result which is similar to the author’s experiment and the difference from other methods is even more significant.\n\n* We found a discrepancy between the implementation of the AdaShift provided by the authors and its description in the paper: in the paper it is repeatedly stated that the result of applying the function \\phi to the gradient should be squared, while in the code the function is applied to the square of the gradient. Could you please elaborate on this?  While we found that on the synthetic and MNIST experiments both orders lead to very similar results, we think that clarification is still needed.\n', 'title': 'ICLR Reproducibility Challenge'}		HkgTkhRcKQ	BJglD8rbzV	ICLR.cc/2019/Conference/-/Paper1028/Public_Comment	[]	3		['everyone']	HkgTkhRcKQ	['~Mikhail_Konobeev1']	1546896984506		1546896984506	['~Mikhail_Konobeev1', 'ICLR.cc/2019/Conference']
926	1546891847159	"{'comment': ""As part of the ICLR 2019 Reproducibility Challenge, we worked to reproduce the results reported in this paper (Variational Sparse Coding). Given no available code for the project, we implemented the variational autoencoder architecture described in the paper from scratch.  We validate the experimental results and further propose improvements or future research directions that may contribute to the machine learning community. A link to the full report, as well as the repository with code, can be found at the end of this message.\n\nThe authors' main motivation for this work lies in developing a model able to learn sparse representations that are informative (for further classification tasks) and interpretable (by exploring the latent sparse space). In this line of thought, they propose an improvement over the Variational AutoEncoder model, explicitly modeling sparsity in the latent space with a Spike and Slab prior distribution and drawing ideas from sparse coding theory.\n\nOverall, the paper describes in enough detail the Variational Sparse Coding (VSC) model implementation. Only some details in the optimization hyperparameters and initialization mechanisms would be ideal. However, given enough training epochs, the VSC model we implemented was able to converge and produce the desired results in the different tasks described in the paper. \n\nFurther development on testing the model and comparing it against other sparse models on well-known benchmarks is critical. In order to assess how interpretable the learned latent features are, we could extract ideas from disentangled representations, to measure the effect of sparsity on a disentanglement metric, against benchmark models such as β-VAE or Factor-VAE. Without a proper benchmark, we are not able to understand how the learned representations can be interpretable. Although for small latent dimensions, visual inspection is enough, there must be a metric for comparison.  \n\nFinally, we suggest using convolutional architectures to obtain less blurry images and richer sparse representations. In addition, by applying the model to the Disentanglement testing Sprites dataset, we may observe and measure the interpretability of the learned latent sparse representations. \n\nWe conclude that the model hypothesis was validated through corroborating results in different experimental setups with our model implementation. Therefore, the research paper is reproducible. \n\nFull report : https://drive.google.com/open?id=1sEmiD2_dOwTJVydIsiZ1bsxdtbaBQT_Z\nCode: https://github.com/Alfo5123/Variational-Sparse-Coding"", 'title': 'Reproducibility study of Variational Sparse Coding paper'}"		SkeJ6iR9Km	Bklk8fNZfN	ICLR.cc/2019/Conference/-/Paper762/Public_Comment	[]	3		['everyone']	SkeJ6iR9Km	['~Alfredo_De_la_Fuente1']	1546891847159		1546891847159	['~Alfredo_De_la_Fuente1', 'ICLR.cc/2019/Conference']
927	1546890448610	"{'title': 'Methods were compared using consistent features - Reported baseline methods already use the features from Zhu', 'comment': ""We appreciate the reviewer's description of the details of their decision. \n\nThe authors of Zhu et al. confirmed that the baseline methods in Zhu et al. that we compared to were trained using the same features of Zhu et al. These experiments were done in Elhoseiny et al, CVPR 2017.  We were able to reach the authors of Zhu et al and Elhoseiny et al and verified this. \n\nSpecifically, the same semantic and visual representations were used for GAZSL, ZSLPP, ESZSL, WAC-linear, WAC-kernel, and ZSLNS (and also correction networks). Thus, the methods are compared using consistent features. ""}"		r1xurn0cKQ	rJlK037-fN	ICLR.cc/2019/Conference/-/Paper1556/Official_Comment	['ICLR.cc/2019/Conference/Paper1556/Reviewers/Unsubmitted']	19		['everyone']	H1gayYnexN	['ICLR.cc/2019/Conference/Paper1556/Authors']	1546890448610		1546890448610	['ICLR.cc/2019/Conference/Paper1556/Authors', 'ICLR.cc/2019/Conference']
928	1538087860766	"{'title': 'Diversity is All You Need: Learning Skills without a Reward Function', 'abstract': ""Intelligent creatures can explore their environments and learn useful skills without supervision.\nIn this paper, we propose ``Diversity is All You Need''(DIAYN), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning."", 'keywords': ['reinforcement learning', 'unsupervised learning', 'skill discovery'], 'authorids': ['beysenba@cs.cmu.edu', 'abhigupta@berkeley.edu', 'julianibarz@google.com', 'svlevine@eecs.berkeley.edu'], 'authors': ['Benjamin Eysenbach', 'Abhishek Gupta', 'Julian Ibarz', 'Sergey Levine'], 'TL;DR': 'We propose an algorithm for learning useful skills without a reward function, and show how these skills can be used to solve downstream tasks.', 'pdf': '/pdf/2d6a7564b82c143d515169e2a6f787a417379a47.pdf', 'paperhash': 'eysenbach|diversity_is_all_you_need_learning_skills_without_a_reward_function', '_bibtex': '@inproceedings{\neysenbach2018diversity,\ntitle={Diversity is All You Need: Learning Skills without a Reward Function},\nauthor={Benjamin Eysenbach and Abhishek Gupta and Julian Ibarz and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJx63jRqFm},\n}'}"		SJx63jRqFm	SJx63jRqFm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	752	rJlS6znFYm	['everyone']		['ICLR.cc/2019/Conference']	1538087860766		1546880365273	['ICLR.cc/2019/Conference']
929	1544893290462	"{'title': 'meta-review', 'metareview': ""The authors have described a navigation method that uses co-grounding between language and vision as well as an explicit self-assessment of progress. The method is used for room 2 room navigation and is tested in unseen environments. On the positive side, the approach is well-analyzed, with multiple ablations and baseline comparisons. The method is interesting and could be a good starting point for a more ambitious grounded language-vision agent. The approach seems to work well and achieves a high score using the metric of successful goal acquisition. On the negative side, the method relies on beam search, which is certainly unrealistic for real-world navigation, the evaluation metric is very simple and may be misleading, and the architecture is quite complex, may not scale or survive the test of time, and has little relevance for the greater ML community. There was a long discussion between the authors and the reviewers and other members of the public that resolved many of these points, with the authors being extremely responsive in giving additional results and details, and the reviewers' conclusion is that the paper should be accepted. "", 'recommendation': 'Accept (Poster)', 'confidence': '4: The area chair is confident but not absolutely certain'}"		r1GAsjC5Fm	SylMd7nMlV	ICLR.cc/2019/Conference/-/Paper669/Meta_Review	[]	1		['everyone']	r1GAsjC5Fm	['ICLR.cc/2019/Conference/Paper669/Area_Chair1']	1544893290462		1546874946170	['ICLR.cc/2019/Conference/Paper669/Area_Chair1']
930	1545006282501	"{'title': 'meta-review', 'metareview': ""The authors propose an approach for visual navigation that leverages a semantic knowledge graph to ground and inform the policy of an RL agent. The agent uses a graphnet to learn relationships and support the navigation. The empirical protocol is sound and uses best practices, and the authors have added additional experiments during the revision period, in response to the reviewers' requests. However, there were some significant problems with the submission - there were no comparisons to other semantic navigation methods, the approach is somewhat convoluted and will not survive the test of time, and the authors did not conclusively show the value of their approach. The reviewers uniformly support the publication of this paper, but with a low confidence. "", 'recommendation': 'Accept (Poster)', 'confidence': '4: The area chair is confident but not absolutely certain'}"		HJeRkh05Km	S1ef02PVxN	ICLR.cc/2019/Conference/-/Paper1033/Meta_Review	[]	1		['everyone']	HJeRkh05Km	['ICLR.cc/2019/Conference/Paper1033/Area_Chair1']	1545006282501		1546874860461	['ICLR.cc/2019/Conference/Paper1033/Area_Chair1']
931	1546873989010	{'title': 'Re: exact the point, not a distraction', 'comment': 'No. The stronger margin-based constraint they enforce is actually:   s_1(x) >= -\\delta => s_2(x) >= \\delta.\n\nNote that delta itself is non-negative i.e. \\delta>=0 and remember s_k(x) >=0 means label k is active for input x. Now, to check what happens to x with  s_1(x) >= 0, see below. \n\nSuppose s_1(x) >= 0. Since 0 >= -delta, it implies s_1(x) >= -delta, so matches with left hand side (LHS ) of the above rule and it triggers it, and as a result RHS of the above rule has to hold. So it is implied that s_2(x) >= delta >=0.\n\nIn summary,  s_1(x) >= 0 => s_2(x) >= 0, for all x.  In other words, whenever label 1 is active for x, label 2 would be active for it too.\n\n[It is easy to check why the assertions you are suggesting in last comment does not work.  Because it does not give any guarantee for x when 0<= s_1(x) < delta.  The label 1 for this specific x is active, but label 2 would not necessarily be since your suggested rule above is not get triggered.]'}		rJlWOj0qF7	ryxaY2kbz4	ICLR.cc/2019/Conference/-/Paper331/Official_Comment	['ICLR.cc/2019/Conference/Paper331/Reviewers/Unsubmitted']	16		['everyone']	H1l5RsAxzE	['ICLR.cc/2019/Conference/Paper331/AnonReviewer1']	1546873989010		1546873989010	['ICLR.cc/2019/Conference/Paper331/AnonReviewer1', 'ICLR.cc/2019/Conference']
932	1546869954577	"{'comment': 'Hi, I noticed that in Table 1 Previous Best CNN on CIFAR-10 with 1k, 2k labels, the listed results are from Mean Teacher [1] (21.55 and 15.73, respectively). But I find that the previous best results are obtained from SNTG [2], where the test errors are\n18.41±0.52, 13.64±0.32. \n\nIt would be better to cite the most recent results in [2] for ""previous best CNN"" in your paper. Thank you.\n\n\n[1]  Antti Tarvainen, Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NeurIPS 2017.\n[2]  Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang. Smooth neighbors on teacher graphs for semi-supervised learning. In CVPR, 2018.\n', 'title': 'Previous Best Results in Table 1'}"		rkgKBhA5Y7	BJgoa2AgzN	ICLR.cc/2019/Conference/-/Paper1559/Public_Comment	[]	5		['everyone']	rkgKBhA5Y7	['(anonymous)']	1546869954577		1546869954577	['(anonymous)', 'ICLR.cc/2019/Conference']
933	1546869713778	{'title': 'exact the point, not a distraction', 'comment': 'if we understand your explanation correctly, the implication rule y_1 ==> y_2 will not be logically expressed in terms of s_1(x) and s_2(x), in both [1] and [2]. The correct way shall be one of the two equivalent assertions:  (1) s_1(x) >= \\delta ==> s_2(x) >= \\delta, (2)  s_1(x) < -\\delta \\or s_2(x) >= \\delta.  \n'}		rJlWOj0qF7	H1l5RsAxzE	ICLR.cc/2019/Conference/-/Paper331/Official_Comment	['ICLR.cc/2019/Conference/Paper331/Reviewers/Unsubmitted']	15		['everyone']	HJlPWIflzN	['ICLR.cc/2019/Conference/Paper331/Authors']	1546869713778		1546869713778	['ICLR.cc/2019/Conference/Paper331/Authors', 'ICLR.cc/2019/Conference']
934	1546868483095	{'title': 'Thanks everyone!', 'comment': 'Thank you everyone for reading and liking our paper, as well as giving many good suggestions. Happy holidays! '}		ryGs6iA5Km	SygoZPCxGE	ICLR.cc/2019/Conference/-/Paper835/Official_Comment	['ICLR.cc/2019/Conference/Paper835/Reviewers/Unsubmitted']	38		['everyone']	Syl605aefE	['ICLR.cc/2019/Conference/Paper835/Authors']	1546868483095		1546868483095	['ICLR.cc/2019/Conference/Paper835/Authors', 'ICLR.cc/2019/Conference']
935	1546867627258	"{'title': 'two points have been neglected by the reviewer', 'comment': '(1) this year\'s submission policy accepts papers from arXiv. ""papers that have appeared on non-peered reviewed websites (like arXiv) or that have been presented at workshops (i.e., venues that do not have a publication proceedings) do not violate the policy"". So, you can detect authors of submissions that exist in arXiv.  \n\n(2) the detective work of your kind may round up names with less or no claims to authorship, because they may be master students from our institute or partner universities who help data collection and implementation. \n\n'}"		rJlWOj0qF7	BkgXhXRezE	ICLR.cc/2019/Conference/-/Paper331/Official_Comment	['ICLR.cc/2019/Conference/Paper331/Reviewers/Unsubmitted']	14		['everyone']	HkguZ1dlG4	['ICLR.cc/2019/Conference/Paper331/Authors']	1546867627258		1546867627258	['ICLR.cc/2019/Conference/Paper331/Authors', 'ICLR.cc/2019/Conference']
936	1546865364963	{'comment': 'I believe you can restate lemma 5 to hold for any finite or infinite (but countable) multiset  with the assumption that each element x appears at most N - 1 times in this multiset. Then, the same function f(x) = N^{-Z(x)} can be used, and the convergent series \\sum_{x \\in X} f(x) would always be injective. This can be proven by induction and it reduces to the following case: if we have 2 series: S = \\sum_{i >= 0} x_i * N^{-i} and T=\\sum_{i >= 0} y_i * N^{-i} , with N-1 > x_i,y_i >=0, then, if x_0 < y_0, one can prove that S < T.  ', 'title': 'this assumption in lemma 5 can be relaxed'}		ryGs6iA5Km	Syl605aefE	ICLR.cc/2019/Conference/-/Paper835/Public_Comment	[]	21		['everyone']	BJe7wWXkGV	['~Octavian_Eugen_Ganea1']	1546865364963		1546865364963	['~Octavian_Eugen_Ganea1', 'ICLR.cc/2019/Conference']
937	1546860154216	"{'title': 'Thank you for the AC & reviewers, and a latest summary of the Q & A', 'comment': 'We would like to thank the AC and the reviewers to engage in discussions during the feedback time. Our presentation has room for improvement and again thank you for the suggestions on this.\nFor reference, we summarise the major questions raised by the reviewers, and our answers to them.\n\n\nQ1: “prior work (Schott et al., 2018) not mentioned”\nA1: We thank the reviewers for pointing out Schott et al., (2018) which is concurrent. Our work is concurrent with Schott et al., (2018). The first version was submitted to arXiv in Feb 2018, and the previous version was presented at ICML 2018 TADGM workshop (oral) in July 2018. \n\nWe cited this paper in revision and discussed the major differences:\n1a) We performed extensive evaluation on both MNIST and CIFAR-10. Schott et al., (2018) only tested their method on MNIST and reached to similar conclusions as ours.\n1b) We investigated different graphical models for both generative and discriminative classifiers. Schott et al., (2018) only tested one architecture.\n1c) We amortized the cost of inference with a recognition network, and the importance sampling only needs K=10 latent variable samples. By contrast, Schott et al., (2018)’s model requires K=5000 samples and 50 optimisation steps to predict y for a single x.\n1d) We proposed new detection methods utilizing the learned model and evaluated their effectiveness in experiments. Schott et al., (2018) did not discuss this.\n1e) We proposed the generative-discriminative fusion model as a scalable solution to big data and big networks. Schott et al., (2018) did not discuss this.\n\nQ2: “focus on minimum perturbation distance only, and take out the whole detection part”\nA2: In revision we added visualisations and tables on the min distortion of the L_inf attacks. However:\nOur detection method is the first to make the classifier and detector use the **same generative model manifold** (as a proxy to the data manifold). This is distinct from all previous attempts, which train a **separate** generative model and thus have no guarantee to have an aligned manifold representation. Therefore previous success on attacking detection methods (Carlini & Wagner 2017, Athayle et al., 2018) do not directly transfer to our proposal, and would need to be reassessed. Our results clearly show that as the perturbation size increases, the robustness of the models naturally decreases but the detection rates increase as well. This means it is hard to fool both the classifier and the detection simultaneously.\n\n\nQ3: ""off-manifold conjecture pre-assumed""\nA3: We did not pre-assume the correctness of the ""off-manifold"" conjecture:\n3a) We were interested in verifying the ""off-manifold"" conjecture for both generative and discriminative classifiers. If for small epsilon the classifiers have high error, then this conjecture might not be true in practice.\n3b) The detection methods depend on the correctness of the ""off-manifold"" conjecture. Therefore, if empirically the detection methods don\'t work even when we have a very good generative model (which is the case for MNIST), then, this conjecture might not be true in practice. \n3c) Our empirical results clearly show that (i) generative classifiers are more robust than discriminative ones (especially when epsilon is small); (ii) the marginal/logit detection methods did work.\n3d) Observing the above, we are confident to say we have empirically verified the ""off-manifold"" conjecture on generative classifiers.\n\n\nQ4. ""two class CIFAR-10 results are not particularly convincing""\nA4: We presented the CIFAR-binary example because:\n4a) Carlini & Wagner (2017b) stated that MNIST results might not transfer to natural images. Therefore we wanted to see if our findings on MNIST is still true on natural images. \n4b) We did try full CIFAR-10, but none of the existing generative models (VAE/GAN/flow-based/autoregressive models) can achieve satisfactory **clean accuracy** on it. Therefore we instead studied this two-class CIFAR-10 problem in order to evaluate **fully generative classifiers**\n4c) We then presented the fusion model for full CIFAR-10 to address the scalability issue. The baseline there was a VGG-16 network. The generative-discriminative fusion model is more robust than VGG-16 and other discriminative classifiers, showing that the robustness properties of generative classifiers do extend to CIFAR-10. \n\nQ5: “possible gradient masking issues”\nA5: We added the SPSA attack (a state-of-the-art score-based attack by Uesato et al. 2018) in revision. We tried our best to make SPSA strong, by using M=2000 ES samples to estimate a single gradient, and it took 3 days on a Tesla P100 GPU to perform attacks against a generative classifier on 1,000 clean MNIST inputs. Both the distillation-based attacks and the SPSA attacks worked worse than the white-box attacks, suggesting that gradient masking is not the explanation of the robustness of the tested generative classifiers.'}"		HygUOoC5KX	HJxzKI2eGV	ICLR.cc/2019/Conference/-/Paper356/Official_Comment	['ICLR.cc/2019/Conference/Paper356/Reviewers/Unsubmitted']	27		['everyone']	HygUOoC5KX	['ICLR.cc/2019/Conference/Paper356/Authors']	1546860154216		1546860154216	['ICLR.cc/2019/Conference/Paper356/Authors', 'ICLR.cc/2019/Conference']
938	1538087739462	{'title': 'Learning Localized Generative Models for 3D Point Clouds via Graph Convolution', 'abstract': 'Point clouds are an important type of geometric data and have widespread use in computer graphics and vision. However, learning representations for point clouds is particularly challenging due to their nature as being an unordered collection of points irregularly distributed in 3D space. Graph convolution, a generalization of the convolution operation for data defined over graphs, has been recently shown to be very successful at extracting localized features from point clouds in supervised or semi-supervised tasks such as classification or segmentation. This paper studies the unsupervised problem of a generative model exploiting graph convolution. We focus on the generator of a GAN and define methods for graph convolution when the graph is not known in advance as it is the very output of the generator. The proposed architecture learns to generate localized features that approximate graph embeddings of the output geometry. We also study the problem of defining an upsampling layer in the graph-convolutional generator, such that it learns to exploit a self-similarity prior on the data distribution to sample more effectively.', 'paperhash': 'valsesia|learning_localized_generative_models_for_3d_point_clouds_via_graph_convolution', 'TL;DR': 'A GAN using graph convolution operations with dynamically computed graphs from hidden features', 'authorids': ['diego.valsesia@polito.it', 'giulia.fracastoro@polito.it', 'enrico.magli@polito.it'], 'authors': ['Diego Valsesia', 'Giulia Fracastoro', 'Enrico Magli'], 'keywords': ['GAN', 'graph convolution', 'point clouds'], 'pdf': '/pdf/23f9c484ef2da7f6685d582c91c2ab013574442e.pdf', '_bibtex': '@inproceedings{\nvalsesia2018learning,\ntitle={Learning Localized Generative Models for 3D Point Clouds via Graph Convolution},\nauthor={Diego Valsesia and Giulia Fracastoro and Enrico Magli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJeXSo09FQ},\n}'}		SJeXSo09FQ	SJeXSo09FQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	76	BJe_VKd1KX	['everyone']		['ICLR.cc/2019/Conference']	1538087739462		1546852409390	['ICLR.cc/2019/Conference']
939	1538087961051	{'title': 'Optimal Control Via Neural Networks: A Convex Approach', 'abstract': 'Control of complex systems involves both system identification and controller design. Deep neural networks have proven to be successful in many identification tasks, however, from model-based control perspective, these networks are difficult to work with because they are typically nonlinear and nonconvex. Therefore many systems are still identified and controlled based on simple linear models despite their poor representation capability.\n\nIn this paper we bridge the gap between model accuracy and control tractability faced by neural networks, by explicitly constructing networks that are convex with respect to their inputs. We show that these input convex networks can be trained to obtain accurate models of complex physical systems. In particular, we design input convex recurrent neural networks to capture temporal behavior of dynamical systems. Then optimal controllers can be achieved via solving a convex model predictive control problem. Experiment results demonstrate the good potential of the proposed input convex neural network based approach in a variety of control applications. In particular we show that in the MuJoCo locomotion tasks, we could achieve over 10% higher performance using 5 times less time compared with state-of-the-art model-based reinforcement learning method; and in the building HVAC control example, our method achieved up to 20% energy reduction compared with classic linear models.\n', 'keywords': ['optimal control', 'input convex neural network', 'convex optimization'], 'authorids': ['yizechen@uw.edu', 'yyshi@uw.edu', 'zhangbao@uw.edu'], 'authors': ['Yize Chen', 'Yuanyuan Shi', 'Baosen Zhang'], 'pdf': '/pdf/a300126e215f3ee78390d89b9c64294ff1049e72.pdf', 'paperhash': 'chen|optimal_control_via_neural_networks_a_convex_approach', '_bibtex': '@inproceedings{\nchen2018optimal,\ntitle={Optimal Control Via Neural Networks: A Convex Approach},\nauthor={Yize Chen and Yuanyuan Shi and Baosen Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1MW72AcK7},\n}'}		H1MW72AcK7	H1MW72AcK7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1332	B1gGTliYYm	['everyone']		['ICLR.cc/2019/Conference']	1538087961051		1546848273591	['ICLR.cc/2019/Conference']
940	1538087841680	"{'title': 'NADPEx: An on-policy temporally consistent exploration method for deep reinforcement learning', 'abstract': ""Reinforcement learning agents need exploratory behaviors to escape from local optima. These behaviors may include both immediate dithering perturbation and temporally consistent exploration. To achieve these, a stochastic policy model that is inherently consistent through a period of time is in desire, especially for tasks with either sparse rewards or long term information. In this work, we introduce a novel on-policy temporally consistent exploration strategy - Neural Adaptive Dropout Policy Exploration (NADPEx) - for deep reinforcement learning agents. Modeled as a global random variable for conditional distribution, dropout is incorporated to reinforcement learning policies, equipping them with inherent temporal consistency, even when the reward signals are sparse. Two factors, gradients' alignment with the objective and KL constraint in policy space, are discussed to guarantee NADPEx policy's stable improvement. Our experiments demonstrate that NADPEx solves tasks with sparse reward while naive exploration and parameter noise fail. It yields as well or even faster convergence in the standard mujoco benchmark for continuous control. "", 'keywords': ['Reinforcement learning', 'exploration'], 'authorids': ['xiesirui@sensetime.com', 'huangjunning@sensetime.com', 'leilanxin@sensetime.com', 'liuchunxiao@sensetime.com', 'mazheng@sensetime.com', 'wayne.zhang@sensetime.com', 'linliang@ieee.org'], 'authors': ['Sirui Xie', 'Junning Huang', 'Lanxin Lei', 'Chunxiao Liu', 'Zheng Ma', 'Wei Zhang', 'Liang Lin'], 'pdf': '/pdf/398797f9c9546ea36eb473661d72eefbac40701f.pdf', 'paperhash': 'xie|nadpex_an_onpolicy_temporally_consistent_exploration_method_for_deep_reinforcement_learning', '_bibtex': '@inproceedings{\nxie2018nadpex,\ntitle={{NADPE}x: An on-policy temporally consistent exploration method for deep reinforcement learning},\nauthor={Sirui Xie and Junning Huang and Lanxin Lei and Chunxiao Liu and Zheng Ma and Wei Zhang and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxciiC9tm},\n}'}"		rkxciiC9tm	rkxciiC9tm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	645	ryg-tz6wtX	['everyone']		['ICLR.cc/2019/Conference']	1538087841680		1546845211333	['ICLR.cc/2019/Conference']
941	1546841856247	"{'comment': '@AC/PC This paper flouts ICLR\'s anonymity guidelines. As the comment by the review, and the other anonymous poster says, the GitHub page lists the first author\'s name under ""contributors"", and the Google Drive lists the first author\'s name as well. It\'s likely a careless error, and may not warrant rejection, but AC/PC should discuss this with the author, and comment on this in the meta-review as a warning.\n\nThe author\'s responds to this is poor e.g. ""therefore, anonymization  is not an issue."", ""do not see the line of your argument. "". Especially since the authors did not issue an apology, this should be flagged as a very important point.', 'title': 'AC/PC Please Note Lack of Anonymity'}"		rJlWOj0qF7	HkguZ1dlG4	ICLR.cc/2019/Conference/-/Paper331/Public_Comment	[]	2		['everyone']	rJgajhp3gE	['(anonymous)']	1546841856247		1546841856247	['(anonymous)', 'ICLR.cc/2019/Conference']
942	1546819071297	{'title': 'Not really...', 'comment': 'It is not clear which part is exactly under question, one variable implying the other one,  or insertion of deltas, etc?\n\nThe bottomline is that once their constraints are held (i.e. once the ball for y_1 is inside the ball for y_2),  then nonnegative s_1(x)  has to imply nonnegative s_2(x). And inserting margin (\\delta) in the proper way in the constraints does not change that.  \n\nWhen they insert margins around balls, for a y_1=> y_2 implication rule , it is enforced that whenever a point is not only inside the inner ball but even margin away from the boundary of it (but outside),  it has to be not only inside the boundary of the outer ball, but also margin away from the boundary (but inside).  Enforcing such constraint guarantees y_1 implies y_2.  This is to provide separation between the balls and facilitate margin based models.\n\nThis lengthy discussion seems more like a distraction from the main point and out of place here.  '}		rJlWOj0qF7	HJlPWIflzN	ICLR.cc/2019/Conference/-/Paper331/Official_Comment	['ICLR.cc/2019/Conference/Paper331/Reviewers/Unsubmitted']	13		['everyone']	SkgQtt5kGN	['ICLR.cc/2019/Conference/Paper331/AnonReviewer1']	1546819071297		1546833393260	['ICLR.cc/2019/Conference/Paper331/AnonReviewer1', 'ICLR.cc/2019/Conference']
943	1545347036369	{'title': 'No response to revisions', 'comment': 'We would like to point out that the reviewers have not posted a response to our revisions. We replied to all concerns listed by the reviewers and made extensive revisions to the paper. Further, two\xa0of the reviewers appeared to like the paper (saying that the quality is good, that it is interesting, that they liked it), but only asked for clarifications. We provided these clarifications, but the reviewers have not acknowledged our response / revisions nor updated their scores. '}		B1lG42C9Km	B1xVyeiKgV	ICLR.cc/2019/Conference/-/Paper1433/Official_Comment	['ICLR.cc/2019/Conference/Paper1433/Reviewers/Unsubmitted']	10		['everyone']	B1lG42C9Km	['ICLR.cc/2019/Conference/Paper1433/Authors']	1545347036369		1546812423780	['ICLR.cc/2019/Conference/Paper1433/Authors', 'ICLR.cc/2019/Conference']
944	1546806441300	"{'title': 'PRNGs are allowed in SGX', 'comment': 'I don\'t understand your comment about the non-colluding assumption. Non-collusion between which parties? As I mentioned in my previous comment, we make no such assumption.\n\nAs for the PRNG, indeed, there is no seedable PRNG in the Intel Sample code (as far as I can tell). But there is also no sample code to implement ML models (for example) and that doesn\'t mean we aren\'t allowed to implement it ourselves.\n\nOf course, for most purposes, using Intel\'s sgx_read_rand will be simpler and faster than using your own PRNG. But for our particular application, using an explicit stateful PRNG makes things easier, without sacrificing any security. \nFinally, as I mentioned above, the Intel SGX developer guide explicitly says:\n""Software vendors that have an existing Pseudo-Random Number Generator (PRNG) should use the RDSEED instruction to benefit from the high-quality entropy source"" \n(https://download.01.org/intel-sgx/linux-2.0/docs/Intel_SGX_Developer_Guide.pdf, page 26). \nSo according to Intel, this is a perfectly natural and secure thing to do.\n\n*edit* It\'s of course worth mentioning that the rand and srand functions commonly available in C/C++ are not cryptographically secure PRNGs and are not available in SGX as they rely on randomness made available by the OS. We of course don\'t use these as they aren\'t secure. Instead, we use a cryptographically secure PRNG, such as AES run in CTR mode (https://en.wikipedia.org/wiki/Cryptographically_secure_pseudorandom_number_generator).'}"		rJVorjCcKQ	ryeZnEJlME	ICLR.cc/2019/Conference/-/Paper119/Official_Comment	['ICLR.cc/2019/Conference/Paper119/Reviewers/Unsubmitted']	8		['everyone']	r1lv411xzN	['ICLR.cc/2019/Conference/Paper119/Authors']	1546806441300		1546806685246	['ICLR.cc/2019/Conference/Paper119/Authors', 'ICLR.cc/2019/Conference']
945	1546805180281	{'comment': 'Seedable PRNG vs RDRAND\n\nSeedable PRNG is not allowed in Intel SGX', 'title': 'HIGHLIGHT'}		rJVorjCcKQ	r1lN611xMV	ICLR.cc/2019/Conference/-/Paper119/Public_Comment	[]	6		['everyone']	r1lv411xzN	['~Yu_Ding1']	1546805180281		1546805180281	['~Yu_Ding1', 'ICLR.cc/2019/Conference']
946	1546805038540	"{'comment': 'For the non-colluding assumption, I think we have nothing to discuss. It\'s your ""assumption"".\n\nJust tell me what you think about the following truth:\n\nThere is no seedable pseudo-random number generation function in the whole Intel SGX SDK/PSW/Sample code. And in Intel SGX developer manual, it says\n\'The tRTS provides a wrapper to the RDRAND instruction to generate a true\nrandom number from hardware. The C/C++ standard library functions rand\nand srand functions are not supported within an enclave because they only\nprovide pseudo random numbers. Instead, enclave developers should use the\nsgx_read_rand function to get true random numbers.\'\n\nYour implementation has a hidden assumption: PRNG in Intel SGX is allowed. However, it\'s not.', 'title': 'Disagree'}"		rJVorjCcKQ	r1lv411xzN	ICLR.cc/2019/Conference/-/Paper119/Public_Comment	[]	5		['everyone']	BklTaiqJMV	['~Yu_Ding1']	1546805038540		1546805038540	['~Yu_Ding1', 'ICLR.cc/2019/Conference']
947	1546789066029	"{'comment': 'Yes, we use our VSR repo as a base-repo to develop RGAN models. You can find GAN architecture defined in VSR/Models/Gan.py. You can run the training and validation referring to README_ICLR.md.\nAlso, our PR to the challenge is at https://github.com/reproducibility-challenge/iclr_2019/pull/135 , where you can find the full report :)\n\nP.S: You may need to delete the dot ""."" when click on that link directly :)', 'title': 'the link is correct'}"		S1erHoR5t7	SJlM0xjJMV	ICLR.cc/2019/Conference/-/Paper87/Public_Comment	[]	3		['everyone']	H1xxkyA2WE	['~Wenyi_Tang1']	1546789066029		1546789425773	['~Wenyi_Tang1', 'ICLR.cc/2019/Conference']
948	1546787781175	"{'title': 'Clarification on online/offline phases and remote clients', 'comment': 'Our scheme requires no non-collusion assumption and no pre-computation by a remote client. Rather, the pre-computation is done by the server (in a trusted enclave).\n\nLet me clarify the setting we consider. We consider a remote client that interacts with an untrusted cloud server. This server hosts a trusted enclave that runs ML workloads on client data (we assume no enclave on the remote client\'s machine).\n\nWhen the client sends an input $x$ to the enclave, we can accelerate the private computation of $F(x)$ on the server by leveraging an untrusted GPU, as long as the enclave has previously computed $F(r)$.\nAs you correctly observe, computing $F(r)$ takes as long as computing $F(x)$, but the advantage is that $F(r)$ is independent of any client data, and can thus be pre-computed by the server\'s enclave at any time (e.g., during periods of low client usage such as outside of business hours). \nOur protocol thus consists of an ""offline phase"", in which the server\'s enclave slowly pre-computes values $F(r)$ and securely stores them, followed by an ""online phase"" in which the enclave can very quickly and privately compute $F(x)$ on a remote client\'s data.\n\nProtocols with such dual offline and online phases are very common in the secure multiparty-computation literature (e.g., if you look at many of the MPC protocols proposed for the 2017 edition of the iDash competition you linked above, you\'ll find that many require an offline pre-computation phase).\nRequiring such an offline phase is a possible limitation of our protocol which we discuss and acknowledge in our paper. Whether the ""price"" of pre-computing on random values in an offline phase in order to accelerate the online phase is worth it will depend heavily on application requirements.\n\nWe\'d also like to note that the ""lifetime"" of the pre-computed values $F(r)$ need not be very long, as these values can be discarded as soon as they are used (e.g., the enclave could compute values $F(r)$ outside of business hours, and then consume them all a few hours later). Even if secret values were held on for a long time, we don\'t understand why this should contradict SGX\'s use-cases. This is exactly what SGX\'s sealing mechanism is for: https://software.intel.com/en-us/blogs/2016/05/04/introduction-to-intel-sgx-sealing\n\nFinally, the comment on seedable (cryptographic) PRNGs being forbidden or bad design in SGX is nonsense. The RDRAND instruction available on Intel processors simply calls a hardware-implemented PRNG that is seeded using a hardware entropy source. SGX enclaves can of course use RDRAND (or even better RDSEED) to generate a secure random seed to be used with any other software-implemented CSPRNG (the SGX developer guide even suggests this in its section on Random Number Generation). This is exactly what we do: we use RDRAND to seed an AES-based CSPRNG.'}"		rJVorjCcKQ	BklTaiqJMV	ICLR.cc/2019/Conference/-/Paper119/Official_Comment	['ICLR.cc/2019/Conference/Paper119/Reviewers/Unsubmitted']	7		['everyone']	Byx9F1EJfN	['ICLR.cc/2019/Conference/Paper119/Authors']	1546787781175		1546787781175	['ICLR.cc/2019/Conference/Paper119/Authors', 'ICLR.cc/2019/Conference']
949	1546787195299	{'title': 'unacceptable comment', 'comment': 'please use your own words to do the explanation (why  s_1(x) only needs to be greater than  minus \\delta (-\\delta))  \n\nthanks. \n\nPS: the theorems you pointed in [1] and [2] do not directly go to the formalism, something is still missing. '}		rJlWOj0qF7	SkgQtt5kGN	ICLR.cc/2019/Conference/-/Paper331/Official_Comment	['ICLR.cc/2019/Conference/Paper331/Reviewers/Unsubmitted']	12		['everyone']	r1giuzlyGN	['ICLR.cc/2019/Conference/Paper331/Authors']	1546787195299		1546787195299	['ICLR.cc/2019/Conference/Paper331/Authors', 'ICLR.cc/2019/Conference']
950	1546768058366	{'title': 'Reproducibility report for: A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit', 'authors': ['Kumar Kshitij Patel', 'Anay Mehrotra', 'Tim Lebailly'], 'authorids': ['kumar.patel@epfl.ch', 'anaymehrotra1@gmail.com', 'tim.lebailly@epfl.ch'], 'pdf': '/pdf/c5497ee5260059cf90a590a663f66868bfe40c3d.pdf', 'paperhash': 'patel|reproducibility_report_for_a_resizable_minibatch_gradient_descent_based_on_a_multiarmed_bandit'}		SJlGaRBJMN	SJlGaRBJMN	OpenReview.net/Archive/-/Direct_Upload	[]	190		['everyone']		['~Kumar_Kshitij_Patel1']	1546768058366		1546768058366	['~Kumar_Kshitij_Patel1']
951	1546760065671	"{'comment': ""The non-colluding assumption is almost impossible in the real world, and this is why you and I are not on the same page. It is because an attacker could easily read the value of $u+r and $r when enclave passes them to the graphic card. So $r **HAS TO** be pre-calculated by one user remotely.\n\nSo I agree that $r is a one-time pad and it masks secret data. But, $r here is a long-lasting one-time pad and its lifetime is much longer than crypto-safe one-time stuff because a client needs to precompute F(r) offline with poor computing performance. Obviously, if the client has the ability to compute F(r) efficiently and quickly, he can directly compute F(x). So in your proposed approach, one potential assumption is that the one who chooses $r does is a poor guy and can only compute slowly.\n\nSo now it turns out to be another problem -- the lifetime of the one-time pad $r. The user needs to protect his $r in a rather long period of time in an untrusted environment -- contradicting with the target of SGX. This is the limitation and you mentioned it in the paper.\n\nThe only solution may be -- to generate $r in user's enclave and calculate F(r) in an SGX enclave slowly -- using only 128MB memory on an i7 or E3 CPU. How many users would like to pay for this price? such as 1000 hrs of local CPU time in exchange for a short predict calculation?\n\nbtw: the proposed solution relies on seedable PRNG. This is forbidden in secure SGX programming. You could find that there is no rand() function in Intel SGX SDK -- because SGX ONLY TAKES RANDOM NUMBERS FROM RDRAND. Using seedable rng in SGX is a really bad design."", 'title': 'Disagree'}"		rJVorjCcKQ	Byx9F1EJfN	ICLR.cc/2019/Conference/-/Paper119/Public_Comment	[]	4		['everyone']	H1ewxd-slV	['~Yu_Ding1']	1546760065671		1546766982220	['~Yu_Ding1', 'ICLR.cc/2019/Conference']
952	1538087978124	{'title': 'G-SGD: Optimizing ReLU Neural Networks in its Positively Scale-Invariant Space', 'abstract': 'It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant. Conventional algorithms like stochastic gradient descent optimize the neural networks in the vector space of weights, which is, however, not positively scale-invariant. This mismatch may lead to problems during the optimization process. Then, a natural question is: \\emph{can we construct a new vector space that is positively scale-invariant and sufficient to represent ReLU neural networks so as to better facilitate the optimization process }? In this paper, we provide our positive answer to this question. First, we conduct a formal study on the positive scaling operators which forms a transformation group, denoted as $\\mathcal{G}$. We prove that the value of a path (i.e. the product of the weights along the path) in the neural network is invariant to positive scaling and the value vector of all the paths is sufficient to represent the neural networks under mild conditions. Second, we show that one can identify some basis paths out of all the paths and prove that the linear span of their value vectors (denoted as $\\mathcal{G}$-space) is an invariant space with lower dimension under the positive scaling group. Finally, we design stochastic gradient descent algorithm in $\\mathcal{G}$-space (abbreviated as $\\mathcal{G}$-SGD) to optimize the value vector of the basis paths of neural networks with little extra cost by leveraging back-propagation. Our experiments show that $\\mathcal{G}$-SGD significantly outperforms the conventional SGD algorithm in optimizing ReLU networks on benchmark datasets. ', 'keywords': ['optimization', 'neural network', 'irreducible positively scale-invariant space', 'deep learning'], 'authorids': ['meq@microsoft.com', 'zhengsx@mail.ustc.edu.cn', 'huzhang@microsoft.com', 'wche@microsoft.com', 'qiwye@microsoft.com', 'mazm@amt.ac.cn', 'ynh@ustc.edu.cn', 'tyliu@microsoft.com'], 'authors': ['Qi Meng', 'Shuxin Zheng', 'Huishuai Zhang', 'Wei Chen', 'Qiwei Ye', 'Zhi-Ming Ma', 'Nenghai Yu', 'Tie-Yan Liu'], 'pdf': '/pdf/d292ca68112f99129d39ce733f31196391c76132.pdf', 'paperhash': 'meng|gsgd_optimizing_relu_neural_networks_in_its_positively_scaleinvariant_space', '_bibtex': '@inproceedings{\nmeng2018gsgd,\ntitle={G-{SGD}: Optimizing Re{LU} Neural Networks in its Positively Scale-Invariant Space},\nauthor={Qi Meng and Shuxin Zheng and Huishuai Zhang and Wei Chen and Zhi-Ming Ma and Tie-Yan Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxfEn09Y7},\n}'}		SyxfEn09Y7	SyxfEn09Y7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1431	BklLeyEROQ	['everyone']		['ICLR.cc/2019/Conference']	1538087978124		1546766397681	['ICLR.cc/2019/Conference']
953	1546756442900	{'title': 'Thanks!', 'comment': 'We appreciate your great suggestion, and we will clarify our assumption accordingly.'}		ryGs6iA5Km	BJe7wWXkGV	ICLR.cc/2019/Conference/-/Paper835/Official_Comment	['ICLR.cc/2019/Conference/Paper835/Reviewers/Unsubmitted']	37		['everyone']	r1gaJ5CAW4	['ICLR.cc/2019/Conference/Paper835/Authors']	1546756442900		1546756442900	['ICLR.cc/2019/Conference/Paper835/Authors', 'ICLR.cc/2019/Conference']
954	1538087915631	{'title': 'MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders', 'abstract': 'Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation.Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.', 'keywords': ['VAE', 'regularization', 'auto-regressive'], 'authorids': ['xuezhem@cs.cmu.edu', 'ctzhou@cs.cmu.edu', 'ehovy@cs.cmu.edu'], 'authors': ['Xuezhe Ma', 'Chunting Zhou', 'Eduard Hovy'], 'pdf': '/pdf/babbf506605364d38ab996f0e6495bc76e74d526.pdf', 'paperhash': 'ma|mae_mutual_posteriordivergence_regularization_for_variational_autoencoders', '_bibtex': '@inproceedings{\nma2018mae,\ntitle={{MAE}: Mutual Posterior-Divergence Regularization for Variational AutoEncoders},\nauthor={Xuezhe Ma and Chunting Zhou and Eduard Hovy},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hke4l2AcKQ},\n}'}		Hke4l2AcKQ	Hke4l2AcKQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1064	HyxEZna9Y7	['everyone']		['ICLR.cc/2019/Conference']	1538087915631		1546746672083	['ICLR.cc/2019/Conference']
955	1546746355180	{'title': 'Carema-Ready version updated', 'comment': 'Carema-Ready version of this paper has been uploaed'}		Hke4l2AcKQ	r1gjl5gJzV	ICLR.cc/2019/Conference/-/Paper1064/Official_Comment	['ICLR.cc/2019/Conference/Paper1064/Reviewers/Unsubmitted']	15		['everyone']	Hke4l2AcKQ	['ICLR.cc/2019/Conference/Paper1064/Authors']	1546746355180		1546746355180	['ICLR.cc/2019/Conference/Paper1064/Authors', 'ICLR.cc/2019/Conference']
956	1546744434736	{'title': 'Re: Not really...', 'comment': 'Theorem 1 in Section 5.2.2 in page 69 of [2] proves it. The proof of the theorem appears in Appendix C.3 page 98 of [2] and the geometric intuition behind it appears in the second paragraph of Section 5.2.2  and Figure 5.2 in page 70 of [2].\nIt is also discussed in Theorem 3 of reference [1].'}		rJlWOj0qF7	r1giuzlyGN	ICLR.cc/2019/Conference/-/Paper331/Official_Comment	['ICLR.cc/2019/Conference/Paper331/Reviewers/Unsubmitted']	11		['everyone']	S1gHx1sOZN	['ICLR.cc/2019/Conference/Paper331/AnonReviewer1']	1546744434736		1546744475430	['ICLR.cc/2019/Conference/Paper331/AnonReviewer1', 'ICLR.cc/2019/Conference']
957	1546738148695	"{'comment': 'In appendix D and E you state and use the fact that ""because multisets X are finite, there exists a number N s.t. |X| < N for all (finite) X"". This is mathematically incorrect, since arbitrarily large finite sets cannot have an upper bound in size. In fact, the set of all finite sets containing elements in a countable set is uncountable. The authors might want to clearly state the assumption that they deal with finite sets of cardinality at most N.', 'title': 'The sizes of all finite multisets cannot be bounded'}"		ryGs6iA5Km	r1gaJ5CAW4	ICLR.cc/2019/Conference/-/Paper835/Public_Comment	[]	20		['everyone']	Syl6WEQn9m	['(anonymous)']	1546738148695		1546738148695	['(anonymous)', 'ICLR.cc/2019/Conference']
958	1546728850998	{'title': 'Thank you', 'comment': 'Thank you for your interest in our paper and highlighting additional related work. We will incorporate these references into the final version of the paper.'}		Hygxb2CqKm	S1xscrn0b4	ICLR.cc/2019/Conference/-/Paper1136/Official_Comment	['ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted']	14		['everyone']	HylIggEFW4	['ICLR.cc/2019/Conference/Paper1136/Authors']	1546728850998		1546728850998	['ICLR.cc/2019/Conference/Paper1136/Authors', 'ICLR.cc/2019/Conference']
959	1538087798046	"{'title': 'Learning a SAT Solver from Single-Bit Supervision', 'abstract': 'We present NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability.  Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations. Moreover, NeuroSAT generalizes to novel distributions; after training only on random SAT problems, at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs.', 'keywords': ['sat', 'search', 'graph neural network', 'theorem proving', 'proof'], 'authorids': ['dselsam@cs.stanford.edu', 'mlamm@cs.stanford.edu', 'buenz@cs.stanford.edu', 'pliang@cs.stanford.edu', 'leonardo@microsoft.com', 'dill@cs.stanford.edu'], 'authors': ['Daniel Selsam', 'Matthew Lamm', 'Benedikt B\\""{u}nz', 'Percy Liang', 'Leonardo de Moura', 'David L. Dill'], 'TL;DR': 'We train a graph network to predict boolean satisfiability and show that it learns to search for solutions, and that the solutions it finds can be decoded from its activations.', 'pdf': '/pdf/868d9f7b066d73e8b692a14f5148c48d16376781.pdf', 'paperhash': 'selsam|learning_a_sat_solver_from_singlebit_supervision', '_bibtex': '@inproceedings{\nselsam2018learning,\ntitle={Learning a {SAT} Solver from Single-Bit Supervision},\nauthor={Daniel Selsam and Matthew Lamm and Benedikt B\\""{u}nz and Percy Liang and Leonardo de Moura and David L. Dill},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJMC_iA5tm},\n}'}"		HJMC_iA5tm	HJMC_iA5tm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	402	BJeic0ucFm	['everyone']		['ICLR.cc/2019/Conference']	1538087798046		1546725108774	['ICLR.cc/2019/Conference']
960	1545238393356	{'title': 'Important problem, interesting solution, however stronger results may be needed', 'metareview': ' The paper addresses an important problem of detecting biases in classifiers (e.g. in face detection), using simulation tools with Bayesian parameter search. While the direction of research and the presented approach seem to be practically useful, there were several concerns raised by the reviewers regarding strengthening the results (e.g., beyond single avatar, etc), and suggestions on possibly a more applied conference as a better venue.  While thourough rebuttals by the authors addressed some of these concerns, which increased some ratings, overall, the paper was still in the borderline range. We hope the suggestions and comments of the reviewers can help to improve the paper.\n\n \n ', 'recommendation': 'Reject', 'confidence': '3: The area chair is somewhat confident'}		BJf_YjCqYX	SygbYDgux4	ICLR.cc/2019/Conference/-/Paper460/Meta_Review	[]	1		['everyone']	BJf_YjCqYX	['ICLR.cc/2019/Conference/Paper460/Area_Chair1']	1545238393356		1546715795435	['ICLR.cc/2019/Conference/Paper460/Area_Chair1']
961	1546669989237	{'title': 'reply', 'comment': 'Method 2 is exactly what we proposed. Our work is not a simple application of KD for multilingual NMT (e.g., method 1). Instead, we need to carefully design the distillation method, including when to distill and which language to distill.\n'}		S1gUsoR9YX	SJgao10a-4	ICLR.cc/2019/Conference/-/Paper625/Official_Comment	['ICLR.cc/2019/Conference/Paper625/Reviewers/Unsubmitted']	24		['everyone']	Syg3ME65WV	['ICLR.cc/2019/Conference/Paper625/Authors']	1546669989237		1546672089121	['ICLR.cc/2019/Conference/Paper625/Authors', 'ICLR.cc/2019/Conference']
962	1538087985783	{'title': ' Reasoning About Physical Interactions with Object-Oriented Prediction and Planning', 'abstract': 'Object-based factorizations provide a useful level of abstraction for interacting with the world. Building explicit object representations, however, often requires supervisory signals that are difficult to obtain in practice. We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties. Our model, Object-Oriented Prediction and Planning (O2P2), jointly learns a perception function to map from image observations to object representations, a pairwise physics interaction function to predict the time evolution of a collection of objects, and a rendering function to map objects back to pixels. For evaluation, we consider not only the accuracy of the physical predictions of the model, but also its utility for downstream tasks that require an actionable representation of intuitive physics. After training our model on an image prediction task, we can use its learned representations to build block towers more complicated than those observed during training.', 'keywords': ['structured scene representation', 'predictive models', 'intuitive physics', 'self-supervised learning'], 'authorids': ['janner@berkeley.edu', 'svlevine@eecs.berkeley.edu', 'billf@mit.edu', 'jbt@mit.edu', 'cbfinn@eecs.berkeley.edu', 'jiajunwu@mit.edu'], 'authors': ['Michael Janner', 'Sergey Levine', 'William T. Freeman', 'Joshua B. Tenenbaum', 'Chelsea Finn', 'Jiajun Wu'], 'TL;DR': 'We present a framework for learning object-centric representations suitable for planning in tasks that require an understanding of physics.', 'pdf': '/pdf/c3d330a568fa726d681ecc56de656382ee31f638.pdf', 'paperhash': 'janner|reasoning_about_physical_interactions_with_objectoriented_prediction_and_planning', '_bibtex': '@inproceedings{\njanner2018reasoning,\ntitle={Reasoning About Physical Interactions with Object-Centric Models},\nauthor={Michael Janner and Sergey Levine and William T. Freeman and Joshua B. Tenenbaum and Chelsea Finn and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJx9EhC9tQ},\n}'}		HJx9EhC9tQ	HJx9EhC9tQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1474	rJgerzR9Km	['everyone']		['ICLR.cc/2019/Conference']	1538087985783		1546639285219	['ICLR.cc/2019/Conference']
963	1545838915765	{'title': 'Please make our rebuttals publicly visible.', 'comment': 'Hello, our responses to reviewers are not visible to everyone. Can you change the visibility of the responses so that they are visible?'}		B1eKk2CcKm	ryenr-7W-4	ICLR.cc/2019/Conference/-/Paper1005/Official_Comment	['ICLR.cc/2019/Conference/Paper1005/Reviewers/Unsubmitted']	4		['everyone']	ByldPHwxxV	['ICLR.cc/2019/Conference/Paper1005/Authors']	1545838915765		1546636433380	['ICLR.cc/2019/Conference/Paper1005/Authors', 'ICLR.cc/2019/Conference']
964	1546624791093	{'title': 'Speeding up Context-based Sentence Representation Learning with Non-autoregressive Convolutional Decoding', 'authors': ['Shuai Tang', 'Hailin Jin', 'Chen Fang', 'Zhaowen Wang', 'Virginia R. de Sa'], 'authorids': ['shuaitang93@ucsd.edu', 'hljin@adobe.com', 'cfang@adobe.com', 'zhawang@adobe.com', 'desa@ucsd.edu'], 'pdf': 'http://www.aclweb.org/anthology/W18-3009.pdf', 'paperhash': 'tang|speeding_up_contextbased_sentence_representation_learning_with_nonautoregressive_convolutional_decoding'}		BJe1myXT-V	BJe1myXT-V	OpenReview.net/Archive/-/Direct_Upload	[]	189		['everyone']		['~Shuai_Tang1']	1546624791093		1546624791093	['~Shuai_Tang1']
965	1546624691211	{'title': 'Exploiting Invertible Decoders for Unsupervised Sentence Representation Learning', 'authors': ['Shuai Tang', 'Virginia R. de Sa'], 'authorids': ['shuaitang93@ucsd.edu', 'desa@ucsd.edu'], 'pdf': 'https://arxiv.org/pdf/1809.02731.pdf', 'paperhash': 'tang|exploiting_invertible_decoders_for_unsupervised_sentence_representation_learning'}		HJgsnAf6ZV	HJgsnAf6ZV	OpenReview.net/Archive/-/Direct_Upload	[]	188		['everyone']		['~Shuai_Tang1']	1546624691211		1546624691211	['~Shuai_Tang1']
966	1546624572936	{'title': 'Rethinking Skip-thought: A Neighbourhood based Approach', 'authors': ['Shuai Tang', 'Hailin Jin', 'Chen Fang', 'Zhaowen Wang', 'Virginia R. de Sa'], 'authorids': ['shuaitang93@ucsd.edu', 'hljin@adobe.com', 'cfang@adobe.com', 'zhawang@adobe.com', 'desa@ucsd.edu'], 'pdf': '/pdf/678009ffa33fe66ac4b583575abee660daefb8d0.pdf', 'paperhash': 'tang|rethinking_skipthought_a_neighbourhood_based_approach'}		HkeSB0GTb4	HkeSB0GTb4	OpenReview.net/Archive/-/Direct_Upload	[]	187		['everyone']		['~Shuai_Tang1']	1546624572936		1546624572936	['~Shuai_Tang1']
967	1538087741492	{'title': ' The relativistic discriminator: a key element missing from standard GAN', 'abstract': 'In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs. \n\nWe show that this property can be induced by using a relativistic discriminator which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function. \n\nEmpirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization.\n\nThe code is freely available on https://github.com/AlexiaJM/RelativisticGAN.', 'paperhash': 'jolicoeurmartineau|the_relativistic_discriminator_a_key_element_missing_from_standard_gan', 'TL;DR': 'Improving the quality and stability of GANs using a relativistic discriminator; IPM GANs (such as WGAN-GP) are a special case.', 'authorids': ['alexia.jolicoeur-martineau@mail.mcgill.ca'], 'authors': ['Alexia Jolicoeur-Martineau'], 'keywords': ['AI', 'deep learning', 'generative models', 'GAN'], 'pdf': '/pdf/9d1feb597fbc059ec906836cb0234e5b2ef0731c.pdf', '_bibtex': '@inproceedings{\njolicoeur-martineau2018,\ntitle={ The relativistic discriminator: a key element missing from standard {GAN}},\nauthor={Alexia Jolicoeur-Martineau},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1erHoR5t7},\n}'}		S1erHoR5t7	S1erHoR5t7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	87	HklMWLIYu7	['everyone']		['ICLR.cc/2019/Conference']	1538087741492		1546621238932	['ICLR.cc/2019/Conference']
968	1546612818658	"{'title': 'Camera Ready Paper Uploaded', 'comment': ""Dear reviewers and readers,\n\nwe have uploaded the camera ready, de-anonymized version of our paper.\nThe code, data, and models to reproduce the paper's results can be found here: https://github.com/tohinz/multiple-objects-gan""}"		H1edIiA9KQ	SJxoUgla-N	ICLR.cc/2019/Conference/-/Paper187/Official_Comment	['ICLR.cc/2019/Conference/Paper187/Reviewers/Unsubmitted']	12		['everyone']	H1edIiA9KQ	['ICLR.cc/2019/Conference/Paper187/Authors']	1546612818658		1546612818658	['ICLR.cc/2019/Conference/Paper187/Authors', 'ICLR.cc/2019/Conference']
969	1538087826744	{'title': 'SOM-VAE: Interpretable Discrete Representation Learning on Time Series', 'abstract': 'High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time.\nTo address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space.\nThis model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty.\nWe evaluate our model in terms of clustering performance and interpretability on static (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two macro states, as well as on a challenging real world medical time series application on the eICU data set. Our learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data.', 'keywords': ['deep learning', 'self-organizing map', 'variational autoencoder', 'representation learning', 'time series', 'machine learning', 'interpretability'], 'authorids': ['fortuin@inf.ethz.ch', 'mhueser@inf.ethz.ch', 'locatelf@inf.ethz.ch', 'heiko.strathmann@gmail.com', 'raetsch@inf.ethz.ch'], 'authors': ['Vincent Fortuin', 'Matthias Hüser', 'Francesco Locatello', 'Heiko Strathmann', 'Gunnar Rätsch'], 'TL;DR': 'We present a method to learn interpretable representations on time series using ideas from variational autoencoders, self-organizing maps and probabilistic models.', 'pdf': '/pdf/15c8cbb113daa27eb888054e58b057bcfeb1a203.pdf', 'paperhash': 'fortuin|somvae_interpretable_discrete_representation_learning_on_time_series', '_bibtex': '@inproceedings{\nfortuin2018deep,\ntitle={Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series},\nauthor={Vincent Fortuin and Matthias Hüser and Francesco Locatello and Heiko Strathmann and Gunnar Rätsch},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygjcsR9Y7},\n}'}		rygjcsR9Y7	rygjcsR9Y7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	562	HJeFJOcqF7	['everyone']		['ICLR.cc/2019/Conference']	1538087826744		1546612154333	['ICLR.cc/2019/Conference']
970	1546604248419	"{'title': 'interesting results!', 'comment': ""It's very interesting as I have generally observed that RaGANs were better in almost every scenarios I tested. This is why replications are so useful :) .\n\nI'd be curious to see in which scenarios each approach worked best. The link is to VideoSuperResolution, is that the correct link?""}"		S1erHoR5t7	H1xxkyA2WE	ICLR.cc/2019/Conference/-/Paper87/Official_Comment	['ICLR.cc/2019/Conference/Paper87/Reviewers/Unsubmitted']	10		['everyone']	S1l1Bash-V	['ICLR.cc/2019/Conference/Paper87/Authors']	1546604248419		1546604248419	['ICLR.cc/2019/Conference/Paper87/Authors', 'ICLR.cc/2019/Conference']
971	1546599813281	{'title': 'A Multi-task Network to Detect Junctions in Retinal Vasculature', 'authors': ['Fatmatulzehra Uslu', 'Anil Anthony Bharath'], 'authorids': ['fatmatulzehra.uslu@btu.edu.tr', 'a.bharath@imperial.ac.uk'], 'pdf': '/pdf/6b5bcf3a3f4a4cf4f1db7e987c580743959cc8bb.pdf', 'paperhash': 'uslu|a_multitask_network_to_detect_junctions_in_retinal_vasculature'}		HJx6FTn3-E	HJx6FTn3-E	OpenReview.net/Archive/-/Direct_Upload	[]	186		['everyone']		['~Fatmatulzehra_Uslu1']	1546599813281		1546599813281	['~Fatmatulzehra_Uslu1']
972	1546599744617	{'title': 'A Recursive Bayesian Approach To Describe Retinal Vasculature Geometry', 'authors': ['Fatmatulzehra Uslu', 'Anil Anthony Bharath'], 'authorids': ['fatmatulzehra.uslu@btu.edu.tr', 'a.bharath@imperial.ac.uk'], 'pdf': '/pdf/eee4e9c8598ea038464cd91c46d9e94231a44ca3.pdf', 'paperhash': 'uslu|a_recursive_bayesian_approach_to_describe_retinal_vasculature_geometry'}		rJetHTn3bE	rJetHTn3bE	OpenReview.net/Archive/-/Direct_Upload	[]	185		['everyone']		['~Fatmatulzehra_Uslu1']	1546599744617		1546599744617	['~Fatmatulzehra_Uslu1']
973	1546596616126	{'comment': 'We participated in ICLR Reproducibility Challenge 2019 and you can find our full report and code, written in Python using PyTorch, on Github: https://github.com/francescobardi/pde_solver_deep_learned.\n\nThis paper was very interesting and challenging, it introduced - for us at least - a novel idea on how to apply ML-techniques.\n\nWe could partially confirm the results reported in the original paper, not every result was reproducible either through lack of time or certainty in how these results where achieved or measured. We did not have the opportunity to test the solver using the MultiGrid method, nor the square-Poisson problem.\n\nWe have some questions regarding different aspects of the paper.\n\n1 Code \n- We have not found any reference to the source code used to produce the results, if it is publicly available you could add a reference in the paper.\n\n2 Reset operator\n- It is not clear if the proposed approach to enforce boundary condition can be extended to boundary conditions other than Dirichlet or to other iterative methods such as Gauss Seidel.\n\n3 Training process\n- How many problem instances were considered to build the loss function?\n- What is the range for the random value that is applied to each edge of the square?\n- How do you obtain the ground truth solution?\n- Are the weights of the convolutional kernels randomly initialized or do you set them?\n- What optimization algorithm do you use if any?\n- How do you propose to include the spectral radius constraint?\n\n4 Model testing:\n- How were the numbers reported in Table 1 obtained?\n- How do you implement the cylindrical geometry in a finite difference framework? Did you use radial coordinates or a non-uniform grid?', 'title': 'ICLR Reproducibility Challenge 2019 - Team Name: zoidberg'}		rklaWn0qK7	Bylef-n3-E	ICLR.cc/2019/Conference/-/Paper1214/Public_Comment	[]	5		['everyone']	rklaWn0qK7	['~francesco_bardi1']	1546596616126		1546597044320	['~francesco_bardi1', 'ICLR.cc/2019/Conference']
974	1546595638995	{'comment': 'We participate the ICLR Reproducibility Challenge and reproduce the experiments (part) of RGAN. \n\nRelativistic discriminator (RGAN) is proposed to improve training stability and visual quality of wide variety of GANs. We conduct a series of experiments, aiming to reproduce the results of the RGAN. We compare frechet inception distance (FID) and inception score (IS) among 10 different loss functions (NSGAN, LSGAN, NSGAN-GP, WGAN-GP and the relativistic version) via 2 GAN architectures (DCGAN and ResNet) and 3 different normalization methods (batch normalization, spectral normalization and no normalization). Our tasks include generating 32x32 natural images (trained by CIFAR10) and 64x64 human faces (trained by CelebA).\n\nWe compare relativistic GAN and relativistic average GAN to their counter-parts in totally 20 cases. We found in most cases, relativistic GAN is an effective method to stabilize training process and can also improve image quality, while relativistic average GAN is not very effective and sometimes corrupt training in our experiments. Apply relativistic GAN with spectral normalization on discriminator is strongly recommended. Apply gradient penalty to discriminator loss function can further improves the image quality at the cost of higher computation.\n\nIn general, the experimental results show that RGAN is able to stabilize training process and improve generated image quality in 15 out of 20 cases, and fail to generate reasonable images (unconverged) in 2 cases. While the relativistic average GAN (RaGAN) can improve the quality in 9 out of 20 cases and fail to generate reasonable images in 4 cases. Our code is available at https://github.com/LoSealL/VideoSuperResolution.\n', 'title': 'Reproducibility Report of ICLR Reproducibility Challenge'}		S1erHoR5t7	S1l1Bash-V	ICLR.cc/2019/Conference/-/Paper87/Public_Comment	[]	2		['everyone']	S1erHoR5t7	['~Wenyi_Tang1']	1546595638995		1546595638995	['~Wenyi_Tang1', 'ICLR.cc/2019/Conference']
975	1538088004802	{'title': 'The Limitations of Adversarial Training and the Blind-Spot Attack', 'abstract': 'The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the “blind-spot attack”, where the input images reside in “blind-spots” (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.', 'keywords': ['Adversarial Examples', 'Adversarial Training', 'Blind-Spot Attack'], 'authorids': ['huan@huan-zhang.com', 'chenhg@mit.edu', 'zhaos@utexas.edu', 'boning@mtl.mit.edu', 'inderjit@cs.utexas.edu', 'chohsieh@cs.ucla.edu'], 'authors': ['Huan Zhang*', 'Hongge Chen*', 'Zhao Song', 'Duane Boning', 'Inderjit S. Dhillon', 'Cho-Jui Hsieh'], 'TL;DR': 'We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.', 'pdf': '/pdf/8a378790c2b538af164f53d751d30d33811f0018.pdf', 'paperhash': 'zhang|the_limitations_of_adversarial_training_and_the_blindspot_attack', '_bibtex': '@inproceedings{\nzhang2018the,\ntitle={The Limitations of Adversarial Training and the Blind-Spot Attack},\nauthor={Huan Zhang and Hongge Chen and Zhao Song and Duane Boning and inderjit dhillon and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylTBhA5tQ},\n}'}		HylTBhA5tQ	HylTBhA5tQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1584	ByxzSCs5K7	['everyone']		['ICLR.cc/2019/Conference']	1538088004802		1546589777103	['ICLR.cc/2019/Conference']
976	1518730171899	{'title': 'Certifying Some Distributional Robustness with Principled Adversarial Training', 'abstract': 'Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n', 'pdf': '/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf', 'TL;DR': 'We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.', 'paperhash': 'sinha|certifying_some_distributional_robustness_with_principled_adversarial_training', 'authors': ['Aman Sinha', 'Hongseok Namkoong', 'John Duchi'], 'keywords': ['adversarial training', 'distributionally robust optimization', 'deep learning', 'optimization', 'learning theory'], 'authorids': ['amans@stanford.edu', 'hnamk@stanford.edu', 'jduchi@stanford.edu'], '_bibtex': '@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}'}		Hk6kPgZA-	Hk6kPgZA-	ICLR.cc/2018/Conference/-/Blind_Submission	[]	596	SJ6kPgWC-	['everyone']		['ICLR.cc/2018/Conference']	1509127909457		1546570475487	['ICLR.cc/2018/Conference']
977	1538087857652	{'title': 'A Unified Theory of Early Visual Representations from Retina to Cortex through Anatomically Constrained Deep CNNs', 'abstract': 'The vertebrate visual system is hierarchically organized to process visual information in successive stages. Neural representations vary drastically across the first stages of visual processing: at the output of the retina, ganglion cell receptive fields (RFs) exhibit a clear antagonistic center-surround structure, whereas in the primary visual cortex (V1), typical RFs are sharply tuned to a precise orientation. There is currently no unified theory explaining these differences in representations across layers. Here, using a deep convolutional neural network trained on image recognition as a model of the visual system, we show that such differences in representation can emerge as a direct consequence of different neural resource constraints on the retinal and cortical networks, and for the first time we find a single model from which both geometries spontaneously emerge at the appropriate stages of visual processing. The key constraint is a reduced number of neurons at the retinal output, consistent with the anatomy of the optic nerve as a stringent bottleneck. Second, we find that, for simple downstream cortical networks, visual representations at the retinal output emerge as nonlinear and lossy feature detectors, whereas they emerge as linear and faithful encoders of the visual scene for more complex cortical networks. This result predicts that the retinas of small vertebrates (e.g. salamander, frog) should perform sophisticated nonlinear computations, extracting features directly relevant to behavior, whereas retinas of large animals such as primates should mostly encode the visual scene linearly and respond to a much broader range of stimuli. These predictions could reconcile the two seemingly incompatible views of the retina as either performing feature extraction or efficient coding of natural scenes, by suggesting that all vertebrates lie on a spectrum between these two objectives, depending on the degree of neural resources allocated to their visual system.', 'keywords': ['visual system', 'convolutional neural networks', 'efficient coding', 'retina'], 'authorids': ['lindsey6@stanford.edu', 'socko@stanford.edu', 'sganguli@stanford.edu', 'sdeny@stanford.edu'], 'authors': ['Jack Lindsey', 'Samuel A. Ocko', 'Surya Ganguli', 'Stephane Deny'], 'TL;DR': 'We reproduced neural representations found in biological visual systems by simulating their neural resource constraints in a deep convolutional model.', 'pdf': '/pdf/b3bc9c780cbeea2cdba4c393e02534afafcfee47.pdf', 'paperhash': 'lindsey|a_unified_theory_of_early_visual_representations_from_retina_to_cortex_through_anatomically_constrained_deep_cnns', '_bibtex': '@inproceedings{\nlindsey2018the,\ntitle={The effects of neural resource constraints on early visual representations },\nauthor={Jack Lindsey and Samuel A. Ocko and Surya Ganguli and Stephane Deny},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=S1xq3oR5tQ},\n}'}		S1xq3oR5tQ	S1xq3oR5tQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	734	SklW9Mo9FQ	['everyone']		['ICLR.cc/2019/Conference']	1538087857652		1546555374831	['ICLR.cc/2019/Conference']
978	1538087882537	{'title': 'BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning', 'abstract': 'Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons, but given the poor data efficiency of the current learning methods, this goal may require substantial research efforts. Here, we introduce the BabyAI research platform to support investigations towards including humans in the loop for grounded language learning. The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty. The levels gradually lead the agent towards acquiring a combinatorially rich synthetic language which is a proper subset of English. The platform also provides a heuristic expert agent for the purpose of simulating a human teacher. We report baseline results and estimate the amount of human involvement that would be required to train a neural network-based agent on some of the BabyAI levels. We put forward strong evidence that current deep learning methods are not yet sufficiently sample efficient when it comes to learning a language with compositional properties.', 'keywords': ['language', 'learning', 'efficiency', 'imitation learning', 'reinforcement learning'], 'authorids': ['maximechevalierb@gmail.com', 'dimabgv@gmail.com', 'salemlahlou9@gmail.com', 'lcswillems@gmail.com', 'chitwaniit@gmail.com', 'thien@cs.uoregon.edu', 'yoshua.bengio@umontreal.ca'], 'authors': ['Maxime Chevalier-Boisvert', 'Dzmitry Bahdanau', 'Salem Lahlou', 'Lucas Willems', 'Chitwan Saharia', 'Thien Huu Nguyen', 'Yoshua Bengio'], 'TL;DR': 'We present the BabyAI platform for studying data efficiency of language learning with a human in the loop', 'pdf': '/pdf/fad157b9c5115703346f602a079d4cb9dd2e8613.pdf', 'paperhash': 'chevalierboisvert|babyai_a_platform_to_study_the_sample_efficiency_of_grounded_language_learning', '_bibtex': '@inproceedings{\nchevalier-boisvert2018babyai,\ntitle={Baby{AI}: First Steps Towards Grounded Language Learning With a Human In the Loop},\nauthor={Maxime Chevalier-Boisvert and Dzmitry Bahdanau and Salem Lahlou and Lucas Willems and Chitwan Saharia and Thien Huu Nguyen and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJeXCo0cYX},\n}'}		rJeXCo0cYX	rJeXCo0cYX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	877	S1gFtMvqt7	['everyone']		['ICLR.cc/2019/Conference']	1538087882537		1546550331521	['ICLR.cc/2019/Conference']
979	1546549579193	{'title': 'Counting Everyday Objects in Everyday Scenes', 'authors': ['Prithvijit Chattopadhyay', 'Ramakrishna Vedantam', 'Ramprasaath R. Selvaraju', 'Dhruv Batra', 'Devi Parikh'], 'authorids': ['prithvijit3@gatech.edu', 'vrama@gatech.edu', 'ramprs@gatech.edu', 'dbatra@gatech.edu', 'parikh@gatech.edu'], 'pdf': 'http://openaccess.thecvf.com/content_cvpr_2017/papers/Chattopadhyay_Counting_Everyday_Objects_CVPR_2017_paper.pdf', 'paperhash': 'chattopadhyay|counting_everyday_objects_in_everyday_scenes'}		BJlXIKgh-V	BJlXIKgh-V	OpenReview.net/Archive/-/Direct_Upload	[]	184		['everyone']		['~Prithvijit_Chattopadhyay1']	1546549579193		1546549579193	['~Prithvijit_Chattopadhyay1']
980	1538087746909	{'title': 'Learning to Understand Goal Specifications by Modelling Reward', 'abstract': 'Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples. As reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed. In a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new expert examples.\n', 'keywords': ['instruction following', 'reward modelling', 'language understanding'], 'authorids': ['dimabgv@gmail.com', 'felixhill@google.com', 'leike@google.com', 'edwardhughes@google.com', 'seyedarian.hosseini@umontreal.ca', 'pushmeet@google.com', 'etg@google.com'], 'authors': ['Dzmitry Bahdanau', 'Felix Hill', 'Jan Leike', 'Edward Hughes', 'Arian Hosseini', 'Pushmeet Kohli', 'Edward Grefenstette'], 'TL;DR': 'We propose AGILE, a framework for training agents to perform instructions from examples of respective goal-states.', 'pdf': '/pdf/5c799f17517a04550e325b5fa5b42a0c088f490e.pdf', 'paperhash': 'bahdanau|learning_to_understand_goal_specifications_by_modelling_reward', '_bibtex': '@inproceedings{\nbahdanau2018learning,\ntitle={Learning to Understand Goal Specifications by Modelling Reward},\nauthor={Dzmitry Bahdanau and Felix Hill and Jan Leike and Edward Hughes and Pushmeet Kohli and Edward Grefenstette},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xsSjC9Ym},\n}'}		H1xsSjC9Ym	H1xsSjC9Ym	ICLR.cc/2019/Conference/-/Blind_Submission	[]	116	H1xvAyiKF7	['everyone']		['ICLR.cc/2019/Conference']	1538087746909		1546548186807	['ICLR.cc/2019/Conference']
981	1546536394819	{'comment': 'Because Wieting et al. only used the most difficult example, which is highly possible to be false-negative. In contrast, KBGAN and self-adversarial sampling do not seem to suffer from the false-negative problem.', 'title': 'Then this is why negative examples in Wieting et al. are not effective'}		HkgEQnRqYQ	S1lX0Hps-V	ICLR.cc/2019/Conference/-/Paper1347/Public_Comment	[]	17		['everyone']	B1lmFrmcbV	['(anonymous)']	1546536394819		1546536394819	['(anonymous)', 'ICLR.cc/2019/Conference']
982	1546523493377	{'title': 'Code Release', 'comment': 'The code includes environments and algorithms: https://github.com/mengf1/DHER .'}		Byf5-30qFX	ryl6DQ9i-V	ICLR.cc/2019/Conference/-/Paper1200/Official_Comment	['ICLR.cc/2019/Conference/Paper1200/Reviewers/Unsubmitted']	25		['everyone']	Byf5-30qFX	['ICLR.cc/2019/Conference/Paper1200/Authors']	1546523493377		1546523711370	['ICLR.cc/2019/Conference/Paper1200/Authors', 'ICLR.cc/2019/Conference']
983	1538087919839	{'title': 'Transfer Learning for Sequences via Learning to Collocate', 'abstract': 'Transfer learning aims to solve the data sparsity for a specific domain by applying information of another domain. Given a sequence (e.g. a natural language sentence), the transfer learning, usually enabled by recurrent neural network (RNN), represent the sequential information transfer. RNN uses a chain of repeating cells to model the sequence data. However, previous studies of neural network based transfer learning simply transfer the information across the whole layers, which are unfeasible for seq2seq and sequence labeling. Meanwhile, such layer-wise transfer learning mechanisms also lose the fine-grained cell-level information from the source domain.\n\nIn this paper, we proposed the aligned recurrent transfer, ART, to achieve cell-level information transfer. ART is in a recurrent manner that different cells share the same parameters. Besides transferring the corresponding information at the same position, ART transfers information from all collocated words in the source domain. This strategy enables ART to capture the word collocation across domains in a more flexible way. We conducted extensive experiments on both sequence labeling tasks (POS tagging, NER) and sentence classification (sentiment analysis). ART outperforms the state-of-the-arts over all experiments.\n', 'keywords': ['transfer learning', 'recurrent neural network', 'attention', 'natural language processing'], 'authorids': ['cui.wanyun@sufe.edu.cn', 'simonzgy@outlook.com', 'shen54@illinois.edu', 'tedjiangfdu@gmail.com', 'weiwang1@fudan.edu.cn'], 'authors': ['Wanyun Cui', 'Guangyu Zheng', 'Zhiqiang Shen', 'Sihang Jiang', 'Wei Wang'], 'TL;DR': 'Transfer learning for sequence via learning to align cell-level information across domains.', 'pdf': '/pdf/a7ffa154ddf6c8b998decf1e4529c8dbacc6c156.pdf', 'paperhash': 'cui|transfer_learning_for_sequences_via_learning_to_collocate', '_bibtex': '@inproceedings{\ncui2018transfer,\ntitle={Transfer Learning for Sequences via Learning to Collocate},\nauthor={Wanyun Cui and Guangyu Zheng and Zhiqiang Shen and Sihang Jiang and Wei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByldlhAqYQ},\n}'}		ByldlhAqYQ	ByldlhAqYQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	1089	rkxTMWt9Fm	['everyone']		['ICLR.cc/2019/Conference']	1538087919839		1546516280098	['ICLR.cc/2019/Conference']
984	1546510746510	{'title': 'Interactive Natural Language Acquisition in a Multi-modal Recurrent Neural Architecture', 'authors': ['Stefan Heinrich', 'Stefan Wermter'], 'authorids': ['heinrich@informatik.uni-hamburg.de', 'wermter@informatik.uni-hamburg.de'], 'pdf': 'https://arxiv.org/pdf/1703.08513.pdf', 'paperhash': 'heinrich|interactive_natural_language_acquisition_in_a_multimodal_recurrent_neural_architecture'}		rJlfsZwi-E	rJlfsZwi-E	OpenReview.net/Archive/-/Direct_Upload	[]	183		['everyone']		['~Stefan_Heinrich1']	1546510746510		1546510746510	['~Stefan_Heinrich1']
985	1538087759734	{'title': 'Generating Multiple Objects at Spatially Distinct Locations', 'abstract': 'Recent improvements to Generative Adversarial Networks (GANs) have made it possible to generate realistic images in high resolution based on natural language descriptions such as image captions. Furthermore, conditional GANs allow us to control the image generation process through labels or even natural language descriptions. However, fine-grained control of the image layout, i.e. where in the image specific objects should be located, is still difficult to achieve. This is especially true for images that should contain multiple distinct objects at different spatial locations. We introduce a new approach which allows us to control the location of arbitrarily many objects within an image by adding an object pathway to both the generator and the discriminator. Our approach does not need a detailed semantic layout but only bounding boxes and the respective labels of the desired objects are needed. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image layout. We perform experiments on the Multi-MNIST, CLEVR, and the more complex MS-COCO data set. Our experiments show that through the use of the object pathway we can control object locations within images and can model complex scenes with multiple objects at various locations. We further show that the object pathway focuses on the individual objects and learns features relevant for these, while the global pathway focuses on global image characteristics and the image background.', 'keywords': ['controllable image generation', 'text-to-image synthesis', 'generative model', 'generative adversarial network', 'gan'], 'authorids': ['hinz@informatik.uni-hamburg.de', 'heinrich@informatik.uni-hamburg.de', 'wermter@informatik.uni-hamburg.de'], 'authors': ['Tobias Hinz', 'Stefan Heinrich', 'Stefan Wermter'], 'TL;DR': 'Extend GAN architecture to obtain control over locations and identities of multiple objects within generated images.', 'pdf': '/pdf/03d7745f16bbb2baa681c15ffb3b73cb61c507a5.pdf', 'paperhash': 'hinz|generating_multiple_objects_at_spatially_distinct_locations', '_bibtex': '@inproceedings{\nhinz2018generating,\ntitle={Generating Multiple Objects at Spatially Distinct Locations},\nauthor={Tobias Hinz and Stefan Heinrich and Stefan Wermter},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1edIiA9KQ},\n}'}		H1edIiA9KQ	H1edIiA9KQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	187	H1xFQ_45Km	['everyone']		['ICLR.cc/2019/Conference']	1538087759734		1546509158604	['ICLR.cc/2019/Conference']
986	1546501119081	"{'title': 'Suggesting authorship', 'comment': ""Dear AnonReviewer3,\n\nWe would like to recognize your very detailed and useful review by including you as a co-author in the next revision (among other pending changes addressing your and other reviewers' final remarks). If you are interested, please let us know your name, email and affiliation.\n\nThank you!""}"		B1g30j0qF7	HJxPb2VoWV	ICLR.cc/2019/Conference/-/Paper930/Official_Comment	['ICLR.cc/2019/Conference/Paper930/Reviewers/Unsubmitted']	33		['everyone']	SkgZdQ0t37	['ICLR.cc/2019/Conference/Paper930/Authors']	1546501119081		1546501119081	['ICLR.cc/2019/Conference/Paper930/Authors', 'ICLR.cc/2019/Conference']
987	1546470419887	{'comment': 'I get your point but let me explain things in a more clear way, there are 3 methods,\n\nMethod 1: NLL + KD Loss (without early stopping pure KD)\n\nMethod 2: NLL+KD or only NLL (early stopping or oscillation with small mag. and freq.) (this is not pure KD, sometimes and for some languages NLL loss (baseline) is used)\n\nMethod 3: NLL loss only (Multilingual)\n\nWe know on Tedtalk dataset Method 2 > Method 3 > Method 1\n\nThis might indicate that Method 2 i.e. early stopping is the cause of significant improvement and KD might not be helping a lot since Method 1 > Method 3 and method 1 uses KD loss always. ', 'title': 'reply'}		S1gUsoR9YX	Syg3ME65WV	ICLR.cc/2019/Conference/-/Paper625/Public_Comment	[]	3		['everyone']	HyxyZDU_-E	['~krtin_kumar1']	1546470419887		1546470419887	['~krtin_kumar1', 'ICLR.cc/2019/Conference']
988	1538087862186	{'title': 'Multi-class classification without multi-class labels', 'abstract': 'This work presents a new strategy for multi-class classification that requires no class-specific labels, but instead leverages pairwise similarity between examples, which is a weaker form of annotation. The proposed method, meta classification learning, optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi-class classifier as a submodule. We formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models. We then demonstrate that this same framework generalizes to the supervised, unsupervised cross-task, and semi-supervised settings. Our method is evaluated against state of the art in all three learning paradigms and shows a superior or comparable accuracy, providing evidence that learning multi-class classification without multi-class labels is a viable learning option.', 'keywords': ['classification', 'unsupervised learning', 'semi-supervised learning', 'problem reduction', 'weak supervision', 'cross-task', 'learning', 'deep learning', 'neural network'], 'authorids': ['yenchang.hsu@gatech.edu', 'zhaoyang.lv@gatech.edu', 'joel.schlosser@gtri.gatech.edu', 'phillip.odom@gtri.gatech.edu', 'zkira@gatech.edu'], 'authors': ['Yen-Chang Hsu', 'Zhaoyang Lv', 'Joel Schlosser', 'Phillip Odom', 'Zsolt Kira'], 'pdf': '/pdf/65e173d93cea9bceab67e6c8caee82e4c764084d.pdf', 'paperhash': 'hsu|multiclass_classification_without_multiclass_labels', '_bibtex': '@inproceedings{\nhsu2018multiclass,\ntitle={Multi-class classification without multi-class labels},\nauthor={Yen-Chang Hsu and Zhaoyang Lv and Joel Schlosser and Phillip Odom and Zsolt Kira},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJzR2iRcK7},\n}'}		SJzR2iRcK7	SJzR2iRcK7	ICLR.cc/2019/Conference/-/Blind_Submission	[]	760	HyeUGb-5YX	['everyone']		['ICLR.cc/2019/Conference']	1538087862186		1546466147924	['ICLR.cc/2019/Conference']
989	1546457063216	"{'title': 'RE: Answer', 'comment': ""Great.\n\n1. Yes, this is the optimism issue raised in Levine 2018. Most control as inference methods use a structured variational objective, which fundamentally gets at this optimism bias.\n\n2. I'm not familiar with RL algorithms that suffer from this issue. If you have references, it would be great to add them to the discussion of related works.\n\n""}"		ByetGn0cYX	r1x1llc5bV	ICLR.cc/2019/Conference/-/Paper1285/Official_Comment	['ICLR.cc/2019/Conference/Paper1285/Reviewers/Unsubmitted']	23		['everyone']	B1xd7Ks8gE	['ICLR.cc/2019/Conference/Paper1285/AnonReviewer1']	1546457063216		1546457063216	['ICLR.cc/2019/Conference/Paper1285/AnonReviewer1', 'ICLR.cc/2019/Conference']
990	1538087897472	{'title': 'Optimal Completion Distillation for Sequence Learning', 'abstract': 'We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.', 'keywords': ['Sequence Learning', 'Edit Distance', 'Speech Recognition', 'Deep Reinforcement Learning'], 'authorids': ['sasabour@google.com', 'williamchan@google.com', 'mnorouzi@google.com'], 'authors': ['Sara Sabour', 'William Chan', 'Mohammad Norouzi'], 'TL;DR': 'Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.', 'pdf': '/pdf/de7507e029f8d5fced948dfa043fa8a39a5a4be6.pdf', 'paperhash': 'sabour|optimal_completion_distillation_for_sequence_learning', '_bibtex': '@inproceedings{\nsabour2018optimal,\ntitle={Optimal Completion Distillation for Sequence Learning},\nauthor={Sara Sabour and William Chan and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkMW1hRqKX},\n}'}		rkMW1hRqKX	rkMW1hRqKX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	963	rke4OqhcK7	['everyone']		['ICLR.cc/2019/Conference']	1538087897472		1546449228534	['ICLR.cc/2019/Conference']
991	1538087851130	{'title': 'Wizard of Wikipedia: Knowledge-Powered Conversational Agents', 'abstract': 'In open-domain dialogue intelligent agents should exhibit the use of knowledge, however there are few convincing demonstrations of this to date. The most popular sequence to sequence models typically “generate and hope” generic utterances that can be memorized in the weights of the model when mapping from input utterance(s) to output, rather than employing recalled knowledge as context. Use of knowledge has so far proved difficult, in part because of the lack of a supervised learning benchmark task which exhibits knowledgeable open dialogue with clear  grounding. To that end we collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia.  We then design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses. Our best performing dialogue models are able to conduct knowledgeable discussions on open-domain topics as evaluated by automatic metrics and human evaluations, while our new benchmark allows for measuring further improvements in this important research direction.', 'keywords': ['dialogue', 'knowledge', 'language', 'conversation'], 'authorids': ['edinan@fb.com', 'roller@fb.com', 'kshuster@fb.com', 'angelafan@fb.com', 'michaelauli@fb.com', 'jase@fb.com'], 'authors': ['Emily Dinan', 'Stephen Roller', 'Kurt Shuster', 'Angela Fan', 'Michael Auli', 'Jason Weston'], 'TL;DR': 'We build knowledgeable conversational agents by conditioning on Wikipedia + a new supervised task.', 'pdf': '/pdf/65b29aba41b3077b22392c1b078f54437129f6cd.pdf', 'paperhash': 'dinan|wizard_of_wikipedia_knowledgepowered_conversational_agents', '_bibtex': '@inproceedings{\ndinan2018wizard,\ntitle={Wizard of Wikipedia: Knowledge-Powered Conversational Agents},\nauthor={Emily Dinan and Stephen Roller and Kurt Shuster and Angela Fan and Michael Auli and Jason Weston},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1l73iRqKm},\n}'}		r1l73iRqKm	r1l73iRqKm	ICLR.cc/2019/Conference/-/Blind_Submission	[]	697	ryehExocFQ	['everyone']		['ICLR.cc/2019/Conference']	1538087851130		1546447803273	['ICLR.cc/2019/Conference']
992	1538087881448	{'title': 'Graph HyperNetworks for Neural Architecture Search', 'abstract': 'Neural architecture search (NAS) automatically finds the best task-specific neural network topology, outperforming many manual architecture designs. However, it can be prohibitively expensive as the search requires training thousands of different networks, while each training run can last for hours. In this work, we propose the Graph HyperNetwork (GHN) to amortize the search cost: given an architecture, it directly generates the weights by running inference on a graph neural network. GHNs model the topology of an architecture and therefore can predict network performance more accurately than regular hypernetworks and premature early stopping. To perform NAS, we randomly sample architectures and use the validation accuracy of networks with GHN generated weights as the surrogate search signal. GHNs are fast - they can search nearly 10× faster than other random search methods on CIFAR-10 and ImageNet. GHNs can be further extended to the anytime prediction setting, where they have found networks with better speed-accuracy tradeoff than the state-of-the-art manual designs.', 'keywords': ['neural', 'architecture', 'search', 'graph', 'network', 'hypernetwork', 'meta', 'learning', 'anytime', 'prediction'], 'authorids': ['cjzhang@edu.uwaterloo.ca', 'mren@cs.toronto.edu', 'urtasun@cs.toronto.edu'], 'authors': ['Chris Zhang', 'Mengye Ren', 'Raquel Urtasun'], 'pdf': '/pdf/b9c01adccfbaae7fad1c8e5c2db9ea6c19313a0c.pdf', 'paperhash': 'zhang|graph_hypernetworks_for_neural_architecture_search', '_bibtex': '@inproceedings{\nzhang2018graph,\ntitle={Graph HyperNetworks for Neural Architecture Search},\nauthor={Chris Zhang and Mengye Ren and Raquel Urtasun},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkgW0oA9FX},\n}'}		rkgW0oA9FX	rkgW0oA9FX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	871	Hyx6kDqqYX	['everyone']		['ICLR.cc/2019/Conference']	1538087881448		1546447542767	['ICLR.cc/2019/Conference']
993	1538087813459	{'title': 'GO Gradient for Expectation-Based Objectives', 'abstract': 'Within many machine learning algorithms, a fundamental problem concerns efficient calculation of an unbiased gradient wrt parameters $\\boldsymbol{\\gamma}$ for expectation-based objectives $\\mathbb{E}_{q_{\\boldsymbol{\\gamma}} (\\boldsymbol{y})} [f (\\boldsymbol{y}) ]$. Most existing methods either ($i$) suffer from high variance, seeking help from (often) complicated variance-reduction techniques; or ($ii$) they only apply to reparameterizable continuous random variables and employ a reparameterization trick. To address these limitations, we propose a General and One-sample (GO) gradient that ($i$) applies to many distributions associated with non-reparameterizable continuous {\\em or} discrete random variables, and ($ii$) has the same low-variance as the reparameterization trick. We find that the GO gradient often works well in practice based on only one Monte Carlo sample (although one can of course use more samples if desired). Alongside the GO gradient, we develop a means of propagating the chain rule through distributions, yielding statistical back-propagation, coupling neural networks to common random variables.', 'keywords': ['generalized reparameterization gradient', 'variance reduction', 'non-reparameterizable', 'discrete random variable', 'GO gradient', 'general and one-sample gradient', 'expectation-based objective', 'variable nabla', 'statistical back-propagation', 'hierarchical', 'graphical model'], 'authorids': ['yulaicong@gmail.com', 'miaoyun9zhao@gmail.com', 'ke.bai@duke.edu', 'lcarin@duke.edu'], 'authors': ['Yulai Cong', 'Miaoyun Zhao', 'Ke Bai', 'Lawrence Carin'], 'pdf': '/pdf/c1850be28bb34be80afe77b50448d3c43fb7b078.pdf', 'paperhash': 'cong|go_gradient_for_expectationbased_objectives', 'TL;DR': 'a Rep-like gradient for non-reparameterizable continuous/discrete distributions; further generalized to deep probabilistic models, yielding statistical back-propagation', '_bibtex': '@inproceedings{\ncong2018go,\ntitle={{GO} Gradient for Expectation-Based Objectives},\nauthor={Yulai Cong and Miaoyun Zhao and Ke Bai and Lawrence Carin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryf6Fs09YX},\n}'}		ryf6Fs09YX	ryf6Fs09YX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	490	BkloSJS-Km	['everyone']		['ICLR.cc/2019/Conference']	1538087813459		1546442896352	['ICLR.cc/2019/Conference']
994	1546442459930	{'title': 'Some Remarks on the Meta-Review', 'comment': 'We thank you very much for your meta review as well as the reviewers’ work that will help us to revise the paper. However, we disagree with several statements and conclusions in this meta-review, some of which we believe do not take into account our rebuttal and substantial portions of the paper. As this meta-review also brings multiple new points of criticism that are not made by the reviewers we would like to conclude by addressing the major ones.\n\n“the performance of the models is significantly worse than the state of the art”. We show for the CIFAR-10 dataset an identical performance to end-to-end training for the same network architecture (Sec 4.3  last par.). We show for imagenet performance of 88.5 compared to a baseline of 90.1 (Sec 4.3;Tab 1, on par with VGG-nets). These results  are more than sufficient to support our claims. There are many instances of alternative training procedures  that can be made to work mildly on small datasets like MNIST or even CIFAR (obtaining 80% accuracy), but simply do not work on large datasets like Imagenet (Sec 4.1 par 1),  sometimes obtaining accuracies below 20% top 5.  Gaps of 50%+ to the state of the art are clearly  not the same as a 1% difference on imagenet, which  we demonstrate  has many ways to be closed.  We have also discussed repeatedly in our paper that the imagenet accuracies obtained by our method correspond to those of models that have been used for countless downstream applications and even deployed in industrial settings. To the best of our knowledge and in communications with members of the community obtaining VGG like accuracies with  these methods is  not  expected. The review of R3 appears to agree with this, even asking for code during the review process to verify the claims. \n \n“such a training procedure is not novel”. We have addressed this statement in our rebuttal, pointing out that even the specific instantiation of our method adds simple but novel aspects that turn out essential for scaling such methods to large images (and datasets). \n\n“could be useful for theoretical understanding of deep nets, but they don’t really perform such analysis in this submission” This is inaccurate. Prop 3.2 is simple approach to fill the gap between 1-hidden layer training literature and deep models with practically relevant performance, by claiming the tractability of the optimum for the deep network optimization, showing how one can use our models to directly connect to existing theoretical results. There are multiple works that conclude that it is extremely hard to show similar optimization results for standard end-to-end learned deep networks. We also point out our empirical study of linear separability (Sec 4.2) that makes several novel observations about the role of this phenomenon that require layerwise training to observe (Sec 4.2, par. 5). We are not aware of any claims by the reviewers or meta-review that show or argue that these contributions are not novel nor relevant. \n\n“unclear whether conclusions of such analysis would extend to deep nets trained end-to-end.”\n(a)If a model and training procedure is easier to analyze and is shown to produce a highly useful model (good enough  to use in industrial settings or downstream applications in countless vision papers)  it is then certainly preferable to analyze  over an end to end approach. We have multiple published references in our paper where theoretical work do just this: analyze greedily trained models because they simplify the analysis. Knowing greedily trained models can produce useful models is thus a desirable fact to demonstrate. (b) In our paper and rebuttal we discuss in detail that hypotheses for layerwise functionality (e.g.  progressive linear separability or mutual information) in end to end trained networks can be more directly evaluated via layerwise training. Please see our Introduction and Sec 4.2 for more detailed discussion of this point.\n \n“it is unclear what other advantages such a training scheme can offer”: Reviewers observed several advantages of the method (R1,bulletpoint strength ; R3 second comment ; R2 par. 5-7), that we also mention (Sec 3.3; Sec 2; Sec 4.3 ; Sec 5). Furthermore, (Sec 2; end of Sec 4.3) is devoted to motivating specific advantages from multiple angles; no precise argument is made against any of those points in this review process. (Please also see our answer to R2(par 4, 9,11) and R1(par 3, 5) )'}		r1Gsk3R9Fm	B1lVyPL9WE	ICLR.cc/2019/Conference/-/Paper1018/Official_Comment	['ICLR.cc/2019/Conference/Paper1018/Reviewers/Unsubmitted']	27		['everyone']	B1xRUVBZg4	['ICLR.cc/2019/Conference/Paper1018/Authors']	1546442459930		1546442459930	['ICLR.cc/2019/Conference/Paper1018/Authors', 'ICLR.cc/2019/Conference']
995	1542459601401	{'title': 'MIDAS: Finding the Right Web Sources to Fill Knowledge Gaps', 'authors': ['Anonymous'], 'authorids': ['AKBC.ws/2019/Conference/Paper11/Authors'], 'keywords': ['Source recommendation', 'Knowledge base'], 'TL;DR': 'This paper focuses on identifying high quality web sources for industrial knowledge base augmentation pipeline.', 'abstract': 'Knowledge bases, massive collections of facts (RDF triples) on diverse topics, support vital modern applications. However, existing knowledge bases contain very little data compared to the wealth of information on the Web. This is because the industry standard in knowledge base creation and augmentation suffers from a serious bottleneck: they rely on domain experts to identify appropriate web sources to extract data from. Efforts to fully automate knowledge extraction have failed to improve this standard: these automated systems are able to retrieve much more data and from a broader range of sources, but they suffer from very low precision and recall. As a result, these large-scale extractions remain unexploited. In this paper, we present MIDAS, a system that harnesses the results of automated knowledge extraction pipelines to repair the bottleneck in industrial knowledge creation and augmentation processes. MIDAS automates the suggestion of good-quality web sources and describes what to extract with respect to augmenting an existing knowledge base. We make three major contributions. First, we introduce a novel concept, web source slices, to describe the contents of a web source. Second, we define a profit function to quantify the value of a web source slice with respect to augmenting an existing knowledge base. Third, we develop effective and highly-scalable algorithms to derive high-profit web source slices. We demonstrate that MIDAS produces high-profit results and outperforms the baselines significantly on both real-word and synthetic datasets.', 'pdf': '/pdf/b9315bdfda346a45417ee2ef41ea8bb17749156b.pdf', 'archival status': 'Non-Archival', 'subject areas': ['Information Extraction', 'Databases'], 'paperhash': 'anonymous|midas_finding_the_right_web_sources_to_fill_knowledge_gaps', '_bibtex': '@inproceedings{    \nanonymous2019midas:,    \ntitle={MIDAS: Finding the Right Web Sources to Fill Knowledge Gaps},    \nauthor={Anonymous},    \nbooktitle={Submitted to Automated Knowledge Base Construction},    \nyear={2019},    \nurl={https://openreview.net/forum?id=BketCg9p6X},    \nnote={under review}    \n}'}		BketCg9p6X	BketCg9p6X	AKBC.ws/2019/Conference/-/Withdrawn_Submission	[]	11	rklFFa3jTm	['everyone']		['AKBC.ws/2019/Conference']	1542459601401		1546439826364	['AKBC.ws/2019/Conference']
996	1546439818936	{'title': 'Thank you!', 'comment': 'Thank you for the references. We will review and add them in our final version.'}		ryf7ioRqFX	B1xmq2H5bV	ICLR.cc/2019/Conference/-/Paper609/Official_Comment	['ICLR.cc/2019/Conference/Paper609/Reviewers/Unsubmitted']	10		['everyone']	SylZld4tZ4	['ICLR.cc/2019/Conference/Paper609/Authors']	1546439818936		1546439818936	['ICLR.cc/2019/Conference/Paper609/Authors', 'ICLR.cc/2019/Conference']
997	1546429818775	"{'comment': 'Sampling and selecting examples are equivalent terms here.\n\nThe method to choose $t_1$ is essentially ""self-adversarial"" negative sampling. Notice that by $t_1 = argmax cos(g(x_1), g(t))$, $t_1$ is the most difficult example regarding the model itself. Wieting et al. went even further with addressing the false-negative problem.\n\nThe current work should focus on showing how they apply it effectively.', 'title': 'Wieting et al. do self-adversarial negative sampling'}"		HkgEQnRqYQ	B1lmFrmcbV	ICLR.cc/2019/Conference/-/Paper1347/Public_Comment	[]	16		['everyone']	ryxb31UJZ4	['(anonymous)']	1546429818775		1546429818775	['(anonymous)', 'ICLR.cc/2019/Conference']
998	1538087903397	{'title': 'InstaGAN: Instance-aware Image-to-Image Translation', 'abstract': 'Unsupervised image-to-image translation has gained considerable attention due to the recent impressive progress based on generative adversarial networks (GANs). However, previous methods often fail in challenging cases, in particular, when an image has multiple target instances and a translation task involves significant changes in shape, e.g., translating pants to skirts in fashion images. To tackle the issues, we propose a novel method, coined instance-aware GAN (InstaGAN), that incorporates the instance information (e.g., object segmentation masks) and improves multi-instance transfiguration. The proposed method translates both an image and the corresponding set of instance attributes while maintaining the permutation invariance property of the instances. To this end, we introduce a context preserving loss that encourages the network to learn the identity function outside of target instances. We also propose a sequential mini-batch inference/training technique that handles multiple instances with a limited GPU memory and enhances the network to generalize better for multiple instances. Our comparative evaluation demonstrates the effectiveness of the proposed method on different image datasets, in particular, in the aforementioned challenging cases. Code and results are available in https://github.com/sangwoomo/instagan', 'keywords': ['Image-to-Image Translation', 'Generative Adversarial Networks'], 'authorids': ['swmo@kaist.ac.kr', 'mscho@postech.ac.kr', 'jinwoos@kaist.ac.kr'], 'authors': ['Sangwoo Mo', 'Minsu Cho', 'Jinwoo Shin'], 'TL;DR': 'We propose a novel method to incorporate the set of instance attributes for image-to-image translation.', 'pdf': '/pdf/52bf3ef01bd32e5149681f2448724d9a2fff070a.pdf', 'paperhash': 'mo|instagan_instanceaware_imagetoimage_translation', '_bibtex': '@inproceedings{\nmo2018instanceaware,\ntitle={Instance-aware Image-to-Image Translation},\nauthor={Sangwoo Mo and Minsu Cho and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxwJhC9YX},\n}'}		ryxwJhC9YX	ryxwJhC9YX	ICLR.cc/2019/Conference/-/Blind_Submission	[]	996	B1eRB-V5tm	['everyone']		['ICLR.cc/2019/Conference']	1538087903397		1546420797996	['ICLR.cc/2019/Conference']
999	1538087773475	{'title': 'DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder', 'abstract': 'Variational autoencoders (VAEs) have shown a promise in data-driven conversation modeling. However, most VAE conversation models match the approximate posterior distribution over the latent variables to a simple prior such as standard normal distribution, thereby restricting the generated responses to a relatively simple (e.g., single-modal) scope. In this paper, we propose DialogWAE, a conditional Wasserstein autoencoder (WAE) specially designed for dialogue modeling. Unlike VAEs that impose a simple distribution over the latent variables, DialogWAE models the distribution of data by training a GAN within the latent variable space. Specifically, our model samples from the prior and posterior distributions over the latent variables by transforming context-dependent random noise using neural networks and minimizes the Wasserstein distance between the two distributions. We further develop a Gaussian mixture prior network to enrich the latent space. Experiments on two popular datasets show that DialogWAE outperforms the state-of-the-art approaches in generating more coherent, informative and diverse responses.', 'keywords': ['dialogue', 'GAN', 'VAE', 'WAE', 'chatbot'], 'authorids': ['guxiaodong1987@126.com', 'kyunghyun.cho@nyu.edu', 'jungwoo.ha@navercorp.com', 'hunkim@cse.ust.hk'], 'authors': ['Xiaodong Gu', 'Kyunghyun Cho', 'Jung-Woo Ha', 'Sunghun Kim'], 'pdf': '/pdf/92ddedb3a4fa2778c27aa10fc10351df323170a6.pdf', 'paperhash': 'gu|dialogwae_multimodal_response_generation_with_conditional_wasserstein_autoencoder', '_bibtex': '@inproceedings{\ngu2018dialogwae,\ntitle={Dialog{WAE}: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder},\nauthor={Xiaodong Gu and Kyunghyun Cho and Jung-Woo Ha and Sunghun Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkgBvsC9FQ},\n}'}		BkgBvsC9FQ	BkgBvsC9FQ	ICLR.cc/2019/Conference/-/Blind_Submission	[]	263	SyegNTw9tQ	['everyone']		['ICLR.cc/2019/Conference']	1538087773475		1546419178191	['ICLR.cc/2019/Conference']
